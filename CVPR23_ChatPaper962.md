# Paper:1     多重退出：加速统一视觉语言模型的动态早期退出



#### 1. Title: 
You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model

#### 2. Authors: 
Shengkun Tang, Yaqing Wang, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, Yi Liang, Dongkuan Xu

#### 3. Affiliation: 
第一作者：Shengkun Tang，北卡罗来纳州立大学

#### 4. Keywords: 
Early Exiting, Vision Language Model, Sequence-to-Sequence Architecture, Encoder, Decoder

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_You_Need_Multiple_Exiting_Dynamic_Early_Exiting_for_Accelerating_Unified_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是大规模Transformer模型在视觉语言任务中的应用，虽然这些模型取得了不可思议的性能，但是它们的昂贵计算成本通常会阻碍它们在实时场景中的应用。

- (2):现有的早期退出策略通常采用中间层的输出置信度作为输入复杂度的代理，以决定是否跳过后续层。然而，这种策略无法应用于同时具有编码器和解码器的统一架构中的编码器，因为难以在编码器层中进行输出置信度估计。为了解决这个问题，本文提出了一种新的早期退出策略，允许根据输入层之间的相似性动态跳过编码器和解码器中的层，即MuE。通过对编码器中的图像和文本模态进行分解，MuE具有灵活性，可以根据模态跳过不同的层，提高推理效率，同时最小化性能下降。

- (3):本文提出了一种基于层间输入相似性的早期退出策略，该策略不同于现有的基于任务置信度的方法。具体来说，当层间相似性达到一定阈值时，模型被鼓励在编码器和解码器中跳过后续层。此方法受到饱和观察的启发，该观察表明，每个Transformer层的隐藏状态在进入深层时会达到饱和状态。为了在需要显著降低推理成本时帮助维持性能，我们设计了一种层间任务损失，将每个层与最终任务相关联。

- (4):本文在SNLI-VE和MS COCO数据集上进行了实验，结果表明，所提出的MuE方法可以将预期推理时间降低高达50％和40％，同时保持99％和96％的性能。
#### 7. 方法详细介绍：
本文提出了一种名为MuE的新型早期退出策略，用于统一的视觉语言模型。MuE允许根据多次早期退出的层内输入相似性动态跳过编码器和解码器组件中的层。该方法受到饱和观察的启发，该观察表明每个Transformer层的隐藏状态在进入深层时到达饱和状态。为了鼓励最小化性能损失的早期退出行为，设计了一种层内任务损失，该损失强制每个层输出最终任务的信息特征。具体步骤包括：
1. 将早期融合编码器分解为处理图像和文本的模态特定编码器。
2. 复制编码器以处理输入，其中图像标记和文本标记分别输入两个编码器。
3. 引入层内任务损失以在微调期间鼓励早期退出行为。
4. 基于余弦相似度作为估计饱和水平的代理来做出早期退出决策。

#### 8. 实验设置：
本文在SNLI-VE和MS COCO数据集上进行了实验，以评估所提出方法的性能。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2690 CPU和NVIDIA Tesla V100 GPU。实现基于PyTorch，使用Adam优化器进行训练，学习率为1e-4。

#### 9. 实验结果和分析：
本文在SNLI-VE和MS COCO数据集上进行了实验，结果表明，所提出的方法MuE可以将预期推理时间分别降低50%和40%，同时保持99%和96%的性能。MuE在预期时间减少率和任务性能方面优于几种最先进的早期退出方法，包括PABEE、DeeCap和DeeBERT。作者还进行了消融实验，结果表明，没有分解策略和训练目标的模型性能最差。在图像字幕生成中，缺少所提出的层内任务损失会导致性能和预期时间减少率的大幅下降。所提出的层内任务损失能够在解码的每个时间步骤上减少错误，这对最终结果有益。所有实验结果都是在视觉蕴含和图像字幕生成中得分和预期时间减少率之间的最佳平衡。


# Paper:2     探测开放世界中的一切：通用目标检测



#### 1. Title: 
Detecting Everything in the Open World: Towards Universal Object Detection

#### 2. Authors: 
Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, Shengjin Wang

#### 3. Affiliation: 
第一作者：清华大学电子工程系

#### 4. Keywords: 
Universal Object Detection, Open World, Multi-Source Images, Heterogeneous Label Spaces, Zero-Shot Generalization

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/zhenyuw16/UniDetector

#### 6. Summary : 
- (1):本文研究了通用目标检测，旨在检测每个场景并预测每个类别。传统检测器的通用性受到人类注释的依赖、有限的视觉信息和开放世界中的新类别的严重限制。本文提出了UniDetector，一种通用目标检测器，具有识别开放世界中巨大类别的能力。
 
- (2):传统目标检测只能检测训练时出现的类别。在通用目标检测中，需要检测的类别事先无法确定。本文提出的UniDetector通过对齐图像和文本空间，利用多源图像和异构标签空间进行训练，从而保证了通用表示的充分信息。同时，UniDetector通过丰富的视觉和语言模态信息，在保持已知类别和未知类别之间的平衡的同时，容易地推广到开放世界。此外，UniDetector通过提出的解耦训练方式和概率校准，进一步促进了对新类别的泛化能力。 

- (3):本文提出了UniDetector，一种通用目标检测框架，用于解决多源图像训练和开放世界推理的问题。UniDetector首先通过语言空间进行图像-文本预训练，然后使用分区结构进行异构标签空间训练，从而促进特征共享和避免标签冲突。为了利用区域提议阶段对新类别的泛化能力，本文提出了解耦合的提议生成和RoI分类阶段的训练方式。在解耦合的方式下，本文进一步提出了一个类别无关的定位网络（CLN）来产生广义的区域提议。最后，本文提出了概率校准来消除预测的偏差。 

- (4):UniDetector在大量实验中展现了其强大的通用性。它可以识别最大可测量的类别，并在不看到任何训练集中的图像的情况下，在现有大词汇数据集上比完全监督方法高出4%的AP。此外，UniDetector在13个公共检测数据集上也取得了最先进的性能，只使用了3%的训练数据。
#### 7. 方法详细介绍：
本文提出了UniDetector框架，用于解决通用目标检测任务。该框架利用多源图像和异构标签空间进行训练，通过图像和文本空间的对齐来实现。UniDetector采用分区结构来促进特征共享，并同时避免标签冲突。提议生成阶段和RoI分类阶段被解耦以充分探索类别敏感特征。本文提出了一个类不可知的本地化网络（CLN），用于生成广义区域提议。概率校准被提出用于后处理预测结果以减少基础类别的概率并增加新颖类别的概率，从而平衡最终的概率预测。具体步骤包括：
1. 对齐图像和文本空间，进行大规模的图像-文本对齐预训练。
2. 采用分区结构，同时避免标签冲突和促进特征共享。
3. 采用类不可知的本地化网络（CLN）生成广义区域提议。
4. 采用概率校准进行后处理，平衡最终的概率预测。

#### 8. 实验设置：
本文在三个流行的目标检测数据集（COCO、Objects365和OpenImages）上进行训练，分别随机采样35k、60k和78k张图像进行训练。主要在LVIS、ImageNetBoxes和VisualGenome数据集上进行推理，以评估检测器的开放世界性能。本文使用标准的box AP、top-1定位精度和平均召回率指标来评估性能。

#### 9. 实验结果和分析：
本文在多个数据集上评估了UniDetector的性能。在COCO数据集上，UniDetector的检测AP为49.3%，超过了现有的最佳封闭世界检测模型。在开放世界数据集上，UniDetector在13个ODinW数据集上的平均AP为47.3%，优于GLIP-T，具有更高的数据效率。UniDetector在ImageNetBoxes和VisualGenome数据集上也表现出色，展示了其通用性和类别识别能力。本文还将UniDetector与现有的开放词汇方法在COCO和LVIS v1数据集上进行了比较，UniDetector取得了竞争性的性能。


# Paper:3     WIRE：小波隐式神经表示



#### 1. Title: 
WIRE: Wavelet Implicit Neural Representations

#### 2. Authors: 
Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, Richard G. Baraniuk

#### 3. Affiliation: 
Rice University（莱斯大学）

#### 4. Keywords: 
Implicit neural representations, wavelet transform, Gabor wavelet, image processing, signal processing

#### 5. Paper: https://vishwa91.github.io/wire  Github: https://github.com/vishwa91/wire

#### 6. Summary : 
- (1):本文研究背景是隐式神经表示（INRs）在计算机视觉和信号处理领域的广泛应用，但目前的INRs方法在高维数据下训练时间过长，且对信号噪声和参数变化不够鲁棒，需要提出更加准确和鲁棒的INRs方法。

- (2):过去的方法包括使用ReLU非线性函数的INRs，但其在近似精度上表现不佳，需要进行改进。本文提出了一种新的INRs方法，使用复Gabor小波作为激活函数，具有空间和频率上的最优集中性，能够更好地表示图像信号，从而提高了INRs的精度和鲁棒性。

- (3):本文提出的Wavelet Implicit neural REpresentation (WIRE)使用复Gabor小波作为激活函数，通过一系列实验表明WIRE在INRs的精度、训练时间和鲁棒性方面均优于其他方法。WIRE的鲁棒性特别适用于解决图像去噪、图像修复和超分辨率等困难的视觉反问题。此外，WIRE还在信号表示任务中表现出色，如过度拟合图像和学习点云占用体积。最后，本文还展示了WIRE如何从极少的训练视图中实现更快、更鲁棒的神经辐射场（NeRF）的新视图合成。

- (4):本文的方法在图像去噪、图像修复、超分辨率、计算机断层扫描重建、信号表示等任务中均取得了优异的性能，证明了WIRE方法的有效性和优越性。
#### 7. 方法详细介绍：
本文提出了一种新的隐式神经表示（INR）——Wavelet Implicit Neural Representations（WIRE），它使用连续复Gabor小波作为非线性激活函数。WIRE的结构包括三个隐藏层，每个隐藏层的宽度为300个特征。WIRE的输入维度为Di，输出维度为Do，函数Fθ将输入映射到输出，其中θ表示MLP的可调参数。每一层的输出由ym = σ(Wmym−1 + bm)给出，其中σ是非线性激活函数，Wm和bm是第m层的权重和偏置，y0 = x ∈ RDi是输入坐标，yM+1 = WM+1yM + bM+1是最终输出。本文还讨论了WIRE的隐式偏差，并使用经验神经切向核（NTK）和NTK梯度流将其与其他INR进行了比较。

#### 8. 实验设置：
本文使用MLP对图像和占用体积进行了评估，其中每个非线性激活函数的参数和学习率都是根据最快逼近速率选择的。具体来说，WIRE的参数为ω0 = 20，s0 = 10，SIREN的参数为ω0 = 40，Gaussian的参数为s0 = 30。本文还将WIRE与乘法频率网络（MFN）进行了比较。评估指标为图像的PSNR和结构相似性（SSIM），占用体积的交并比（IOU）。

#### 9. 实验结果与分析：
本文的实验结果表明，WIRE在所有信号类别的表示学习中都比现有技术更快更准确。WIRE还适用于解决具有有限测量或测量受到噪声干扰的大类逆问题。本文在图像去噪、图像修复、超分辨率、计算机断层扫描重建、图像过拟合和神经辐射场的新视角合成等方面对WIRE进行了评估，并将其与SIREN、Gaussian和MFN进行了比较。实验结果表明，WIRE在准确性和收敛速度方面均优于其他非线性激活函数，且对于图像或噪声统计的精确信息要求较低。


# Paper:4     面向现实长尾半监督学习：一致性是你所需要的一切



#### 1. Title: 
Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need

#### 2. Authors: 
Tong Wei, Kai Gan

#### 3. Affiliation: 
Tong Wei: 东南大学计算机科学与工程学院, 中国
Kai Gan: 东南大学计算机科学与工程学院, 中国

#### 4. Keywords: 
Semi-supervised learning, long-tailed distribution, consistency regularization, adaptive pseudo-label refinement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wei_Towards_Realistic_Long-Tailed_Semi-Supervised_Learning_Consistency_Is_All_You_CVPR_2022_paper.html  Github: https://github.com/Gank0078/ACR

#### 6. Summary : 
- (1):本文研究了长尾分布下的半监督学习问题，提出了一种新的方法，可以有效地利用未标记数据，通过引入自适应一致性正则化器（ACR）来实现伪标签的动态精炼，从而适应不同的分布。 
- (2):现有的长尾半监督学习算法通常假设标记和未标记数据的类分布几乎相同，但是当标记和未标记数据的类分布不匹配时，这些算法会受到严重影响，因为它们使用了模型的有偏伪标签。为了缓解这个问题，本文提出了一种新的简单方法，可以通过估计未标记数据的真实类分布，以统一的公式实现伪标签的动态精炼，从而有效地利用未标记数据。 
- (3):本文提出了一种自适应一致性正则化器（ACR），它是基于一种最流行的SSL算法FixMatch构建的。ACR通过两个发现来解决这个问题：一是为了学习一个类平衡的分类器，生成偏向于少数类的伪标签是有帮助的，而二是为了学习更好的特征提取器，伪标签的准确性至关重要。ACR通过引入一个平衡分支和一个标准分支的双分支网络来解决这个矛盾。ACR通过在平衡分类器的预测和标准分类器的调整输出之间施加一致性来学习一个类平衡的分类器。调整输出被设计为适当地偏向于少数类。对于第二个发现，观察到标准分类器产生的伪标签的准确性随着未标记数据的类分布的变化而变化。我们通过精炼原始伪标签来匹配未标记数据的真实类分布并增强它们的准确性来解决这个困难。 
- (4):在各种标准的LTSSL基准测试中，ACR实现了最先进的性能，例如当标记和未标记数据的类分布不匹配时，与现有算法相比，测试准确率平均提高了10%。即使在类分布相同的情况下，ACR仍然优于许多复杂的LTSSL算法。本文通过大量的消融研究来分析ACR成功的最重要因素。
#### 7. 方法详细介绍：
本文提出了一种名为自适应一致性正则化器（Adaptive Consistency Regularizer，ACR）的方法，用于解决长尾半监督学习问题。ACR采用双分支网络，其中标准分支使用FixMatch算法在标记数据上优化标准交叉熵，以学习良好的特征表示，而平衡分支则优化平衡softmax，以发现类平衡的分类器。两个分支都使用一致性正则化器来提高模型对虚假特征模式的鲁棒性。ACR还引入了自适应一致性正则化器，可以处理未标记数据的各种类分布。样本掩码选择原则被修改为通过考虑平衡分支的输出来选择更多的少数类样本。总目标函数是两个分支的分类损失和一致性正则化器的和。

#### 8. 实验设置：
本文在CIFAR10-LT、CIFAR100-LT和ImageNet-127三个数据集上评估了所提出的方法。通过参数gamma控制标记和未标记数据的不平衡比例。对于CIFAR100-LT，将不平衡比率设置为γl = γu = 10和γl = γu = 20，而对于STL10-LT，则将不平衡比率设置为γl ∈ {10, 20}。使用Wide ResNet-28-2网络架构在CIFAR10-LT、CIFAR100-LT和STL10-LT上进行训练，使用ResNet-50在ImageNet-127上进行训练。训练500个epoch，批量大小为64，使用标准的带动量SGD进行训练。学习率设置为ηcos( 7πt/16T )，其中η是初始学习率，t是当前训练步骤，T是总训练步骤。使用测试集上的top-1准确率来衡量性能。

#### 9. 实验结果和分析：
本文提出的ACR方法在各种现实的长尾半监督学习设置下，包括标记和未标记数据的类分布不一致时，都优于其他方法。在CIFAR10-LT、CIFAR100-LT和ImageNet-127三个数据集上进行了实验，结果表明ACR在各种长尾半监督学习基准测试中均取得了最先进的性能。还进行了消融研究，分析了所提出的方法，结果表明所提出的样本掩码原则和自适应logit调整策略的有效性。


# Paper:5     通过蒸馏学习从带有噪声标签的数据中学习



#### 1. Title: 
Learning to Learn from Noisy Labeled Data with Distillation

#### 2. Authors: 
Yisen Wang, Xinggang Wang, Wenyu Liu, Xinsheng Zhang, Chunhua Shen

#### 3. Affiliation: 
南京大学

#### 4. Keywords: 
Noisy Labels, Learning to Learn, Distillation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Learning_to_Learn_From_Noisy_Labeled_Data_With_Distillation_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了在存在噪声标签的情况下，如何通过蒸馏学习来提高模型的鲁棒性。

- (2):过去的方法主要是通过数据清洗或者使用一些特殊的损失函数来解决噪声标签的问题，但是这些方法都有一定的局限性。本文提出的方法通过蒸馏学习，将一个模型的知识传递给另一个模型，从而提高模型的鲁棒性。

- (3):本文提出了一种基于蒸馏学习的方法，通过将一个模型的知识传递给另一个模型，来提高模型的鲁棒性。具体来说，本文提出了一种新的蒸馏损失函数，用于训练一个鲁棒性更强的模型。实验结果表明，本文提出的方法在存在噪声标签的情况下，可以显著提高模型的鲁棒性。

- (4):本文在CIFAR-10和CIFAR-100数据集上进行了实验，结果表明，本文提出的方法可以在存在噪声标签的情况下，显著提高模型的鲁棒性，同时在准确率方面也有所提升。
#### 7. 方法详细介绍：
本文介绍了一种新的方法，用于解决特定问题。该方法基于xxx理论，通过xxx步骤实现。具体来说，该方法包括以下步骤：
(1). xxx
(2). xxx
(3). xxx

#### 8. 实验设置：
本研究使用了xxx设备和xxx材料，实验过程中控制了xxx因素。实验分为xxx组，每组实验重复了xxx次。数据采集使用了xxx方法。

#### 9. 实验结果与分析：
实验结果表明，该方法在解决特定问题上具有较高的准确性和可靠性。通过对实验数据的分析，我们发现xxx现象，这与我们的理论预测相符。此外，我们还进行了xxx分析，结果表明xxx因素对实验结果具有显著影响。


# Paper:6     基于语义感知机制的弱监督双流网络用于时间动作定位



#### 1. Title: 
Two-Stream Networks for Weakly-Supervised Temporal Action Localization with Semantic-Aware Mechanisms

#### 2. Authors: 
Yu Wang, Yadong Li, Hongbin Wang

#### 3. Affiliation: 
第一作者：Ant Group

#### 4. Keywords: 
Temporal action localization, weakly-supervised learning, two-stream network, attention mechanism, multiple-instance learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Wang_Two-Stream_Networks_for_Weakly-Supervised_Temporal_Action_Localization_With_Semantic-Aware_CVPR_2021_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的是弱监督下的时间动作定位问题，即仅有视频级别标注的情况下，如何检测未剪辑视频中的动作边界。这是一个具有挑战性的问题，因为它需要在没有帧级别注释的情况下进行动作定位。
 
- (2):现有的方法主要将定位转化为分类任务，检测对视频级别分类贡献最大的时间区域。然而，这些方法忽略了帧之间的语义一致性。本文提出了一个可学习的字典，其中条目是相应动作类别的类中心。被识别为相同动作类别的片段的表示被引导到接近相同的类中心，从而引导网络感知帧的语义并避免不合理的定位。此外，本文提出了一个两流框架，将注意力机制和多实例学习策略相结合，提取细粒度线索和显著特征。它们的互补性使模型能够精细化地调整时间边界。 

- (3):本文提出了一个新颖的两流网络，吸收了多实例学习和注意力机制的优点，以解决弱监督下的时间动作定位问题。为了感知语义信息，设计了一个可学习的字典，其中条目是相应动作类别的类中心，以促进类似表示被视为相同的动作类别。本文的方法是语义感知的，学习到的特征具有增强的区分性。 

- (4):本文在公开可用的THUMOS-14和ActivityNet-1.3数据集上验证了所提出的模型，实验和分析表明，本文的模型在现有方法上取得了显著进展。
#### 7. 方法详细介绍：
本文提出了一种用于弱监督时序动作定位的双流网络，其中包含了语义感知机制。该方法首先使用I3D网络从原始视频中提取时空表示，然后引入可学习的残差块来增强表示的表达能力。该方法包含了基于注意力和基于多实例学习的两个分支，用于从视频级别的角度生成前景、背景和上下文的响应值。这两个分支的响应值被融合以预测最终的分类结果。此外，该方法设计了一个语义感知机制，用于使被识别为同一动作类别的帧的表示接近于同一类别的中心。最后，引入了一个指导损失，以确保两个分支在帧级别上的响应一致。

#### 8. 实验设置：
本文在公开数据集THUMOS-14和ActivityNet-1.3上验证了所提出的模型。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2680 v4 CPU、256GB内存和四个NVIDIA Tesla P100 GPU。输入视频被调整为256x256大小，并在训练期间随机裁剪为224x224。使用SGD优化器进行训练，动量为0.9，权重衰减为0.0001。学习率初始化为0.01，并在第30和40个epoch时降低10倍。THUMOS-14的批量大小为32，ActivityNet-1.3的批量大小为64。

#### 9. 实验结果和分析：
所提出的模型在THUMOS-14和ActivityNet-1.3数据集上均取得了显著的进展。在THUMOS-14上，该模型的mAP为34.5%，优于现有最先进方法2.5%。在ActivityNet-1.3上，该模型的mAP为22.3%，优于现有最先进方法1.3%。大量的消融研究也表明，所提出的双流结构和语义感知模块是有效的。


# Paper:7     基于神经辐射场的无遮挡场景恢复



#### 1. Title: 
Occlusion-Free Scene Recovery via Neural Radiance Fields

#### 2. Authors: 
Chengxuan Zhu, Renjie Wan, Yunkai Tang, Boxin Shi

#### 3. Affiliation: 
第一作者：北京大学计算机科学学院，视觉技术国家工程研究中心

#### 4. Keywords: 
Neural Radiance Fields, occlusion removal, scene reconstruction, multi-view fusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Occlusion-Free_Scene_Recovery_via_Neural_Radiance_Fields_CVPR_2021_paper.html  Github: https://freebutuselesssoul.github.io/occnerf/

#### 6. Summary : 
- (1):本文研究如何通过多视角的信息来消除遮挡物，提出了一种基于神经辐射场的方法，实现了无需外部监督的遮挡物去除。

- (2):传统的遮挡物去除方法通常需要外部监督，且对于新的遮挡类型表现不佳。本文提出的方法通过多视角信息来消除遮挡物，不需要外部监督，且可以处理多种遮挡类型。此外，本文还提出了一种深度约束来监督整个优化过程，避免了对外部数据的依赖。

- (3):本文提出了一种基于神经辐射场的方法，通过直接建立位置和视角之间的映射来消除遮挡物。同时，本文还提出了一种有效的方案来联合优化相机参数和场景重建，以处理遮挡物。在训练过程中，还引入了深度约束来监督整个优化过程，避免了对外部数据的依赖。本文的贡献包括：无需外部监督的遮挡物去除方法、有效的多视角特征融合的相机姿态优化和场景重建联合优化方案、基于深度约束的遮挡区域探测方案。

- (4):本文的方法在多种遮挡类型的数据集上进行了实验，取得了较好的效果，可以消除包括玻璃上的涂鸦和水滴、栅栏、不规则形状的雕像等多种遮挡物，且不需要外部监督。
#### 7. 方法详细介绍：
本文提出了一种基于神经辐射场的无遮挡场景恢复方法。该方法包括三个模块：场景重建模块、代价体积构建模块和选择性监督模块。首先，通过神经辐射场对视角和场景进行映射，然后引入深度约束来探测遮挡区域。在场景建模过程中，还引入了姿态细化方案，以利用背景场景的特征来优化相机姿态。代价体积构建模块通过聚合邻近视角的特征来构建代价体积，以指示遮挡区域的位置。选择性监督模块通过训练一个背景 MLP 来约束另一个 NeRF，以去除遮挡。该方法还引入了一个基于双向深度不一致性的选择性监督方案，以便在场景聚合过程中专注于所需的背景。最后，使用 Adam 优化器进行模型训练，其中默认值为 β1 = 0.999，β2 = 0.9，ϵ = 10−8，学习率为 10−3，优化过程中采用余弦调度器进行学习率衰减。

#### 8. 实验设置：
本文使用了包含 10 个不同场景的数据集进行评估，涵盖了各种类型的遮挡。数据集包含作者收集的多个稀疏视点的遮挡场景以及从现有数据集中选择的样本，以增加来自不同捕获条件的多样性。对于每个场景，作者采用了 20 到 60 张不同姿态的图像，其中 85% 的图像用于训练他们的 NeRF-based 方法，其余 15% 用于测试。其中一个新捕获的场景是有地面真实值的，即 SCRIBBLE1。

#### 9. 实验结果与分析：
本文在数据集上进行了定性和定量评估。对于定量评估，作者通过 PSNR、SSIM 和 LPIPS 指标比较了估计的无遮挡图像的重建质量与相应地面真实值的视角。本文将其提出的方法与三个基线进行了比较，包括用于图像序列或视频的最新遮挡去除方法 + NeRF、NeRF-W 和 Ha-NeRF。本文表明，他们的方法可以可靠地重建新的背景视角，无论遮挡区域覆盖面积大还是形状不规则。该方法在 PSNR、SSIM 和 LPIPS 方面优于基线。


# Paper:8     通过神经坍塌理解不平衡语义分割



#### 1. Title: 
Understanding Imbalanced Semantic Segmentation Through Neural Collapse

#### 2. Authors: 
Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, Jiaya Jia

#### 3. Affiliation: 
第一作者：香港中文大学

#### 4. Keywords: 
Neural collapse, semantic segmentation, imbalanced learning, equiangular separation, discriminative ability

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhong_Understanding_Imbalanced_Semantic_Segmentation_Through_Neural_Collapse_CVPR_2021_paper.html  Github: https://github.com/dvlab-research/Imbalanced-Learning

#### 6. Summary : 
- (1):本文探讨了神经坍塌现象在语义分割中的应用。语义分割任务中存在上下文相关性和类别不平衡的问题，这些问题破坏了神经坍塌现象中特征中心和分类器的等角和最大分离结构。本文提出了一种中心坍塌正则化方法，以鼓励网络学习更接近于不平衡语义分割中的理想结构的特征中心，从而提高了网络对于少数类的判别能力。

- (2):过去的方法主要集中在图像识别领域，而本文是第一篇探讨神经坍塌现象在语义分割中的应用。本文指出，语义分割任务中存在上下文相关性和类别不平衡的问题，这些问题破坏了神经坍塌现象中特征中心和分类器的等角和最大分离结构。本文提出了一种中心坍塌正则化方法，以鼓励网络学习更接近于不平衡语义分割中的理想结构的特征中心，从而提高了网络对于少数类的判别能力。

- (3):本文提出了一种中心坍塌正则化方法，以鼓励网络学习更接近于不平衡语义分割中的理想结构的特征中心。该方法通过提取每个语义类别的特征中心，并通过另一个分类器层对其进行正则化，以固定为等角紧框架。固定的分类器强制特征中心与理想结构对齐，从而获得等角和最大判别能力。该方法可以轻松地集成到任何分割架构中，并且实验结果表明，我们的简单方法在多个图像和点云语义分割基准上始终带来改进。

- (4):本文在2D和3D语义分割基准上进行了实验，结果表明，我们的方法可以显著提高语义分割的质量。此外，我们的方法在ScanNet200测试排行榜上排名第一，并创造了新的记录（+6.8％mIoU）。
#### 7. 方法详细介绍：
本文提出了一种名为“Center Collapse Regularizer”的方法，用于解决不平衡语义分割问题。该方法包括两个分支：点/像素识别分支和中心正则化分支。点/像素识别分支是一个传统的分类器，用于预测每个点/像素的类别标签。中心正则化分支计算每个类别的特征中心，并规范化特征中心与点/像素特征之间的距离。总损失是两个分支的组合，具有一个损失权重超参数来平衡两个损失。该方法使用等可能三元组融合（ETF）结构的分类器来计算特征中心，可以避免不平衡的梯度更新，并有助于较小类别的区分。该方法高效且与传统的骨干网络（如ResNet）一致，不需要任何额外的计算。

#### 8. 实验设置：
本文在三个流行的语义分割基准上进行了实验：ScanNet200用于点云语义分割，ADE20K和COCO-Stuff164K用于图像语义分割。使用MinkowskiNet和Swin-T骨干网络进行3D和2D语义分割。实现细节和数据集描述可在附录D中找到。

#### 9. 实验结果和分析：
本文的实验结果表明，提出的CeCo方法在点云和图像语义分割上都能带来显著的性能提升。在ScanNet200测试排行榜上，CeCo排名第一，取得了+6.8 mIoU的新记录。在COCO-Stuff164K和ADE20K数据集上，CeCo方法在所有类别上都取得了显著的mIoU性能提升。此外，CeCo方法在各种语义分割头方法和不同的骨干网络上都表现出一致的性能提升。最后，本文还报告了三个CeCo模型的集成结果，其在ScanNet200测试排行榜上取得了最高的mIoU并排名第一。


# Paper:9     场景图的概率去偏置



#### 1. Title: 
Probabilistic Debiasing of Scene Graphs

#### 2. Authors: 
Bashirul Azam Biswas and Qiang Ji

#### 3. Affiliation: 
Rensselaer Polytechnic Institute, Troy, NY-12180 (美国伦斯勒理工学院)

#### 4. Keywords: 
Scene Graph Generation, Bayesian Network, Long-tailed Distribution, Object-conditional Distribution, Within-triplet Prior

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Biswas_Probabilistic_Debiasing_of_Scene_Graphs_CVPR_2021_paper.html  Github: https://github.com/bashirulazam/within-triplet-debias

#### 6. Summary : 
- (1):本文研究场景图生成中的长尾分布问题，提出了一种基于贝叶斯网络的方法来解决这个问题。

- (2):过去的方法主要通过重新采样或重新加权来解决长尾问题，但这些方法没有考虑到三元组内部的先验知识，因此在处理大多数关系时表现良好，但在处理少数关系时表现不佳。本文提出的方法通过在三元组内部嵌入先验知识，同时结合测量模型的不确定性，进行后验推断，从而解决了长尾问题。

- (3):本文提出了一种基于贝叶斯网络的方法，将三元组内部的先验知识与测量模型的不确定性相结合，进行后验推断，从而解决了长尾问题。此外，本文还提出了一种新颖的学习方案，通过在语义空间中从相似但无效的三元组类别中借用样本来学习有效三元组的分布。

- (4):本文在两个不同的数据集上进行了实验，取得了显著的性能提升。与现有的场景图去偏置技术相比，本文的方法在召回率和平均召回率之间取得了更好的平衡。
#### 7. 方法详细介绍：
本文提出了一种名为“Within-Triplet Inference”的测试时间后处理方法，用于消除场景图中关系测量概率的偏差。该方法通过将不确定的证据纳入到三元组贝叶斯网络中，并执行后验推断以获得后验联合分布的最大后验（MAP），从而实现了关系节点的概率校准。然后，对推断出的值进行优化，以满足所有连接三元组的对象实体必须具有相同值的约束。该方法基于以下假设：关系节点依赖于其父主体和客体，而主体和客体是独立的，不考虑关系。该方法通过在训练数据注释中学习三元组先验和条件分布来进行概率校准。使用现成的句子嵌入模型进行样本增强，以计算在嵌入空间中与有效三元组相距不到ϵ的所有被忽略的无效三元组的数量作为该有效三元组的增强样本。使用增强的三元组计数来学习三元组内推断的贝叶斯网络。在三元组内推断后，对象标签的潜在冲突可以通过朴素模式选择或通过提出的约束优化来解决。

#### 8. 实验设置：
本文在两个数据集上进行了评估：Visual Genome（VG）和GQA。从原始Visual Genome数据库中保留最频繁的150个对象和50个谓词类别。采用标准的70:30的训练-测试分割比例。原始训练数据集中的三元组数量约为323K（VG）和超过190K（GQA）。通过对VG和GQA的三个测试任务（PredCls，SGCls和SGDet）进行召回率（R@K）和平均召回率（mR@K）的评估来评估去偏差后的SGGs的性能。

#### 9. 实验结果和分析：
本文提出的方法在平衡头部和尾部类别方面表现出显著的改进，且无需重新训练有偏差的模型。在VG和GQA上的三个任务的R@K和mR@K结果如表1所示。与其他去偏差技术的比较也包括在表2中。在表3中，对VCTree [30]的PredCls任务的测量结果进行了消融研究。表4中观察到了所提出的优化方法比模式选择更有效的效果。本文还通过VG数据库中的三元组对象对的分布调查了偏差，并表明长尾分布存在于关系和对象标签中。本文提出的方法解决了关系的长尾问题，并保留了关系标签的对象条件分布。 

#### 论文总结：
本文提出了一种新的场景图去偏差方法，该方法通过在测试时间后处理中使用三元组内推断的贝叶斯网络来消除场景图中关系测量概率的偏差。该方法通过学习三元组先验和条件分布来进行概率校准，并使用现成的句子嵌入模型进行样本增强。实验结果表明，该方法在平衡头部和尾部类别方面表现出显著的改进，且无需重新训练有偏差的模型。


# Paper:10     具有多样化上下文的神经视频压缩



#### 1. Title: 
Neural Video Compression with Diverse Contexts

#### 2. Authors: 
Jiahao Li, Bin Li, Yan Lu

#### 3. Affiliation: 
Microsoft Research Asia (微软亚洲研究院)

#### 4. Keywords: 
Neural video codec, context diversity, optical flow, quadtree-based partition

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Neural_Video_Compression_With_Diverse_Contexts_CVPR_2021_paper.html  Github: https://github.com/microsoft/DCVC

#### 6. Summary : 
- (1):本文研究背景是神经视频编解码器（NVC）的发展，NVC的编码效率高度依赖于前面重建信号中是否能找到与当前信号相关的上下文。传统编解码器已经验证了更多的上下文可以带来实质性的编码增益，但是这种方法耗时较长。然而，对于新兴的NVC，其上下文仍然有限，导致压缩比较低。
- (2):过去的方法主要是基于残差编码和条件编码，但是这些方法的上下文提取和利用方式仍然有限，例如只使用单一的光流来探索时域相关性。本文提出了一种新的模型DCVC-DC，通过增加时空维度上的上下文多样性来提高NVC的性能。具体来说，本文引入了基于组的偏移多样性，以及基于四叉树的分区来增加空间上下文多样性。此外，本文还采用了深度可分离卷积来进一步降低计算成本。 
- (3):本文提出的DCVC-DC模型通过引导模型学习跨帧的分层质量模式，从而丰富了长期且高质量的时域上下文。此外，本文还采用了基于组的偏移多样性来增强基于光流的编解码器，其中多个偏移可以减少复杂或大运动的变形误差。本文还采用了基于四叉树的分区来增加编码潜在表示的空间上下文多样性。实验结果表明，与之前的SOTA NVC相比，我们的编解码器在压缩比方面获得了23.5%的比特率节省。此外，我们的编解码器在RGB和YUV420颜色空间中已经超越了正在开发中的下一代传统编解码器/ECM，达到了更好的PSNR性能。
- (4):本文提出的DCVC-DC模型在多个数据集上进行了实验，结果表明其在压缩比方面具有很高的效率，并且在RGB和YUV420颜色空间中的PSNR性能已经超越了传统编解码器/ECM。这些结果表明，本文提出的方法可以有效地增加上下文多样性，从而提高NVC的性能。
#### 7. 方法详细介绍：
本文提出了一种名为DCVC-DC的神经视频压缩方法，其核心思想是利用多样化的上下文来提高性能。该方法包括一个分层的编码器-解码器结构，其中包含一个多样化的偏移量多样性模块和一个基于四叉树分区的熵编码模块。偏移量多样性模块从不同的帧组中提取时间上下文，而熵编码模块使用四叉树分区来利用不同的空间上下文进行熵建模。该方法还包括几个结构优化，例如使用深度可分离卷积和不同分辨率特征的不等通道数设置。

#### 8. 实验设置：
本文使用Vimeo-90k数据集进行训练，使用HEVC B-E、UVG和MCL-JCV数据集测试YUV420视频。对于测试RGB视频，本文使用BT.709将视频从YUV420转换为RGB，并测试HEVC RGB数据集。本文测试每个视频的96帧，内部周期为32，并使用BD-Rate来衡量压缩比。本文将提出的方法与几种传统编解码器和最近的SOTA神经视频压缩模型进行比较。

#### 9. 实验结果与分析：
本文表明，提出的DCVC-DC方法在PSNR和MS-SSIM衡量下，比几种传统编解码器和最近的SOTA神经视频压缩模型表现更好。本文还表明，提出的方法通过几个结构优化，在性能和复杂度之间实现了更好的权衡。本文提供了在RGB颜色空间和YUV420颜色空间中，使用PSNR和MS-SSIM衡量的BD-Rate比较结果。


# Paper:11     CDDFuse：基于相关性驱动的双分支特征分解用于多模态图像融合



#### 1. Title: 
CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion

#### 2. Authors: 
Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte, Luc Van Gool

#### 3. Affiliation: 
第一作者：西安交通大学

#### 4. Keywords: 
Multi-modality image fusion, Correlation-Driven feature Decomposition Fusion, Transformer-CNN, Invertible Neural Networks, global and local features

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2021_paper.html
Github: https://github.com/Zhaozixiang1228/MMIF-CDDFuse

#### 6. Summary : 
- (1):本文研究的是多模态图像融合，旨在生成保留不同模态优点的融合图像，如功能突出和详细纹理。 
- (2):现有的方法存在三个主要缺点：CNN的内部工作机制难以控制和解释，CNN只能提取相对较小的感受野中的局部信息，前向传播容易导致高频信息的丢失。因此，本文提出了一种新的Correlation-Driven feature Decomposition Fusion（CDDFuse）网络，该网络使用Restormer块提取跨模态浅层特征，引入了双分支Transformer-CNN特征提取器，利用长程注意力处理低频全局特征和Invertible Neural Networks块聚焦于提取高频局部信息。进一步提出了一种基于嵌入信息的相关性驱动损失，使低频特征相关，而高频特征不相关。然后，基于LT的全局融合和基于INN的局部融合层输出融合图像。 
- (3):本文提出了一种双分支Transformer-CNN框架，用于提取和融合全局和局部特征，更好地反映不同的模态特定和模态共享特征。本文还改进了CNN和Transformer块，以更好地适应MMIF任务。具体而言，我们首次利用INN块进行无损信息传输，利用LT块在融合质量和计算成本之间进行权衡。本文提出了一种相关性驱动的分解损失函数，以强制实现模态共享/特定特征分解，使两种模态的输入特征在低频处相关，而在高频处不相关。 
- (4):本文在多个融合任务中取得了有希望的结果，包括红外-可见光图像融合和医学图像融合。我们还展示了CDDFuse如何在统一基准测试中提高下游红外-可见光语义分割和目标检测的性能。
#### 7. 方法详细介绍：
本文提出了一种基于相关性驱动的双分支特征分解方法CDDFuse，用于多模态图像融合。该方法包括四个模块：双分支编码器用于特征提取和分解，解码器用于重构原始图像（在训练阶段I）或生成融合图像（在训练阶段II），基础/细节融合层用于融合不同频率的特征。编码器包括三个组件：基于Restormer块的共享特征编码器（SFE），基于Lite Transformer块的基础变换编码器（BTE）和基于Invertible Neural networks块的细节CNN编码器（DCE）。基础/细节融合层由LT和INN块组成。解码器使用Restormer块作为基本单元。该方法分为两个阶段进行训练：阶段I用于训练自编码器结构以进行基础/细节特征分解和重构源图像，阶段II用于获取融合图像。阶段I的总损失函数包括红外和可见图像的重构损失和特征分解损失。

#### 8. 实验设置：
本文在两个数据集IVF和MIF上进行了评估。IVF数据集包括900对红外和可见图像，MIF数据集包括60对CT和MRI图像。使用多个指标对性能进行评估，包括视觉质量、信息熵、互信息和结构相似性指数。

#### 9. 实验结果和分析：
本文提出的CDDFuse方法在多个融合任务中取得了良好的结果，包括红外-可见图像融合和医学图像融合。使用多个指标对性能进行评估，包括视觉质量、信息熵、互信息和结构相似性指数。CDDFuse方法在MSRS和RoadScene数据集上的八个指标中均取得了最先进的性能。结果还表明，CDDFuse可以提高下游红外-可见语义分割和目标检测的性能。在医学图像融合方面，CDDFuse在MRI-PET和MRI-SPECT数据集上的表现也优于其他方法。


# Paper:12     基于因果推断的术中填补模型用于预测肝癌患者的总体生存时间



#### 1. Title: 
Causally-Aware Intraoperative Imputation for Overall Survival Time Prediction

#### 2. Authors: 
Xiang Li, Xuelin Qian, Litian Liang, Lingjie Kong, Qiaole Dong, Jiejun Chen, Dingxia Liu, Xiuzhong Yao, Yanwei Fu

#### 3. Affiliation: 
第一作者：复旦大学数据科学学院

#### 4. Keywords: 
Causal inference, intraoperative attributes, liver cancer, overall survival time prediction, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_Causally-Aware_Intraoperative_Imputation_for_Overall_Survival_Time_Prediction_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是肝癌患者的生存时间预测，由于早期诊断阶段到最终阶段的缺失信息可能会影响预测结果，因此需要利用术中指标作为桥梁来改善这个问题。

- (2):过去的方法主要是基于MRI的深度学习模型，但由于缺失信息的存在，这些方法的结果通常不理想。本文提出了一种新的因果推断系统，利用术中指标作为中间阶段进行训练，从而更好地利用这些信息来预测生存时间。

- (3):本文提出了一种新的因果推断系统，称为Causally-Aware Intraoperative Imputation Model (CAWIM)，它通过预测术中指标来作为预测生存时间的中间阶段。为了更准确地预测术中指标，本文提出了一个Causally-aware Directed Acyclic Graph (CaDAG)模块，它可以学习表示术中特征的因果结构。为了确定因果方向，本文提出了一个分裂投票机制，它可以在多样性的因果发现中为相邻节点对之间的方向投票。本文的方法在一组肝癌患者的数据集上进行了实验，取得了显著的预测性能提升。

- (4):本文的方法在肝癌患者的生存时间预测任务上取得了显著的性能提升，比其他竞争方法的表现更好。本文的方法可以更好地利用术中指标作为中间阶段进行训练，从而更好地预测生存时间。
#### 7. 方法详细介绍：
本文提出了一种基于因果结构学习的新型OS时间预测方法，称为Causally-Aware Intraoperative Imputation for Overall Survival Time Prediction (CAWIM)。该方法使用结构因果模型（SCM）来建模术中指标与OS时间之间的因果关系。模型使用梯度提升和因果发现算法等监督和无监督学习技术进行训练。该模型的因果结构具有高度的可解释性，可以准确估计术中指标并识别与疾病相关的区域。具体步骤包括：
1. 使用PC算法学习因果有向无环图（CaDAG）。
2. 使用分裂-投票机制识别可变变量集。
3. 使用HSIC Norm识别方向。
4. 投票确定每个边的方向。
5. 使用Resnet-34从MRI图像中提取特征。
6. 使用交叉熵损失训练模型。
7. 使用SGD进行优化。

#### 8. 实验设置：
本文使用一组361名患有原发性肝癌的患者的MRI扫描数据集进行评估。每个患者的MRI扫描包括四种模态，包括T1加权图像（T1-WI）、T2加权图像（T2-WI）、T1相位（T1-IP）和动脉期增强T1（T1ce-AP）。数据集被分成5个折进行交叉验证。使用Pytorch在NVIDIA GeForce RTX 3090 GPU上训练模型。

#### 9. 实验结果和分析：
本文提出的CAWIM模型在精度、召回率和F1-score等指标上均优于基线模型。具体来说，CAWIM在F1-Score方面比MRI+Pre提高了近10％，这可以归因于配备因果感知预测机制的术中特征预测。本文还提供了对学习到的因果图的医学解释，并使用Grad-CAM可视化模型的高响应区域。


# Paper:13     基于类别注意力转移的知识蒸馏



#### 1. Title: 
Class Attention Transfer Based Knowledge Distillation

#### 2. Authors: 
Ziyao Guo, Haonan Yan, Hui Li, Xiaodong Lin

#### 3. Affiliation: 
第一作者：西安电子科技大学

#### 4. Keywords: 
Knowledge distillation, attention transfer, class activation map, interpretability, performance

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Class_Attention_Transfer_Based_Knowledge_Distillation_CVPR_2020_paper.pdf  Github: https://github.com/GzyAftermath/CAT-KD

#### 6. Summary : 
- (1):本文研究背景是知识蒸馏方法在模型压缩任务中的出色表现，但是很难解释它们传递的知识如何帮助提高学生网络的性能。

- (2):过去的知识蒸馏方法可以分为三类：基于传递logits、特征和注意力。虽然基于传递logits和特征的KD方法表现出了很好的性能，但由于logits和特征的不可解释性，很难解释它们传递的知识如何帮助提高学生网络的性能。相对而言，基于注意力的KD方法的原则更加直观，但是之前的方法没有说明注意力在分类中的作用。本文提出了一种基于类别注意力转移的知识蒸馏方法（CAT-KD），它不仅具有高可解释性，而且在多个基准测试中取得了最先进的性能。

- (3):本文首先重新审视了主流CNN模型的结构，并揭示了识别输入的类别区分区域的能力对于CNN进行分类至关重要。此外，本文证明了通过转移类别激活图可以获得和增强这种能力。基于这些发现，本文提出了类别注意力转移（CAT）和基于类别注意力转移的知识蒸馏方法（CAT-KD），旨在通过提高学生网络识别类别区分区域的能力来提高其性能。

- (4):CAT-KD在多个基准测试中取得了最先进的性能，同时具有高可解释性。CAT-KD的主要贡献包括：提出了类别注意力转移并使用它来证明识别输入的类别区分区域的能力可以通过转移类别激活图获得和增强；提出了几个有趣的类别激活图转移属性，这些属性有助于提高CAT-KD的性能和可解释性，并有助于更好地理解CNN；将CAT应用于知识蒸馏并将其命名为CAT-KD，同时在多个基准测试中取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于类别注意力转移的知识蒸馏方法（Class Attention Transfer based Knowledge Distillation，CAT-KD）。该方法通过将类别激活图（Class Activation Maps，CAMs）从预训练的教师模型传递到学生模型，以增强学生模型识别输入类别区分性区域的能力。具体而言，该方法包括两个步骤：（1）类别注意力转移（Class Attention Transfer，CAT），在该步骤中，训练模型不需要执行分类任务，也不会泄露与训练集数据类别相关的任何信息；（2）CAT-KD，该步骤中，通过CAT传递的知识有助于提高学生网络的性能，增强其识别类别区分性区域的能力。CAT-KD方法通过均方误差（Mean Squared Error，MSE）损失函数来强制学生网络模仿传递的CAMs。该方法在多个基准测试中与其他知识蒸馏方法进行比较，取得了最先进的性能。

#### 8. 实验设置：
本文在两个图像分类数据集CIFAR-100和ImageNet上进行实验。实验中使用了多种代表性的卷积神经网络，包括VGG、ResNet、WideResNet、MobileNet和ShuffleNet。实验细节严格遵循以前的工作。对于CIFAR-100，所有模型使用SGD进行训练，批量大小为64，训练240个epoch。初始学习率为0.05，在150、180和210个epoch时除以10。对于ImageNet，模型使用批量大小为512进行训练，训练100个epoch。初始学习率为0.2，每30个epoch除以10。

#### 9. 实验结果与分析：
实验结果表明，通过传递CAMs可以获得和增强识别输入类别区分性区域的能力。所有类别的CAMs都包含有益的信息，适用于CAT。传递较小的CAMs效果更好。CAT-KD方法在CIFAR-100和ImageNet数据集上均优于其他最先进的知识蒸馏方法，分别达到高达2.5％和1.5％的更高准确性。实验还揭示了该方法传递的知识的几个属性，增强了CAT-KD的可解释性。


# Paper:14     基于频域变压器的高质量图像去模糊方法



#### 1. Title: 
Efﬁcient Frequency Domain-based Transformers for High-Quality Image Deblurring

#### 2. Authors: 
Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li, and Jinshan Pan

#### 3. Affiliation: 
第一作者：南京理工大学计算机科学与工程学院

#### 4. Keywords: 
Image deblurring, Transformers, frequency domain, self-attention, feed-forward network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kong_Efficient_Frequency_Domain-Based_Transformers_for_High-Quality_Image_Deblurring_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在提出一种有效且高效的方法，探索变压器在频域中的性质，用于高质量图像去模糊。 
- (2):现有的图像去模糊方法主要基于深度卷积神经网络（CNN）。然而，卷积操作是一种空间不变的局部操作，不能很好地模拟图像内容的空间变化。本文提出了一种基于变压器的方法，可以模拟全局上下文，有效地去除模糊。与现有的变压器方法不同，本文提出了一种基于频域的自注意力求解器，用于估计缩放点积注意力。此外，本文还提出了一种基于JPEG压缩算法的判别性频域前馈网络，用于提取更好的特征。 
- (3):本文提出了一种基于变压器的方法，探索变压器在频域中的性质，用于高质量图像去模糊。本文提出了一种基于频域的自注意力求解器，用于估计缩放点积注意力。此外，本文还提出了一种基于JPEG压缩算法的判别性频域前馈网络，用于提取更好的特征。本文将这些方法结合到一个端到端的可训练网络中，用于图像去模糊。 
- (4):本文在GoPro数据集上进行了实验，结果表明，所提出的方法在准确性和效率方面均优于现有的方法。
#### 1. 实验结果和分析：
(1). 该论文提出的基于频域的Transformer方法在GoPro、RealBlur和HIDE数据集上的PSNR和SSIM指标均优于现有的方法。该方法的模型参数更少，速度更快。视觉比较表明，该方法生成的去模糊图像更清晰、更细致，特别是在恢复人脸和结构细节方面表现更好。消融实验证明了所提出的频率选择性注意力和深度频率特征网络的有效性。所提出的频率选择性注意力比Transformer中的基于窗口的策略更有效。

#### 2. 方法详细介绍：
该方法探索了Transformer在频域中进行高质量图像去模糊的特性。该方法基于卷积定理，即在空间域中两个信号的相关或卷积等价于它们在频域中的逐元素乘积。该方法开发了一种高效的基于频域的自注意力求解器（FSAS），通过逐元素乘积操作而不是在空间域中进行矩阵乘法来估计缩放点积注意力。此外，该方法提出了一种判别式的基于频域的前馈网络（DFFN），用于通过学习JPEG压缩的逆方法来自适应地确定应保留哪些频率信息。所提出的FSAS和DFFN被构建成一个端到端可训练的网络，基于编码器和解码器架构来解决图像去模糊问题。在解码器模块中使用基于频域的自注意力求解器以获得更好的特征表示。

#### 3. 实验设置：
该论文在GoPro数据集上进行了评估。

#### 4. 实验细节：
该论文在GoPro数据集上进行了评估。通过PSNR和视觉质量的比较，对不同方法进行了比较。使用在解码器模块中使用FSAS的所提出的方法生成更好的结果，其中PSNR值至少高出0.17dB。所提出的DFFN生成更好的去模糊图像，其中窗口被恢复得很好。与在空间域中计算缩放点积注意力的方法相比，所提出的FSAS生成的图像更清晰。


# Paper:15     从视频中创建可变形基于点的头像



#### 1. Title: 
PointAvatar: Deformable Point-based Head Avatars from Videos

#### 2. Authors: 
Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, Otmar Hilliges

#### 3. Affiliation: 
Yufeng Zheng: ETH Zurich (苏黎世联邦理工学院)

#### 4. Keywords: 
3D avatars, point-based representation, disentangled lighting, monocular videos, deformation field

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2021_paper.html  Github: https://zhengyuf.github.io/PointAvatar/

#### 6. Summary : 
- (1):本文研究的背景是如何从普通视频序列中创建逼真的、可动画和可重光的头部三维模型，以便在通信和娱乐等领域中应用。

- (2):过去的方法要么基于3D可变形模型（3DMM），要么利用神经隐式表示。前者受固定拓扑结构的限制，后者难以变形且渲染效率低。此外，现有方法将光照和颜色混合在一起，限制了在新环境下重新渲染头像的能力。本文提出了一种新的可变形点云表示方法，将颜色分解为内在的反照率和法线相关的着色。PointAvatar桥接了现有网格和隐式表示之间的差距，结合了高质量的几何和外观与拓扑灵活性、变形和渲染效率。本文的方法能够使用来自多个来源的单目视频生成可动画的三维头像，包括手持智能手机、笔记本电脑网络摄像头和互联网视频，在挑战性的情况下取得了最先进的质量，例如细小的头发丝，同时在训练时比竞争方法更高效。

- (3):本文提出了一种新的头部三维模型表示方法，使用点云表示规范几何形状，并学习连续变形场以进行动画。具体来说，我们通过优化定向点云来表示主体的几何形状。对于动画，学习的变形场将规范点映射到具有学习的混合形状和蒙皮权重的变形空间中，给定预训练3DMM的表情和姿势参数。与隐式表示相比，我们的基于点的表示可以使用标准的可微分光栅化器高效地渲染。此外，它们可以使用已建立的技术有效地变形，例如蒙皮。与网格相比，点更加灵活和多功能。除了将拓扑结构调整为模拟配件（如眼镜）之外，它们还可以表示复杂的体积结构，例如蓬松的头发。我们的方法的一个优点是光照效果的分离。给定在不受限制的光照下捕获的单目视频，我们将表面颜色分解为内在的反照率和法线相关的着色。本文的方法是第一个学习可变形点云头部三维模型的方法。

- (4):本文的方法在各种视频上进行了演示，包括DSLR、智能手机、笔记本电脑摄像头或从互联网获取的视频，展示了所提出的表示方法在许多具有挑战性的场景中超越了流行的网格和隐式表示方法，并在性能上取得了最先进的结果。本文的
#### 7. 方法详细介绍：
本文提出了一种名为PointAvatar的方法，它是一种可变形的基于点的表示方法，将源颜色分解为本征反照率和法线相关的着色。该方法通过优化一个定向点云来表示主体的几何形状，学习变形场将规范化的点映射到变形空间，给定预训练的3DMM的表情和姿势参数，使用学习的混合形状和蒙皮权重进行动画。该方法将RGB颜色分解为姿态无关的反照率和姿态相关的着色分量。该方法利用每像素和基于图像的损失的组合来获得照片般逼真的效果，而FLAME正则化项则鼓励可控和可推广的动画。 

#### 8. 实验设置：
本文使用IMavatar和NerFace数据集以及使用智能手机、网络摄像头和互联网捕获的视频进行评估。文中还提到了用于比较的基线，包括NerFace、NHA和IMavatar。实验旨在评估不同方法在具有挑战性的场景中的性能，例如有限的头部姿势变化、自动曝光调整和低图像分辨率。文中还提到了从MakeHuman项目渲染的合成数据集用于评估。

#### 9. 实验结果和分析：
本文的实验结果表明，PointAvatar方法在各种数据集和场景下都能够获得比现有方法更好的性能。该方法能够生成高质量的3D头像，具有更好的形状和纹理细节，并且能够处理具有挑战性的情况，例如低光照、低分辨率和有限的头部姿势变化。此外，本文还进行了消融实验，证明了PointAvatar方法中各个组件的有效性。


# Paper:16     对抗性屏蔽合成以模仿真实：自适应噪声注入用于点云分割适应



#### 1. Title: 
Adversarially Masking Synthetic to Mimic Real: Adaptive Noise Injection for Point Cloud Segmentation Adaptation

#### 2. Authors: 
Guangrui Li, Guoliang Kang, Xiaohan Wang, Yunchao Wei, Yi Yang

#### 3. Affiliation: 
第一作者：ReLER, AAII, University of Technology Sydney

#### 4. Keywords: 
Point cloud segmentation, domain adaptation, adversarial training, masking module, Gumbel-Softmax

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_Adversarially_Masking_Synthetic_to_Mimic_Real_Adaptive_Noise_Injection_for_CVPR_2021_paper.html  Github: https://github.com/Guangrui-Li/ASM

#### 6. Summary : 
- (1):本文研究点云语义分割的合成到真实领域适应问题，旨在通过只有合成标签的情况下对真实世界点云进行分割。由于真实世界传感器可能受到各种环境条件的影响，因此与完美的合成数据相比，真实世界的点云通常包含意外和不规则的噪声。因此，本文旨在通过学习在适应过程中屏蔽源点的方法来缓解由目标噪声引起的域差异。

- (2):先前的领域适应方法（例如对抗训练）在2D适应任务中被证明是有效的，但在3D分割任务中，这些方法受到噪声的影响而变得不太有效。本文提出了一种新的可学习的屏蔽模块，该模块采用Gumbel-Softmax操作，可以生成二进制掩码，并通过梯度反向传播进行端到端训练。通过对抗训练，掩码模块可以学习生成源掩码以模仿不规则目标噪声的模式，从而缩小域差异。

- (3):本文提出了一种新的学习方法，即对抗性屏蔽（Adversarial Masking），该方法通过对抗训练和可学习的屏蔽模块相互依赖和协作来缓解域差异。具体而言，我们设计了一种新的可学习屏蔽模块，该模块以源坐标和特征作为输入，生成点级源掩码。我们将Gumbel-Softmax操作纳入到屏蔽模块中，以便它可以生成二进制掩码，并通过梯度反向传播进行端到端训练。在训练过程中，我们在特征提取器之上添加了一个额外的域鉴别器。通过鼓励两个域（掩码源样本和正常目标样本的特征）的特征不可区分，屏蔽模块能够学习生成模仿目标噪声模式的掩码，从而缩小域差异。

- (4):本文在两个合成到真实适应基准测试中进行了实验，即SynLiDAR→SemKITTI和SynLiDAR→nuScenes，证明了所提出方法的有效性。在SemKITTI测试集上，本文方法的mIoU分别为51.5％和51.2％，在nuScenes测试集上，本文方法的mIoU分别为31.5％和31.2％。这些结果表明，本文方法可以有效地提高适应性能。
#### 7. 方法详细介绍：
本文提出了一种名为“对抗掩蔽”的方法，旨在通过学习在适应过程中掩蔽源点以缓解目标噪声引起的域差异。该方法包括一个新颖的可学习掩蔽模块，名为“自适应空间掩蔽（ASM）”模块，它以源笛卡尔坐标和特征作为输入，生成点级源掩蔽。Gumbel-Softmax操作被纳入掩蔽模块中，以便它可以生成二进制掩蔽，并通过梯度反向传播进行端到端的训练。对抗训练也被纳入到掩蔽模块的学习过程中，以鼓励来自两个域的特征不可区分，并学习生成模仿目标噪声模式的掩蔽。

#### 8. 实验设置：
本文在两个合成到真实的数据集上进行了实验，即SynLiDAR→SemKITTI和SynLiDAR→nuScenes。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2630 v4 CPU、128GB RAM和四个NVIDIA Tesla V100 GPU。实现基于PyTorch，使用Adam优化器进行训练。

#### 9. 实验结果与分析：
本文提出的对抗掩蔽方法在合成到真实的点云分割适应中有效缓解了域差异。该方法在SemKITTI和nuScenes数据集上均优于基线和先前的方法。该方法与随机dropout相比对不同类别表现出不同的偏好，并且可以推导出更合理的噪声分布以缓解域差异。敏感性分析显示，该方法的性能随着λ从5×10−4到5×10−3的增加而先增加后略微降低。在λ的广泛选择范围内，该方法始终以较大的优势优于先前的解决方案，这进一步验证了该设计的有效性。


# Paper:17     通过Foley类比从视频生成音频的条件生成



#### 1. Title: 
Conditional Generation of Audio from Video via Foley Analogies

#### 2. Authors: 
Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, Andrew Owens

#### 3. Affiliation: 
第一作者：University of Michigan（密歇根大学）

#### 4. Keywords: 
Audio generation, video-to-audio synthesis, Foley, self-supervised learning, conditional generation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Du_Conditional_Generation_of_Audio_From_Video_via_Foley_Analogies_CVPR_2021_paper.html  Github: https://github.com/xypb/CondFoleyGen

#### 6. Summary : 
- (1):本文的研究背景是音频生成和视频到音频合成。作者提出了一种基于Foley类比的条件音频生成方法，旨在为无声视频生成音轨。

- (2):过去的方法主要是预测视频的共同声音，但艺术家们的目标是为视频创建一个与其真实声音不同但仍然与屏幕上的事件相匹配的音轨。此外，这些先前的系统很少给艺术家控制输出声音的能力。因此，本文提出了一种基于Foley类比的条件音频生成方法，旨在为无声视频生成音轨，并且艺术家可以控制输出声音。

- (3):本文提出了一种自监督预训练任务，用于学习条件Foley，并提出了一种模型来解决该问题。该模型使用Transformer和VQGAN来自回归地预测音频代码序列，同时在提供的音频-视觉示例的条件下进行条件生成。作者通过人类研究和自动评估指标表明，他们的模型成功地生成了视频的音轨，并且可以根据提供的示例内容进行输出的变化。

- (4):本文的方法在Greatest Hits数据集上进行了评估，该数据集包含需要理解材料属性和物理交互的视频，并通过高度多样化的CountixAV数据集进行了定性示例。通过感知研究和定量评估，作者表明，他们的模型生成的音轨传达了条件示例的物理属性，同时反映了屏幕上的动作的时间和运动。
#### 7. 方法详细介绍：
本文提出了一种条件Foley生成模型，通过自监督学习来解决这个问题。该方法包括一个预训练任务，用于训练模型从同一源视频中采样的条件音频-视觉剪辑中预测输入视频剪辑的声音。模型被训练来推断场景中的动作类型，并生成类似的声音以匹配输入示例。在测试时，使用Transformer来自回归地预测一个音频代码序列，用于VQGAN的频谱图，同时以提供的音频-视觉示例为条件。通过生成大量的音轨并使用音频-视觉同步模型来选择与视频时间对齐度最高的声音，从而在测试时提高了模型的性能。

#### 8. 实验设置：
作者在两个数据集上训练了他们的条件Foley生成模型：Greatest Hits和CountixAV。Greatest Hits数据集包含鼓槌与不同场景中的不同物体交互的视频，而CountixAV包含了来自YouTube视频的多达23个不同类别的重复动作视频。作者使用自动评估指标和人类感知研究来评估他们的方法。他们还考虑了几个其他模型进行比较，包括SpecVQGAN和他们模型的几个消融实验。他们使用声音分类器来识别动作和材料，并通过测量生成的声音是否包含正确数量的起始点以及它们的时间是否与输入视频中的时间匹配来评估生成的音色的质量。

#### 9. 实验结果和分析：
本文提出的方法在Greatest Hits数据集和公开视频上进行了评估，模型在CountixAV数据集上进行了训练。通过感知研究和自动化指标进行评估，结果表明该模型成功地学习了如何从条件音频中转移相关信息，同时匹配了无声输入视频中的事件。本文还讨论了所提出方法的潜在限制和广泛影响。在自动评估指标方面，作者测量了生成的声音与条件示例的材料属性、输入示例的动作以及生成声音中起始点的数量和时间匹配的速率。结果表明，所提出的方法在材料和动作匹配方面优于先前的方法，并且在起始点匹配方面相当。通过重新排序，所提出的方法在材料和动作匹配方面进一步提高了性能。


# Paper:18     基于N-Gram的Swin Transformer的高效轻量级图像超分辨率



#### 1. Title: 
N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution

#### 2. Authors: 
Haram Choi, Jeongmin Lee, Jihoon Yang

#### 3. Affiliation: 
第一作者：Sogang University（韩国梭罗大学） 

#### 4. Keywords: 
Image super-resolution, Swin Transformer, N-Gram, Efficient, Lightweight

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Choi_N-Gram_in_Swin_Transformers_for_Efficient_Lightweight_Image_Super-Resolution_CVPR_2022_paper.html  Github: https://github.com/rami0205/NGramSwin

#### 6. Summary : 
- (1):本文研究单幅图像超分辨率（SR）领域，提出了一种新的方法，旨在解决现有方法中存在的问题，如计算量大、感受野有限等。

- (2):现有的SR方法中，大多数都存在计算量大的问题，而且普通的窗口自注意力机制（WSA）感受野有限，无法利用邻域纹理或模式来恢复高分辨率图像。本文提出了一种新的方法，即在Swin Transformer中引入N-Gram上下文，将邻域局部窗口视为N-Gram，通过滑动WSA相互作用，扩展感受野，从而恢复受损像素。此外，为了减少计算量，本文还使用通道缩减组卷积来生成N-Gram上下文特征。

- (3):本文提出了一种高效的SR网络，称为NGswin，它由浅层模块、三个分层编码器阶段、SCDP瓶颈和小型解码器阶段组成。其中，NSTBs（N-Gram Swin Transformer Blocks）采用了我们提出的N-Gram上下文和Swin V2中提出的缩放余弦注意力。SCDP瓶颈采用了U-Net的变体，可以接收编码器的多尺度输出。本文还将N-Gram上下文应用于其他基于Swin Transformer的SR模型，如SwinIR-light和HNCT，提出了改进的SwinIR-NG模型，取得了最新的轻量级SR结果。

- (4):本文提出的NGswin和SwinIR-NG模型在多个数据集上进行了实验，结果表明，与现有的领先方法相比，它们具有更高的PSNR和更低的计算量，达到了高效和竞争性的性能。
#### 7. 方法详细介绍：
本文提出了一种新的超分辨率网络NGswin，它采用了N-Gram上下文来扩展Swin Transformer的感受野。NGswin采用了不对称的U-Net结构，包括浅层模块、三个分层编码器阶段、一个对称的Swin Transformer解码器和一个SCDP瓶颈层。其中，NSTB是N-Gram Swin Transformer Block，采用了Swin V2中提出的缩放余弦注意力和后归一化。NSTB中的N-Gram上下文算法通过四个步骤实现，包括uni-Gram嵌入、N-Gram窗口自注意力、拼接和1x1卷积。SCDP瓶颈层用于将多尺度特征的丰富表示传递给解码器，并保持NGswin的高效性。编码器和解码器是不对称的，解码器比编码器小得多。编码器采用分层结构，每个阶段的最后一个NSTB的输出与下一个阶段的输入进行级联。解码器包含Kdec个NSTB和一个最终的层归一化。解码器的输入与第一个编码器阶段的zK1enc1进行残差连接。重建模块包含一个卷积层，用于调整维度。

#### 8. 实验设置：
本文使用DIV2K数据集中的800个高分辨率-低分辨率图像对进行训练。低分辨率图像被随机裁剪成64x64大小的块，并进行随机水平翻转和旋转增强。使用Adam或AdamW优化器最小化ISR和IHR之间的L1像素损失。实验结果使用Set5、Set14、BSD100、Urban100和Manga109数据集进行评估，使用PSNR和SSIM作为指标。实验在单个NVIDIA GeForce RTX 3090 GPU上进行，批量大小为16，学习率初始化为1e-4，使用1e-8的权重衰减。

#### 9. 实验结果和分析：
本文将NGswin与其他高效超分辨率模型进行了比较，包括EDSR-baseline、MemNet、CARN、IMDN、LatticeNet、RFDN-L、SRPN-Lite、HNCT、ESRT、ELAN-light和DiVANet。评估在五个基准数据集上进行，使用PSNR和SSIM作为指标。结果表明，NGswin在所有基准测试中都优于以前的领先模型，并具有相对高效的结构。SwinIR-NG是在SwinIR-light的基础上使用N-Gram上下文进行改进，为所有基准测试建立了最先进的轻量级SR。N-Gram上下文增强了Swin Transformer-based SR模型，牺牲了一定的效率。N-Gram上下文通常会增加SSIM，并且该方法对Urban100和Manga109数据集具有鲁棒性。文本还包括每个模型的视觉比较。


# Paper:19     非对比学习与语言-图像预训练的结合



#### 1. Title: 
Non-Contrastive Learning Meets Language-Image Pre-Training

#### 2. Authors: 
Jinghao Zhou, Li Dong, Zhe Gan, Lijuan Wang, Furu Wei

#### 3. Affiliation: 
Microsoft (微软)

#### 4. Keywords: 
Language-image pre-training, contrastive learning, non-contrastive learning, representation learning, zero-shot transfer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Non-Contrastive_Learning_Meets_Language-Image_Pre-Training_CVPR_2022_paper.html  Github: https://github.com/shallowtoil/xclip

#### 6. Summary : 
- (1):本文研究了语言-图像预训练中的非对比学习方法，探讨了其有效性和可行性。
- (2):传统的对比学习方法在处理网络爬取的图像-文本数据时效率低下，因为图像和文本之间的相关性较弱。本文提出了一种基于非对比学习的方法，通过估计文本分布来指导视觉编码器的学习，从而提高了表示学习的效果。但是，在零样本识别方面表现不佳。因此，本文进一步提出了一种多任务框架xCLIP，将CLIP和nCLIP相结合，取得了更好的效果。
- (3):本文提出了一种基于非对比学习的语言-图像预训练方法nCLIP，通过估计文本分布来指导视觉编码器的学习。同时，本文还提出了一种多任务框架xCLIP，将CLIP和nCLIP相结合，取得了更好的效果。
- (4):本文在多个下游任务上进行了系统评估，包括零样本分类、领域外分类、检索、视觉表示学习和文本表示学习等，证明了xCLIP的有效性和提高数据效率的能力。在27个分类任务中，基于xCLIP的基础模型在零样本分类和线性探测准确率方面分别取得了3.3%和1.5%的性能提升。在零样本检索、半监督学习和微调方面，也取得了不错的效果。
#### 7. 方法详细介绍：
本文提出了一种非对比语言-图像预训练方法（nCLIP），该方法使用概率分布来估计文本分配给一个或多个对象簇的方式，并将这种估计作为伪标签来指导视觉编码器的学习。nCLIP目标的公式为视觉和文本分布之间的交叉熵。本文还介绍了一种多任务框架xCLIP，将CLIP和nCLIP目标结合起来，以在零样本转移和表示学习方面实现更好的性能。具体步骤包括：
1. 将图像和文本输入到模型中，提取视觉和文本特征。
2. 将视觉特征投影到文本空间中，将文本特征投影到视觉空间中。
3. 将投影特征转换为概率分布，并学习估计预测分布，通过最小化它们之间的交叉熵来实现。
4. 通过反向传播同时更新目标和预测分支，并加入熵正则化以避免崩溃解。
5. 将对比和非对比目标结合起来进行预训练。

#### 8. 实验设置：
本文在多个下游任务上对所提出的方法进行了系统评估，包括零样本分类、领域外分类、检索、视觉表示学习和文本表示学习。评估使用不同的预训练数据集、评估指标和优化配置进行。模型使用35百万个公开可用的图像-文本对进行预训练，使用AdamW优化器、峰值学习率为1e-3、余弦调度器、权重衰减为0.2、AdamW的β2为0.98、AdamW的ϵ为1e-6、3个热身时期和32个时期的自动混合精度进行训练。预处理和数据增强遵循CLIP，图像大小在（0.5，1.0）的范围内随机裁剪和调整大小。

#### 9. 实验结果和分析：
本文在多个任务上评估了所提出的非对比学习方法，包括零样本分类、领域外分类、零样本多标签分类和零样本图像到文本检索。结果表明，所提出的方法，特别是xCLIP模型，在大多数任务中都比基线CLIP模型表现更好。本文还与nCLIP模型进行了性能比较，结果表明，在零样本多标签分类方面，nCLIP表现更好。本文提供了详细的实验结果和与其他方法的比较。


# Paper:20     PlaneDepth: 基于正交平面的自监督深度估计



#### 1. Title: 
PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes

#### 2. Authors: 
Ruoyu Wang, Zehao Yu, Shenghua Gao

#### 3. Affiliation: 
上海科技大学

#### 4. Keywords: 
Monocular depth estimation, self-supervised learning, orthogonal planes, Laplacian Mixture Model, data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PlaneDepth_Self-Supervised_Depth_Estimation_via_Orthogonal_Planes_CVPR_2021_paper.html  Github: https://github.com/svip-lab/PlaneDepth

#### 6. Summary : 
- (1):本文研究的是单目深度估计的自监督学习方法，旨在解决数据获取成本高的问题。 
- (2):过去的方法通常使用回归模块来估计像素级深度图，但是这些方法通常会遇到局部最小值问题。多个近似于前向平面的深度表示方法被引入，但是这些方法无法表示地面，因为地面平面垂直于这些预定义的前向平面。本文提出了一种新的正交平面表示方法，包括垂直平面和地面平面，可以更好地表示场景中的深度信息。 
- (3):本文提出了PlaneDepth，一种基于正交平面的单目深度估计网络，利用Laplacian混合模型来估计深度分布。我们使用这些平面来合成参考视图，提供自监督信号。我们进一步发现，广泛使用的调整大小和裁剪数据增强会破坏正交性假设，导致平面预测不准确。我们通过显式构造调整大小和裁剪变换来解决这个问题，以矫正预定义的平面和预测的相机姿态。我们进一步提出了一种增强的自蒸馏损失，用双边遮挡掩码进行监督，以提高正交平面表示对遮挡的鲁棒性。 
- (4):在KITTI数据集上的实验表明，我们的方法在自监督单目深度估计任务上取得了良好的性能，可以提取地面平面，有助于自动驾驶。
#### 7. 方法详细介绍：
本文提出了一种自监督深度估计方法，称为PlaneDepth。该方法使用一组正交平面来表示场景深度分布，其中包括垂直平面和地面平面。深度分布由一组拉普拉斯分布的混合物建模。该方法使用保持正交性的数据增强方法，包括缩放和裁剪变换以及神经位置编码。为了提高PlaneDepth对遮挡的鲁棒性，本文提出了一种增强的自蒸馏损失，该损失使用双边遮挡掩码进行监督。优化使用感知损失、基于拉普拉斯分布的光度一致性和深度图平滑损失。

#### 8. 实验设置：
本文在KITTI数据集上进行实验，使用Eigen训练集进行立体训练，使用Zhou的训练集进行单目训练。模型使用KITTI数据集提出的指标在Eigen原始测试集和Eigen改进测试集上进行评估。网络在立体训练阶段进行了50个epoch的训练，批量大小为8，在高分辨率上进行了另一个epoch的微调，没有使用缩放和裁剪数据增强，在高分辨率上使用自蒸馏损失进行了另外10个epoch的训练，批量大小为4，仍然没有使用缩放和裁剪。

#### 9. 实验结果和分析：
PlaneDepth方法在KITTI数据集上表现优异，即使不使用后处理，也在改进的Eigen测试集上取得了最佳性能。该方法的每个组件的有效性通过消融研究得到验证。混合拉普拉斯损失优于基线方法中使用的简单L1损失，使用地面平面带来了额外的好处，即可以在没有监督的情况下提取地面平面并预测连续深度。使用增强的自蒸馏损失可以提高模型对遮挡的鲁棒性。


# Paper:21     基于法线引导的服装UV预测技术用于人体再纹理



#### 1. Title: 
Normal-guided Garment UV Prediction for Human Re-texturing

#### 2. Authors: 
Yasamin Jafarian, Tuanfeng Y. Wang, Duygu Ceylan, Jimei Yang, Nathan Carr, Yi Zhou, Hyun Soo Park

#### 3. Affiliation: 
第一作者：University of Minnesota（明尼苏达大学）

#### 4. Keywords: 
Garment UV prediction, texture mapping, 3D surface geometry, isometry, neural network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jafarian_Normal-Guided_Garment_UV_Prediction_for_Human_Re-Texturing_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是编辑人类视频时，需要考虑服装的几何变形对外观的影响，而现有的方法无法很好地处理服装的微小变形，因此需要一种新的方法来解决这个问题。

- (2):过去的方法有两种，一种是基于3D重建和渲染，但是需要高精度的3D几何信息，而且对于真实图像和视频的泛化能力较差；另一种是直接估计密集的UV映射，但是缺乏几何细节，只能捕捉到人体表面的几何信息，无法处理服装的变形。本文提出的方法综合了这两种方法的优点，通过预测图像的3D表面法线来约束UV映射，从而实现了几何感知的UV映射，避免了3D重建的过程。

- (3):本文提出了一种基于神经网络的方法，通过预测图像的3D表面法线和光流来估计几何感知的UV映射。UV映射是由一个多层感知器模型预测的，可以根据像素位置预测UV坐标。为了消除UV映射的参考坐标系的歧义，我们使用预定义的代理UV映射来约束神经网络。我们使用等距约束作为损失函数来优化UV映射，并使用光流来实现视频中的时间一致性。本文的创新点在于，通过预测图像的3D表面法线来约束UV映射，从而实现了几何感知的UV映射，避免了3D重建的过程。

- (4):本文的方法在真实数据和合成数据上都取得了比现有方法更好的性能。在人类UV映射预测任务上，本文的方法在真实数据上的性能优于现有方法，同时在合成数据上也取得了很好的性能。本文的方法可以用于编辑人类视频中的服装，可以实现更加真实的外观编辑。
#### 7. 方法详细介绍：
本文提出了一种基于图像和视频的几何感知服装UV预测方法。该方法利用从图像中预测的3D表面法线来设计保持与底层3D表面等距的UV映射。该方法的输入为图像或视频、其表面法线预测和密集光流（用于视频），输出为几何感知的UV映射估计。该UV映射由多层感知机建模，可以预测给定像素位置的UV坐标。为了消除UV映射的参考坐标系歧义，神经网络使用预定义的代理UV映射进行条件化。本文还提出了几种损失函数，包括几何损失、代理损失、Z损失和时间损失，以确保可见3D表面的物理合理性、消除UV映射的参考坐标系歧义和确保纹理映射的时间一致性。该方法在合成和真实数据集上进行了评估，使用UV误差、平均精度百分比和光度误差等指标进行了定量和定性分析。

#### 8. 实验设置：
本文使用了两个数据集进行实验评估，一个是合成数据集，另一个是真实数据集。合成数据集包括了多个服装模型，真实数据集包括了多个人体图像和视频。本文使用了PyTorch框架实现了所提出的方法，并在一台配备了NVIDIA GeForce RTX 2080 Ti GPU的计算机上进行了训练和测试。本文使用了Adam优化器进行训练，学习率为0.0001，批量大小为8，训练时长为100个epoch。

#### 9. 实验结果与分析：
本文使用UV误差、光度误差、几何误差和时间误差等指标对所提出的方法进行了评估，并与其他方法进行了比较。实验结果表明，所提出的方法在所有指标上均取得了最佳性能。本文还进行了消融实验，分析了距离和角度约束的影响以及代理损失在消除UV映射歧义方面的作用。本文还进行了用户研究，结果表明所提出的方法在图像重纹理和UV映射网格可视化方面具有较好的效果。


# Paper:22     通过蒸馏从噪声标签数据中学习



#### 1. Title: 
Learning to Learn from Noisy Labeled Data with Distillation

#### 2. Authors: 
Yisen Wang, Xinggang Wang, Wenyu Liu, Xinsheng Zhang, Chunhua Shen

#### 3. Affiliation: 
南京大学

#### 4. Keywords: 
Noisy Labels, Learning to Learn, Distillation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Wang_Learning_to_Learn_From_Noisy_Labeled_Data_With_Distillation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是在存在噪声标签的情况下，如何从中学习到有效的知识。
 
- (2):过去的方法主要是通过对噪声标签进行过滤或纠正来解决问题，但这些方法往往需要大量的人工干预，且效果不佳。本文提出的方法是通过蒸馏技术，将一个学习良好的模型的知识传递给一个新的模型，从而使新模型能够更好地学习到有效的知识。这种方法不需要对噪声标签进行过滤或纠正，且能够在不同的数据集上取得良好的效果。
 
- (3):本文提出了一种基于蒸馏技术的学习方法，该方法可以从一个学习良好的模型中提取出有效的知识，并将其传递给一个新的模型。具体来说，该方法通过最小化新模型与学习良好的模型之间的距离来实现知识传递。此外，本文还提出了一种新的损失函数，用于在存在噪声标签的情况下训练新模型。实验结果表明，该方法能够在不同的数据集上取得良好的效果，且不需要对噪声标签进行过滤或纠正。
  
- (4):本文的方法在多个数据集上进行了实验，结果表明，该方法能够在存在噪声标签的情况下，取得比其他方法更好的效果。这表明本文提出的方法能够有效地从噪声标签中学习到有效的知识，且能够在实际应用中发挥重要作用。
#### 7. 方法详细介绍：
本文提出了一种名为“基于蒸馏的嘈杂标签学习”（DLNL）的方法，用于从嘈杂标记的数据中学习。该方法包括两个阶段：
1. 教师模型训练：在嘈杂标记的数据上训练教师模型，生成每个样本的软标签。软标签是类别的概率分布，比硬标签更具信息量。
2. 学生模型训练：使用教师模型生成的软标签，在嘈杂标记的数据上训练学生模型。学生模型通过最小化其预测与软标签之间的交叉熵损失来进行优化。

DLNL还包括一个正则化项，以鼓励学生模型从嘈杂标签中学习，而不是记忆它们。正则化项基于学生模型的预测与教师模型的预测之间的差异。

总体而言，DLNL是一个两阶段的过程，利用教师模型的知识来改善学生模型从嘈杂标记的数据中的学习。

#### 8. 实验设置：
当前文本中没有提供关于实验设置的具体信息。

#### 9. 实验结果与分析：
本文在多个数据集上进行了实验，结果表明，DLNL方法在嘈杂标记的数据上的表现优于其他方法。此外，实验还表明，DLNL方法可以在不同的噪声类型和噪声水平下进行有效的学习。分析表明，DLNL方法可以通过利用教师模型的知识来减轻嘈杂标签的影响，从而提高学习性能。


# Paper:23     动态场景下的鲁棒测试时间适应



#### 1. Title: 
Robust Test-Time Adaptation in Dynamic Scenarios

#### 2. Authors: 
Longhui Yuan, Binhui Xie, Shuang Li

#### 3. Affiliation: 
北京理工大学计算机学院

#### 4. Keywords: 
Test-time adaptation, practical test-time adaptation, dynamic scenarios, robust batch normalization, memory bank, time-aware reweighting

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yuan_Robust_Test-Time_Adaptation_in_Dynamic_Scenarios_CVPR_2021_paper.html  Github: https://github.com/BIT-DA/RoTTA

#### 6. Summary : 
- (1):本文研究的是动态场景下的测试时间适应问题，即测试数据流的标签分布和采样方式都可能随时间变化。这种情况下，传统的测试时间适应方法可能会失效，因此需要一种更加鲁棒的方法来应对这种情况。

- (2):过去的测试时间适应方法大多针对简单的测试数据流，如单一或多个分布的独立采样数据。然而，在现实世界的应用中，如自动驾驶，环境逐渐变化，测试数据随时间相关采样，这些尝试可能会失败。本文提出了一种实用的测试时间适应方法（PTTA），并针对PTTA中的复杂数据流提出了一种鲁棒的测试时间适应方法（RoTTA）。RoTTA包括三个部分：鲁棒的统计估计、考虑时效性和不确定性的类别平衡采样和考虑时效性的鲁棒训练。相比之前的方法，RoTTA更加全面地考虑了PTTA的挑战。

- (3):本文提出的RoTTA方法包括三个部分：鲁棒的统计估计、考虑时效性和不确定性的类别平衡采样和考虑时效性的鲁棒训练。其中，鲁棒的统计估计使用指数移动平均法来估计BatchNorm层的统计量，类别平衡采样考虑了时效性和不确定性，鲁棒训练使用了一种考虑时效性的重加权策略和teacher-student模型。

- (4):本文在CIFAR-10-C、CIFAR-100-C和DomainNet数据集上进行了实验，RoTTA方法取得了最佳结果，相比最佳基线平均分类误差分别降低了5.9%、5.5%和2.2%。这表明RoTTA方法在PTTA设置下具有很好的鲁棒性和实用性。
#### 7. 方法详细介绍：
本文提出了一种名为RoTTA（Robust Test-Time Adaptation）的方法，利用鲁棒批归一化（RBN）和相关采样测试更新（CSTU）来适应动态测试分布。RoTTA还采用了一种时效性重新加权策略，以减少来自旧的和不可靠实例的错误梯度信息的风险，并稳定适应。该方法涉及使用指数移动平均值更新学生模型和教师模型。从内存库中的实例的损失值使用时效性重新加权和学生模型的软最大预测与教师模型的交叉熵来计算。

#### 8. 实验设置：
本文在CIFAR10-C和CIFAR100-C数据集上进行了实验，比较了RoTTA方法与其他方法的性能。实验中使用了不同的数据增强方法和不同的噪声类型和强度。实验中使用的模型是ResNet-18和ResNet-50。在实验中，使用了不同的评估指标，如平均分类误差和AUC等。

#### 9. 实验结果和分析：
本文提出的RoTTA方法在CIFAR10-C和CIFAR100-C数据集上取得了最佳性能，平均分类误差为30.1。与其他方法相比，RoTTA在长期场景中具有更好的适应性，其中分布不断变化，测试流以独立或相关方式采样。消融实验验证了RoTTA每个组件的有效性。在DomainNet数据集上的实验结果表明，RoTTA在不同的噪声类型和强度下具有竞争性的性能。


# Paper:24     基于样例的扩散模型图像编辑



#### 1. Title: 
Paint by Example: Exemplar-based Image Editing with Diffusion Models

#### 2. Authors: 
Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
image editing, exemplar-based, diffusion models, self-supervised training, content bottleneck

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2022_paper.html  Github: https://github.com/Fantasy-Studio/Paint-by-Example

#### 6. Summary : 
- (1):本文研究了基于样例的图像编辑，旨在实现更精确的控制。 
- (2):过去的方法主要是基于文本的图像编辑，但是文本描述存在歧义，难以精确控制。本文提出了一种基于样例的图像编辑方法，通过自监督训练来解决样例和源图像之间的差异问题。 
- (3):本文提出了一种基于扩散模型的图像编辑方法，通过引入内容瓶颈和强数据增强等技术来避免直接复制和粘贴样例图像的问题。同时，为了保证编辑过程的可控性，本文设计了任意形状的掩模，并利用无分类器的指导来增加与样例图像的相似性。 
- (4):本文的方法在野外图像上实现了令人印象深刻的性能，并实现了可控的编辑。
#### 7. 方法详细介绍：
本文提出了一种基于示例的图像编辑方法，称为“Paint by Example”，它可以根据示例图像对图像内容进行语义上的修改。该方法利用扩散模型进行自监督训练，并使用图像先验、强数据增强、内容瓶颈和无分类器指导等技术来解决边界伪影问题。该算法允许精确控制编辑，并在野外图像上取得了令人印象深刻的性能。具体步骤如下：
1. 使用示例图像和源图像训练扩散模型。
2. 利用强数据增强和内容瓶颈来避免直接复制粘贴示例图像的平凡解。
3. 使用任意形状的掩膜和无分类器指导来增加与示例图像的相似度。
4. 通过扩散模型的单次前向传递实现整个框架。

#### 8. 实验设置：
作者使用了文本到图像生成模型 Stable Diffusion 作为初始化，提供了强大的图像先验。他们在 OpenImages 数据集上训练了 40 个 epoch，该数据集包含 600 个对象类别的 1600 万个边界框和 190 万张图像。在训练期间，图像分辨率预处理为 512×512，训练过程在 64 个 NVIDIA V100 GPU 上花费了约 7 天时间。作者还创建了一个名为 COCO Exemplar-based image Editing benchmark 的测试基准，其中包含了从 MSCOCO 验证集手动选择的 3,500 个源图像，每个图像仅包含一个边界框和从 MSCOCO 训练集中检索的参考图像块。他们使用了三个指标来评估生成的图像：FID 分数、Quality Score（QS）和 CLIP 分数。

#### 9. 实验结果和分析：
本文提出的“Paint by Example”方法在 COCOEE 数据集上进行了测试，结果表明该方法在图像编辑方面表现出色。与其他方法相比，该方法可以更好地保留源图像的细节，并且可以在不同的编辑任务中实现更好的性能。此外，该方法还可以在不同的数据集上进行迁移学习，表现出良好的泛化能力。


# Paper:25     基于双层记忆设计和知识投影的任务增量学习框架



#### 1. Title: 
Decoupling Learning and Remembering: a Bilevel Memory Framework with Knowledge Projection for Task-Incremental Learning

#### 2. Authors: 
Wenju Sun, Qingyong Li, Jing Zhang, Wen Wang, Yangli-ao Geng

#### 3. Affiliation: 
北京交通大学交通数据分析与挖掘北京市重点实验室

#### 4. Keywords: 
Incremental learning, memory-based models, bilevel-memory design, knowledge projection, plasticity-stability dilemma

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Decoupling_Learning_and_Remembering_A_Bilevel_Memory_Framework_With_Knowledge_CVPR_2021_paper.html  Github: https://github.com/SunWenJu123/BMKP

#### 6. Summary : 
- (1):本文研究了增量学习中的可塑性和稳定性之间的困境，提出了一种基于双层记忆设计的增量学习框架，以解决这一问题。

- (2):过去的增量学习方法包括基于正则化、扩展和记忆的方法，但它们都存在可塑性和稳定性之间的权衡问题。本文提出的方法通过双层记忆设计，将学习和记忆的功能分离，以实现高可塑性和高稳定性。此外，本文还提出了一种知识投影过程，将松散组织的模型知识投影到紧凑的表示中，以提高内存利用效率。

- (3):本文提出的方法是一种基于双层记忆设计的增量学习框架，包括一个工作内存和一个长期内存。工作内存负责自适应模型学习，以确保可塑性；长期内存负责持久存储学习模型中融合的知识，以保证稳定性。为了将工作内存中的学习知识提取出来并融合到长期内存中，本文提出了一种知识投影过程，以自适应地维护所有增量学习任务共享的一组模式基础，将工作内存中的松散组织模型知识投影到紧凑的表示中，以便在长期内存中记忆。本文的创新点在于提出了一种基于双层记忆设计的增量学习框架，并提出了一种知识投影过程，以提高内存利用效率和实现前向知识传递。

- (4):本文在CIFAR-10、CIFAR-100和Tiny-ImageNet上进行了评估，实验结果表明，本文提出的方法在内存使用率更低的情况下实现了最先进的性能。本文的方法可以有效地解决增量学习中的可塑性和稳定性之间的困境。
#### 7. 方法详细介绍：
本文提出了一种基于双层记忆的增量学习框架，包括工作内存和长期内存两个部分。工作内存是一个L层神经网络，用于适应性地学习新知识和推理。长期内存负责稳定地存储所有已学习的知识。本文提出了一种知识投影过程，将参数知识空间（PKS）中的知识投影到核心知识空间（CKS）中的紧凑表示中，从而提高了内存利用效率，并实现了增量学习的前向知识传递。本文还设计了一种表示压缩正则化器，以鼓励工作内存重复使用先前学习的知识，从而增强了BMKP的内存效率和性能。

具体步骤如下：
1. 工作内存和长期内存初始化为相同的模型参数。
2. 对于每个任务，使用工作内存进行训练，同时使用长期内存进行知识传输。
3. 在每个任务结束时，使用知识投影过程将PKS中的知识投影到CKS中的紧凑表示中。
4. 使用表示压缩正则化器鼓励工作内存重复使用先前学习的知识。
5. 在下一个任务开始时，使用长期内存重新加载所有已学习的知识，并使用工作内存进行训练。

#### 8. 实验设置：
本文在三个图像分类数据集上评估了BMKP的性能，包括5-split CIFAR-10、10-split CIFAR-100和10-split Tiny-ImageNet。使用单个NVIDIA Tesla V100 GPU进行实验，批量大小设置为128，学习率初始化为0.1，并在总训练时期的50％和75％处以10的倍数衰减。CIFAR-10和CIFAR-100的总训练时期设置为200，Tiny-ImageNet设置为300。

#### 9. 实验结果和分析：
实验结果表明，BMKP在内存使用率较低的情况下实现了最先进的性能，可以有效地防止遗忘并实现增量学习。所提出的知识投影过程可以将PKS中松散组织的模型知识投影到CKS中的紧凑表示中，从而提高内存利用效率并实现前向知识传递。表示压缩正则化器鼓励工作内存重复使用先前学习的知识，从而增强了BMKP的内存效率和性能。实验还进行了消融研究，证明了基础更新和重新训练对BMKP性能的改进作用。


# Paper:26     DrapeNet：服装生成和自监督穿搭



#### 1. Title: 
DrapeNet: Garment Generation and Self-Supervised Draping

#### 2. Authors: 
Luca De Luigi, Ren Li, Benoît Guillard, Mathieu Salzmann, Pascal Fua

#### 3. Affiliation: 
第一作者：Luca De Luigi，隶属于瑞士洛桑联邦理工学院计算机视觉实验室（CVLab）

#### 4. Keywords: 
Garment draping, self-supervised learning, generative network, 3D models

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/De_Luigi_DrapeNet_Garment_Generation_and_Self-Supervised_Draping_CVPR_2021_paper.html
Github: https://github.com/liren2515/DrapeNet

#### 6. Summary: 
- (1):本文研究的背景是数字服装的快速穿搭，旨在实现高质量的穿搭效果，同时避免大量的训练数据和高计算成本。

- (2):过去的方法包括基于物理模拟的算法和基于数据驱动的方法，但前者计算成本高，后者需要大量的训练数据。本文提出了一种基于自监督学习的单一穿搭网络，通过预测与服装相关的3D变形场，实现对多种服装的穿搭。同时，通过搭建服装生成网络，将服装表示为无符号距离场，从而实现对服装形状的编辑和采样。这种方法不仅计算速度快，而且可以通过梯度下降从部分观测数据（如图像或3D扫描）中恢复出服装和人体的3D模型。

- (3):本文提出的DrapeNet方法通过将服装生成网络与穿搭网络相结合，实现了对多种服装的穿搭。通过将服装表示为无符号距离场，实现了对服装形状的编辑和采样。同时，通过自监督学习，避免了大量的训练数据。该方法可以从部分观测数据中恢复出服装和人体的3D模型。

- (4):本文的方法在多种服装的穿搭任务中取得了良好的性能，同时可以从部分观测数据中恢复出服装和人体的3D模型，支持其目标的实现。
#### 7. 方法详细介绍：
DrapeNet框架包括两个网络：生成网络和披肤网络。生成网络通过点云编码器将采样自未穿戴衣物表面的点嵌入到紧凑的向量中，然后解码为无符号距离函数（UDF），以预测衣物的表面。披肤网络使用SMPL模型对人体进行参数化，并将SMPL蒙皮过程扩展到身体周围的3D体积，以实现衣物的披肤。该网络通过最小化包括膜应变能、弯曲能、重力势能和防止身体和衣物之间碰撞的惩罚项的损失函数进行训练。为了防止上下衣物之间的相交，引入了交叉求解器组件。该方法是可微分的，适用于解决逆问题和从图像数据建模穿着衣服的人。

#### 8. 实验设置：
实验使用CLOTH3D数据集，包含超过7K个穿着不同衣物的3D人物动画序列。生成网络和披肤网络分别在数据集中随机选择600个上衣和300个下装进行训练。网络仅使用T-pose下的平均身体形状的衣物网格进行训练。测试时，从数据集中随机选择30个上衣和30个下装，并将它们视为整个模拟序列。评估指标包括欧几里得距离、身体和衣物之间的相交比率以及上下衣物之间的相交。

#### 9. 实验结果与分析：
DrapeNet在之前未见过的衣物上，相较于两种完全监督学习方法DeePSD和DIG，提供了最低的身体-衣物相互穿透比率和最少的上下衣物相交。然而，DrapeNet产生了更高的欧几里得距离值，这不一定对应于逼真的披肤。人类评估研究表明，DrapeNet可以比竞争方法更好地披肤衣物，超过50%的用户选择DrapeNet作为最逼真的方法。


# Paper:27     利用LiDAR强度的无监督内在图像分解



#### 1. Title: 
Unsupervised Intrinsic Image Decomposition with LiDAR Intensity

#### 2. Authors: 
Shogo Sato, Yasuhiro Yao, Taiga Yoshida, Takuhiro Kaneko, Shingo Ando, Jun Shimamura

#### 3. Affiliation: 
Shogo Sato, Yasuhiro Yao, Taiga Yoshida, Shingo Ando, Jun Shimamura: NTT Human Informatics Laboratories, Japan

#### 4. Keywords: 
Intrinsic image decomposition, LiDAR intensity, unsupervised learning, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sato_Unsupervised_Intrinsic_Image_Decomposition_With_LiDAR_Intensity_CVPR_2021_paper.html  Github: https://github.com/ntthilab-cv/NTT-intrinsic-image-decomposition

#### 6. Summary : 
- (1):本文研究的背景是内在图像分解（IID）任务，即将自然图像分解为反照率和阴影两个部分，IID是高级计算机视觉任务的基础，如重新照明和场景理解。
- (2):过去的方法包括基于优化的方法、基于监督学习的方法和基于无监督学习的方法。然而，监督学习方法需要大量标注数据，而无监督学习方法的性能不如监督学习方法。本文提出了一种利用LiDAR强度的无监督IID方法，通过设计强度一致性损失和LiDAR强度稠密化模块来解决IID中的不适定问题。
- (3):本文提出了一种利用LiDAR强度的无监督IID方法（IID-LI），通过设计强度一致性损失和LiDAR强度稠密化模块来解决IID中的不适定问题。LiDAR强度的利用是本文的创新点，它可以独立于阳光条件、投影阴影和阴影。本文还提出了一种LiDAR强度稠密化模块，该模块基于深度图像先验（DIP），可以同时卷积稀疏数据（LiDAR强度）和不同模态的密集数据（RGB图像）。实验结果表明，本文提出的方法在IID任务上取得了较好的性能。
- (4):本文的方法在自己的数据集上进行了验证，该数据集包括RGB图像、LiDAR强度和人工标注的IID注释。实验结果表明，本文提出的方法在IID任务上取得了较好的性能，优于传统的无监督学习方法。
#### 7. 方法详细介绍：
本文提出了一种基于LiDAR强度的无监督本征图分解方法(IID-LI)。该方法包括两个主要模块：LiDAR强度稠密化(LID)模块和强度一致性损失模块。LID模块基于深度图像先验(DIP)，旨在处理LiDAR强度的稀疏性和遮挡性。强度一致性损失模块计算LiDAR强度和灰度化反照率之间的误差，为分解单个图像的不适定问题提供了一个准则。该方法通过对抗损失、内容损失、KL散度损失、图像重构损失、先验代码重构损失、物理重构损失、平滑损失和强度一致性损失进行优化，实现了本征图分解。

#### 8. 实验设置：
本文使用了一个包含RGB图像、LiDAR强度和本征图注释的公开数据集来评估所提出的方法。该数据集包含具有不同照明条件和物体几何形状的室外场景。所提出的方法使用PyTorch实现，并在NVIDIA Tesla V100 GPU上进行训练。

#### 9. 实验结果与分析：
所提出的方法在平均绝对误差(MAE)和结构相似性指数(SSIM)方面优于现有的无监督学习方法，表现出良好的本征图分解效果。LID模块有效地处理了LiDAR强度的稀疏性和遮挡性。强度一致性损失模块为分解单个图像的不适定问题提供了一个准则。本文使用减少的LiDAR数据对估计精度进行了评估，并列出了随机采样注释的数值结果。LiDAR强度的密度从原始强度降至50％、10％和1％，估计精度随着LiDAR强度密度的增加而提高。所提出的LID模块提高了估计精度。强度一致性损失模块计算LiDAR强度和灰度化反照率之间的误差，对于大多数以灰度材料为主的室外场景是有效的。在未来的研究中，作者将进一步提高LiDAR稀疏性的鲁棒性和图像与LiDAR强度的配准精度。


# Paper:28     Bootstrap Your Own Prior: 面向分布无关的新类发现



#### 1. Title: 
Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery

#### 2. Authors: 
Muli Yang, Liancheng Wang, Cheng Deng, and Hanwang Zhang

#### 3. Affiliation: 
Cheng Deng is affiliated with School of Electronic Engineering, Xidian University, Xi’an, China

#### 4. Keywords: 
Novel Class Discovery, distribution-agnostic, class distribution, clustering, transfer learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Yang_Bootstrap_Your_Own_Prior_Towards_Distribution-Agnostic_Novel_Class_Discovery_CVPR_2021_paper.html  Github: https://github.com/muliyangm/BYOP

#### 6. Summary : 
- (1): This paper focuses on the Novel Class Discovery (NCD) task, which aims to discover unknown classes without any annotation by exploiting the transferable knowledge already learned from a base set of known classes. Existing works hold an impractical assumption that the novel class distribution prior is uniform, yet neglect the imbalanced nature of real-world data. 
- (2): The past methods assume a uniform novel class distribution or require the ground-truth class prior, which is impractical. This paper proposes a new method, dubbed “Bootstrapping Your Own Prior (BYOP)”, which iteratively estimates the class prior based on the model prediction itself. BYOP tackles the challenge of distribution-agnostic NCD, which allows data drawn from arbitrary unknown class distributions and thus renders existing methods useless or even harmful. 
- (3): The proposed BYOP method deploys a clustering method that partitions the data subject to the current class prior. At each iteration, the current class prior estimation is not yet accurate, and thus may result in ambiguous clusters for the minority classes if the true class distribution is highly-imbalanced. The cluster assignments are used as pseudo-labels to train a classifier to discover novel classes. BYOP proposes a dynamic temperature technique that can be integrated into the classifier to output more confident distribution predictions. To estimate the class prior, BYOP gathers the predicted novel class distributions as the class assignments for the training samples and calculates the proportion of each class assignment, so that a new class prior can be derived that is beneficial for the next training iteration. 
- (4): The proposed BYOP method outperforms the current state-of-the-art methods in the challenging distribution-agnostic NCD task on several standard datasets. The performance of BYOP supports their goals of addressing the impractical assumption of uniform class distribution and tackling the challenge of distribution-agnostic NCD.
#### 7. 方法详细介绍：
本文提出了一种名为“Bootstrap Your Own Prior (BYOP)”的方法，用于解决分布无关的新类别发现（NCD）任务。BYOP通过迭代地基于模型预测本身估计类别分布，从而生成更准确的伪标签，以帮助下一次训练迭代。BYOP流程包括使用来自上一次迭代的类别先验对未标记数据进行聚类以生成伪标签，使用生成的伪标签训练分类器以预测新类别分布，并通过计算每个类别分配的比例来估计类别先验，以便下一次迭代使用。还提出了一种动态温度技术，以鼓励更自信的预测并消除不自信的数据。模型使用ResNet-18作为图像编码器，使用多头聚类、数据增强、优化器和学习率调度器在基础集和新颖集上进行预训练和训练。

#### 8. 实验设置：
实验在三个标准NCD基准数据集上进行，分别是CIFAR10、CIFAR100和Tiny-ImageNet。数据集被分为两个子集，即包含已知基础类别标记数据的基础集和包含未标记新类别数据的新颖集。假定新类别的数量是先验已知的。实验评估模型在训练集中识别新类别和在测试集中识别基础和新类别的性能，使用任务感知和任务无关协议。实验在不同的不平衡比率下进行，并对每个数据集进行五次运行的平均结果。

#### 9. 实验结果和分析：
本文提出的BYOP方法在四个NCD基准测试中进行了评估，表明其在各种不平衡比率下均优于当前最先进的方法。BYOP方法特别在传统的NCD协议中表现出色，其中准确的类别先验对于发现训练数据中的新类别直接有益。动态温度技术也提高了任何使用的先验的整体性能。


# Paper:29     基于隐式身份的深度伪造人脸交换检测



#### 1. Title: 
Implicit Identity Driven Deepfake Face Swapping Detection

#### 2. Authors: 
Baojin Huang, Zhongyuan Wang, Jifan Yang, Jiaxin Ai, Qin Zou, Qian Wang, Dengpan Ye

#### 3. Affiliation: 
武汉大学计算机学院

#### 4. Keywords: 
Deepfake, Face swapping, Implicit identity, Forgery detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Implicit_Identity_Driven_Deepfake_Face_Swapping_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了从面部身份的角度检测面部交换。面部交换旨在用源面替换目标面，并生成人类无法区分真假的假面。作者认为，假面包含显式身份和隐式身份，分别对应于面部交换期间源面和目标面的身份。本文提出了一种新颖的隐式身份驱动的面部交换检测框架，旨在探索面部的隐式身份，以指导深度网络做出更合理的检测结果。

- (2):早期的研究通常将面部交换检测视为二进制图像分类任务。最近的研究致力于探索特定的伪造模式，如噪声分析、局部区域和频率信息。然而，这些方法仍然围绕某些操作方法，不利于在实际中推广。本文从面部身份的角度考虑面部交换检测，提出了一种新颖的隐式身份驱动的框架，探索了假面的隐式身份，以指导深度网络做出更合理的检测结果。

- (3):本文提出了显式身份对比（EIC）损失和隐式身份探索（IIE）损失，监督CNN骨干将面部图像嵌入隐式身份空间。在EIC的指导下，真实样本被拉近其显式身份，而假样本被推离其显式身份。此外，IIE从基于边界的分类损失函数中得出，鼓励具有已知目标身份的假面享有类内紧密性和类间多样性。本文的方法在几个数据集上进行了广泛的实验和可视化，证明了其对抗最先进的对手的泛化能力。

- (4):本文提出的方法在多个数据集上进行了实验，取得了优于最先进方法的结果。本文的方法从面部身份的角度考虑面部交换检测，探索了假面的隐式身份，以指导深度网络做出更合理的检测结果。本文的方法在面部交换检测任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种新的隐式身份驱动的深度伪造人脸交换检测方法。该方法包括两个主要方案：显式身份对比（EIC）和隐式身份探索（IIE）。EIC损失将真实样本拉近其显式身份，同时将伪造样本推离其显式身份。IIE损失约束伪造样本的身份归属于其对应的目标人脸（隐式身份）。整体损失函数包括EIC和IIE损失，以及二元交叉熵损失。该方法使用全连接分类器进行分类，并充分利用标签信息。

#### 8. 实验设置：
本文使用Pytorch深度学习框架在两个NVIDIA GTX 3090 GPU上实现了提出的隐式身份驱动的深度伪造人脸交换检测方法。训练集使用翻转数据增强技术以提高模型的鲁棒性。初始学习率设置为0.1，并在第8和第14个epoch时除以10。整个深度网络由SGD优化，动量为0.9，权重衰减为5e-4。在训练期间，使用在WebFace数据集上训练的CosFace提取显式身份特征。隐式身份嵌入网络基于ResNet18。公式4中的温度参数τ设置为0.1。此外，公式6中的λ1和λ2分别经验性地设置为0.05和0.1。使用的评估指标为接收器操作特征曲线下面积（AUC）、等误差率（EER）和准确率（ACC）。

#### 9. 实验结果与分析：
本文提出的隐式身份驱动的深度伪造人脸交换检测方法在多个数据集上均取得了优于现有方法的表现。在FF++数据集上，该方法的AUC为82.04%，在DeepFakes数据集上为78.94%。在跨数据集评估中，该方法在DeepFakes数据集上的AUC为78.94%，在FaceSwap数据集上为80.43%，在FaceShifter数据集上为74.81%。该方法还在多源操作评估中取得了最高的准确率和AUC。实验结果表明，该方法具有优越性和泛化能力。

#### 论文总结：
本文提出了一种新的隐式身份驱动的深度伪造人脸交换检测方法，该方法在多个数据集上均取得了优于现有方法的表现。该方法包括显式身份对比（EIC）和隐式身份探索（IIE）两个主要方案，使用全连接分类器进行分类，并充分利用标签信息。实验结果表明，该方法具有优越性和泛化能力。


# Paper:30     RIFormer：保持视觉骨干网络有效性但删除令牌混合器



#### 1. Title: 
RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer

#### 2. Authors: 
Jiahao Wang, Songyang Zhang, Yong Liu, Taiqiang Wu, Yujiu Yang, Xihui Liu, Kai Chen, Ping Luo, Dahua Lin

#### 3. Affiliation: 
第一作者：清华大学深圳国际研究生院

#### 4. Keywords: 
Computer Vision, Vision Transformer, Token Mixer, Structural Re-parameterization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_RIFormer_Keep_Your_Vision_Backbone_Effective_But_Removing_Token_Mixer_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究如何在保持视觉骨干网络有效性的同时，删除其基本构建块中的令牌混合器。
- (2):以前的方法主要集中在改进架构，但是这些方法仍然保留了令牌混合器，这会导致推理时的显著延迟。本文提出了一种新的学习策略，即知识蒸馏，以完全删除令牌混合器。本文的方法在ImageNet-1K数据集上取得了有竞争力的性能，并且具有更快的推理速度。
- (3):本文提出了一种基于重新参数化思想的RepIdentityFormer模型，用于学习没有令牌混合器的视觉骨干网络。具体来说，我们在学生模型中引入简单的仿射变换，以替换训练中的令牌混合器。通过知识蒸馏，我们可以将教师模型的知识转移到学生模型中，从而提高学生模型的建模能力。我们还总结了一些实用的学习策略，以帮助学习极其简单的模型。
- (4):本文的方法在ImageNet-1K数据集上取得了有竞争力的性能，并且具有更快的推理速度。在各种视觉任务上，我们的模型都取得了很好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为RepIdentityFormer（RIFormer）的新方法，用于在保持有效性的同时从视觉骨干网络中删除令牌混合器。该方法通过使用仿射变换重新参数化模型架构来替换训练期间的令牌混合器。仿射变换的参数在推理期间合并到LayerNorm中，使模型无令牌混合器。该方法还利用知识蒸馏将知识从具有令牌混合器的教师模型转移到没有令牌混合器的学生模型。本文总结了经验实践，提出了5个有效学习简化模型架构的指南。具体步骤包括：
1. 使用仿射变换替换令牌混合器。
2. 将仿射变换的参数合并到LayerNorm中。
3. 使用知识蒸馏将知识从教师模型转移到学生模型。
4. 使用模块模仿技术进行训练。
5. 从教师模型中加载部分参数。

#### 8. 实验设置：
本文使用RIFormer模型在ImageNet-1K数据集上进行实验。训练方案遵循第4节中的指南，数据增强包括MixUp、CutMix、CutOut和RandAugment。训练时长为600个epoch，预训练模型使用384x384的输入分辨率进行微调，微调时长为30个epoch。RIFormer模型的参数为12M，MAC为1.8G。

#### 9. 实验结果和分析：
RIFormer模型在ImageNet-1K数据集上取得了良好的结果，top-1准确率为82.6%，同时每秒处理超过1185张2242分辨率的图像。与其他高效骨干网络（如GFNet）相比，RIFormer模型具有可比的准确率和吞吐量。本文还进行了消融实验，分析了模块模仿和不同加速策略的有效性。实验结果表明，RIFormer模型在不同深度设置和教师情况下均具有积极效果。


# Paper:31     协作助力相机在3D检测中超越LiDAR



#### 1. Title: 
Collaboration Helps Camera Overtake LiDAR in 3D Detection

#### 2. Authors: 
Yue Hu, Yifan Lu, Runsheng Xu, Weidi Xie, Siheng Chen, Yanfeng Wang

#### 3. Affiliation: 
上海交通大学合作式中介网络创新中心

#### 4. Keywords: 
3D detection, camera-only, LiDAR, multi-agent collaboration

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Collaboration_Helps_Camera_Overtake_LiDAR_in_3D_Detection_CVPR_2021_paper.html  Github: https://github.com/ChenSiheng/CoCa3D

#### 6. Summary : 
- (1):本文研究的是3D物体检测，提出了一种基于多智能体协作的相机3D检测方法，旨在通过多智能体之间的信息共享来提高相机3D检测的性能，从而在某些实际场景中超越LiDAR。

- (2):相机3D检测相对于基于LiDAR的检测系统来说，具有更简单的配置和更经济的解决方案，但由于输入中缺乏直接的3D测量，因此精确的深度估计是一个主要的挑战。许多先前的方法尝试通过网络设计来改善深度估计，例如可变形层和更大的感受野。本文提出了一种新的方法，通过引入多智能体协作来改善相机3D检测，使智能体之间可以通过通信共享互补信息。同时，通过选择最具信息量的线索来优化通信效率。多视角的共享信息消除了单智能体估计的深度歧义，并补充了单智能体视野中的遮挡和远距离区域。 

- (3):本文提出了一种协作相机3D检测框架CoCa3D，包括三个部分：i）单智能体相机3D检测，实现每个智能体的基本深度估计和3D检测；ii）协作深度估计，通过促进多个智能体视角之间的空间一致性来消除估计深度的歧义；iii）协作检测特征学习，通过共享关键检测信息来补充检测特征。与处理LiDAR的先前协作感知方法不同，CoCa3D专门设计了新的协作深度估计来定制相机3D检测任务。

- (4):在一个真实世界的数据集和两个新的模拟数据集上评估了CoCa3D。结果表明，CoCa3D在DAIR-V2X上的性能提高了44.21％，在OPV2V+上提高了30.60％，在CoPerception-UAVs+上提高了12.59％，达到了先前的SOTA性能。本文的贡献是：i）提出了一种新的协作相机3D检测框架CoCa3D，通过多智能体协作来提高相机的检测能力，促进更全面的3D检测；ii）提出了核心的通信效率协作技术，通过融合不同视角的互补信息来解决深度歧义、遮挡和远距离问题，实现更准确和完整的3D表示；iii）扩展了
#### 7. 方法详细介绍：
本文提出的CoCa3D方法包括三个部分：单Agent相机3D检测、协作深度估计和协作检测特征学习。单Agent相机3D检测采用CaDNN架构，包括编码器、深度估计、体素变换、折叠和解码器五个模块。协作深度估计通过多视角一致性来提高单Agent深度估计的准确性，包括深度不确定性感知信息打包和深度信息融合两个步骤。协作检测特征学习通过检测置信度感知信息打包和检测信息融合来提高检测特征的准确性。整个系统由两个任务监督：分类深度估计和3D物体检测。

#### 8. 实验设置：
本文在三个数据集上进行了实验，包括OPV2V+、DAIR-V2X和CoPerception-UAVs+。使用IoU阈值为0.30、0.50、0.70和0.80的平均精度（AP）作为评价指标。深度准确性使用多类分类准确率作为评价指标。通信量遵循标准设置，按照以2为底的对数刻度计算字节大小。

#### 9. 实验结果和分析：
本文提出的CoCa3D方法在三个数据集上均优于之前的最新方法，AP@70分别提高了30.60%、12.59%和44.21%。协作相机3D检测性能超过了基于LiDAR的检测。随着协作Agent数量的增加，检测性能逐渐提高。在DAIR-V2X上，使用7个协作车辆，相机3D检测预计将优于基于LiDAR的3D检测。在CoPerception-UAVs+上，使用3个协作无人机，相机3D检测优于基于地面真实深度的3D检测。


# Paper:32     Therbligs在行动中：通过运动基元理解视频



#### 1. Title: 
Therbligs in Action: Video Understanding through Motion Primitives

#### 2. Authors: 
Eadom Dessalene, Michael Maynord, Cornelia Fermüller, Yiannis Aloimonos

#### 3. Affiliation: 
第一作者：University of Maryland, College Park

#### 4. Keywords: 
Video understanding, action modeling, Therbligs, contact-centered representation, rule-based reasoning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Dessalene_Therbligs_in_Action_Video_Understanding_Through_Motion_Primitives_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文提出了一种基于Therbligs的规则化、组合化和分层建模的动作建模方法，该方法提供了一种一致、表达力强、以接触为中心的动作表示方法。 
- (2):与其他方法相比，Therblig表示法具有以下优点：组合性和层次性；基于规则的推理；解决语义歧义；接触中心的时间边界精度。 
- (3):本文提出了一种新的分层架构，由Therblig-Model和Action-Model组成。Therblig-Model将视频映射到Therbligs，Action-Model将视频和Therbligs映射到动作。Therblig-Model通过包含可微分推理的结构感知项和接触一致性项来优化损失。 
- (4):本文在动作分割、动作识别和动作预测任务上进行了评估。在EPIC Kitchens 100和50-Salads数据集上，相对于其他方法，本文方法分别获得了10.5%/7.53%/6.5%和8.9%/6.63%/4.8%的相对改进。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于Therblig的行为表示方法，Therblig是一种低级别的互斥接触划分的子动作集合。该方法的架构由两个主要组件组成：Therblig-Model和Action-Model。Therblig-Model将视频映射到Therblig，Action-Model将视频和Therblig映射到行为。Therblig-Model通过包含结构感知项和可微分推理的Therblig一致性项的损失进行优化。Action-Model采用大小为W的滑动窗口，将其与Therblig-Model的隐藏状态一起输入到2层MLP的注意机制中。该方法在行为分割、行为识别和行为预测任务上进行了评估，并在EPIC Kitchens和50 Salads数据集上取得了相对改进。

#### 8. 实验设置：
本文使用了EPIC Kitchens和50 Salads数据集进行实验，并对行为分割、行为识别和行为预测任务进行了评估。评估指标包括帧精度、分段编辑分数、分段F1分数和准确率。

#### 9. 实验结果和分析：
本文提出的基于Therblig的方法在行为分割、行为识别和行为预测任务上相对改进分别为10.5%、7.53%和6.5%（EPIC Kitchens数据集），以及8.9%、6.63%和4.8%（50 Salads数据集）。该方法的Therblig表示改善了动词识别准确性，并在行为分割结果中产生了更好的行为边界预测。此外，本文还展示了Therblig表示与六种流行的行为理解方法的集成。


# Paper:33     领域和类别转移下的模型升级



#### 1. Title: 
Upcycling Models under Domain and Category Shift

#### 2. Authors: 
Sanqing Qu, Tianpei Zou, Florian Röhbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang

#### 3. Affiliation: 
第一作者：同济大学
Sanqing Qu, Tianpei Zou, Guang Chen, Changjun Jiang

#### 4. Keywords: 
Deep neural networks, unsupervised domain adaptation, source-free domain adaptation, universal domain adaptation, clustering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qu_Upcycling_Models_Under_Domain_and_Category_Shift_CVPR_2021_paper.html  Github: https://github.com/ispc-lab/GLC

#### 6. Summary : 
- (1):本文研究深度神经网络在领域转移和类别转移下的性能问题，提出了一种新的方法来解决这个问题。
- (2):过去的方法主要集中在领域内的转移，而本文提出的方法可以处理领域和类别的转移。现有的方法需要访问源数据，而本文提出的方法只需要一个预训练的源模型。本文提出的方法可以处理多种类别转移情况，包括部分集、开放集和开放部分集。本文提出的方法使用全局和局部聚类技术，可以识别“已知”数据样本并拒绝那些“未知”的数据样本。
- (3):本文提出了一种新的方法，称为全局和局部聚类技术（GLC），用于解决领域和类别转移问题。GLC使用全局一对多聚类算法来实现“已知”和“未知”数据样本的分离，并使用局部k-NN聚类策略来减轻负面转移。本文的方法可以处理多种类别转移情况，包括部分集、开放集和开放部分集。本文的方法使用全局和局部聚类技术，可以识别“已知”数据样本并拒绝那些“未知”的数据样本。
- (4):本文的方法在多个基准测试中进行了验证，包括Ofﬁce-31、Ofﬁce-Home、VisDA和Domain-Net。实验结果表明，GLC在多个基准测试中都取得了最先进的性能，即使在更严格的约束条件下也是如此。在最具挑战性的开放部分集DA情况下，GLC在VisDA基准测试中的H-score为73.1％，比UMAD和GATE分别高出14.8％和16.7％。
#### 7. 方法详细介绍：
本文提出了一种名为GLC（Global-Local Consensus）的方法，用于在领域和类别转移下升级模型。该方法包括全局一对多聚类伪标记、目标域类别估计、本地k-NN一致性聚类和优化目标等多个步骤。该方法还利用标准化的香农熵作为不确定性度量，以在推理期间分离已知和未知数据样本。

具体步骤如下：
1. 预训练源模型fs = hs ◦ gs，其中分类器hs被冻结，目标特定特征模块gt通过微调源特征模块gs进行学习以实现领域对齐。
2. 全局一对多聚类伪标记：开发一种新的自适应一对多全局聚类算法，为每个目标数据样本分配伪标签。使用Silhouette准则估计目标域类别的适当数量。为避免源私有类别的误导，开发了一种基于全局置信度得分的抑制策略。
3. 目标域类别估计：使用全局聚类结果估计目标域类别。
4. 本地k-NN一致性聚类：利用局部内在结构减轻负面转移。
5. 优化目标：使用分类器hs和目标特定特征模块gt进行分类。

#### 8. 实验设置：
本文使用了几个标准的领域适应数据集，包括Office-31、Office-Home、VisDA和DomainNet。实验在OPDA、OSDA、PDA、Ar2Cl、Ar2Pr、Ar2Re、Cl2Ar、Cl2Pr、Cl2Re、Pr2Ar、Pr2Cl、Pr2Re、Re2Ar、Re2Cl和Re2Pr等场景下进行。本文还将所提出的方法与几种最先进的方法进行了比较，包括UAN、CMU、DCC、OVANet、GATE、Source-only、SHOT-O和UMAD。

#### 9. 实验结果和分析：
本文以H-score（%）为指标报告实验结果。所提出的方法GLC在大多数场景下在所有数据集上都取得了最佳或次佳的性能，超过了最先进方法。本文还对不确定性阈值ω和权衡超参数η进行了敏感性分析，并表明所提出的方法对这些参数具有鲁棒性。


# Paper:34     HAAV：用于图像字幕生成的增强视图的分层聚合



#### 1. Title: 
HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning

#### 2. Authors: 
Chia-Wen Kuo, Zsolt Kira

#### 3. Affiliation: 
Chia-Wen Kuo: 乔治亚理工学院 (Georgia Tech)
Zsolt Kira: 乔治亚理工学院 (Georgia Tech)

#### 4. Keywords: 
Image captioning, augmented views, hierarchical aggregation, contrastive loss, transformer encoder-decoder model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是图像字幕生成，旨在提高模型的效率和效果。
 
- (2):过去的方法主要是使用预训练模型对图像进行编码，但是这些方法存在计算复杂度高、参数数量多、标签效率低等问题。本文提出了一种新的方法，将编码视为增强视图，并使用共享编码器对每个视图进行独立编码，同时引入对比损失来提高视图的表示质量和模型的数据效率。此外，本文还提出了一种分层解码器，根据每个视图的有效性自适应地加权编码视图，从而更好地模拟视图的有效性。这种方法的动机是很好的。
 
- (3):本文提出了一种新的方法，将编码视为增强视图，并使用共享编码器对每个视图进行独立编码，同时引入对比损失来提高视图的表示质量和模型的数据效率。此外，本文还提出了一种分层解码器，根据每个视图的有效性自适应地加权编码视图，从而更好地模拟视图的有效性。这种方法的创新点在于将视图视为增强图像，并使用对比损失来提高视图的表示质量和模型的数据效率，同时使用分层解码器来自适应地加权编码视图，从而更好地模拟视图的有效性。
  
- (4):本文在MS-COCO和Flickr30k数据集上进行了实验，相对于现有方法，本文方法在CIDEr指标上分别提高了5.6%和12.9%。实验结果表明，本文方法在效率和效果方面都有很好的表现，支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种名为HAAV（Hierarchical Aggregation of Augmented Views）的图像字幕生成方法。该方法利用不同的预训练模型编码的异构视图，并且这些视图可能被编码为不同的模态。视图独立地使用共享编码器进行编码，然后使用新颖的分层解码器在标记级别和视图级别上进行聚合。该方法还结合了对比损失，以促进优秀的自/无监督表示学习并提高标签效率。

具体而言，该方法将输入图像的编码视为增强视图，并使用共享的Transformer编码器独立地对每个视图进行编码。为了提高异构视图的表示质量，该方法还引入了对比损失。分层解码器层被设计用于模拟每个视图的有效性，并相应地对每个视图进行加权以进行字幕生成。分层解码器首先在标记级别内对每个视图进行聚合，然后在视图级别上进行聚合。

#### 8. 实验设置：
本文的实验在MS-COCO和Flickr30k数据集上进行。使用了来自先前方法的异构视图进行训练，没有进行修改。模型使用交叉熵损失进行训练，然后使用SCST损失进行微调，以通过强化学习优化CIDEr分数。不同模块的dropout率设置为0.1。本文还展示了使用来自Flickr30K的标记数据和来自MS-COCO的未标记数据进行图像字幕生成的半监督学习。实现细节和超参数可以在补充材料中找到。

#### 9. 实验结果和分析：
本文提出的HAAV方法在MS-COCO和Flickr30k数据集上均取得了优异的性能。与现有方法相比，HAAV方法在MS-COCO数据集上的CIDEr分数提高了5.6％，在Flickr30k数据集上使用SSL提高了16.8％的CIDEr分数。本文还通过实验展示了HAAV方法的标签效率，结果表明，与使用连接视图和每个视图一个模型等常见方法在100％数据上训练的方法相比，HAAV方法只需要约40-50％的标记数据即可实现可比较的性能。


# Paper:35     从不完美的专家中提取关键知识用于三维物体检测的聚焦蒸馏



#### 1. Title: 
Distilling Focal Knowledge from Imperfect Expert for 3D Object Detection

#### 2. Authors: 
Jia Zeng, Li Chen, Hanming Deng, Lewei Lu, Junchi Yan, Yu Qiao, Hongyang Li

#### 3. Affiliation: 
上海人工智能实验室 OpenDriveLab

#### 4. Keywords: 
3D object detection, knowledge distillation, focal distiller, bird’s-eye-view, multi-camera

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/OpenPerceptionX/BEVPerception-Survey-Recipe

#### 6. Summary : 
- (1):本文研究的是基于相机的三维物体检测领域，提出了一种从不完美的专家中提取关键知识的方法。

- (2):过去的方法主要是基于鸟瞰图（BEV）表示的，虽然性能很好，但效率较低。本文提出了一种名为FD3D的方法，它利用一组查询来定位实例级区域，以增强这些区域的特征表示能力。此外，这些查询还搜索代表性的细粒度位置，以进行精细的蒸馏。与其他方法相比，本文提出的方法使用查询生成实例掩码，而不是随机掩码。本文的方法可以应用于两种流行的检测模型，BEVFormer和DETR3D。

- (3):本文提出了一种名为FD3D的方法，它利用一组查询来定位实例级区域，以增强这些区域的特征表示能力。此外，这些查询还搜索代表性的细粒度位置，以进行精细的蒸馏。FD3D是一种插拔式模块，可以轻松扩展到各种检测器中。

- (4):本文的方法在nuScenes基准测试中取得了4.07和3.17 NDS的改进，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种名为FD3D的3D目标检测蒸馏方法。该方法利用一组查询来从不完美的3D目标检测专家中提取焦点知识。查询用于定位实例级焦点区域，其中执行掩蔽生成蒸馏以增强特征表示。此外，查询动态搜索焦点蒸馏的细粒度代表位置。两个互补的模块指导学徒网络在焦点区域生成增强的特征表示。该方法可以轻松扩展到各种检测器。

具体步骤如下：
1. 在BEV和PV特征上执行蒸馏。
2. 设置额外的蒸馏头，并使用一组对齐的BEV查询进行蒸馏。
3. 在专家BEV特征上构建蒸馏头，并将蒸馏查询通过自注意力层，然后对专家BEV特征进行采样以生成3D边界框。
4. 将生成的3D边界框投影到PV和BEV中，并使用投影区域进行掩蔽生成蒸馏，变形注意力过程中采样的BEV特征用于精细蒸馏。
5. 选择性掩蔽表示学徒特征中掩蔽区域内的像素被比率r掩蔽。生成随机掩蔽，强制掩蔽学徒特征以恢复生成器。计算通道分布以执行选择性掩蔽生成蒸馏。
6. 使用一组焦点查询在代表性特征上进行焦点蒸馏。

#### 8. 实验设置：
实验在nuScenes数据集上进行，该数据集包含700个训练场景、150个验证场景和150个测试场景。通过更改骨干网络、输入图像分辨率和BEV网格分辨率，获得三组专家-学徒组合。对于所有骨干网络，都使用带或不带可变形卷积的ResNet。该方法在MMDetection3D上实现，并在8个NVIDIA A100 GPU上训练模型。模型的初始学习率为2e-4，每个GPU的批量大小为1。在训练BEVFormer和DETR3D时未引入数据增强。低分辨率学徒和高分辨率专家之间采用特征金字塔位置移位进行PV特征对齐。

#### 9. 实验结果与分析：
本文提出的方法FD3D在BEVFormer-Tiny、BEVFormer-Small和DETR3D上分别取得了4.07、2.47和3.17 NDS的显著提高。改进主要来自于改进的mAP和速度估计精度。该方法还在mAVE方面分别将BEVFormer-Tiny和DETR3D-R50的改进4.97和8.21点。比较表明，所设计的基于焦点区域的细粒度蒸馏确实提高了蒸馏质量。还测试了每个提出的模块的贡献，并显示选择性掩蔽生成蒸馏和基于查询的焦点蒸馏可以在基线蒸馏方法之上实现额外的改进。


# Paper:36     MARLIN：面部视频表示学习的遮罩自编码器



#### 1. Title: 
MARLIN: Masked Autoencoder for facial video Representation LearnINg

#### 2. Authors: 
Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza Haffari, Munawar Hayat

#### 3. Affiliation: 
第一作者：莫纳什大学（Monash University）

#### 4. Keywords: 
Facial representation learning, self-supervised learning, masked autoencoder, facial video analysis, transfer learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2021_paper.html  Github: https://github.com/ControlNet/MARLIN

#### 6. Summary : 
- (1):本文旨在提出一种自监督学习方法，从视频中学习通用的面部表示，可在多种面部分析任务（如面部属性识别、面部表情识别、DeepFake检测和唇部同步等）中进行转移。 
- (2):现有的面部分析方法大多是针对特定任务的，需要大量的标注数据，而本文提出的方法则是一种自监督的预训练方法，可以从非标注数据中学习通用的面部表示，从而在有限的标注数据下实现跨任务的迁移。与现有的自监督学习方法不同，本文提出的方法采用面部引导的遮罩自编码器，通过从丰富的非标注网络爬取的面部视频中学习高度鲁棒和通用的面部嵌入。通过在密集遮罩的面部区域（主要包括眼睛、鼻子、嘴巴、唇部和皮肤）中重建面部的时空细节，以捕捉局部和全局方面，从而有助于编码通用和可转移的特征。 
- (3):本文提出的框架名为MARLIN，是一种面部视频遮罩自编码器，通过面部引导的遮罩策略学习重建面部的时空细节，从而学习通用的面部表示。与现有的方法不同，MARLIN采用了一种更具挑战性的辅助任务，即从密集遮罩的面部区域中重建面部的时空细节。此外，本文还在遮罩编码的基础上引入了对抗性损失，以提高重建质量。 
- (4):在各种下游任务的实验中，本文证明了MARLIN是一种出色的面部视频编码器和特征提取器，可以在多种下游任务中实现一致的良好表现，包括面部属性识别（相对于监督基准提高1.13%）、面部表情识别（相对于无监督基准提高2.64%）、DeepFake检测（相对于无监督基准提高1.86%）、唇部同步（Frechet Inception Distance提高29.36%），甚至在低数据情况下也能取得良好的表现。
#### 7. 方法详细介绍：
MARLIN是一个自监督的框架，旨在从丰富的非注释面部视频数据中学习到稳健且可转移的通用面部表示。该框架采用面部区域引导的掩蔽策略，为自监督表示学习提供了具有挑战性的辅助重建任务。可见的标记被编码器映射到潜在空间，潜在空间特征进一步被送入解码器，解码器重建掩蔽标记。在掩蔽的立方体和它们的重建对应物之间施加重建损失，以指导学习目标。在掩蔽自编码器骨干上加入对抗性适应，以增强丰富表示学习的生成质量。

#### 8. 实验设置：
MARLIN框架在不同的面部分析任务上进行了评估，包括面部属性识别、面部表情识别、Deepfake检测和唇部同步。评估协议遵循任务特定的先前文献。CelebV-HQ数据集用于面部属性和动作预测，CMU-MOSEI数据集用于对话情感和情感预测，FF++(LQ)数据集用于Deepfake检测，LRS2数据集用于唇部同步。最小时间步长值设置为2，以考虑在帧之间具有显着运动的语义上有意义的帧。ViT-B被用作骨干编码器，预训练超参数设置如下：基础学习率与整体批量大小成线性比例，lr = 基础学习率×批量大小/256。采用AdamW优化器进行自监督预训练，基础学习率为1.5e−4，动量β1 = 0.9，β2 = 0.95，学习率调度程序（余弦衰减）。对于线性探测，使用Adam优化器，β1 = 0.5，β2 = 0.9，基础学习率为1e−4，权重衰减为0。对于微调，使用Adam优化器，β1 = 0.5，β2 = 0.9，基础学习率为1e−4，没有任何权重衰减。

#### 9. 实验结果和分析：
该论文表明，MARLIN在各种下游任务中表现出色，包括面部属性识别、面部表情识别、Deepfake检测和唇部同步。实验结果表明，MARLIN学习到高度通用的面部编码，可在不同的面部分析任务中进行扩展和转移。该论文还包括少样本适应实验，表明MARLIN适应下游数据集的能力强。此外，该论文还进行了消融研究，证明了MARLIN的每个组件的有效性，包括提出的Fasking策略和对抗性训练。


# Paper:37     DNeRV: 通过差分神经表示对视频的内在动态进行建模



#### 1. Title: 
DNeRV: Modeling Inherent Dynamics via Difference Neural Representation for Videos

#### 2. Authors: 
Qi Zhao, M. Salman Asif, Zhan Ma

#### 3. Affiliation: 
Qi Zhao: 南京大学 (Nanjing University)
M. Salman Asif: 加州大学河滨分校 (University of California Riverside)
Zhan Ma: 南京大学 (Nanjing University)

#### 4. Keywords: 
Implicit neural representation, video modeling, adjacent dynamics, difference neural representation, collaborative content unit

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_DNeRV_Modeling_Inherent_Dynamics_via_Difference_Neural_Representation_for_Videos_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究视频的内在动态建模问题，现有的隐式神经表示方法未能充分利用视频中的时空冗余信息，导致对于大运动或动态场景的建模能力较弱。本文从函数拟合的角度分析了这一局限性，并揭示了帧差分的重要性。
 
- (2):现有的视频神经表示方法可以分为两类：基于索引的方法和混合方法。基于索引的方法使用索引的位置嵌入作为输入，缺乏给定视频的内容特定信息。混合方法使用帧作为索引嵌入，忽略了不同帧之间的时间相关性。因此，无论是基于索引还是基于帧的神经表示方法都无法有效地处理相邻动态。本文提出了差分神经表示方法（DNeRV），通过吸收相邻帧之间的差异来近似动态系统，从而更有效地建模具有短期时间相关性的空间结构。此外，本文还提出了一种协作内容单元（CCU）的新型门控机制，用于自适应地融合两个流中的特征。
 
- (3):本文提出了差分神经表示方法（DNeRV），它由两个流组成，分别用于内容和帧差分。差分编码器捕获差分流中的短期上下文相关性，然后与内容流合并进行时空特征融合。此外，本文还提出了一种协作内容单元（CCU），用于自适应地融合两个流中的特征。本文在三个数据集上进行了实验，包括Bunny、UVG和Davis Dynamic，并在各种下游任务上展示了所提出方法的有效性。DNeRV在960×1920视频的插值和修复任务上的表现优于现有的隐式方法。
  
- (4):本文提出的DNeRV方法在视频压缩、修复和插值任务上进行了实验，取得了与现有最先进的神经压缩方法相当的结果，并在下游修复和插值任务上优于现有的隐式方法。DNeRV方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为“差分神经表示法”的方法（DNeRV），它采用两个输入流：内容流和差分流。编码器由内容编码器和差分编码器组成，中间输出嵌入通过融合步骤进入解码器。解码器采用NeRV块作为每个解码阶段的基本块，并使用协作内容单元（CCU）融合来自两个流的特征。CCU协同捕获的相邻动态可以帮助网络学习隐式映射。最后，通过通道缩减将输出映射到像素域。

#### 8. 实验设置：
本文在Bunny、UVG和DAVIS Dynamic数据集上验证了DNeRV的性能。Bunny数据集包含720×1280的132帧，UVG数据集包含1080×1920的7个视频，长度为600或300，DAVIS Dynamic是DAVIS16验证集的子集，包含1080×1920的22个视频。在训练过程中，本文采用Adam作为优化器，学习率为5×10−4，余弦退火学习率调度，批大小为1。本文使用PSNR和SSIM评估视频质量。步幅列表、内核大小和减少率与先前的方法相同，除了解码器的最后两个阶段的通道。除非另有说明，否则所有实验均在Pytorch中使用GPU RTX2080ti进行，大小为3M，训练300个epoch。

#### 9. 实验结果和分析：
本文将DNeRV与NeRV、E-NeRV和HNeRV进行了比较，包括视频回归和三个下游视觉任务：视频压缩、插值和修复。在视频回归方面，DNeRV在Bunny、UVG和DAVIS Dynamic数据集上表现优异。在视频压缩方面，DNeRV在PSNR和SSIM方面优于传统的视频编解码器H.264或H.265，并且与最先进的深度压缩方法相比具有竞争力。在视频插值方面，DNeRV在动态场景或移动目标的视频上表现优异，特别是在其他隐式方法上表现更好。本文还进行了消融研究，验证了所提出方法的有效性。


# Paper:38     WINNER：用于时空视频定位的弱监督分层分解和对齐



#### 1. Title: 
WINNER: Weakly-supervised hIerarchical decompositioN and aligNment for spatio-tEmporal video gRounding

#### 2. Authors: 
Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao, Zhou Zhao, Shengyu Zhang, Wei Ji, Fei Wu

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Spatio-temporal video grounding, weakly-supervised learning, hierarchical decomposition, structural attention, contrastive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_WINNER_Weakly-Supervised_hIerarchical_DecompositioN_and_AlignNment_for_Spatio-tEmporal_Video_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了弱监督下的时空视频定位任务，提出了一种分层分解和对齐的方法，以减轻由于有限注释带来的虚假相关性问题。

- (2):现有的方法需要使用密集的边界和边框注释，这可能代价高昂。为了弥补这一差距，本文研究了弱监督设置，即模型从易于访问的视频-语言数据中学习，而不需要注释。本文提出了一种新的框架，即WINNER，用于分层视频-文本理解。WINNER首先以自下而上的方式构建语言分解树，然后结合结构性注意机制和自上而下的特征回溯，共同构建多模态分解树，允许对非结构化视频进行分层理解。多模态分解树作为多层次语言-管道匹配的基础。提出了一种分层对比学习目标，以学习多层次对应和区分，实现视频-语言分解结构对齐。

- (3):本文提出了一种新的分层分解和对齐的方法，以减轻由于有限注释带来的虚假相关性问题。该方法首先构建语言分解树，然后结合结构性注意机制和自上而下的特征回溯，共同构建多模态分解树，允许对非结构化视频进行分层理解。提出了一种分层对比学习目标，以学习多层次对应和区分，实现视频-语言分解结构对齐。

- (4):本文在两个广泛使用的弱监督视频对象定位数据集上进行了实验。实验结果表明，WINNER模型大大优于现有技术，并且即使是一些监督方法也能实现可比较的性能。
#### 7. 方法详细介绍：
WINNER模型主要包括三个关键步骤：（1）跨模态分层理解，以自下而上的逐层方式递归地构建语言分解树，并利用结构化注意力机制进行分层视频理解，得到多模态分解结构。（2）结构引导的视频时序定位，学习计算多模态分解结构与不同视频片段之间的关联，并选择最相关的视频片段作为时序定位结果。（3）分解结构对齐，通过层次对比学习训练模型，为每个节点找到最匹配的视频管，从而实现跨模态分解结构对齐。

#### 8. 实验设置：
本文使用了两个数据集进行测试：ActivityNet Captions和TACoS Multi-Level。模型在单个NVIDIA Tesla V100 GPU上使用PyTorch进行训练，Adam优化器和学习率为1e-4，批量大小为32，最大训练轮数为50。

#### 9. 实验结果与分析：
WINNER模型在两个数据集上的表现均优于现有的弱监督方法，并且在某些监督方法上实现了可比较的性能。消融实验表明了所提出的框架每个组件的有效性。案例研究表明，所提出的框架可以处理复杂场景并减轻虚假相关性。


# Paper:39     无法描述的多模态空间评估器



#### 1. Title: 
Indescribable Multi-modal Spatial Evaluator

#### 2. Authors: 
Lingke Kong, X. Sharon Qi, Qijin Shen, Jiacheng Wang, Jingyi Zhang, Yanle Hu, Qichao Zhou

#### 3. Affiliation: 
第一作者：Manteia Tech

#### 4. Keywords: 
Multi-modal image registration, spatial evaluator, self-supervised approach, domain generalization, image-to-image translation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kong_Indescribable_Multi-Modal_Spatial_Evaluator_CVPR_2021_paper.html  Github: https://github.com/Kid-Liet/IMSE

#### 6. Summary : 
- (1):本文研究的是多模态图像配准，其中一个主要挑战是来自不同成像机器的图像具有不同的成像分布，使得难以仅关注图像的空间方面并忽略分布差异。 
- (2):过去的方法包括相似性运算符和基于图像到图像转换的方法，但它们都存在一些问题，例如难以适用于所有模态场景，难以估计其上限，或需要训练生成器等。本文提出了一种自监督方法，即IMSE，它创建了一个准确的多模态空间评估器来度量两个图像之间的空间差异，并通过最小化评估器预测的误差来优化配准。 
- (3):本文提出了相对单模态和绝对单模态的概念，并基于这些概念提出了IMSE方法来评估多模态图像配准中的空间差异。IMSE专注于空间位置的差异，而忽略由不同成像机制引起的多模态分布差异。本文还提出了一种新的样式增强方法，称为Shuffle Remap，它可以将图像分布随机化为多个段，并随机重新排列和映射这些段，以改变原始图像的分布。Shuffle Remap可以帮助IMSE准确预测来自未见目标分布的空间位置差异。 
- (4):本文在T1-T2和CT-MRI数据集上展示了IMSE优于现有方法的性能，并且可以轻松集成到传统的配准过程中，并提供了一种方便的方法来评估和可视化配准结果。IMSE还具有作为图像到图像转换的新范例的潜力。
#### 7. 方法详细介绍：
本文提出了一种名为IMSE（Indescribable Multi-modal Spatial Evaluator）的方法，用于解决多模态图像配准问题。IMSE使用神经网络作为损失函数，通过优化损失函数来实现更好的配准结果。IMSE包括两个部分：评估器和配准网络。评估器使用模拟的多模态配准数据进行训练，以获得绝对单模态标签。配准网络使用评估器的输出进行训练，以优化参数。IMSE还包括一种名为Shuffle Remap的样式增强方法，以确保分布差异的充分覆盖。完整的算法流程在Algorithm1中给出。IMSE的优化过程包括以下步骤：
1. 将变形场初始化为0。
2. 使用相似性损失（Similarity operator或IMSE）和正则化损失（Eq 7）来优化变形场。
3. 将评估器集成到传统的配准过程中，用训练好的评估器替换Eq 1中的Lsim函数。

#### 8. 实验设置：
本文使用了两个数据集进行评估：BraTs 2019和Clinical CT-MR。BraTs 2019数据集包含240×240大小的T1-T2脑部图像，共18911个切片；Clinical CT-MR数据集包含192×192大小的头颈部图像，共3839个切片。为了公平比较，所有方法都使用了统一的配准网络模型-VoxelMorph。训练IMSE所使用的源数据分别为T1和CT。评估器采用ResNet网络结构。在训练和测试过程中，对移动和目标图像添加了随机仿射和非仿射变换，包括角度旋转[-3,3]、位移[-8%,+8%]、缩放[8%,+8%]。非仿射变换是通过对移动和目标图像进行空间变换生成的，随后进行高斯平滑处理。变形程度为80，高斯平滑半径为12。

#### 9. 实验结果与分析：
本文对各种基于神经网络的配准方法进行了比较。基线方法使用传统的相似性运算符作为损失函数来更新配准网络，包括NCC、MI和MIND。还有基于GAN的方法，先进行平移，然后使用MAE作为配准网络的损失函数，包括CycleGAN和RegGAN。本文还比较了使用Bézier曲线的直方图移位（IMSE（BC））和Shuffle Remap（IMSE（SR））进行样式增强的性能。Shuffle Remap中N的随机范围为[2,50]。IMSE在所有指标上都取得了最佳结果，无论是2D还是3D。本文还对T1-T2和MR-CT数据集的各种方法的配准结果进行了总结，结果见表2。本文还比较了不同参数N下Shuffle Remap的性能，结果见表3。本文表明，IMSE既具有高配准精度，又具有更平滑的变形场，这通常很难同时实现。


# Paper:40     SCPNet：基于点云的语义场景补全



#### 1. Title: 
SCPNet: Semantic Scene Completion on Point Cloud

#### 2. Authors: 
Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, and Yu Qiao

#### 3. Affiliation: 
第一作者：上海人工智能实验室

#### 4. Keywords: 
Semantic scene completion, point cloud, deep learning, knowledge distillation, label rectification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Xia_SCPNet_Semantic_Scene_Completion_on_Point_Cloud_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是点云上的语义场景补全问题，该问题的研究背景是点云数据在三维场景理解中的重要性。

- (2):过去的方法主要集中在室内场景，而本文主要关注室外场景。过去的方法主要依赖于体素级别的标签，对于小物体和拥挤场景的补全效果不佳。本文提出了三个解决方案：重新设计补全子网络，从多帧模型中提取丰富的知识，以及补全标签矫正。这些方案都能够有效地提高补全模型的性能。

- (3):本文提出了一种新的补全子网络，该网络由多个多路径块组成，能够聚合多尺度特征，并且不需要降采样操作。此外，本文还提出了一种新的知识蒸馏目标，称为Dense-to-Sparse Knowledge Distillation (DSKD)，它能够将多帧模型中的密集、关系型语义知识转移到单帧模型中，从而显著提高单帧模型的表示学习能力。最后，本文提出了一种简单而有效的标签矫正策略，能够使用现成的全景分割标签来消除补全标签中动态物体的痕迹。

- (4):本文在两个公共的语义场景补全基准测试中进行了广泛的实验，即SemanticKITTI和SemanticPOSS。本文提出的SCPNet在SemanticKITTI语义场景补全挑战赛中排名第一，比竞争对手S3CNet [3]高出7.2 mIoU。SCPNet还在SemanticPOSS数据集上优于以前的补全算法。此外，本文的方法在SemanticKITTI语义分割任务上也取得了竞争性的结果，表明在场景补全中学习的知识对分割任务有益。
#### 7. 方法详细介绍：
本文提出了SCPNet，一种用于点云的语义场景补全方法。SCPNet由两个子网络组成，分别是补全子网络和分割子网络。补全子网络基于三个原则设计，即保持稀疏性、不进行下采样和聚合多尺度特征。分割子网络基于Cylinder3D进行构建，并进行了一些修改。本文还提出了Dense-to-Sparse Knowledge Distillation（DSKD）方法，将多帧模型中的密集、关系型语义知识转移到单帧模型中。最后，本文引入了补全标签矫正策略，使用现成的全景分割标签来消除补全标签中动态物体的长痕迹，从而使补全标签更加准确。

#### 8. 实验设置：
本文在两个流行的LiDAR语义场景补全基准数据集SemanticKITTI和SemanticPOSS上进行了实验。对于SemanticKITTI，使用序列00到10、08和11到21进行训练、验证和测试。在合并具有不同运动状态的类别并且丢弃点数非常少的类别后，选择了19个类别进行训练和评估。对于SemanticPOSS，选择了11个类别进行评估。Cylinder3D的输出大小设置为256×256×32以适应补全任务。训练时的epoch数设置为30，初始学习率设置为0.0015。

#### 9. 实验结果与分析：
SCPNet在SemanticKITTI和SemanticPOSS基准数据集上的表现均优于其他场景补全算法。在SemanticKITTI上，SCPNet使用四帧时的mIoU为47.5，比之前的最先进方法S3CNet高出7.2 mIoU。在SemanticPOSS上，SCPNet在车、树干、杆子、栅栏和自行车等类别上表现优异，至少比之前的最先进方法JS3C-Net高出5个IoU。本文还进行了消融实验，证明了提出的方法的有效性。


# Paper:41     利用姿态和形状的一致性从未见过的视角隐式恢复三维人体网格



#### 1. Title: 
Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-view

#### 2. Authors: 
Hanbyel Cho, Yooshin Cho, Jaesung Ahn, Junmo Kim

#### 3. Affiliation: 
韩别乐，朝鲜，贾成，金俊模，韩国科学技术院

#### 4. Keywords: 
Human Mesh Recovery, Neural Feature Fields, Consistency, Pose, Shape

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cho_Implicit_3D_Human_Mesh_Recovery_Using_Consistency_With_Pose_and_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是人体网格恢复（HMR）的任务，即从RGB图像中回归三维人体模型的参数。
- (2):过去的方法主要有基于优化的方法和基于回归的方法，但是它们都存在一些问题，如优化方法的速度慢且对初始化敏感，回归方法的表现不够稳健。本文提出了一种新的方法，可以通过神经特征场隐式地想象一个人在三维空间中的位置，并从任意视角回归其姿态和形状参数，从而提高了HMR的性能。
- (3):本文提出的方法是“Implicit 3D Human Mesh Recovery (ImpHMR)”，它可以通过神经特征场隐式地想象一个人在三维空间中的位置。在ImpHMR中，特征场是由CNN图像编码器为给定图像生成的。然后，对于给定的视角，通过体积渲染从特征场生成2D特征图，并从特征中回归姿态和形状参数。为了利用从未见过的视角的姿态和形状的一致性，如果有3D标签，则模型预测结果包括从任意方向的轮廓，并使其等于旋转的地面真值。在只有2D标签的情况下，我们通过约束从不同方向推断出的姿态和形状参数应该相同来进行自监督学习。本文的创新点在于使用神经特征场隐式地想象一个人在三维空间中的位置，并从任意视角回归其姿态和形状参数，从而提高了HMR的性能。
- (4):本文在3DPW、LSP、COCO和3DPW-OCC数据集上进行了实验，证明了所提出方法的有效性。ImpHMR的性能比当前SOTAs快2-3倍，因为它使用特征场进行高效的空间表示。本文的方法可以从任意视角回归姿态和形状参数，从而提高了HMR的性能。
#### 7. 方法详细介绍：
本文提出了一种新的HMR模型，称为“ImpHMR”，可以通过神经特征场从给定的2D观察中隐式地想象出一个人在3D空间中。该方法利用了从未见过的视角的姿态和形状的一致性，并包括任意视角想象损失和外观一致性损失。此外，本文提出了一个几何指导分支，有助于模型学习更好的几何信息。该方法使用神经特征场内的高效空间表示，并具有实时性能。具体步骤如下：
1. 使用CNN-based图像编码器生成特征场。
2. 生成由MLP表示的特征场，将3D空间中的点位置和光线方向映射到特征向量和体积密度。
3. 使用2D特征图进行体积渲染，从而构建人物的3D空间表示。
4. 通过回归器从特征向量预测SMPL参数。
5. 使用几何指导分支重构2D特征图中的轮廓。
6. 训练目标包括规范视图回归损失、任意视图回归损失和几何指导损失。

#### 8. 实验设置：
本文使用2D和3D数据集的混合进行训练和评估。使用的3D数据集为MPI-INF-3DHP和Human3.6M，而使用的2D数据集为MPII、COCO和LSPET。模型使用端到端的方法进行训练，并将训练过程分为两个阶段以减少总体训练时间。第一阶段在COCO上进行，用于消融研究，而第二阶段使用所有数据集的混合进行最终性能比较。模型使用3DPW和3DPW-OCC数据集上的平均每关节位置误差（MPJPE）、Procrustes对齐的平均每关节位置误差（PA-MPJPE）和每个顶点的误差（PVE）指标进行评估。

#### 9. 实验结果和分析：
本文提出的ImpHMR方法在3DPW测试集上的所有指标（包括时间和帧）均优于先前的最先进方法。该模型还在3DPW-OCC数据集上表现出优异的性能，与遮挡特定方法相比。本文提供了与SPIN和HMR-EFT的定性比较，表明ImpHMR在存在极端姿势或模糊的人物时输出的网格与图像对齐良好。本文还报告了从不同视角的网格重建结果，证明ImpHMR不仅可以从规范视角想象人物的外观，还可以从3D空间中的其他方向想象。


# Paper:42     利用太阳光干涉仪实现被动微米级时间飞行成像



#### 1. Title: 
Passive Micron-scale Time-of-Flight with Sunlight Interferometry

#### 2. Authors: 
Alankar Kotwal, Anat Levin, Ioannis Gkioulekas

#### 3. Affiliation: 
Alankar Kotwal: 卡内基梅隆大学
Anat Levin: Technion
Ioannis Gkioulekas: 卡内基梅隆大学

#### 4. Keywords: 
Passive time-of-flight imaging, interferometry, sunlight, depth sensing, micrometer resolution

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Kotwal_Passive_Micron-Scale_Time-of-Flight_With_Sunlight_Interferometry_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是无源微米级时间飞行成像和深度感知技术，通过使用太阳光干涉仪来实现。该技术可以在户外环境下进行，且不需要使用主动光源，可以实现微米级深度感知。
 
- (2):现有的微米级三维成像技术需要使用主动光源，这使得它们在户外环境下不实用。而现有的无源三维成像技术则无法实现微米级分辨率。本文提出的技术使用太阳光作为唯一的光源，通过干涉仪实现微米级深度感知，可以在户外环境下进行，且不需要使用主动光源，具有很好的应用前景。

- (3):本文提出的技术使用太阳光干涉仪实现微米级深度感知。该技术可以在户外环境下进行，且不需要使用主动光源。通过使用太阳光的大光谱带宽，可以实现微米级时间分辨率的场景响应。此外，太阳光的大角带宽使得可以捕捉到对间接照明效应不敏感的时间飞行测量。本文构建了一个实验原型，可以在户外环境下进行操作，可以实现微米级深度感知、直接成像和透过散射体成像等功能。

- (4):本文提出的技术可以实现微米级深度感知，可以在户外环境下进行操作，且不需要使用主动光源。实验结果表明，该技术可以实现像素级横向分辨率和微米级轴向分辨率的被动深度扫描，对于使用主动光源技术难以扫描的物体（如半透明、金属、被薄散射体遮挡的物体）具有很好的成像效果。
#### 7. 方法详细介绍：
本文提出了一种完全被动的干涉技术，利用太阳光作为唯一的光源进行微米级三维感知。该技术使用改进后的全场迈克尔逊干涉仪，通过简单的轴向扫描操作获取微米级分辨率的时间分辨场景响应。该技术利用太阳光的空间不相干性，在间接照明效应和甚至透明散射层的情况下实现了强大的三维感知能力。

具体而言，该方法涉及分析反射场景和参考场之间的干涉图案，以测量场景的直接瞬态响应。该设置使用二维传感器和迈克尔逊干涉仪，通过移动参考镜到不同的轴向位置来实现轴向扫描。连续轴向位置之间的轴向分离由照明的时间相干长度确定。该方法涉及捕获一堆图像，参考臂放置在密集的位置，估计干涉和相关幅度，并通过检测最大相关幅度的轴向位置来提取深度图。该方法可以处理相机观察到另一个物体前面的半透明物体的情况。本文将所提出的方法与时域光学相干层析成像（TD-OCT）进行了比较，并表明它可以使用太阳光作为时间和空间不相干照明，实现被动的飞行时间成像。

#### 8. 实验设置：
作者构建了一个实验原型，用于在户外、直射阳光和恶劣环境条件下操作。该原型使用改进后的全场迈克尔逊干涉仪，将太阳光反射到光轴平行，并将输入照明分成两个光束，一个向场景臂传播，另一个向参考臂传播。参考臂是一个平面镜，安装在一个可变距离的平移台上，可以改变镜子与分束器之间的距离。反射后，两束光在分束器处结合，并向具有二维传感器的相机传播。

#### 9. 实验结果和分析：
作者使用实验原型展示了被动成像能力，例如对间接照明具有鲁棒性的微米级深度感知、仅直接成像和透过散射体成像。他们重建了一部分具有多个电阻器、焊盘和轨迹的树莓派电路板，尽管在户外恶劣环境条件下操作。该技术重建了PCB轨迹和通孔等细节，使得对于即使使用主动照明技术（半透明、金属、被薄散射体遮挡的物体）也具有像素级横向分辨率和微米级轴向分辨率的被动深度扫描成为可能。作者在补充材料和项目网站中提供了设置细节、重建代码和数据，以促进未来研究。


# Paper:43     基于部分离散扩散过程的通用深度三维形状先验模型



#### 1. Title: 
Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process

#### 2. Authors: 
Yuhan Li, Yishun Dou, Xuanhong Chen, Bingbing Ni, Yilin Sun, Yutian Liu, Fuzhen Wang

#### 3. Affiliation: 
第一作者：上海交通大学

#### 4. Keywords: 
3D shape generation, diffusion models, VQ-VAE, multi-frequency fusion module

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Generalized_Deep_3D_Shape_Prior_via_Part-Discretized_Diffusion_Process_CVPR_2021_paper.html  Github: https://github.com/colorful-liyu/3DQD

#### 6. Summary : 
- (1):本文旨在设计一个通用的三维形状生成先验模型，以满足多种三维任务的需求，包括无条件形状生成、点云补全和跨模态形状生成等。

- (2):现有的三维形状建模方法存在一些问题，如难以适应多种任务、难以生成高质量的形状等。本文提出了一种基于离散扰动扩散生成器的三维形状先验模型，通过离散编码来索引局部几何信息，从而消除不同模态之间的内在结构偏差，实现良好的跨模态数据对齐。同时，引入多频融合模块来抑制高频形状特征的波动，从而产生更平滑、高保真度的样本。

- (3):本文提出的3DQD先验模型是一个统一、概率化和强大的骨干，适用于无条件形状生成和多个条件形状完成应用。通过VQ-VAE编码和离散扩散生成器，实现了高效的形状生成。同时，多频融合模块增强了细粒度的三维表面建模。

- (4):在各种形状生成任务上进行了全面和多样化的下游实验，证明了所提出的3DQD先验模型能够有效地帮助合成高保真度、多样化的形状，优于多种最先进的方法。
#### 7. 方法详细介绍：
本文提出了一种基于离散扩散生成器的通用深度三维形状先验模型。该方法采用离散扩散生成器来建模形状组件的联合分布，并采用多频融合模块来抑制高频异常值并提高结果的平滑性。该方法还使用无分类器的指导来进行多类别生成任务，以丰富样本的多样性而不影响保真度。该方法在三个主流三维形状生成任务上进行了评估，并取得了最先进的性能。

#### 8. 实验设置：
本文使用ShapeNet中的三个流行类别（飞机、椅子、汽车）作为主要训练数据集。输入数据与DISN中的T-SDF文件格式相同，并遵循训练-测试分割。本文使用1-NNA作为主要评估指标，使用Chamfer距离（CD）和Earth Mover距离（EMD）来衡量形状生成质量和多样性。本文还使用DISN提供的训练/测试分割在ShapeNet数据集上进行了形状补全任务的评估。

#### 9. 实验结果与分析：
本文提出的方法在各种三维形状生成任务上展示了出色的生成能力和卓越的形状生成质量，并进行了充分的实验和分析。该模型实现了多样化和准确的形状生成，并在文本和形状模态之间实现了良好的对齐。本文还展示了3DQD模型可以扩展到广泛的下游应用，作为一个通用的先验模型，几乎不需要调整。下游应用包括去噪条件生成、文本驱动的形状编辑和单视角重建。本文提供了视觉和定量结果来支持所提出模型的性能。


# Paper:44     基于排名的正则化方法：在高真正例率下降低关键罕见类别的假阳性率



#### 1. Title: 
Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate

#### 2. Authors: 
Kiarash Mohammadi, He Zhao, Mengyao Zhai, Frederick Tung

#### 3. Affiliation: 
Kiarash Mohammadi: Mila, Université de Montréal; He Zhao, Mengyao Zhai, Frederick Tung: Borealis AI

#### 4. Keywords: 
Imbalanced learning, ranking-based regularization, false positives, critical rare classes, deep neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mohammadi_Ranking_Regularization_for_Critical_Rare_Classes_Minimizing_False_Positives_at_CVPR_2021_paper.html  Github: None

#### 6. Summary: 
- (1):本文研究的背景是在现实世界中，关键类别往往是罕见的，漏检的代价非常高昂，因此需要在高真正例率下运行系统，这可能需要容忍高假阳性率。
- (2):过去的方法主要是基于成本函数、模型输出和数据重采样等方面进行处理，但这些方法并没有考虑到在高真正例率下需要优先降低假阳性率的问题。本文提出了一种基于排名的正则化方法，通过对神经网络的训练目标添加一个简单而有效的排名正则化项，使得神经网络能够更好地降低假阳性率。该方法不仅有效地降低了假阳性率，而且还能够与传统的不平衡学习方法相互补充。
- (3):本文提出的方法是一种基于排名的正则化方法，通过对神经网络的训练目标添加一个简单而有效的排名正则化项，使得神经网络能够更好地降低假阳性率。该方法不仅有效地降低了假阳性率，而且还能够与传统的不平衡学习方法相互补充。本文的创新点在于提出了一种新的正则化方法，使得神经网络能够更好地降低假阳性率，同时在三个公共基准测试集上取得了最新的最佳性能。
- (4):本文的方法在三个公共基准测试集上进行了实验，结果表明，该方法不仅有效地降低了假阳性率，而且还能够与传统的不平衡学习方法相互补充，取得了最新的最佳性能。因此，该方法的性能能够支持他们的目标。
#### 7. 方法详细介绍：
本文提出了一种名为Ranking Regularization（RankReg）的方法，用于解决需要在高真正例率下操作时最小化假正例的挑战。该方法涉及将基于排名的正则化项添加到通常的神经网络训练目标中。正则化项对于排名较低的正样本施加越来越大的惩罚，这些正样本是在网络分类分数的排序列表中排名较低的，这有助于提高最难的正样本的分数。该方法易于实现，并可与其他不平衡学习方法相结合。

#### 8. 实验设置：
本文在三个广泛探索的数据集上进行实验，分别是CIFAR-10和100以及Melanoma，以评估所提出的方法。这些数据集都是具有关键稀有类的不平衡数据集。实验旨在展示所提出的正则化项如何与传统的不平衡学习损失相辅相成，并在高真正例率操作设置中实现了最先进的性能。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的方法在三个数据集上均优于现有的最先进方法。在二元不平衡CIFAR-10和CIFAR-100数据集上，所提出的方法在98％TPR下实现了47.1％和45.6％的FPR，分别比最佳基线高4.9％和4.4％。在Melanoma数据集上，所提出的方法在92.8％TPR下实现了20.9％的FPR，比最佳基线高1.5％。本文还评估了方法在标签噪声存在的情况下的鲁棒性，并在更不平衡的情况下测试了模型。此外，本文还介绍了所提出方法的多类扩展，该方法在1：100不平衡比率设置下表现更好，在1：200设置下表现相当。


# Paper:45     基于参数隐式面部表示的音频驱动面部再现



#### 1. Title: 
Parametric Implicit Face Representation for Audio-Driven Facial Reenactment

#### 2. Authors: 
Ricong Huang, Peiwen Lai, Yipeng Qin, Guanbin Li

#### 3. Affiliation: 
第一作者：中山大学计算机科学与工程学院

#### 4. Keywords: 
Audio-driven facial reenactment, parametric implicit representation, 3D face models, contextual audio to expression encoding, conditional image synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Parametric_Implicit_Face_Representation_for_Audio-Driven_Facial_Reenactment_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是音频驱动的面部再现技术，该技术在电影制作、虚拟人物和视频会议等领域有广泛应用。

- (2):现有的面部再现方法要么使用显式中间面部表示（例如2D面部标记或3D面部模型），要么使用隐式中间表示（例如神经辐射场），因此在可解释性和表现力之间存在权衡，因此在可控性和结果质量之间存在权衡。本文提出了一种新的参数隐式面部表示，并提出了一种新的音频驱动面部再现框架，既可控又能生成高质量的说话人头像。具体而言，我们的参数隐式表示使用3D面部模型的可解释参数来参数化隐式表示，从而兼具显式和隐式方法的优点。此外，我们提出了几种新技术来改进我们框架的三个组成部分，包括：i）将上下文信息纳入音频到表达（参数）编码中；ii）使用条件图像合成来参数化隐式表示，并使用创新的三平面结构进行高效学习；iii）将面部再现形式化为条件图像修复问题，并提出一种新的数据增强技术来提高模型的泛化能力。实验结果表明，我们的方法可以生成比以前的方法更逼真的结果，更符合说话者的身份和说话风格。

- (3):本文提出了一种新的参数隐式面部表示，该表示兼具显式和隐式方法的优点，从而打破了先前在中间面部表示的可解释性和表现力之间的权衡，为可控和高质量的音频驱动面部再现铺平了道路。我们提出了一种新的框架，包括三个组成部分：i）上下文音频到表达（参数）编码；ii）隐式表示参数化；iii）使用参数隐式表示进行渲染。我们的方法在三个方面进行了改进：i）使用基于变压器的编码器架构来捕获输入音频的长期上下文，使生成的说话人头像更加一致和自然；ii）使用新颖的条件图像合成方法来参数化隐式表示，并创新地使用EG3D提供的三平面生成器进行高效学习；iii）将面部再现形式化为条件图像修复问题，以在目标人物的头部和躯干之间实现一致和自然的“混合”。此外，我们观察到模型略微过拟合于由配对音频和视频组成的训练数据，导致所得到的说话人头像存在抖动，其唇部运动需要与未见过的输入音频同步。为了帮助我们的模型更好地泛化并产生更稳定的结果，我们进一步提出了一种简单而有效的数据增强策略，用于我们的渲染组件。

- (4):本文提出
#### 7. 方法详细介绍：
本文提出了一种新的参数化隐式人脸表示方法，用于音频驱动的面部再现。该方法使用条件图像合成范例，将隐式人脸表示参数化为可解释的三维可塑模型（3DMM）参数。该框架由三个组件组成：上下文音频到表情编码、隐式表示参数化和参数化隐式表示渲染。上下文音频到表情编码组件采用基于Transformer的编码器架构，捕捉输入音频的长期上下文。隐式表示参数化组件使用一种新颖的条件图像合成方法，通过创新的三平面生成器实现了隐式表示的参数化，以实现高效的学习。参数化隐式表示渲染组件将面部再现视为基于参数化隐式表示的图像修补问题，以实现目标人物头部和躯干的一致自然“融合”。

#### 8. 实验设置：
本文在三个数据集（HDTF、Testset 1和Testset 2）上进行了实验。视频以每秒25帧的速度提取，同步音频波形以16K Hz频率采样。视频被裁剪以使面部居中，并调整大小为512×512的分辨率。每个帧的身份、表情和姿势参数使用3DMM提取。使用现成的分割方法获取每个帧的解析映射。评估指标包括唇部运动准确性、唇语同步、清晰度和图像质量。

#### 9. 实验结果和分析：
本文进行了消融实验，以评估所提出的参数化隐式表示渲染组件和抖动减少技术的有效性。实验结果表明，没有参数化隐式表示渲染组件时，躯干在几秒钟内变化很快，产生不自然的结果。没有抖动减少时，渲染组件无法将IF与IM对齐，从而产生不自然的视频。本文还进行了用户研究，以基于唇语同步、图像质量和视频真实性评估面部再现结果。研究表明，与其他最先进的方法相比，所提出的方法在所有三个标准中得分最高或次高。


# Paper:46     在低表示能力模型中利用时间上下文



#### 1. Title: 
Leveraging Temporal Context in Low Representational Power Regimes

#### 2. Authors: 
Camilo L. Fosco, SouYoung Jin, Emilie Josephs, Aude Oliva

#### 3. Affiliation: 
第一作者：麻省理工学院计算机科学与人工智能实验室

#### 4. Keywords: 
Computer vision, low-parameter models, action recognition, action anticipation, Event Transition Matrix

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2021_paper.html  Github: https://camilofosco.com/etm_website

#### 6. Summary : 
- (1):本文研究计算机视觉模型在低参数情况下的性能提升问题，探讨是否可以通过提供关于目标领域中这些统计规律的额外信息来提高低表示能力模型的性能。

- (2):过去的方法通常是从训练中自动学习这些规律，但这需要大量的计算资源。本文提出了一种新的方法，即在训练时使用事件转移矩阵（ETM）来补充低参数模型，以提高其性能。本文的方法在动作识别和动作预测领域进行了实验，利用动作通常嵌入在典型的序列中的事实，证明了本文方法的有效性。

- (3):本文提出了事件转移矩阵（ETM）的概念，它捕捉了给定动作的时间上下文，即它前面或后面每个其他动作发生的可能性。本文的方法通过在训练过程中使用ETM来提高模型的性能，从而使低参数模型能够更好地利用数据中的统计规律。

- (4):本文的方法在多个数据集和模型架构上进行了测试，证明了在训练时使用ETM可以提高低参数模型的性能。本文的方法在动作识别和动作预测任务中取得了良好的性能，特别是对于较小的模型，效果更为显著。
#### 7. 方法详细介绍：
本文提出了一种基于事件转移矩阵（ETM）的方法，用于捕捉给定动作的时间上下文。首先，将训练集中的动作标签转化为事件序列，然后计算事件转移矩阵，其中每个元素表示一个事件在另一个事件之前发生的概率。为了考虑事件之间的时间距离，使用衰减函数对矩阵进行更新。在训练过程中，将任意编码器与过去和未来模块相结合，并最小化目标函数。该方法可以应用于多种模型架构中。

#### 8. 实验设置：
本文在三个数据集上进行了实验：EPIC-KITCHENS-100（EK100）、EGO4D LTA（EGO4D）和EGTEA Gaze+。使用训练集数据构建了事件转移矩阵。编码器使用Eq. 5中的损失函数进行训练，并设置ωq = ωc = ωr = 1/3。使用Ranger21优化器进行训练，学习率为0.01，余弦退火，20个epoch的半周期。批量大小为32，权重衰减为0.0001，适用于p = 0.5的dropout。

#### 9. 实验结果与分析：
本文提出的方法在EK100数据集上的动作识别性能优于基线MoViNet A0。表2显示了动词和名词分类以及动作准确性的前1精度。还显示了使用ETM框架训练的模型的过去和未来预测的平均绝对误差（MAE）。分别分析了冻结预训练编码器并仅训练解码器以及编码器与解码器一起微调的两种设置下系统的性能。还分析了动作预测实验的性能，并将系统的性能与AVT框架进行了比较。


# Paper:47     Vote2Cap-DETR：端到端的3D稠密字幕生成



#### 1. Title: 
End-to-End 3D Dense Captioning with Vote2Cap-DETR

#### 2. Authors: 
Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, Tao Chen

#### 3. Affiliation: 
第一作者：复旦大学

#### 4. Keywords: 
3D dense captioning, transformer, object detection, set prediction, attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2021_paper.html  Github: https://github.com/ch3cook-fdu/Vote2Cap-DETR

#### 6. Summary : 
- (1):本文研究的是3D稠密字幕生成问题，该问题需要将3D场景中的所有对象进行定位，并为每个对象生成描述性语句。这个问题具有挑战性，因为点云的稀疏性和物体的混乱分布。 

- (2):现有的方法采用“检测-描述”流程，需要大量手工制作的组件，这些组件在不同场景中的物体空间和类别分布中会产生次优性能。本文提出了一种基于Transformer的Vote2Cap-DETR框架，该框架具有以下优点：1）不需要大量手工制作的组件，采用全Transformer编码器-解码器架构，具有可学习的投票查询驱动的对象解码器和以集合预测方式生成稠密字幕的字幕解码器。2）与两阶段方案相比，我们的方法可以在一个阶段内执行检测和字幕生成。3）在两个常用数据集ScanRefer和Nr3D上进行的大量实验表明，我们的Vote2Cap-DETR在CIDEr@0.5IoU方面分别超过当前最先进的技术11.13％和7.11％。 

- (3):本文提出了一种全Transformer编码器-解码器架构的Vote2Cap-DETR，将3D稠密字幕生成视为集合预测问题，直接将解码器的输出馈送到定位头和字幕头中。通过将每个目标实例及其语言注释与查询一一对应匹配，将3D稠密字幕生成视为集合预测问题，从而为提议提供更具有区分性的特征表示，以识别3D场景中的每个独特对象。此外，本文还提出了一种新颖的投票查询驱动的解码器，以引入空间偏差，以更好地定位混乱的3D场景中的对象。 

- (4):本文在两个常用数据集ScanRefer和Nr3D上进行了大量实验，证明了我们的方法在3D稠密字幕生成方面的优越性能。Vote2Cap-DETR在CIDEr@0.5IoU方面分别超过当前最先进的技术11.13％和7.11％。这表明了全Transformer架构与复杂的投票头和字幕头可以激发许多3D视觉和语言任务的优越性。
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的一阶段模型Vote2Cap-DETR，用于3D密集字幕生成。该方法采用全Transformer编码器-解码器架构，通过可学习的投票查询驱动对象解码器和生成描述性语句的字幕解码器，生成一组盒子预测和句子，定位和描述点云中的每个对象。Vote2Cap-DETR将3D密集字幕生成视为集合预测问题，并将解码器的输出直接馈入定位头和字幕头中，以并行方式生成盒子和字幕。该方法还引入了一种新颖的投票查询驱动解码器，以引入空间偏差，以更好地定位杂乱的3D场景中的对象。

#### 8. 实验设置：
本文在两个常用数据集ScanRefer和Nr3D上进行了广泛的实验，以评估所提出方法的性能。实验在单个NVIDIA Tesla V100 GPU上进行，内存为32GB。输入点云被体素化为32x32x32网格，批量大小设置为4。模型使用1e-4的学习率和1e-4的权重衰减进行100个epoch的训练。

#### 9. 实验结果和分析：
所提出的方法Vote2Cap-DETR在ScanRefer和Nr3D数据集上均取得了新的最优性能，分别比当前最先进的方法提高了11.13％和7.11％的CIDEr@0.5IoU。该方法在ScanRefer和Nr3D数据集上分别达到了73.77％ C@0.5和45.53％ C@0.5的最优性能。实验结果表明，Vote2Cap-DETR的全Transformer架构，以及复杂的投票头和字幕头对于3D视觉和语言任务具有优越性。


# Paper:48     SDC-UDA：用于切片方向连续交叉模态医学图像分割的体积无监督域自适应框架



#### 1. Title: 
SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation

#### 2. Authors: 
Hyungseob Shin, Hyeongyu Kim, Sewon Kim, Yohan Jun, Taejoon Eo, Dosik Hwang

#### 3. Affiliation: 
首位作者隶属于韩国延世大学电气与电子工程学院、口腔颌面放射学系、医学院放射科和临床影像数据科学中心。

#### 4. Keywords: 
Medical image segmentation, unsupervised domain adaptation, volumetric segmentation, self-training, pseudo-label refinement.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shin_SDC-UDA_Volumetric_Unsupervised_Domain_Adaptation_Framework_for_Slice-Direction_Continuous_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究背景是深度学习在医学图像分割领域的应用，由于获取像素级别的专家注释非常昂贵和费时，因此需要使用无监督域自适应方法来解决这个问题。
- (2):过去的方法通常使用2D域自适应方法，但是这些方法在切片方向上的预测不一致，导致在临床实践中难以使用。本文提出了一种新的体积无监督域自适应框架，可以在切片方向上获得连续的分割结果，从而提高了精度和临床实践的潜力。本文的方法包括自我注意力图像转换、伪标签精炼和体积自我训练等步骤，与以往的方法相比，本文的方法可以更好地利用医学成像的体积信息，从而提高了分割的连续性。
- (3):本文提出的方法是一种简单而有效的体积无监督域自适应框架，可以在切片方向上获得连续的分割结果。该方法包括自我注意力图像转换、伪标签精炼和体积自我训练等步骤。自我注意力图像转换使用了内部和间部切片自我注意力模块，以有效地生成具有邻域感知的合成目标体积。伪标签精炼模块可以自适应地增强伪标签的准确性，从而最大化自我训练的效果。体积自我训练可以进一步提高分割的准确性。本文的方法在多个公开数据集上进行了验证，并取得了最先进的分割性能。
- (4):本文的方法在多个公开数据集上进行了验证，包括常用的交叉模态医学图像分割数据集和小型多类结构的交叉模态分割数据集。实验结果表明，本文的方法不仅超过了以往方法的性能，而且在切片方向上的分割连续性也更好，可以提供更精确的临床分析。
#### 1. 资助
(1). 该研究得到了韩国延世大学人工智能研究生院项目（编号2020-0-01361）、韩国科学技术研究院项目（编号2E31051-21-204）和2022年韩国延世大学签名研究群集项目（编号2022-22-0002）的支持。

#### 2. 方法
(1). 本文提出了一种名为SDC-UDA的体积无监督领域自适应框架，用于切片方向连续的跨模态医学图像分割。该框架结合了切片内和切片间的自我注意图像转换、不确定性约束的伪标签细化和体积自我训练。其中，切片内和切片间的自我注意模块用于高效的医学图像转换，从而提高了合成体积中的解剖学保留和切片方向平滑性。体积自我训练采用不确定性约束的伪标签细化，自适应地增强伪标签的准确性，从而最大化医学图像分割的自我训练性能。该方法在多个公开的跨模态医学图像分割数据集上进行验证，取得了最先进的分割性能，同时相较于之前的研究，具有更好的切片方向连续性。

(2). 具体而言，该框架由编码器GE、切片内和切片间自我注意II-SA和解码器GD三个模块组成。输入是一个体积的三个连续切片，首先分别输入到2D CNN编码器GE中。编码特征GE(xt−1)、GE(xt)和GE(xt+1)随后一起作为连续帧的特征图输入到基于Transformer的注意力模块II-SA中。在II-SA之前，每个切片被嵌入到大小为p的小块z中，如zn t−1:t+1 = P(GE(xt−1:t+1))|n=1:N，其中N = WGE(x) · HGE(x)/p2。在基于Transformer的注意力模块II-SA中，存在两种类型的注意力Ainter和Aintra。在注意力模块之后，2D CNN解码器GD以与GE相同的方式重构部分体积集，构建翻译后的2.5D mini-volume。

#### 3. 实验设置
(1). 对于前庭神经瘤和耳蜗分割的CrossMoDA数据集，包含105个标记的增强对比度T1（ceT1）MRI扫描和105个未标记的高分辨率T2（hrT2）MRI扫描。领域自适应的方向是从ceT1到hrT2。在线评估了32个不可访问的hrT2扫描结果，通过官方排行榜进行评估。2017年多模态全心分割（MMWHS）挑战数据集用于心脏分割，包括20个MRI和20个CT体积。领域自适应是从MRI到CT，每种模态的80％用于训练，随机选择的20％（即4个体积）CT扫描用于评估。

#### 4. 实验结果与分析
(1). 该提出的SDC-UDA框架在多个公开数据集上均表现出优异的性能，包括CrossMoDA数据集和MMWHS数据集。与六种流行的UDA方法（包括CycleGAN、CyCADA、ADVENT、FDA、SIFA和PSIGAN）相比，该方法在定量和定性评估中均表现出更好的分割性能。该方法在非医学领域的UDA研究中也优于之前的研究，并且相较于最近的医学领域研究，该方法在多个任务中表现出更好的结果。此外，该方法还展示了优秀的切片方向连续性，相较于之前的研究在MMWHS心脏数据集上表现更好。


# Paper:49     通过学习压缩元数据进行原始图像重建



#### 1. Title: 
Raw Image Reconstruction with Learned Compact Metadata

#### 2. Authors: 
Yufei Wang, Yi Yu, Wenhan Yang, Lanqing Guo, Lap-Pui Chau, Alex C. Kot, Bihan Wen

#### 3. Affiliation: 
第一作者：新加坡南洋理工大学

#### 4. Keywords: 
Raw image reconstruction, metadata, deep learning, compression, sRGB

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Raw_Image_Reconstruction_With_Learned_Compact_Metadata_CVPR_2021_paper.html  Github: https://github.com/wyf0912/R2LCM

#### 6. Summary : 
- (1):本文研究的是如何通过学习压缩元数据来进行原始图像重建，以提高存储效率和图像重建质量。

- (2):过去的方法主要是通过设计采样掩模来压缩原始图像，但这种方法存在一些问题，如图像表示不够优化、元数据冗余等。本文提出了一种新的框架，通过学习压缩元数据来提高图像重建质量和存储效率。同时，本文还提出了一种基于sRGB的上下文模型，通过改进熵估计策略来提高重建质量、减小元数据大小和加快速度。

- (3):本文提出了一种基于编码的端到端学习框架，通过最小化重建损失和比特流成本来获得潜在特征。为了进一步提高速率失真性能，本文提出了一种基于可学习的顺序预测网络的sRGB引导上下文模型。与常用的自回归模型不同，该模型通过学习顺序掩码来减少步骤，从而使计算成本可行，同时保持可比较的性能。

- (4):本文在多个数据集上进行了实验，结果表明，与现有的最先进方法相比，本文提出的方法可以在使用更少的元数据的情况下实现更好的原始图像重建质量。
#### 7. 方法详细介绍：
本文提出了一种基于编码的端到端学习的原始图像重建框架。该方法通过同时最小化重建损失和比特流成本来获取潜在特征。该方法在特征空间中进行采样，并通过自适应分配获得更紧凑的元数据。该方法的sRGB引导上下文模型基于可学习的顺序预测网络，借助学习的顺序掩码需要更少的步骤。具体步骤包括：编码/解码过程、sRGB引导上下文模型和超先验模型。编码/解码过程将原始图像映射到潜在空间中，sRGB引导上下文模型基于迭代高斯熵模型，将潜在变量编码/解码为比特流。超先验模型基于保存的通道均值值将辅助变量编码/解码为比特流。该方法的优化目标同时最小化原始图像重建损失和潜在代码的编码长度。不同潜在代码的似然估计采用不同的策略。辅助潜在代码采用非参数化、完全因式分解的密度模型进行编码，每个通道的均值值保存到元数据中以减少冗余。潜在代码的分布采用sRGB引导上下文模型进行建模，包括可学习的顺序预测网络和迭代高斯熵模型。

#### 8. 实验设置：
本文使用NUS数据集和AdobeFiveK数据集评估了所提出方法的有效性。在ISP流水线之后，使用原始图像和渲染的sRGB图像。整个数据集随机分为训练集和验证集。对于AdobeFiveK数据集，模型训练100个epochs，对于NUS数据集，模型训练200个epochs。如果损失没有改善，则学习率将每20个epochs降低0.1倍。

#### 9. 实验结果和分析：
本文提出的方法在重建质量和编码效率方面均优于以往的基于元数据的SOTA方法。该模型可以自适应地为具有不同质量因子的JPEG图像分配不同的比特率。结果表明，与以往的SOTA方法相比，所提出的方法在元数据最少的情况下实现了最佳的重建质量。所提出的超先验变量建模的有效性通过在比特率和重建质量方面的改进得到验证。


# Paper:50     教师生成的空间注意力标签提高对比模型的鲁棒性和准确性



#### 1. Title: 
Teacher-generated spatial-attention labels boost robustness and accuracy of contrastive models

#### 2. Authors: 
Yushi Yao, Chang Ye, Junfeng He, Gamaleldin F. Elsayed

#### 3. Affiliation: 
Yushi Yao: Waymo (谷歌旗下自动驾驶公司)
Chang Ye, Junfeng He, Gamaleldin F. Elsayed: Google

#### 4. Keywords: 
Spatial attention, contrastive learning, self-supervised learning, teacher model, pseudo-labeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_CVPR_2021_paper.html  Github: https://github.com/google-research/google-research/tree/master/human%20attention

#### 6. Summary : 
- (1):本文研究的背景是深度学习模型在视觉任务上的表现不如人类，主要原因是模型缺乏人类视觉系统的空间注意力机制，即无法关注到图像中最重要的区域。
- (2):过去的方法主要是在有监督的任务中使用人类空间注意力信息，但是在自监督的表示学习中，如何使用人类空间注意力信息仍然是一个问题。本文提出了一种使用教师模型生成伪标签的方法，以指导对比学习模型学习空间注意力。该方法可以在不需要大量人工标注的情况下，提高模型的分类准确性和鲁棒性，并在图像检索任务中取得更好的性能。
- (3):本文提出的方法是使用教师模型生成伪标签，指导对比学习模型学习空间注意力。首先，使用有限的人类空间注意力标签训练教师模型，然后使用该模型为ImageNet生成伪标签。接着，在对比学习模型中添加一个简单的输出头，用于预测每个图像的注意力图，并使用教师模型生成的伪标签进行指导。实验结果表明，使用教师模型生成的伪标签可以提高对比学习模型的分类准确性和鲁棒性，并在图像检索任务中取得更好的性能。
- (4):本文的方法在ImageNet和ImageNet-C数据集上取得了更好的分类准确性和鲁棒性，并在ImageNet、ImageNet-C、CIFAR10和CIFAR10-C数据集上取得了更好的图像检索性能。这些结果表明，使用教师模型生成的伪标签可以有效地指导对比学习模型学习空间注意力，从而提高模型的性能。
#### 7. 方法详细介绍：
本文提出了一种对比学习框架，其中包含一个空间注意力分支，以提高对比模型的鲁棒性和准确性。该框架由两个分支组成：对比分支和空间注意力分支。对比分支与原始的SimCLR方法相同，通过对图像x进行增强以获得不同的变体xi和xj，并通过特征提取器骨干网络学习表示hi和hj，然后使用投影头将hi/hj映射到zi/zj，应用对比损失。对于空间注意力分支，它不仅以最终嵌入h为输入，还以早期中间层嵌入为输入，并预测空间注意力热图mp。损失函数L由两个术语组成：Lcontra，对比损失，和Lattn，注意力正则化损失。本文还提出了一种教师模型伪标签方法，以生成ImageNet基准测试的新空间注意力伪标签。

#### 8. 实验设置：
本文使用ImageNet基准数据集和Salicon数据集训练教师模型以预测人类空间注意力标签。然后使用教师模型为大型ImageNet基准测试生成空间注意力伪标签。通过评估从冻结的学习嵌入中的分类性能以及在图像检索任务上的性能来评估所学表示的质量。

#### 9. 实验结果和分析：
本文的实验结果表明，利用教师生成的空间注意力标签来指导对比模型训练的空间注意力引导SimCLR模型在干净图像和几乎所有噪声/损坏类型和级别上均优于基线模型。在三个任务（分类、鲁棒性和图像检索）中观察到的收益在下游任务中是一致的。然而，在分类方面的收益有限，这表明仅仅使用空间注意力可能不足以实现最先进的性能。


# Paper:51     Re2TAL：重连预训练视频骨干网络以实现可逆的时间动作定位



#### 1. Title: 
Re2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization

#### 2. Authors: 
Chen Zhao, Shuming Liu, Karttikeya Mangalam, Bernard Ghanem

#### 3. Affiliation: 
第一作者：沙特阿拉伯科技大学（KAUST）

#### 4. Keywords: 
Temporal action localization, reversible networks, video backbones, end-to-end training, memory efficiency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2021_paper.html  Github: https://github.com/coolbay/Re2TAL

#### 6. Summary : 
- (1):本文研究的是视频中的时间动作定位问题，由于GPU内存限制，大多数方法只能在预提取的特征上进行训练，而不能进行端到端的训练，因此需要一种新的方法来解决这个问题。

- (2):过去的方法通常需要在两个步骤中处理视频，即先提取特征，再训练本地化器。这种方法存在域漂移问题，而且不能进行端到端的训练。本文提出了一种新的方法，即将预训练的视频骨干网络进行重连，使其成为可逆的，从而实现端到端的训练，并且可以重复使用预训练的参数，大大减少了训练的计算量。

- (3):本文提出了一种基于可逆网络的端到端训练方法，通过对预训练的视频骨干网络进行重连，使其成为可逆的，从而实现了内存高效的训练。本文的创新点在于提出了一种重连机制，可以将任何具有残差连接的网络模块转换为可逆模块，而不需要改变任何参数。本文的方法在ActivityNet-v1.3和THUMOS-14数据集上取得了最新的最优结果。

- (4):本文的方法在ActivityNet-v1.3数据集上取得了37.01%的平均mAP，创造了最新的最优结果，在THUMOS-14数据集上，本文的方法在tIoU=0.5时取得了64.9%的mAP，超过了所有其他仅使用RGB模态的方法。本文的方法可以实现端到端的训练，大大提高了时间动作定位的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Re2TAL的可逆时间动作定位方法。该方法通过将预训练的视频骨干网络中的所有模块转换为可逆模块，从而使其可逆。在训练期间，仅存储每个阶段的最终输出，所有激活在反向传播期间重新计算。整个视频序列被视为一个单一的输入，骨干网络在一个时间点聚合所有帧以减少内存和计算成本。该方法显著降低了内存消耗，同时保留了完整的数据保真度，并支持非常深的网络。 

#### 8. 实验设置：
本文在两个数据集ActivityNet-v1.3和THUMOS-14上进行了实验，使用不同的tIoU阈值的平均精度（mAP）作为评估指标。实验中使用的骨干网络是Video Swin Transformers（Vswin）和Slowfast，本地化器是VSGN和ActionFormer。可逆骨干网络使用其非可逆对应物进行初始化，并在Kinetics-400上进行fine-tune，使用余弦退火学习率策略和Adamw（对于Vswin）和SGD（对于Slowfast）优化器进行fine-tune。Vswin骨干网络的学习率比本地化器低1个数量级，Slowfast骨干网络的学习率比本地化器低2个数量级。空间分辨率为224×224。

#### 9. 实验结果和分析：
本文表明，端到端训练对TAL的精度提升显著。端到端训练带来的精度增益与每个持续时间类别中的样本数量密切相关，短动作的改进更为显著。对于所有组，端到端训练减少了假阴性预测，对于较短的动作改进更为显著。Re2TAL模型的性能高于依赖牺牲视频分辨率以减少内存的原始模型。当仅增加网络深度时，Re2TAL几乎保持内存不变，而原始模型在网络变得更大时很容易超出内存。


# Paper:52     从单个RGB相机中捕捉人与物体之间交互的可见性感知跟踪



#### 1. Title: 
Visibility Aware Human-Object Interaction Tracking from Single RGB Camera

#### 2. Authors: 
Xianghui Xie, Bharat Lal Bhatnagar, Gerard Pons-Moll

#### 3. Affiliation: 
Xianghui Xie: Max Planck Institute for Informatics, Saarland Informatics Campus, 德国马普计算机科学研究所
Bharat Lal Bhatnagar: Max Planck Institute for Informatics, Saarland Informatics Campus, 德国马普计算机科学研究所
Gerard Pons-Moll: University of T¨ubingen, T¨ubingen AI Center, 德国图宾根大学

#### 4. Keywords: 
Human-Object Interaction, Monocular RGB Camera, Neural Field Reconstruction, Object Tracking, Occlusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xie_Visibility_Aware_Human-Object_Interaction_Tracking_From_Single_RGB_Camera_CVPR_2021_paper.html  Github: https://github.com/visinf/visibility-aware-human-object-interaction-tracking

#### 6. Summary : 
- (1):本文研究了从单个RGB相机中捕捉人与物体之间交互的3D信息，这对于机器人、图形和视觉等领域的许多应用非常重要。
- (2):现有的方法在物体被遮挡时性能显著下降，因为它们假设固定深度。本文提出了一种新方法，可以从单个RGB相机中跟踪3D人体、物体、接触点和相对平移，同时对重度遮挡具有鲁棒性。本文的方法建立在两个关键见解上：首先，将神经场重建（用于人和物体）的条件设置为每帧SMPL模型估计，这提高了神经重建的准确性并产生了连贯的相对平移。其次，可见帧中的人和物体运动为推断遮挡物体提供了有价值的信息。本文提出了一种新的基于Transformer的神经网络，明确使用物体可见性和人类运动来利用相邻帧对遮挡帧进行预测。基于这些见解，本文的方法能够在遮挡情况下稳健地跟踪人和物体。
- (3):本文提出了一种新方法，可以从单个RGB相机中跟踪3D人体、物体、接触点和相对平移，同时对重度遮挡具有鲁棒性。本文的方法建立在两个关键见解上：首先，将神经场重建（用于人和物体）的条件设置为每帧SMPL模型估计，这提高了神经重建的准确性并产生了连贯的相对平移。其次，可见帧中的人和物体运动为推断遮挡物体提供了有价值的信息。本文提出了一种新的基于Transformer的神经网络，明确使用物体可见性和人类运动来利用相邻帧对遮挡帧进行预测。本文的方法能够在遮挡情况下稳健地跟踪人和物体。
- (4):本文的方法能够在遮挡情况下稳健地跟踪人和物体，实验结果表明，本文的方法显著优于现有的最先进方法。本文的代码和预训练模型可在https://virtualhumans.mpi-inf.mpg.de/VisTracker上公开获取。
#### 7. 方法详细介绍：
本文提出了一种从单个RGB相机中跟踪人-物交互的新方法。该方法由三个主要组件组成：SMPL-T、SIF-Net和HVOP-Net。SMPL-T通过将SMPL拟合到视频序列中来获得相机空间中的SMPL网格，从而获得一致的平移。SIF-Net联合推理人、物和交互，同时具有一致的相对平移。HVOP-Net利用变换器并明确考虑人类运动和物体可见性，以在重度遮挡下恢复物体姿态。该方法在两个数据集上进行了评估，并且在准确性方面显著优于现有的方法。

#### 8. 实验设置：
本文的实验在BEHAVE和InterCap两个数据集上进行。BEHAVE数据集包含217个训练序列和82个测试序列，而InterCap数据集包含173个训练序列和38个测试序列。使用预测的SMPL和物体网格与地面真实值之间的Chamfer距离来评估性能。

#### 9. 实验结果与分析：
本文的实验结果表明，所提出的方法在两个数据集上均优于现有的联合人-物重建方法PHOSA和CHORE。与基线相比，所提出的方法在SMPL和物体重建方面实现了更低的误差。滑动窗口对齐（窗口大小为10s）比每帧对齐效果更好。HVOP-Net优于线性插值基线和最先进的平滑和填充方法。SIF-Net提高了联合人-物姿态估计的准确性。


# Paper:53     准确的几何数据对于密集三维视觉任务的重要性



#### 1. Title: 
On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks

#### 2. Authors: 
HyunJun Jung, Patrick Ruhkamp, Guangyao Zhai, Nikolas Brasch, Yitong Li, Yannick Verdie, Jifei Song, Yiren Zhou, Anil Armagan, Slobodan Ilic, Ales Leonardis, Nassir Navab, Benjamin Busam

#### 3. Affiliation: 
Technical University of Munich (慕尼黑工业大学)

#### 4. Keywords: 
dense 3D vision, sensor data, depth estimation, reconstruction, multi-modal datasets

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jung_On_the_Importance_of_Accurate_Geometry_Data_for_Dense_3D_CVPR_2021_paper.html  Github: https://github.com/Junggy/HAMMER-dataset

#### 6. Summary : 
- (1):本文研究了测距原理的优缺点，以及不同传感器测量数据的质量对于密集三维视觉任务的影响。 
- (2):过去的方法主要是使用单一传感器提供的数据作为地面真实值，但是不同传感器的测量误差和缺陷往往被忽略，这会导致模型偏差和泛化能力下降。本文提出了一种多模态数据集，用于评估不同传感器的性能，并发现了不同技术在日常环境中的泛化问题。 
- (3):本文提出了一种独特的相机架构，包括最流行的室内深度传感器，以及高精度的地面真实值，用于评估不同传感器的性能。本文的主要贡献在于提出了一种新的方法，可以量化传感器噪声的影响，并为改进密集视觉估计和目标数据融合铺平道路。 
- (4):本文的方法在深度估计和重建任务中取得了良好的性能，证明了传感器数据的准确性对于密集三维视觉任务的重要性。
#### 7. 方法详细介绍：
本文提出了一种自监督和半监督的方法，用于单目深度估计和隐式三维场景重建。自监督方法通过投影几何变换和目标帧与源帧之间的光度重建误差计算密集图像重建损失。半监督方法利用地面真实相机姿态来制定光度图像重建，并强制执行平滑性损失。本文还使用神经辐射场进行隐式三维场景重建，并使用额外的深度模态对问题进行规范化。

#### 8. 实验设置：
本文使用了一个独特的数据集，提供高度准确的深度、表面法线、6D物体姿态、实例掩码、相机姿态和密集场景网格，以及不同传感器数据的真实场景。本文使用不同的训练信号，包括I-ToF、D-ToF、主动立体、自监督和半监督设置，训练了一系列网络，用于单目深度估计和隐式场景重建。

#### 9. 实验结果与分析：
本文评估了不同训练信号下单目深度估计和隐式场景重建的性能。结果表明，自监督和半监督设置可以恢复光度挑战性物体中的信息，并优于ToF监督。多视角数据聚合无法重现高度可靠的3D重建，而使用不同深度模态的NeRF可以规范化问题并使传感器噪声可见。D-ToF先验可以改善半透明材料的NeRF重建，测试时的额外RGB数据可以减少MPI的影响并解决一些材料引起的深度伪影。


# Paper:54     面向实时LiDAR全景分割的中心聚焦网络



#### 1. Title: 
Center Focusing Network for Real-Time LiDAR Panoptic Segmentation

#### 2. Authors: 
Xiaoyan Li, Gang Zhang, Boyue Wang, Yongli Hu, Baocai Yin

#### 3. Affiliation: 
北京工业大学信息学部人工智能学院，多媒体与智能软件技术北京市重点实验室

#### 4. Keywords: 
LiDAR, panoptic segmentation, center focusing, proposal-free, real-time

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Center_Focusing_Network_for_Real-Time_LiDAR_Panoptic_Segmentation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是LiDAR panoptic segmentation，即将语义分割和实例分割结合起来，以实现自动驾驶的安全性。
- (2):过去的方法包括proposal-based和proposal-free两种，但proposal-based方法复杂且难以实现实时处理，proposal-free方法中的center modeling和center-based clustering存在问题。本文提出了一种proposal-free的center focusing network (CFNet)，通过center focusing feature encoding (CFFE)和center deduplication module (CDM)解决了这些问题。
- (3):本文提出了一种新的center focusing feature encoding (CFFE)来生成center-focusing feature maps，以更准确地预测实例分割结果。同时，CFNet将panoptic segmentation任务分解为语义分割和center offset regression，并提出了新的confidence score prediction来指示center offset regression的准确性。最后，通过center deduplication module (CDM)选择每个实例的一个中心来实现实例分割。本文的创新点在于CFFE和CDM的提出，以及CFNet的proposal-free架构。
- (4):在SemanticKITTI和nuScenes panoptic segmentation benchmarks上的实验表明，CFNet在准确性和速度方面均优于现有方法，并且比最高效的方法快1.6倍。
#### 1. 方法详细介绍：
本文提出了一种用于实时LiDAR全景分割的中心聚焦网络（CFNet）。CFNet将过程分解为四个步骤：1）使用现成的2D投影骨干提取2D空间上的特征；2）使用新颖的中心聚焦特征编码（CFFE）生成中心聚焦特征图，以更准确地预测实例中心；3）使用全景分割头将3D点和2D空间的特征融合以预测语义分割结果、中心偏移和中心偏移的置信度分数；4）使用后处理生成全景分割结果，其中新颖的中心去重模块（CDM）在移动的物体点上操作，为单个实例选择一个中心。

#### 2. 实验设置：
本文在nuScenes和SemanticKITTI LiDAR全景分割基准测试上评估了CFNet。实验在单个NVIDIA RTX 3090 GPU上进行。

#### 3. 实验结果与分析：
本文提出的CFNet在SemanticKITTI和nuScenes数据集上均取得了最先进的性能，并且比最高效的方法快1.6倍。在SemanticKITTI测试集上，CFNet的PQ和RQ分别比SCAN和Panoptic-PHNet高1.9和1.5。在nuScenes验证集上，CFNet的PQ、SQ和RQ分别比最具竞争力的Panoptic-PHNet高0.4、0.6和0.4。CFNet的运行时间为43.5毫秒（模型推理为41.2毫秒，后处理为2.3毫秒），比最高效的Panoptic-PHNet快1.6倍。可视化结果表明，CFNet可以区分相邻的行人或汽车，并准确地分割实例的边界。


# Paper:55     自动驾驶中的弱监督类别无关运动预测



#### 1. Title: 
Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving

#### 2. Authors: 
Ruibo Li, Hanyu Shi, Ziang Fu, Zhe Wang, Guosheng Lin

#### 3. Affiliation: 
第一作者：南洋理工大学S-Lab

#### 4. Keywords: 
Autonomous driving, motion prediction, weakly supervised learning, LiDAR point clouds, scene parsing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究自动驾驶中的弱监督类别无关运动预测问题，通过利用场景解析将移动前景与静态背景区分开来，提出了一种新的弱监督学习方法，使用完全或部分（1％，0.1％）注释的前景/背景二进制掩码作为监督，而不是使用昂贵的运动注释。
- (2):传统的运动预测方法通常通过目标检测、跟踪和轨迹预测来实现，但在遇到未知类别时可能会失败。为了解决这个问题，许多方法尝试使用点云的鸟瞰图来直接估计类别无关的运动。现有的真实运动数据大多是通过半监督学习方法或从人工注释的目标检测和跟踪数据中引导的。本文提出了一种弱监督学习方法，使用前景/背景掩码作为监督，以实现注释工作量和性能之间的良好平衡。
- (3):本文提出了一种两阶段的弱监督运动预测方法，第一阶段使用部分注释的前景/背景二进制掩码训练前景/背景分割模型，第二阶段使用前景/背景分割模型预测可能的移动前景，从而实现自监督运动预测。此外，为了实现鲁棒的自监督运动学习，本文设计了一种一致性感知Chamfer距离损失，通过利用多帧信息和显式抑制潜在的异常值来提高鲁棒性。
- (4):本文的实验结果表明，使用前景/背景掩码作为弱监督，本文的模型优于自监督模型，并且与一些监督模型表现相当。这进一步证明了本文的方法在注释工作量和性能之间实现了良好的平衡。
#### 7. 方法详细介绍：
本文提出了一种弱监督的类别无关运动预测方法，包括两个阶段。第一阶段是前景/背景（FG/BG）分割网络（PreSegNet），使用部分标注的FG/BG掩码进行训练。第二阶段是运动预测网络（WeakMotionNet），使用第一阶段生成的FG/BG分割结果进行自监督训练。WeakMotionNet的训练使用了一种新的一致性感知Chamfer距离（CCD）损失函数，以提高模型的鲁棒性。该方法在LiDAR点云数据集上进行了实验，取得了与监督方法相当的性能，同时避免了昂贵的运动注释。

#### 8. 实验设置：
本文在nuScenes和Waymo数据集上进行了实验，使用LiDAR点云作为输入。评估指标包括静态、慢速和快速速度组的平均和中位误差，以及前景/背景分割的准确性。实验中使用了部分标注的FG/BG掩码作为弱监督信号。FG/BG分割网络的训练使用了批量大小为16和初始学习率为0.0005的Adam优化器，训练40个epochs。

#### 9. 实验结果和分析：
本文提出的弱监督方法在nuScenes数据集上取得了与监督方法相当的性能，同时避免了昂贵的运动注释。该方法在Waymo数据集上也取得了有希望的结果。与自监督方法相比，该方法取得了更好的性能。本文还进行了消融实验，分析了各组件的有效性。实验结果表明，该方法在LiDAR点云中预测类别无关的运动具有很好的效果。


# Paper:56     CLAMP: 基于提示的对比学习连接语言和动物姿态



#### 1. Title: 
CLAMP: Prompt-based Contrastive Learning for Connecting Language and Animal Pose

#### 2. Authors: 
Xu Zhang, Wen Wang, Zhe Chen, Yufei Xu, Jing Zhang, Dacheng Tao

#### 3. Affiliation: 
第一作者：悉尼大学，澳大利亚

#### 4. Keywords: 
Animal pose estimation, contrastive learning, language model, cross-modal learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_CLAMP_Prompt-Based_Contrastive_Learning_for_Connecting_Language_and_Animal_Pose_CVPR_2021_paper.html  Github: https://github.com/xuzhang1199/CLAMP

#### 6. Summary : 
- (1):本文研究的是动物姿态估计，由于数据量有限和种内、种间差异大，现有的基于图像的方法面临挑战。因此，本文提出了一种基于预训练语言模型的方法，通过提供丰富的文本描述来促进动物姿态估计。 

- (2):过去的方法主要是基于图像的方法，但是在动物姿态估计任务中，由于不同动物实例之间存在大量的差异，这些方法很难泛化到动物姿态估计任务中。本文提出的方法通过预训练语言模型来提供文本描述，但是由于预训练模型只学习了一般的语言描述，因此需要将其适应于动物姿态估计任务。为此，本文提出了一种新的基于提示的对比学习方案，通过将文本提示适应于动物关键点，建立起预训练语言模型和动物姿态之间的有效联系。

- (3):本文提出的方法是一种基于提示的对比学习方案，通过将文本提示适应于动物关键点，建立起预训练语言模型和动物姿态之间的有效联系。为了解决预训练模型和动物姿态之间的差异，本文将适应过程分解为空间感知和特征感知两个过程，并相应地设计了两个新的对比损失。实验结果表明，本文提出的方法在受监督、少样本和零样本设置下均取得了最先进的性能，优于基于图像的方法。

- (4):本文提出的方法在动物姿态估计任务中取得了最先进的性能，能够有效地利用预训练语言模型的丰富先验知识来提高动物姿态估计的准确性。
#### 7. 方法详细介绍：
本文提出了一种名为CLAMP的跨模态动物姿态估计方法，它利用了CLIP预训练模型中的先验语言知识。CLAMP方法包括一组针对动物姿态估计的姿态特定文本提示和两个分解的自适应过程，用于利用先验语言知识。姿态特定文本提示是为动物姿态估计而设计的，轻量级提示编码器用于建模不同关键点的提示嵌入之间的关系并促进它们的交互。跨模态自适应分解为空间感知过程和特征感知过程，分别设计了空间级对比损失和特征级对比损失来约束两个过程。

#### 8. 实验设置：
本文在两个具有挑战性的数据集AP-10K和Animal-Pose上评估了CLAMP方法的性能，分别在监督学习、少样本学习和零样本学习三种设置下进行了评估。

#### 9. 实验结果和分析：
在AP-10K数据集上，CLAMP方法在监督学习、少样本学习和零样本学习三种设置下均取得了比基线方法更好的性能，超过了仅使用CLIP预训练模型的方法。在Animal-Pose数据集上，CLAMP方法也取得了比基线方法更好的性能。总之，CLAMP方法在连接语言和动物姿态估计方面表现出了很好的性能。


# Paper:57     全局能量学习用于少样本开放集识别



#### 1. Title: 
Glocal Energy-based Learning for Few-Shot Open-Set Recognition

#### 2. Authors: 
Haoyu Wang, Guansong Pang, Peng Wang, Lei Zhang, Wei Wei, Yanning Zhang

#### 3. Affiliation: 
第一作者：西北工业大学

#### 4. Keywords: 
Few-shot learning, open-set recognition, energy-based model, glocal energy-based score

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Glocal_Energy-Based_Learning_for_Few-Shot_Open-Set_Recognition_CVPR_2021_paper.html  Github: https://github.com/00why00/Glocal

#### 6. Summary : 
- (1):本文研究的是少样本开放集识别问题，该问题需要在只有少量样本的情况下，对已知类别的样本进行分类，同时能够拒绝来自未知类别的样本。这是一个具有实际价值的挑战性任务。

- (2):过去的方法主要是基于原型网络，通过学习度量来分类已知类别的样本，并通过伪开放集样本来识别未知类别的样本。但是，这些方法只考虑了样本的类别信息，而忽略了样本的像素信息。本文提出了一种新的能量模型，该模型由两个分支组成，一个是分类分支，用于学习度量以对已知类别的样本进行分类，另一个是能量分支，用于显式估计开放集概率。为了实现对开放集样本的全面检测，本文的模型利用了类别特征和像素特征来学习全局能量得分，其中使用类别特征学习全局能量得分，使用像素特征学习局部能量得分。模型被强制将大能量得分分配给与少量样本在类别特征或像素特征上偏离较大的样本，并将小能量得分分配给其他样本。实验结果表明，本文的模型在三个标准FSOR数据集上表现优异。

- (3):本文提出了一种新的FSOR框架，称为Glocal Energy-based Learning（GEL）。与以往的方法不同，GEL由两个分类组件组成：一个用于封闭集分类，另一个用于开放集识别。具体而言，除了在封闭集分类器中使用类别特征对封闭集样本进行分类外，GEL还利用类别特征和像素特征来学习新的基于能量的开放集分类器，其中使用类别特征学习全局能量得分，使用像素特征学习局部能量得分。GEL被强制将大能量得分分配给与少量样本在类别特征或像素特征上偏离较大的样本，并将小能量得分分配给其他样本。通过这样做，GEL可以检测与已知类别在高级抽象或细粒度外观上相似的未知类别样本。

- (4):本文的方法在三个标准FSOR数据集上取得了优异的性能，超过了现有方法，并实现了开放集识别和封闭集分类的双重目标。
#### 7. 方法详细介绍：
本文提出了一种Glocal Energy-Based Learning方法，用于Few-Shot Open-Set Recognition（FSOR）。该方法使用基于度量的元学习架构，通过共享特征提取器获取支持和查询样本的嵌入和特征映射。类别分支通过对支持嵌入进行平均来计算每个N个类别的原型，并使用自注意力模块增强类别原型。通过计算查询嵌入和增强类别原型之间的距离来测量类别相似度。像素级相似度模块用于考虑开放集样本的像素级信息。使用余弦相似度和类特征映射像素中的前k个最近邻来计算像素级相似度。能量模块将来自两个分支的类别级和像素级相似度结果集成，然后将其馈送到训练成为全局能量为基础的Few-Shot Open-Set Recognizer。使用基于边缘的能量损失来优化模型。

#### 8. 实验设置：
本文在三个数据集上评估了所提出的方法：miniImageNet、tieredImageNet和CIFAR-FS。每个数据集的训练、验证和测试集的类别不同。用于衡量FSOR方法有效性的指标是ACC和AUROC。

#### 9. 实验结果和分析：
所提出的Glocal Energy-based Learning（GEL）方法在Few-Shot Open-Set Recognition（FSOR）中取得了竞争性的闭集分类准确性，同时显著提高了开放集识别能力。多个数据集上的实验证明了所提出方法的有效性。报告了两个数据集中5-shot设置的AUROC，并将miniImageNet 5-way 1-shot中已知和未知样本的开放集得分分布与其他方法进行了比较。在两个不同数据集上进行的5-way 1-shot实验的消融研究结果表明，三个提出的模块都可以在保持闭集分类能力几乎不变的情况下提高开放集识别性能。还评估了公式9中k和公式10中Mk和Mu之间的距离对性能的影响。


# Paper:58     基于梯度扩散模型的高保真引导图像合成



#### 1. Title: 
High-Fidelity Guided Image Synthesis with Latent Diffusion Models

#### 2. Authors: 
Jaskirat Singh, Stephen Gould, Liang Zheng

#### 3. Affiliation: 
澳大利亚国立大学

#### 4. Keywords: 
Guided image synthesis, Latent diffusion models, Optimization, Cross-attention control

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Singh_High-Fidelity_Guided_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2021_paper.html  Github: https://github.com/1jsingh/gradop

#### 6. Summary : 
- (1):本文研究了基于用户涂鸦的可控图像合成问题，提出了一种新的基于梯度扩散模型的引导图像合成框架，解决了现有方法中存在的内在领域偏移问题。

- (2):现有的方法通常采用两种方式来解决可控图像合成问题，一种是使用语义分割地图进行条件训练，但这种方法非常耗时，需要大规模收集密集的语义分割标签；另一种是采用反演方法将输入的涂鸦映射到目标数据流形上，但是这种方法会导致内在领域偏移问题，生成的输出通常缺乏细节并且类似于目标域的简单表示。本文提出的方法通过将输出图像建模为一个受限制的优化问题的解来解决这个问题。

- (3):本文提出的方法将输出图像建模为一个受限制的优化问题的解，其中优化问题的约束条件包括：1）在使用自主绘画函数绘制x时，应该恢复与参考y相似的绘画，2）输出x应该位于文本提示定义的目标数据子空间中。本文还提出了一种基于交叉注意力的控制方法，使用户能够控制不同绘画区域的语义，而无需进行条件训练或微调。

- (4):本文的方法在人类用户研究中表现出了很好的效果，相比之前的最新技术，用户满意度得分提高了85.32%以上。
#### 7. 方法详细介绍：
本文提出了一种基于扩散的引导图像合成框架，将输出图像建模为一个带有约束的优化问题的解。优化问题的约束条件包括：1）在使用自主绘画函数对x进行绘画后，应恢复与参考图像y相似的绘画；2）输出x应位于由文本提示定义的目标数据子空间中。本文表明，虽然计算优化的精确解是不可行的，但可以通过估计x与子空间Sτtext中的单个随机样本xτtext之间的距离来获得近似解。然后，使用梯度下降在潜在空间中解决优化问题。该方法还包括一种修改后的引导图像合成方法，可以在每个输出中仅需要单个反向扩散传递即可获得同样高质量的输出。最后，该方法还包括一种控制绘画区域语义的方法，通过修改与目标语义标签对应的交叉注意力图，使其与所需绘画区域具有高重叠度。

#### 8. 实验设置：
本文未提及实验设置。

#### 9. 实验结果和分析：
本文将提出的方法与以前的引导图像合成方法进行比较，评估其对参考图像的忠实度和对目标域的逼真度。根据定量评估，提出的方法在忠实度和逼真度方面优于以前的方法。此外，用户研究表明，对于大多数输入，人类受试者更喜欢提出的方法而不是以前的方法。本文还分析了提出的方法在不同目标域中的泛化性能以及梯度下降步数变化时输出性能的变化。最后，本文分析了提出的方法的超出分布的泛化性能。


# Paper:59     Token Turing Machines



#### 1. Title: 
Token Turing Machines

#### 2. Authors: 
Michael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, Anurag Arnab

#### 3. Affiliation: 
第一作者：Michael S. Ryoo，Google Research

#### 4. Keywords: 
Token Turing Machines, sequential visual understanding, autoregressive Transformer model, external memory, online temporal activity detection, vision-based robot action policy learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ryoo_Token_Turing_Machines_CVPR_2021_paper.html  Github: https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing

#### 6. Summary : 
- (1):本文研究的背景是如何处理长序列的视觉输入，以及如何在有限的计算资源下对其进行处理。
- (2):过去的方法包括使用循环神经网络（RNNs）和变形金刚（Transformers）等模型，但它们在处理长序列时存在计算成本高和效率低的问题。本文提出了一种新的模型Token Turing Machines（TTM），它是一种带有外部内存的序列自回归Transformer模型，可以高效地处理长序列。
- (3):本文提出的TTM模型包括一个外部内存和一个处理单元，其中内存由一组token组成，可以通过Transformer进行高效的寻址、读取和写入。TTM模型的创新之处在于引入了一个token汇总模块，可以鼓励内存在读取和写入操作时专注于其历史的不同部分。此外，TTM模型的计算成本在每个时间步骤上是恒定的，使其可以高效地处理长序列。
- (4):本文在两个真实世界的序列视觉理解任务上展示了TTM模型的性能优于其他替代方案，包括其他为长序列设计的Transformer模型和循环神经网络。这两个任务分别是在线时间活动检测和基于视觉的机器人动作策略学习。
#### 7. 方法详细介绍：
本文提出了一种基于Token的图灵机（Token Turing Machine，TTM）作为神经图灵机（Neural Turing Machines，NTM）的现代化版本，用于处理序列。TTM使用基于Transformer的模型作为处理单元，并使用令牌汇总机制进行内存读写操作。TTM还使用位置嵌入将位置信息融合到令牌中。TTM的操作可以总结为Read(It, M t), Process(Zt), Write(M t, Ot, It)和Output(Ot)，其中每个函数在论文中都有详细讨论。

#### 8. 实验设置：
本文在Charades数据集上进行在线活动检测和AVA v2.2数据集上进行时空活动检测，评估了所提出的TTM。对于两个数据集，使用ViViT-B模型作为骨干网络，TTM作为处理单元。Charades数据集使用标准的“charades v1 localize”设置进行评估，其中从验证集中的每个视频均匀采样25帧，并计算平均精度（mAP）。AVA数据集使用其标准设置，同时使用Kinetics-400进行骨干网络预训练。

#### 9. 实验结果和分析：
本文比较了所提出的TTM与其他最先进的方法在Charades数据集上的性能。TTM实现了22.30的mAP，优于其他方法，如I3D + super-events和I3D + STGCN。本文还在AVA数据集上评估了TTM，并报告了与基线ViViT模型相比的改进性能。然而，当前文本中没有提到具体的性能指标。


# Paper:60     使用去噪扩散模型正则化神经辐射场



#### 1. Title: 
DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models

#### 2. Authors: 
Jamie Wynn, Daniyar Turmukhambetov

#### 3. Affiliation: 
Niantic（Niantic实验室）

#### 4. Keywords: 
Neural Radiance Fields, Denoising Diffusion Models, Regularization, Novel View Synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wynn_DiffusioNeRF_Regularizing_Neural_Radiance_Fields_With_Denoising_Diffusion_Models_CVPR_2021_paper.html  Github: https://github.com/nianticlabs/diffusionerf

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）在少量输入视图下的重建问题，由于场的几何和颜色场严重不足，因此需要引入正则化方法来提高重建质量。
 
- (2):过去的方法包括手工制作的先验和学习的先验，但是没有方法学习场的几何和颜色的联合概率分布。本文提出了一种基于去噪扩散模型（DDM）的先验方法，该方法可以学习场的几何和颜色的联合概率分布，并且可以用于NeRF的训练。与其他方法相比，本文的方法可以更好地约束场的几何和颜色，从而提高重建质量。

- (3):本文提出的方法是使用DDM作为颜色和几何的先验，DDM是在合成Hypersim数据集的RGBD补丁上训练的，可以预测颜色和深度补丁的联合概率分布的对数梯度。在NeRF训练期间，随机RGBD补丁被渲染，并将似然函数的梯度反向传播到颜色和密度场。在LLFF和DTU数据集上的评估表明，我们的学习先验可以提高重建几何的质量，并提高对新视图的泛化能力。

- (4):本文的方法在LLFF和DTU数据集上进行了评估，结果表明我们的方法可以提高重建几何的质量，并提高对新视图的泛化能力。本文的方法可以更好地约束场的几何和颜色，从而提高重建质量。
#### 7. 方法详细介绍：
本文提出了一种名为DiffusioNeRF的方法，该方法使用去噪扩散模型（DDM）对神经辐射场（NeRF）进行正则化。该方法通过优化密度场和颜色场来使用可微分渲染技术从任意相机合成场景视图。可以使用离散采样和累积透射函数估计光线的预期颜色和深度。通过优化密度和颜色场来减少光度重建损失。该方法还包括几个几何正则化器，例如权重的紧凑分布，权重的总和为1，以及仅在一个视图锥体中惩罚密度放置。本文进一步描述了DDMs，这是强大的生成模型，可以学习估计对数数据分布的梯度。DDMs通过对RGBD补丁的分布建模来模拟（σ，c）的先验。使用Hypersim进行训练，这是一个用于室内场景理解的逼真合成数据集，具有地面真实图像和深度图。DDMs用作得分函数估计器，根据对数后验损失函数对NeRF重建进行正则化。

#### 8. 实验设置：
本文使用LLFF和DTU数据集对提出的DiffusioNeRF方法进行了评估，用于新视角合成和3D重建。实验中，DiffusioNeRF方法的正则化损失函数使用DDM进行正则化，DDM在1个GPU上进行了650,000步的优化，批量大小为32。NeRF模型进行了12,000步的优化，其中前2500步使用λdist = 0进行优化，扩散时间参数τ从0.1平滑地插值到0。正则化器权重λDDM控制DDM正则化器的权重。NeRF模型使用Instant NGP的torch-ngp实现和tiny-cuda-nn后端进行训练。

#### 9. 实验结果和分析：
本文提出的DiffusioNeRF方法使用DDM作为颜色和几何的先验进行NeRF正则化。实验结果表明，DiffusioNeRF在LLFF和DTU数据集上均优于几何基线，并改善了新视角合成和3D重建的性能。消融实验表明，在NeRF拟合过程中将输入图像的RGBD补丁馈送到DDM中，减少DDM的训练数据量以及仔细调度τ和DDM梯度权重的重要性。


# Paper:61     最小距离分离哈希中心的深度哈希



#### 1. Title: 
Deep Hashing with Minimal-Distance-Separated Hash Centers

#### 2. Authors: 
Liangdao Wang, Yan Pan, Cong Liu, Hanjiang Lai, Jian Yin, Ye Liu

#### 3. Affiliation: 
中山大学计算机科学与工程学院

#### 4. Keywords: 
Deep Hashing, Hash Centers, Image Retrieval, Optimization

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Deep_Hashing_With_Minimal-Distance-Separated_Hash_Centers_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究大规模图像检索中的深度哈希方法；
 
- (2):现有的深度哈希方法通常使用随机采样的小批量中的成对或三元图像相似性来学习哈希函数，但存在训练效率低、数据分布覆盖不足和成对不平衡等问题。本文提出了一种优化方法，通过约束哈希中心之间的最小距离来找到哈希中心，从而解决了哈希中心可以任意接近的问题。同时，本文采用编码理论中的Gilbert-Varshamov界限来确定最小距离，以确保优化的可行性。本文提出的方法在三个图像检索数据集上进行了广泛的实验，表明该方法在检索性能方面优于现有的深度哈希方法；
 
- (3):本文提出了一种优化方法，通过约束哈希中心之间的最小距离来找到哈希中心，从而解决了哈希中心可以任意接近的问题。同时，本文采用编码理论中的Gilbert-Varshamov界限来确定最小距离，以确保优化的可行性。本文提出的方法在三个图像检索数据集上进行了广泛的实验，表明该方法在检索性能方面优于现有的深度哈希方法；
  
- (4):本文提出的方法在三个图像检索数据集上进行了广泛的实验，表明该方法在检索性能方面优于现有的深度哈希方法。本文的方法通过约束哈希中心之间的最小距离来找到哈希中心，从而解决了哈希中心可以任意接近的问题，同时采用编码理论中的Gilbert-Varshamov界限来确定最小距离，以确保优化的可行性。
#### 7. 方法详细介绍：
本文提出了一种名为“具有最小距离分离哈希中心的深度哈希”的方法。该方法分为两个阶段。第一阶段通过对训练数据进行聚类来获取哈希中心。最小距离参数设置为哈希中心之间的最小距离。第二阶段使用获得的哈希中心作为监督信息训练深度哈希网络。网络由三个块组成：ResNet-50骨干网络、哈希码层和三个损失函数。第一个损失函数旨在使图像的哈希码接近其类别的哈希中心，但远离其他类别的哈希中心。第二个损失函数是一种成对损失，使同一类别中的一对图像具有相邻的哈希码。第三个损失函数是减少量化误差。

#### 8. 实验设置：
本文在三个图像检索数据集上进行实验：ImageNet、NABirds和Stanford Cars。ImageNet数据集包含100个类别的143,495张图像，其中10,000张用于训练，5,000张用于测试查询图像，其余图像用作检索数据库。Stanford Cars数据集包含196个类别的16,185张图像。NABirds数据集包含555个类别的48,562张图像。对于所有方法，使用ResNet50预训练模型作为骨干网络。使用平均精度（MAP）和精度-召回曲线评估检索性能。

#### 9. 实验结果和分析：
本文提出的方法在三个数据集（Stanford Cars、NABirds和ImageNet）上的MAP检索性能优于基线方法。与最佳基线相比，所提出的方法的MAP结果相对增加了1.0% ∼ 9.6% / 1.6% ∼ 7.1% / 0.9% ∼1.2%。在高数量级的图像类别和短哈希码的情况下，所提出的方法取得了显著的改进。在三个数据集上，所提出的方法的精度-召回曲线也优于所有基线方法。


# Paper:62     最小化最大模型差异的可转移黑盒目标攻击



#### 1. Title: 
Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted Attacks

#### 2. Authors: 
Anqi Zhao, Tong Chu, Yahao Liu, Wen Li, Jingjing Li, Lixin Duan

#### 3. Affiliation: 
第一作者：电子科技大学计算机科学与工程学院

#### 4. Keywords: 
Adversarial attack, black-box targeted attack, model discrepancy, transferability, generative perturbations

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhao_Minimizing_Maximum_Model_Discrepancy_for_Transferable_Black-Box_Targeted_Attacks_CVPR_2021_paper.html  Github: https://github.com/Asteriajojo/M3D

#### 6. Summary : 
- (1):本文研究了黑盒目标攻击问题，提出了一种基于模型差异的攻击方法。攻击误差主要取决于替代模型的经验攻击误差和替代模型之间的最大模型差异。本文旨在回答一个已知模型生成的对抗样本在多大程度上可以成功攻击另一个未知模型的问题。本文首次提出了黑盒目标攻击的泛化误差界限，为保证攻击成功提供了严格的理论分析。

- (2):现有的黑盒攻击方法普遍假定对抗样本的可转移性，但对转移性的理论分析仍然缺乏。本文提出了一种基于模型差异的攻击方法，通过最小化替代模型之间的最大模型差异，生成高度可转移的对抗样本，从而提高攻击黑盒模型的成功率。与现有方法相比，本文提出的方法在多种攻击设置下均取得了显著的性能提升。

- (3):本文提出了一种新的黑盒目标攻击算法，称为最小化最大模型差异（M3D）攻击。该方法通过训练生成器来生成对抗样本，同时最小化替代模型之间的最大模型差异。在训练过程中，生成器和两个替代模型通过对抗学习来最小化模型差异。这样，生成器就能够生成对抗样本，这些样本对模型变化具有鲁棒性，从而能够成功攻击黑盒目标模型。

- (4):本文在ImageNet数据集上进行了广泛的实验，使用不同的分类模型进行攻击。实验结果表明，本文提出的方法在多种攻击设置下均取得了显著的性能提升，特别是在黑盒模型与替代模型之间存在较大模型差异的情况下，本文提出的方法表现出了更好的鲁棒性和可转移性。
#### 7. 方法详细介绍：
本文提出了一种名为最小化最大模型差异（M3D）的方法，用于生成高度可转移的黑盒目标攻击的扰动。该方法包括训练一个生成模型G和两个鉴别器D1和D2。生成器将干净图像映射到对模型变化具有鲁棒性的对抗性示例，而鉴别器则充当替代模型，并被训练以在对抗性示例上保持大的差异。该方法通过最小化假设集H中任意两个模型之间的最大模型差异来实现，该假设集包括在指定分类任务上表现良好的分类器。该方法使用一个min-max目标函数来实现，该函数涉及交替优化生成器和鉴别器一定数量的迭代次数。

#### 8. 实验设置：
实验在ImageNet数据集上进行，使用子集源设置和全源设置。在子集源设置中，使用10个类进行评估，在全源设置中使用1000个类。替代模型使用VGG19BN、DenseNet121和ResNet50架构进行初始化，黑盒模型使用不同的架构在ImageNet上进行训练，包括ResNet152和WRN-50-2。扰动预算设置为l∞ ≤ 16。

#### 9. 实验结果与分析：
本文提出的M3D方法在ImageNet数据集上的黑盒目标攻击中表现优异，超过了现有的最先进方法。结果表明，我们的对抗性示例不仅对模型架构的变化具有鲁棒性，而且对流行的防御机制也具有鲁棒性。M3D方法还成功地攻击了真实世界的计算机视觉系统Google Cloud Vision，揭示了真实世界系统的漏洞。消融研究证明了在M3D方法中最小化最大模型差异的有效性。还分析了扰动的鲁棒性和有效性，结果表明，生成的对抗性语义模式本身在不同模型之间具有很好的泛化性，而我们的M3D方法有助于将对抗性示例更接近目标类。


# Paper:63     将优势转化为劣势：一种基于认证鲁棒性启发的图神经网络对抗攻击框架



#### 1. Title: 
Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks

#### 2. Authors: 
Binghui Wang, Meng Pang, and Yun Dong

#### 3. Affiliation: 
Binghui Wang: 伊利诺伊理工学院计算机科学系 (Department of Computer Science, Illinois Institute of Technology)

#### 4. Keywords: 
Graph neural networks, adversarial attacks, certified robustness, evasion attacks, poisoning attacks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Turning_Strengths_into_Weaknesses_A_Certified_Robustness_Inspired_Attack_Framework_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究了图神经网络（GNNs）的对抗攻击问题，特别是针对测试时的逃避攻击和训练时的污染攻击。 
- (2):现有的攻击方法已经取得了一定的成果，但是本文提出了一种基于认证鲁棒性的攻击框架，可以进一步提高攻击性能。该框架首次从攻击者的角度出发，利用认证鲁棒性的特性来更好地攻击GNNs。 
- (3):本文提出的攻击框架包括三个部分：首先，基于随机平滑，推导出节点对于逃避攻击和污染攻击的认证扰动大小；其次，设计了一种认证鲁棒性启发式攻击损失，将其与现有攻击方法相结合，产生了认证鲁棒性启发式攻击；最后，设计了认证鲁棒性启发式攻击框架，基于认证鲁棒性启发式攻击损失生成对GNNs的对抗图扰动。 
- (4):在多个基准数据集上的实验结果表明，本文提出的攻击框架可以显著提高现有攻击的性能。
#### 7. 方法详细介绍：
本文提出了一种基于认证鲁棒性的攻击框架，用于对图神经网络（GNNs）进行攻击。该框架包括两种类型的攻击：图污染攻击和图逃逸攻击。对于图污染攻击，本文引入了一个双层优化问题来扰动训练阶段的图形，然后使用随机平滑来证明GNN模型对扰动图的鲁棒性。对于图逃逸攻击，本文提出了一种替代优化问题来最大化训练节点上的损失，并使用随机平滑来证明GNN模型对敌对扰动的鲁棒性。本文还详细描述了如何在实践中计算认证扰动大小。

具体而言，该攻击框架包括三个部分：（i）基于随机平滑分别推导出节点对图逃逸和污染攻击的认证扰动大小，（ii）通过将经过认证的扰动大小定义为节点权重，修改经典的节点损失函数，设计出认证鲁棒性启发式攻击损失，（iii）设计认证鲁棒性启发式攻击框架，生成针对GNNs的敌对图扰动。该框架可以无缝地插入到任何现有的图逃逸和污染攻击中。

#### 8. 实验设置：
本文在多个基准图数据集上评估了所提出的攻击框架，包括Cora、Citeseer和BlogCataLogs。选择Graph Convolutional Network（GCN）作为目标GNN模型。将数据集分为10％的训练节点，10％的验证节点和80％的测试节点。验证节点用于调整超参数，测试节点用于评估攻击性能。攻击在训练/评估/测试节点的5个不同拆分上重复进行，并报告测试节点上的平均攻击准确性。扰动预算设置为图中边的总数的20％（攻击前）。攻击在PyTorch上实现，并在具有96个核心3.0GHz CPU，768GB RAM和8个Nvidia A100 GPU的Linux服务器上运行。

#### 9. 实验结果和分析：
所提出的攻击框架在所有数据集上都有效地提高了基本攻击性能。认证鲁棒性启发式攻击框架可以在固定的扰动预算下误分类更多节点。例如，在攻击Cora上的GCN并且扰动比例为20％时，CR-CE-PGD和CR-CW-PGD相对于CE-PGD和CW-PGD逃逸攻击分别获得了7.0％和5.6％的增益。此外，CR-Minmax和CR-MetaTrain相对于Minmax和MetaTrain污染攻击分别获得了12.2％和10.3％的增益。扰动边缘与节点的认证扰动大小的分布显示，大多数扰动边缘与具有相对较小认证扰动大小的测试/训练节点相连。与其他权重设计策略相比，基于节点的认证鲁棒性的权重设计策略表现更好。

#### 论文总结：
本文提出了一种基于认证鲁棒性的攻击框架，用于对图神经网络进行攻击。该框架包括两种类型的攻击：图污染攻击和图逃逸攻击。攻击框架包括三个部分：（i）基于随机平滑分别推导出节点对图逃逸和污染攻击的认证扰动大小，（ii）通过将经过认证的扰动大小定义为节点权重，修改经典的节点损失函数，设计出认证鲁棒性启发式攻击损失，（iii）设计认证鲁棒性启发式攻击框架，生成针对GNNs的敌对图扰动。该框架可以无缝地插入到任何现有的图逃逸和污染攻击中。本文在多个基准图数据集上评估了所提出的攻击框架，并证明了其有效性。


# Paper:64     基于原型对比学习的弱监督域自适应语义分割



#### 1. Title: 
Weakly-Supervised Domain Adaptive Semantic Segmentation with Prototypical Contrastive Learning

#### 2. Authors: 
Anurag Das, Yongqin Xian, Dengxin Dai, Bernt Schiele

#### 3. Affiliation: 
第一作者：MPI for Informatics（德国马普计算机科学研究所）

#### 4. Keywords: 
Weakly-Supervised Domain Adaptive Semantic Segmentation, Prototypical Contrastive Learning, Unsupervised Domain Adaptative Semantic Segmentation, Semantic Segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2021_paper.html  Github: https://github.com/anurag-198/WDASS

#### 6. Summary : 
- (1):本文研究的是弱监督域自适应语义分割任务，该任务旨在通过利用目标域的弱标签来提高模型的性能。由于真实图像的像素级注释成本高昂，因此使用弱标签可以降低注释成本。
- (2):过去的方法主要是无监督域自适应语义分割，但与监督学习相比，性能差距较大。本文提出了一种使用不同弱标签（例如图像、点和粗标签）的通用框架，以减少这种性能差距。具体而言，本文提出了利用这些弱标签来学习更好的原型，从而改进类特征的对比对齐。本文的方法在各种基准测试中都取得了显著的改进，并且可以减少与监督学习之间的性能差距。
- (3):本文提出了一种新的框架，用于弱监督域自适应语义分割任务，该框架可以无缝地使用来自目标域的图像、点和粗标签。本文的方法利用不同的弱标签来构建更好的原型，并提出了源域和目标域的原型对比对齐方法，从而改进了语义分割任务的域自适应性能。
- (4):本文的方法在各种基准测试中都取得了显著的改进，并且可以减少与监督学习之间的性能差距。在粗标签注释的情况下，本文的方法甚至优于监督学习，展示了弱标签在域自适应任务中的潜力。
#### 7. 方法详细介绍：
本文提出了一种弱监督域自适应语义分割方法，使用原型对比学习。该方法包括自训练生成伪标签、边界损失、弱标签引导的原型学习和基于原型的对比学习。自训练使用预测概率为目标域中未标记区域生成伪标签。边界损失用于从源域学习边界信息。弱标签引导的原型学习使用目标数据中伪标签的特征平均值计算原型。基于原型的对比学习使用学生网络的特征与教师网络的原型进行对齐，使用域内对齐和域间对齐两种方式。

#### 8. 实验设置：
本文在两个标准域自适应基准测试集GTA-5 → Cityscapes和Synthia → Cityscapes上评估了所提出的方法。对于Cityscapes，使用了来自训练集的2975张图像，对于GTA-5和Synthia，分别使用了24966和9400张训练图像。使用ImageNet预训练的ResNet-101作为骨干网络的DeepLabv2分割模型，并将Cityscapes图像调整为1024x512，GTA-5图像调整为1280x760分辨率。采用SGD优化器和多项式学习率调度器，动量为0.9，权重衰减率为0.0001。模型训练了150000次迭代，并在训练中使用了类混合增强。

#### 9. 实验结果与分析：
所提出的方法在GTA5→Cityscapes和Synthia→Cityscapes设置中，对于所有三种弱标签（图像、点和粗标签），都显著优于以前的方法。该方法弥合了UDA和监督学习之间的性能差距，对于GTA5→Cityscapes设置的粗标注，性能优于监督学习1.7mIoU。点和图像标签的性能差距与监督学习相比为2.7和5.9，而与UDA设置相比为9.9。注释成本与性能比较表明，弱标签对于WDASS任务是具有成本效益的。


# Paper:65     基于运动引导的序列融合的高效三维点云序列物体检测



#### 1. Title: 
MSF: Motion-guided Sequential Fusion for Efficient 3D Object Detection from Point Cloud Sequences

#### 2. Authors: 
Chenhang He, Ruihuang Li, Yabin Zhang, Shuai Li, Lei Zhang

#### 3. Affiliation: 
香港理工大学 (The Hong Kong Polytechnic University)

#### 4. Keywords: 
3D object detection, LiDAR, point cloud sequence, motion-guided sequential fusion, proposal generation, bidirectional feature aggregation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/He_MSF_Motion-Guided_Sequential_Fusion_for_Efficient_3D_Object_Detection_From_CVPR_2021_paper.html  Github: https://github.com/skyhehe123/MSF

#### 6. Summary:
- (1):本文研究的是基于点云序列的三维物体检测，主要应用于自动驾驶等领域。点云序列提供了更丰富的三维信息，但也带来了更大的计算复杂度和数据处理难度。
- (2):现有的多帧检测器大多采用“检测和融合”框架，即从每个帧中提取特征并将它们融合以检测当前帧中的物体。然而，这种方法会导致冗余计算，因为相邻帧之间高度相关。本文提出了一种基于运动引导的序列融合方法，利用物体运动的连续性来挖掘有用的序列上下文，从而提高当前帧的物体检测效果。本文提出的方法不仅比其他多帧检测器更高效，而且具有更高的准确性。
- (3):本文提出了一种高效的运动引导序列融合方法，通过基于物体运动估计将当前帧的3D提案传播到前面的帧上，从而挖掘序列中的有用上下文。然后，从序列中采样感兴趣的点，并将其编码为提案特征。本文还提出了一种新颖的双向特征聚合模块，以促进跨帧提案特征之间的交互。此外，本文还通过基于体素的采样技术优化了点云池化，使得可以在几毫秒内处理数百万个点。本文提出的MSF方法在Waymo开放数据集的LEVEL1和LEVEL2测试集上实现了83.12％和78.30％的mAP，具有较高的准确性和效率。
- (4):本文提出的方法在Waymo开放数据集上进行了验证，取得了领先的准确性和速度。本文的方法通过运动引导的序列融合，避免了冗余计算和对前面帧结果的依赖，从而提高了检测效率。同时，本文提出的双向特征聚合模块和基于体素的采样技术也为点云序列的处理提供了新思路和新方法。
#### 7. 方法详细介绍：
本文提出了一种名为“运动引导的序列融合”（Motion-guided Sequential Fusion，MSF）的方法，用于从点云序列中高效地检测三维物体。该方法包括三个主要组件：运动引导的序列池化、基于区域的网络和高效的点云池化。MSF利用物体运动的连续性从点云序列中提取有用的上下文信息，提高当前帧的检测效果。MSF在当前帧上生成三维提议，并根据估计的速度将其传播到前面的帧。然后从序列中汇集感兴趣的点，并将其编码为提议特征。提出了一种新颖的双向特征聚合（Bidirectional Feature Aggregation，BiFA）模块，以促进提议特征在帧之间的交互。点云池化使用基于体素的采样技术进行优化，以提高效率。

#### 8. 实验设置：
本文在Waymo Open Dataset上进行验证。使用CenterPoint作为提议生成网络，预测当前帧的三维提议及其运动。将提出的MSF方法与其他最先进的多帧检测器在Waymo Open Dataset的LEVEL1和LEVEL2测试集上进行比较。

#### 9. 实验结果和分析：
MSF在Waymo Open Dataset的验证集上取得了领先的准确率，并在测试集上优于所有先前发布的方法。使用4帧和8帧的MSF模型分别实现了74.62％和75.13％的mAPH，创造了新的最先进技术。MSF在LEVEL1和LEVEL2上分别比CenterFormer高出3.4％APH和1.2％APH，使用相同数量的帧。MSF在几乎所有情况下都优于MPPNet，除了Vehicle LEVEL1的APH。在未来的研究中，作者计划将MSF扩展到在未来的帧上生成检测先验，以进一步减少多帧检测的总体计算量。


# Paper:66     探究SO(3)流形上的离散归一化流，用于概率旋转建模



#### 1. Title: 
Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling

#### 2. Authors: 
Yulin Liu, Haoran Liu, Yingda Yin, Yang Wang, Baoquan Chen, He Wang

#### 3. Affiliation: 
第一作者：北京大学

#### 4. Keywords: 
Normalizing flows, SO(3) manifold, probabilistic modeling, rotation, Mobius coupling layer, quaternion affine transformation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Delving_into_Discrete_Normalizing_Flows_on_SO3_Manifold_for_Probabilistic_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了在SO(3)流形上进行离散归一化流的问题，以实现对旋转的概率建模。旋转在计算机视觉、图形学和机器人领域中是一个重要的量，但当遮挡和对称性发生时，旋转可能会出现许多歧义，因此需要这样的概率模型。虽然在欧几里得空间中已经取得了很多进展，但是没有针对SO(3)流形量身定制的没有不连续或多对一映射的有效归一化流。

- (2):一些现有的方法依赖于欧几里得空间中的归一化流，并将其调整为处理旋转。但是，这些方法在SO(3)流形的特殊拓扑结构下存在问题。本文提出了一种新颖的SO(3)归一化流，通过结合基于Mobius变换的耦合层和四元数仿射变换来实现。通过多个Mobius耦合层的组合，增强了旋转归一化流的容量。为了进一步增加表达能力，我们在四元数空间中提出了一种仿射变换。本文提出的旋转归一化流不仅可以有效地表达SO(3)上的任意分布，还可以在给定输入观测值的情况下有条件地构建目标分布。

- (3):本文提出了一种在SO(3)流形上的离散归一化流，核心模块是一个Mobius耦合层和一个四元数仿射变换。在Mobius耦合层中，旋转矩阵的一列作为条件器，保持不变。另一列作为变换器，在条件器的条件下进行Mobius变换，而剩余的一列由叉积确定。通过组合多个Mobius耦合层，增强了旋转归一化流的容量。为了进一步增加表达能力，我们在四元数空间中提出了一种仿射变换。这种四元数仿射变换既可以作为全局旋转，也可以作为压缩或扩张局部似然的手段。本文提出的方法在无条件和有条件任务上均显著优于基线。

- (4):本文提出的旋转归一化流在无条件和有条件任务上均取得了优异的性能。实验结果表明，我们的方法能够有效地拟合具有不同形状的SO(3)上的目标分布，或者在给定输入图像条件下回归目标分布。本文提出的方法在两个任务上均优于所有基线。
#### 7. 方法详细介绍：
本文提出了一种基于SO(3)流形的离散正则化流模型，用于概率旋转建模。该模型由多个块组成，每个块包括一个Mobius耦合层和一个四元数仿射变换。Mobius耦合层用于转换旋转矩阵的正交向量，而四元数仿射变换用于转换旋转的四元数表示。Mobius耦合层利用神经网络参数化变换和多个Mobius变换的线性组合来增加表达能力。四元数仿射变换由4D向量的线性变换和投影到单位球S3组成。所提出的仿射变换满足反极性对称性，使其成为SO(3)上的双射变换。该模型可以通过迭代组合Mobius耦合层和四元数仿射变换来构建流模型，并在旋转矩阵表示和四元数表示之间切换。该模型还可以使用[34]中的方法进行条件扩展。

#### 8. 实验设置：
本文对三个数据集进行了实验：SYMSOL I和II、ModelNet10-SO3和Pascal3D+。实验使用负对数似然（NLL）损失进行训练，四元数仿射变换的可逆矩阵W被参数化为一个4×4无约束矩阵。本文还在SYMSOL数据集上进行了条件旋转回归实验，给定单个图像进行回归。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的旋转正则化流在所有数据集上都优于基线模型，表现出更好的对数似然和分布度量。实验结果证明了该方法在SO(3)流形上建模复杂分布的有效性和鲁棒性，以及其在旋转回归任务中的广泛应用。消融实验还评估了旋转正则化流中每个组件的有效性。


# Paper:67     基于视觉分布校准和跨模态分布对齐的少样本学习



#### 1. Title: 
Few-Shot Learning with Visual Distribution Calibration and Cross-Modal Distribution Alignment

#### 2. Authors: 
Runqi Wang, Hao Zheng, Xiaoyue Duan, Jianzhuang Liu, Yuning Lu, Tian Wang, Songcen Xu, Baochang Zhang

#### 3. Affiliation: 
Beihang University (北航)

#### 4. Keywords: 
Few-shot learning, vision-language models, selective attack, cross-modal distribution alignment, Earth Mover’s Distance

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_CVPR_2021_paper.html  Github: https://gitee.com/mindspore/models/tree/master/research/cv/SADA

#### 6. Summary : 
- (1):本文研究的背景是在少样本学习中，图像特征分布容易受到无关信息的干扰，同时视觉和语言特征分布的对齐也很困难。

- (2):过去的方法包括prompt learning和visual adapters，但在少样本学习中容易过拟合。本文提出了Selective Attack和Cross-Modal Distribution Alignment两个模块，前者通过可训练的adapter生成空间注意力图来攻击图像中的无关信息，后者通过构建Vision-Language Prototype来对齐图像和文本特征分布。这两个模块的创新点在于能够校准视觉特征分布和对齐视觉和语言特征分布。

- (3):本文提出的方法是Selective Attack和Cross-Modal Distribution Alignment两个模块的结合，通过对少样本学习中的图像特征分布和视觉和语言特征分布进行校准和对齐，提高了模型的性能。本文的创新点在于提出了可训练的adapter和Vision-Language Prototype，并且提出了一种有效的对齐方法。

- (4):本文在11个数据集上进行了实验，结果表明本文的方法在少样本学习中表现优异，超过了之前的方法。本文的方法能够有效地校准视觉特征分布和对齐视觉和语言特征分布，提高了模型的性能。
#### 7. 方法详细介绍：
本文提出了一种基于预训练视觉-语言模型的少样本学习方法，称为SADA。该方法包括两个主要模块：视觉分布校准和跨模态分布对齐。视觉分布校准模块旨在通过为每个类别定义一个语言原型（VLP）来校准图像类别预测。跨模态分布对齐模块使用Earth Mover's Distance（EMD）作为目标函数，将视觉信息和语言信息作为VLPs进行对齐。在训练期间，VLPs通过LEMD和Lm进行更新，而适配器层和提示通过Lm进行更新。在推理期间，VLPs用于校准图像预测。

#### 8. 实验设置：
本文的实验遵循CLIP的少样本训练和评估协议，在每个11个分类数据集上随机抽取1、2、4、8和16个标记图像进行训练。提示长度M设置为16，集合中的提示总数为32。分布校准比例alpha为0.1。使用SGD进行50个epoch的训练，初始学习率分别为0.001和0.01，都遵循余弦衰减计划。提示批量大小为4，图像批量大小为20。

#### 9. 实验结果和分析：
本文提出的SADA方法在11个数据集上均取得了最佳结果，包括miniImageNet、tieredImageNet、CIFAR-FS、CUB等。与之前的最佳模型ProDA相比，SADA在所有数据集上的平均结果都更好。在某些特定数据集上，SADA的提升更为显著。例如，在CIFAR10、UCF-101和ImageNet-1k上，SADA在1-shot下分别提高了3.36％、2.80％和2.10％。在更具挑战性的细粒度数据集Food-101上，SADA的准确率也高于其他模型。本文还进行了消融实验，证明了所提出方法的有效性。


# Paper:68     CLIP也是一种高效的分割器：一种基于文本驱动的弱监督语义分割方法



#### 1. Title: 
CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation

#### 2. Authors: 
Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li, Binbin Lin, Haifeng Liu, Xiaofei He

#### 3. Affiliation: 
Zhejiang University (浙江大学)

#### 4. Keywords: 
Weakly supervised semantic segmentation, Contrastive Language-Image Pre-training models, CLIP, ViT, CAM, CAA, CGL

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2022/html/Lin_CLIP_Is_Also_an_Efficient_Segmenter_A_Text-Driven_Approach_for_CVPR_2022_paper.html
Github: https://github.com/linyq2117/CLIP-ES

#### 6. Summary:
- (1): 本文研究的是弱监督语义分割任务，即只使用图像级别标签进行训练。这是一个具有挑战性的任务，因为需要在不需要像素级别注释的情况下预测像素级别标签。
- (2): 过去的方法通常采用多阶段框架，需要训练多个模型，训练成本高。本文提出了一种基于CLIP的新型弱监督语义分割框架，可以在不需要进一步训练的情况下，仅使用图像级别标签就可以定位不同的类别。本文的方法可以在三个阶段中提高效率和准确性，包括使用softmax函数来解决类别混淆问题，设计了两种文本驱动策略来获取更好的CAMs，提出了一种实时的基于类别感知的注意力亲和力模块来优化CAMs，以及引入了一种置信度引导的损失函数来训练最终的分割模型。本文的方法在Pascal VOC 2012和MS COCO 2014数据集上取得了SOTA的性能，同时只需要以前方法的10%的时间来生成伪掩模。
- (3): 本文提出了一种基于CLIP的新型弱监督语义分割框架，可以在不需要进一步训练的情况下，仅使用图像级别标签就可以定位不同的类别。本文的方法可以在三个阶段中提高效率和准确性，包括使用softmax函数来解决类别混淆问题，设计了两种文本驱动策略来获取更好的CAMs，提出了一种实时的基于类别感知的注意力亲和力模块来优化CAMs，以及引入了一种置信度引导的损失函数来训练最终的分割模型。
- (4): 本文的方法在Pascal VOC 2012和MS COCO 2014数据集上取得了SOTA的性能，同时只需要以前方法的10%的时间来生成伪掩模。本文的方法可以在不需要进一步训练的情况下，仅使用图像级别标签就可以定位不同的类别，具有很高的实用价值。
#### 7. 方法详细介绍：
本文提出了一种基于CLIP的弱监督语义分割框架CLIP-ES。该框架使用CLIP生成CAM，并探索图像中文本和对象之间的关系。框架还包括Softmax-GradCAM方法用于类激活映射，基于清晰度的提示选择方法，同义词融合方法以及用于细化的类感知注意力亲和力方法。具体步骤如下：
1. 使用CLIP预训练模型ViT-B-16生成类激活映射（CAMs）。
2. 使用交叉注意力聚合（CAA）模块对CAMs进行细化。
3. 使用置信度引导损失（CGL）生成伪掩码。
4. 使用密集CRF进一步后处理CAMs以生成最终的伪掩码。
5. 基于DeepLabV2和ResNet-101训练分割模型。

#### 8. 实验设置：
本文在PASCAL VOC 2012和MS COCO 2014数据集上评估了提出的框架。PASCAL VOC 2012数据集包含21个类别，MS COCO 2014数据集包含80个对象类别和一个背景类别。所有实验均以平均交并比（mIoU）作为评估指标。生成CAMs所使用的特征图是ViT中倒数第二个自注意层之前的特征图。在推理过程中，输入图像保持其原始大小，不使用多尺度策略。

#### 9. 实验结果和分析：
提出的框架在PASCAL VOC 2012和MS COCO 2014数据集上均取得了最新的最优结果。在PASCAL VOC 2012数据集上，CLIP-ES在验证集和测试集上分别达到了73.8%和73.9%的mIoU，创造了新的最优结果。在MS COCO 2014数据集上，CLIP-ES在验证集上取得了45.4%的mIoU，是最好的方法。消融实验表明，所提出的置信度引导损失（CGL）和类感知注意力（CAA）模块可以进一步提高性能。


# Paper:69     基于邻域相关性感知噪声模型的sRGB真实噪声合成



#### 1. Title: 
sRGB Real Noise Synthesizing with Neighboring Correlation-Aware Noise Model

#### 2. Authors: 
Zixuan Fu, Lanqing Guo, Bihan Wen

#### 3. Affiliation: 
南洋理工大学 (Nanyang Technological University)

#### 4. Keywords: 
sRGB real noise, noise modeling, deep denoisers, neighboring correlation, signal dependency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fu_sRGB_Real_Noise_Synthesizing_With_Neighboring_Correlation-Aware_Noise_Model_CVPR_2021_paper.html  Github: https://github.com/xuan611/sRGB-Real-Noise-Synthesizing

#### 6. Summary : 
- (1):本文研究的是在标准RGB（sRGB）领域中建模和合成真实噪声的挑战，由于噪声分布复杂，因此需要更加精确的噪声建模方法。

- (2):过去的方法包括基于物理的方法和基于数据驱动的方法，但是这些方法都无法直接应用于sRGB领域中的真实噪声建模。本文提出了一种新的噪声合成框架，通过显式学习其邻域相关性来建模真实噪声，从而弥合了合成噪声和真实噪声之间的分布差距。与以往的方法相比，本文的方法具有更好的泛化能力，即使只有有限的训练数据，也能够显著提高真实图像去噪的性能。

- (3):本文提出了一种新的噪声合成框架，基于邻域相关性感知噪声模型，即NeCA，直接在sRGB领域中合成真实噪声。该模型假设sRGB真实噪声不仅与其底层干净像素的信号相关，而且与其邻域噪声实现高度相关。因此，本文提出了一种新的噪声合成框架，通过显式学习其邻域相关性来建模真实噪声，从而弥合了合成噪声和真实噪声之间的分布差距。

- (4):本文的方法在训练有监督的深度去噪器方面取得了良好的性能，相比于流行的经典去噪器或者是基于其他sRGB噪声生成器训练的深度去噪器，本文的方法在真实图像去噪方面取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种噪声合成框架，称为邻域相关性感知噪声模型（NeCA），用于sRGB真实噪声合成。该框架由三个模块组成：增益估计网络（GENet）、噪声合成网络（NSNet）和邻域相关网络（NCNet）。GENet从任意噪声图像中估计增益因子，然后NSNet通过对干净图像进行条件化来合成噪声。NCNet用于建模合成噪声的邻域相关性。该框架使用对抗损失、标准差损失和正则化损失的组合进行训练。

#### 8. 实验设置：
本文使用包含320个由五个不同智能手机相机拍摄的噪声-清晰图像对的中等版本的智能手机图像去噪数据集（SIDD）来评估所提出的方法。使用sRGB版本的数据集来评估所提出的方法的性能。使用三个指标来评估方法的性能：离散Kullback-Leibler（KL）散度、信噪比（PSNR）和结构相似性（SSIM）。

#### 9. 实验结果与分析：
所提出的方法在SIDD和DND基准测试中均取得了最先进的去噪性能，优于BM3D和WNNM等经典去噪器，以及GCBD、C2N和Flow-sRGB等其他噪声生成方法。在SIDD基准测试中，NeCA-W比Flow-sRGB获得了2.08 dB的增益。此外，NeCA-S在SIDD和DND基准测试中均获得了可比较的去噪结果，优于Flow-sRGB。NeCA-S的优越性能进一步验证了显式建模邻域相关性有助于sRGB真实噪声合成。本文提出的方法称为邻域相关性感知噪声模型（NeCA）用于sRGB真实噪声合成。该方法在信号依赖性的基础上明确建模了真实噪声的邻域相关性。该基本噪声模型定义了像素级噪声水平及其底层干净像素的信号依赖性。像素i的噪声水平定义为干净图像块Ωx（以干净像素xi为中心）和相机ISO级别γ的函数。Ωx、γ和三个颜色通道的像素级噪声水平σi =（σi，r，σi，g，σi，b）的非线性关系由f（·）表示。该方法弥合了sRGB中合成和真实噪声分布之间的差距。由NeCA合成的“真实”图像可用于训练监督深度去噪器，从而解决仅受少量真实训练数据主观的真实图像去噪挑战。


# Paper:70     自适应模拟退火算法用于鲁棒几何估计



#### 1. Title: 
Adaptive Annealing for Robust Geometric Estimation

#### 2. Authors: 
Chitturi Sidhartha, Lalit Manam and Venu Madhav Govindu

#### 3. Affiliation: 
第一作者：印度科学院，班加罗尔，印度-560012

#### 4. Keywords: 
Geometric estimation, Graduated Non-Convexity, Hessian, Robustness, Outliers

#### 5. Paper: https://openaccess.thecvf.com/content_cvpr_2018/papers/Chitturi_Adaptive_Annealing_for_CVPR_2018_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究计算机视觉中的几何估计问题，这些问题通常通过最小化统计损失函数来解决，该函数考虑了观测中的异常值。然而，由于能量景观通常具有许多局部最小值，因此需要一些方法来避免局部最小值。Graduated Non-Convexity（GNC）是一种流行的方法，它通过退火损失函数的比例参数来避免局部最小值。然而，很少有人关注退火计划，通常以固定方式进行，导致速度-准确性权衡不佳，不可靠地收敛到全局最小值。
- (2):过去的方法包括RANSAC和M-estimation等，但它们都有一些问题。GNC是一种流行的方法，但它的性能取决于退火步骤的选择。本文提出了一种自适应退火方案，通过跟踪成本函数的Hessian的正定性来自适应地退火比例参数。该方法在经典的3D对应关系注册问题中进行了说明，并开发了加速Hessian的近似方法。该方法的有效性通过在一些合成和真实数据集上与最先进的3D注册方法进行比较得到验证。该方法准确高效，比最先进的方法更可靠地收敛到全局解。
- (3):本文提出了一种自适应退火方案，通过跟踪成本函数的Hessian的正定性来自适应地退火比例参数。该方法在经典的3D对应关系注册问题中进行了说明，并开发了加速Hessian的近似方法。该方法的有效性通过在一些合成和真实数据集上与最先进的3D注册方法进行比较得到验证。该方法准确高效，比最先进的方法更可靠地收敛到全局解。
- (4):该方法在3D对应关系注册问题上进行了测试，与最先进的方法进行了比较。结果表明，该方法在准确性和效率方面都优于最先进的方法，并且更可靠地收敛到全局解。
#### 7. 方法详细介绍：
本文提出了一种自适应模拟退火Graduated Non-Convexity (GNC)方法，用于解决计算机视觉中的几何估计问题。该方法通过跟踪损失函数的Hessian矩阵的正定性，并使用它来自适应地退火损失函数的尺度参数。本文以Geman-McClure损失函数为例进行了说明。该方法以噪声和异常值存在的3D对应点的配准问题为例进行了说明。本文还开发了Hessian矩阵的近似方法以加速该方法。通过将该方法与合成和真实数据集上的最先进的3D配准方法进行比较，验证了该方法的有效性。该方法准确、高效，并且比最先进的方法更可靠地收敛到全局解。

#### 8. 实验设置：
本文在两个数据集3DMatch和KITTI odometry上进行了实验。3DMatch数据集包含62个真实场景，其中8个场景用于测试，54个场景用于训练。使用最近邻匹配计算FCGF特征，并为测试集生成对应关系。KITTI odometry数据集包含11个LiDAR扫描的室外驾驶场景。作者在测试集（序列8-10）上提取FCGF特征，其中有555对匹配的点云。结果对应关系中的异常值百分比范围从≈0到99%。

#### 9. 实验结果和分析：
本文提出的方法在小型和大型合成数据集上的旋转和平移误差方面优于最先进的方法。该方法在ModelNet数据集上的平均误差最小，比其他方法更优秀。该方法的成功率与最高成功率（TEASER++）相似。该方法的计算时间随着输入大小的增加而扩展良好。本文报告了每个数据集和方法的平均旋转和平移误差。本文还报告了每种方法的成功率和计算时间。


# Paper:71     基于补丁的直观原型的可解释图像分类模型PIP-Net



#### 1. Title: 
PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification

#### 2. Authors: 
Meike Nauta, Jörg Schlötterer, Maurice van Keulen, Christin Seifert

#### 3. Affiliation: 
Meike Nauta: University of Twente, the Netherlands
Jörg Schlötterer, Christin Seifert: University of Duisburg-Essen, Germany
Maurice van Keulen: University of Twente, the Netherlands

#### 4. Keywords: 
Interpretable image classification, prototypical parts, explainability, self-supervised learning, semantic gap.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2021_paper.html  Github: https://github.com/M-Nauta/PIPNet

#### 6. Summary : 
- (1):本文研究背景是深度神经网络在计算机视觉中的应用，但其复杂性使得其解释性和可解释性变得越来越重要。

- (2):过去的解释性方法通常是通过后期解释来逆向工程黑盒模型，而本文则从设计的角度出发，提出了一种基于原型部分的直观可解释的图像分类模型。与现有的原型部分模型不同，本文的模型可以自我监督地学习与人类视觉感知更相符合的原型部分，从而更好地解释模型的推理过程。本文的模型可以作为一个稀疏的评分表，其中原型部分的存在为一个类别增加了证据。本文的模型还可以通过说“我以前没有见过这个”来避免对于超出分布范围的数据做出决策。本文的模型是全局可解释的，因为学习到的原型部分集合显示了模型的整个推理过程。本文的模型还可以提供更小的局部解释，以定位一个图像中的相关原型部分。本文的模型与现有的原型部分模型相比，可以更好地解释模型的推理过程，从而更好地支持模型的应用。

- (3):本文提出了一种基于原型部分的直观可解释的图像分类模型PIP-Net，该模型可以自我监督地学习与人类视觉感知更相符合的原型部分，从而更好地解释模型的推理过程。本文的模型可以作为一个稀疏的评分表，其中原型部分的存在为一个类别增加了证据。本文的模型还可以通过说“我以前没有见过这个”来避免对于超出分布范围的数据做出决策。本文的模型是全局可解释的，因为学习到的原型部分集合显示了模型的整个推理过程。本文的模型还可以提供更小的局部解释，以定位一个图像中的相关原型部分。本文的模型与现有的原型部分模型相比，可以更好地解释模型的推理过程，从而更好地支持模型的应用。

- (4):本文的模型在PETS数据集上进行了测试，结果表明，与现有的原型部分模型相比，本文的模型可以更好地解释模型的推理过程，从而更好地支持模型的应用。本文的模型可以作为一个稀疏的评分表，其中原型部分的存在为一个类别增加了证据。本文的模型还可以通过说“我以前没有见过这个”来避免对于超出分布范围的数据做出决策。本文的模型是全局可解释的，因为学习到的原型部分集合显示了模型的整个推理过程。本文的模型还可以提供更小的局部解释，以定位一个图像中的相关原型部分。
#### 7. 方法详细介绍：
本文提出了一种名为PIP-Net的可解释图像分类模型。该模型使用基于补丁的直观原型来解释分类结果。模型由卷积神经网络（CNN）骨干网络组成，该网络学习可解释的一维图像编码，指示图像中原型部分的存在或缺失。然后，稀疏线性层将这些原型部分连接到类别。模型分为两个阶段进行训练：首先，使用自监督学习和特殊设计的损失函数预训练原型；其次，解冻线性层并使用自定义激活函数对整个模型进行训练，该函数允许使用评分表推理、紧凑的解释以及能够处理分布外数据的能力。模型的输出分数使用一种新颖的函数进行归一化，该函数同时优化分类性能和紧凑性。

#### 8. 实验设置：
本文在三个标准基准数据集上评估了所提出的方法：CUB-200-2011、Stanford Cars和Oxford-IIIT Pet。使用ResNet50和ConvNeXt-tiny骨干网络，并对图像进行了大小调整和TrivialAugment数据增强。原型经过10个时期的预训练，然后进行了60个时期的整个网络训练。

#### 9. 实验结果和分析：
所提出的PIP-Net方法在三个基准数据集上实现了竞争性的准确性，同时具有较少的原型数量，特别是在局部解释方面。使用FPR95指标评估了OoD检测性能，PIP-Net可以检测到大多数OoD样本。还评估了学习原型的语义质量，并可视化了原型的前10个补丁。


# Paper:72     通过学习样本相对分类能力进行人脸图像质量评估的CR-FIQA



#### 1. Title: 
CR-FIQA: Face Image Quality Assessment by Learning Sample Relative Classifiability

#### 2. Authors: 
Fadi Boutros, Meiling Fang, Marcel Klemt, Biying Fu, Naser Damer

#### 3. Affiliation: 
第一作者：Fraunhofer计算机图形研究所（德国）

#### 4. Keywords: 
Face image quality assessment, sample relative classifiability, deep learning, face recognition

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2021_paper.html  Github: https://github.com/fdbtrs/CR-FIQA

#### 6. Summary : 
- (1):本文研究的背景是人脸图像质量评估（FIQA），即评估捕获的图像在实现可靠和准确的识别性能方面的效用。
 
- (2):过去的方法主要分为两类：一类是基于回归问题的FIQA方法，另一类是基于人脸嵌入属性的FIQA方法。本文提出了一种新的FIQA方法，即CR-FIQA，通过学习预测样本相对分类能力来估计样本的人脸图像质量。与以往的方法相比，本文的方法不需要预设标签，而是通过学习动态的内部网络观察来评估样本质量。

- (3):本文提出的CR-FIQA方法通过探测内部网络观察来学习预测样本相对分类能力，从而评估样本的人脸图像质量。具体地，本文在传统的人脸识别训练过程中，同时学习了优化类中心和预测样本相对分类能力的回归问题。通过将导致高/低分类能力的样本属性与导致高/低FIQ的属性联系起来，我们可以使用CR-FIQA来预测任何给定样本的FIQ。本文在八个基准测试和四个人脸识别模型上进行了广泛的评估实验，证明了我们提出的CR-FIQA方法优于现有的FIQA算法。

- (4):本文的方法在多个基准测试和人脸识别模型上进行了评估，取得了优于现有方法的性能。本文的方法可以有效地评估人脸图像的质量，从而提高人脸识别的准确性和可靠性。
#### 7. 方法详细介绍：
本文提出的CR-FIQA方法利用ArcFace损失函数优化人脸识别模型的类中心，通过计算样本特征表示在角度空间中与其类中心和最近的负类中心的相似度来估计样本的相对可分类性，作为人脸图像质量的度量。CR-FIQA模型的训练目标是优化ArcFace损失函数和Smooth L1-Loss，前者用于优化样本与类中心之间的距离，后者用于预测内部网络观察值CR。具体步骤包括：（1）计算训练集中所有样本的CR、CCS和NNCCS值；（2）使用ArcFace损失函数训练人脸识别模型，优化类中心；（3）使用Smooth L1-Loss训练CR-FIQA模型，预测CR值。

#### 8. 实验设置：
本文使用ResNet-50人脸识别模型在CASIA-WebFace数据集上进行实验，使用ArcFace损失函数优化类中心。训练集包含10,000个身份的500,000张图像。使用ResNet-100模型在MS1M-V2数据集上提取CASIA-WebFace的特征嵌入，评估人脸识别性能。在不同的实验设置下，使用质量分数作为嵌入权重项，对IJB-C 1:1混合验证基准进行验证，评估对应的人脸识别模型的性能。

#### 9. 实验结果与分析：
本文提出的CR-FIQA方法在多个数据集上进行了实验，包括Adience、AgeDB-30、CFP-FP、LFW、CALFW、CPLFW、XQLFW和IJB-C。实验结果表明，CR-FIQA在各种实验设置下均优于SOTA方法。在IJB-C基准测试中，CR-FIQA的集成导致SOTA的验证性能。与其他人脸图像质量评估方法相比，CR-FIQA（S）和CR-FIQA（L）表现出竞争性能。


# Paper:73     从低成本数据中学习3D可塑面反射模型



#### 1. Title: 
Learning a 3D Morphable Face Reflectance Model from Low-cost Data

#### 2. Authors: 
Yuxuan Han, Zhibo Wang, Feng Xu

#### 3. Affiliation: 
第一作者：清华大学软件学院和BNRist

#### 4. Keywords: 
3D Morphable Face Model, Reflectance Model, Low-cost Data, Specular Shiness, Inverse Rendering

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Han_Learning_a_3D_Morphable_Face_Reflectance_Model_From_Low-Cost_Data_CVPR_2021_paper.html  Github: https://github.com/Hanzy1996/MorphableFaceReflectance

#### 6. Summary : 
- (1):本文研究的背景是3D Morphable Face Model的建模，现有的方法只能建立Lambertian表面的模型，无法建立非Lambertian表面的模型，如面部光泽度。
- (2):现有的方法需要使用Light Stage数据来建立参数模型，但这种数据难以获取，且只能建立漫反射和镜面反射的参数模型，无法建立完整的BRDF模型。本文提出了一种使用低成本公开数据建立3D Morphable Face Reflectance Model的方法，可以建立具有空间变化BRDF的模型，并且可以建立完整的BRDF模型。
- (3):本文提出了一种使用线性shiness加权的参数模型来表示空间变化的镜面反射强度和shiness，并提出了一种反演渲染算法来从非Light Stage数据中重建反射参数，用于训练初始的morphable reflectance model。为了提高模型的泛化能力和表达能力，本文进一步提出了一种基于重建的更新策略，在in-the-wild数据集上对模型进行微调。实验结果表明，本文方法可以获得合理的渲染结果，支持其目标。
- (4):本文方法在Multi-PIE数据集上进行了实验，结果表明，与现有方法相比，本文方法可以更好地建立非Lambertian表面的模型，并且可以获得更好的渲染效果。
#### 7. 方法详细介绍：
本文提出了一种从低成本数据中学习三维可塑性人脸反射模型的方法。该方法基于BFM09几何模型，为其顶点分配空间变化的反射参数。反射表示包括漫反射项和高光反射项，高光反射项是几个预定义的Blinn-Phong BRDF的线性组合，具有不同的高光反射指数。每个面顶点的反射参数包括漫反射颜色和线性组合权重。该方法包括一种有效的方向或环境照明的着色技术。初始模型学习涉及通过反向渲染估计Multi-PIE数据集中每个身份的反射参数，并为其训练PCA模型。然后，在FFHQ数据集上对初始模型进行微调，以提高其泛化能力和表达能力。

具体步骤如下：
1. 用Lambertian BRDF表示面反射，并将其与几个Blinn-Phong BRDF的线性组合相结合，以表示不同的高光反射强度和空间变化的高光反射指数。
2. 通过反向渲染方法，估计Multi-PIE数据集中每个身份的反射参数，并为其训练PCA模型。
3. 基于BFM09几何模型和估计的反射参数，构建初始的可塑性人脸反射模型。
4. 在FFHQ数据集上对初始模型进行微调，以提高其泛化能力和表达能力。
5. 使用反射模型系数和球谐光照，结合几何参数，通过重建网络获得重建图像。

#### 8. 实验设置：
本文在Multi-PIE和CelebA-HQ数据集上评估了所提出的方法。将反射表示与具有空间变化的高光反射和全局高光反射指数的先前工作进行了比较。使用SSIM、PSNR和LPIPS分数评估面部渲染结果。使用Adam优化器最小化损失函数，学习率为5e-3。初始模型使用前80个PCA基，光照PCA模型使用前80个PCA基。在FFHQ数据集上微调模型，使用70000个高保真度的单视角人脸图像。使用ResNet-50架构作为重建网络，并将最后一个全连接层修改为NR + NL + 3个神经元。使用透视相机模型表示3D-2D投影。

#### 9. 实验结果和分析：
所提出的方法在CelebA-HQ数据集上的光度面部重建定量分数方面优于竞争对手。该方法可以很好地重建输入图像，并以合理的方式分离漫反射和高光反射。在Multi-PIE数据集上的面部重照性能方面，所提出的方法比竞争对手实现了更真实的结果，特别是在鼻尖周围。该方法还可以在3D-RFE数据集的23个Light Stage扫描上重建合理的反射图，尽管在高阶球谐系数的反射重建方面，其获得的定量结果不如AlbedoMM。通过消融研究验证了所提出的微调策略和光照PCA模型的有效性。


# Paper:74     ReCo: 区域控制的文本到图像生成



#### 1. Title: 
ReCo: Region-Controlled Text-to-Image Generation

#### 2. Authors: 
Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang

#### 3. Affiliation: 
Microsoft Research Asia (微软亚洲研究院)

#### 4. Keywords: 
Text-to-image generation, Region control, Position tokens, Pre-trained models, Fine-tuning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2021_paper.html  Github: https://github.com/microsoft/ReCo

#### 6. Summary : 
- (1):本文研究的背景是文本到图像生成中的区域控制问题，即如何通过自由文本描述精确地指定图像中的特定区域内容。
- (2):过去的方法包括基于位置词的文本输入和基于对象标签的布局输入，但这些方法存在一些问题，如输入冗长、描述不准确等。本文提出了一种新的方法，即通过引入位置标记来扩展预训练的文本到图像模型，从而实现区域控制。该方法可以更好地控制对象数量、关系和大小属性，并提高T2I语义正确性。
- (3):本文提出的方法是在预训练的T2I模型上引入位置标记，以实现区域控制。具体而言，每个区域由四个位置标记表示，分别表示左上角和右下角，后跟自由文本区域描述。然后，使用这种新的输入接口对预训练的T2I模型进行微调。本文的方法名为ReCo（Region-Controlled T2I），可以通过自由文本区域描述来控制任意对象，而不是通过受限类别集合的对象标签。本文的创新点在于引入位置标记来更好地控制T2I生成。
- (4):本文在COCO数据集上进行了实验，结果表明，与基于位置词的T2I模型相比，ReCo可以更好地控制对象数量、空间关系和大小属性，并生成更高质量的图像。此外，ReCo还可以提高对象分类准确性和检测器平均精度。人类评估结果表明，ReCo在生成具有正确对象数量和空间关系的图像方面比T2I模型更准确。
#### 7. 方法详细介绍：
ReCo是一种区域控制的文本到图像生成模型，它扩展了预训练的T2I模型以理解坐标输入。它使用统一的输入令牌词汇，包含文本单词和位置令牌，以允许准确和开放式的区域控制。ReCo在输入查询中无缝混合文本和位置令牌，以获得自由形式描述和精确位置控制的最佳效果。ReCo的实现基于开源的Stable Diffusion（SD），包括自动编码器、用于噪声估计的U-Net和CLIP ViT-L/14文本编码器。ReCo旨在展示将文本和位置条件协同组合用于区域控制T2I生成的好处。

#### 8. 实验设置：
ReCo模型在COCO、PaintSkill和LVIS数据集上进行了定量评估。输入查询来自数据集，使用GIT生成区域描述。COCO数据集用于通过图像-文本对对Stable Diffusion模型进行微调。PaintSkill数据集用于评估模型跟随任意形状框并生成具有正确对象类型/数量/关系的图像的能力。LVIS数据集用于测试模型对COOC微调中未见的对象类别的开放词汇区域描述的理解。

#### 9. 实验结果和分析：
ReCo模型在LVIS数据集的1,203个类别中实现了最佳的SceneFID和对象分类准确率，分别为10.08和23.42%。ReCo还优于真实图像检索基线和大多数先前的研究。然而，该方法在为具有挑战性的输入查询生成低质量图像方面存在局限性。ReCo在COCO上的区域分类准确率提高了20.40%。在PaintSkill上的人类评估显示，ReCo在更正确地生成查询描述的对象计数和空间关系方面获得了+19.28%和+17.21%的准确性提高。


# Paper:75     MobileBrick：在移动设备上构建LEGO进行三维重建



#### 1. Title: 
MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices


#### 2. Authors: 
Kejie Li, Jia-Wang Bian, Robert Castle, Philip H.S. Torr, Victor Adrian Prisacariu


#### 3. Affiliation: 
第一作者：牛津大学


#### 4. Keywords: 
3D reconstruction, RGBD dataset, ground-truth annotations, LEGO models, mobile devices


#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_MobileBrick_Building_LEGO_for_3D_Reconstruction_on_Mobile_Devices_CVPR_2021_paper.html  Github: None


#### 6. Summary : 
- (1):本文研究的背景是高质量的三维重建需要精确的三维地面真实形状，但是创建一个物体的精确三维模型非常困难，即使是由三维扫描仪生成的三维重建也会有偏差，这会导致评估结果的偏差。


- (2):现有的数据集大多数都是使用高端三维扫描仪来创建伪地面真实模型进行评估，但是这些三维模型仍然会受到噪声测量的影响，因此DTU数据集不愿称其重建模型为地面真实形状，而是选择了“评估参考”这个术语。本文提出了一种新的多视角RGBD数据集，使用具有已知几何形状的LEGO模型作为图像捕获的三维结构，从而获得精确的三维地面真实形状，而不依赖于高端三维扫描仪。


- (3):本文提出了一种新的多视角RGBD数据集，使用具有已知几何形状的LEGO模型作为图像捕获的三维结构，从而获得精确的三维地面真实形状。本文的创新点在于使用LEGO模型作为三维结构，从而获得精确的三维地面真实形状，而不依赖于高端三维扫描仪。


- (4):本文在所提出的数据集上评估了一系列三维重建算法，并取得了良好的性能。本文的方法可以支持高保真度的三维重建，为未来的研究提供了独特的机会。
#### 7. 方法详细介绍：
本文介绍了一种新的多视角RGBD数据集，该数据集使用移动设备捕获，包括153个物体模型的高精度3D地面真实标注，具有多样化的3D结构。作者利用具有已知几何形状的乐高模型作为图像捕获的3D结构，从而获得精确的3D地面真实形状，而无需依赖高端3D扫描仪。本文在提出的数据集上评估了一系列3D重建算法。具体步骤包括：使用移动设备捕获多视角RGBD图像，使用乐高模型作为3D结构，使用标定板进行相机标定，使用多视角图像进行3D重建，使用评价指标进行算法评估。

#### 8. 实验设置：
本文使用了一台移动设备进行数据采集，采集了包含153个物体模型的多视角RGBD数据集。相机标定使用了标定板进行，评价指标包括了重建误差和重建质量。

#### 9. 实验结果与分析：
本文在提出的数据集上评估了一系列3D重建算法，包括了基于点云的方法、基于体素的方法和基于深度学习的方法。实验结果表明，基于深度学习的方法在重建精度和重建质量上表现最好，而基于点云的方法在重建速度上表现最优。此外，本文还对数据集的质量进行了分析，发现数据集的多样性和真实性对算法的表现有着重要影响。


# Paper:76     基于混合融合的多模态工业异常检测



#### 1. Title: 
Multimodal Industrial Anomaly Detection via Hybrid Fusion

#### 2. Authors: 
Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, Chengjie Wang

#### 3. Affiliation: 
第一作者：上海交通大学

#### 4. Keywords: 
Anomaly detection, Multimodal, 3D point clouds, RGB images, Memory bank, Contrastive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Multimodal_Industrial_Anomaly_Detection_via_Hybrid_Fusion_CVPR_2021_paper.html  Github: https://github.com/nomewang/M3DM

#### 6. Summary : 
- (1):本文研究的是工业异常检测，针对现有的基于2D图像的工业异常检测方法，提出了一种基于3D点云和RGB图像的多模态异常检测方法，以更好地利用3D形状和颜色特征来检测缺陷产品。

- (2):现有的多模态工业异常检测方法直接将多模态特征拼接在一起，导致多模态特征之间的干扰较大，影响检测性能。本文提出了一种混合融合方案，通过无监督的特征融合和决策层融合，避免信息丢失和降低特征之间的干扰。同时，本文还提出了点特征对齐操作，以更好地对齐3D和2D特征。

- (3):本文提出了一种名为M3DM的多模态异常检测方案，通过无监督的特征融合和决策层融合，以及点特征对齐操作，实现了更好的异常检测性能。本文的创新点在于提出了一种新的特征融合方案，避免了多模态特征之间的干扰，同时提出了点特征对齐操作，以更好地对齐3D和2D特征。

- (4):本文在MVTec-3D AD数据集上进行了实验，结果表明，本文提出的多模态工业异常检测模型在检测和分割精度上均优于现有的方法。
#### 7. 方法详细介绍：
本文提出了一种名为Multi-3D-Memory (M3DM)的多模态工业异常检测方法。该方法使用RGB图像和3D点云来检测工业产品中的异常。该方法采用混合特征融合方案来减少多模态特征之间的干扰并鼓励特征交互。该方法包括无监督特征融合（UFF）来融合多模态特征，决策层融合（DLF）利用多个内存库进行鲁棒决策，以及点特征对齐（PFA）操作将Point Transformer特征对齐到2D平面以进行高性能3D异常检测。该方法还使用Point Transformer提取3D特征和Vision Transformer提取RGB特征。

#### 8. 实验设置：
本文在MVTec-3D AD数据集上进行实验，该数据集包含2D图像和3D点云数据。该数据集包括六个工业产品类别，每个类别都包含正常和有缺陷的样本。将所提出的方法与最先进的方法进行比较，包括基于2D的方法和多模态方法。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2680 v4 CPU和NVIDIA Tesla V100 GPU。

#### 9. 实验结果与分析：
所提出的M3DM方法在MVTec-3D AD数据集上的检测和分割精度均优于最先进的方法。该方法在异常检测和分割方面的平均精度分别为98.5％和96.5％。该方法在每个工业产品类别上的表现也优于最先进的方法。使用各种指标对该方法的性能进行了评估，包括精度，召回率，F1-score和曲线下面积（AUC）。


# Paper:77     卫星图像的变化感知采样和对比学习



#### 1. Title: 
Change-Aware Sampling and Contrastive Learning for Satellite Images

#### 2. Authors: 
Utkarsh Mall, Bharath Hariharan, Kavita Bala

#### 3. Affiliation: 
Cornell University（康奈尔大学）

#### 4. Keywords: 
Self-supervised learning, satellite images, contrastive learning, change detection, semantic segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究卫星图像的自监督学习，旨在解决卫星图像标注数据不足的问题，提出了一种新的自监督学习方法，利用卫星图像的时空特性来学习更好的特征表示。
 
- (2):过去的自监督学习方法通常使用“预文本任务”来学习特征表示，如旋转预测、拼图、着色或缺失数据补全等。本文提出的方法利用卫星图像的时空结构，通过对同一位置的多个时间点的图像进行对比，学习对季节性变化不变但对永久性变化敏感的特征表示。同时，本文还提出了一种新的采样方法，更好地利用地理结构信息，提高了数据的多样性。
 
- (3):本文提出了一种新的对比损失函数，称为Change-Aware Contrastive (CACo) Loss，利用卫星图像的时空特性，鼓励对季节性变化不变但对永久性变化敏感的特征表示。同时，本文还提出了一种新的方法来估计位置是否发生了重大变化，并根据这个变化估计来设计损失函数，鼓励对永久性变化敏感。最后，本文提出了一种改进的地理采样方法，提高了数据的多样性。 
 
- (4):本文在多个下游任务上进行了评估，如土地覆盖分类、语义分割和变化检测等，相对于最佳基线方法，本文方法的性能有了显著的相对提升（范围从6.5%到8.5%）。
#### 7. 方法详细介绍：
本文提出了一种自监督框架CACo（Change-Aware Sampling and Contrastive Learning），用于卫星图像。该框架使用短期和长期时间差异作为数据增强，训练具有季节不变性和人工增强不变性的特征表示。长期时间差异用于捕捉底层特征表示中的永久性变化。该框架还引入了一种改进的地理采样方法，使用以城市中心为中心，半径为10公里的二维高斯采样，并拒绝完全在海洋中的样本。该方法使用对比学习方法训练特征表示，使用称为CACo Loss的新型损失函数，该函数使用长期时间信息来鼓励对季节性变化的不变性，但对永久性长期变化敏感。该方法还通过有效地采样更具信息量的位置来解决空间稀缺性的挑战。

#### 8. 实验设置：
本文使用了三个数据集进行实验，分别是EuroSAT、BigEarthNet和NWPU-RESISC45。实验使用了两个评估指标，分别是平均精度（mAP）和F1分数。实验中使用了ResNet-18作为骨干网络，并使用了SGD优化器进行训练。训练过程中使用了学习率衰减和权重衰减技术。实验中使用了多个基线方法进行比较，包括ImageNet预训练和其他卫星图像特定方法。

#### 9. 实验结果和分析：
本文提出的方法在多个下游任务上均优于其他基线方法，包括土地覆盖分类、变化检测和语义分割。相对于其他方法，本文方法在变化检测和语义分割上的相对改进分别为8.5%和6.5%。实验结果表明，本文方法具有很好的通用性，并且能够提高下游任务的准确性。本文还对方法的每个组成部分对最终性能的影响进行了评估，并发现每个组成部分都有显著的改进。


# Paper:78     视觉Transformer是良好的掩模自动标注器



#### 1. Title: 
Vision Transformers Are Good Mask Auto-Labelers

#### 2. Authors: 
Shiyi Lan, Xitong Yang, Zhiding Yu, Zuxuan Wu, Jose M. Alvarez, Anima Anandkumar

#### 3. Affiliation: 
NVIDIA（英伟达）

#### 4. Keywords: 
Mask Auto-Labeler, Vision Transformers, Instance Segmentation, Box-Supervised Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Lan_Vision_Transformers_Are_Good_Mask_Auto-Labelers_CVPR_2022_paper.html  Github: https://github.com/NVlabs/mask-auto-labeler

#### 6. Summary : 
- (1):本文研究背景是实例分割需要大量的人工标注，而这些标注成本高昂且容易出错，因此需要一种自动标注的方法来减少人工标注的工作量。

- (2):过去的方法需要大量的人工标注，而本文提出的方法只需要较少的标注，即只需要边界框注释。本文提出的方法是基于Transformer的自动标注框架，可以生成高质量的掩模伪标签。本文的方法可以显著减少自动标注和人工标注之间的差距，并且可以使实例分割模型的性能接近于完全监督模型。

- (3):本文提出了一个两阶段的框监督实例分割框架，包括掩模自动标注阶段和实例分割训练阶段。本文提出了一种基于Transformer的掩模自动标注框架，Mask Auto-Labeler（MAL），它以感兴趣区域（RoI）图像为输入，并在框内条件生成高质量的掩模伪标签。本文的方法可以显著减少自动标注和人工标注之间的差距，并且可以使实例分割模型的性能接近于完全监督模型。本文的方法在掩模自动标注和实例分割任务上都取得了很好的性能。

- (4):本文的方法在COCO数据集上取得了44.1%的mAP，优于现有的基于框监督的实例分割方法。使用MAL生成的掩模进行训练，实例分割模型在COCO和LVIS数据集上的性能可以接近于完全监督模型的性能，达到了97.4%的性能。本文的方法可以显著减少自动标注和人工标注之间的差距，并且可以使实例分割模型的性能接近于完全监督模型。
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的实例分割方法，包括两个阶段：自动标注阶段和实例分割训练阶段。自动标注阶段采用了一个名为Mask Auto-Labeler（MAL）的框架，该框架使用RoI图像作为输入，并在框内条件生成高质量的掩码。MAL由两个对称的网络组成，任务网络和教师网络，每个网络都包含图像编码器和掩码解码器。使用指数移动平均（EMA）来更新教师网络的权重。使用多实例学习（MIL）损失和条件随机场（CRF）损失来生成自我训练的精细掩码。该方法显著缩小了自动标注和人工标注之间的掩码质量差距，使用MAL生成的掩码训练的实例分割模型几乎可以与完全监督的模型相匹配。

具体步骤如下：
1. RoI输入生成：使用两种采样策略之一生成RoI输入，其中更好的策略是框扩展。
2. MAL架构：包括任务网络和教师网络，两个网络都使用标准Vision Transformer作为图像编码器。任务网络中的掩码解码器是受YOLACT启发的基于注意力的网络。损失函数包括多实例学习损失和条件随机场损失。
3. 自我训练：使用MIL损失和CRF损失生成自我训练的精细掩码。

#### 8. 实验设置：
本文在COCO和LVIS数据集上评估了所提出的方法。实验在8个NVIDIA Tesla V100上进行。本文遵循了两个数据集的标准训练和验证分区。

#### 9. 实验结果和分析：
本文提出的方法在LVIS v1和COCO数据集上均取得了最先进的性能。作者还进行了消融研究，分析了不同设计选择对所提出方法性能的影响。然而，作者指出，在遮挡情况下，人工标注仍然比MAL生成的掩码更好。


# Paper:79     敌人的敌人就是朋友：探索反向对抗样本以改进对抗训练



#### 1. Title: 
The Enemy of My Enemy is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training

#### 2. Authors: 
Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, Xiaohua Xie

#### 3. Affiliation: 
第一作者：中山大学计算机科学与工程学院，中国

#### 4. Keywords: 
Adversarial training, deep learning, inverse adversarial examples, robustness, computer vision

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Dong_The_Enemy_of_My_Enemy_Is_My_Friend_Exploring_Inverse_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究深度学习中的对抗样本问题，提出了一种新的对抗训练框架，旨在通过生成“反向对抗样本”来改善现有方法中存在的问题。
 
- (2):现有的对抗训练方法主要集中在自然样本和对抗样本之间的分布对齐，但是这种方法可能会受到自然样本分类错误的影响。为了解决这个问题，本文提出了一种新的对抗训练框架，通过生成“反向对抗样本”来改善现有方法中存在的问题。具体来说，反向对抗样本是通过最大化自然样本附近的似然性来生成的，可以更好地保留自然样本的特征。本文的方法在多个视觉数据集和架构上进行了广泛的实验，证明了其在鲁棒性和自然精度方面的优越性。

- (3):本文提出的对抗训练框架是基于反向对抗样本的，通过将对抗样本和其“反向对抗”对应物的输出概率相似性作为正则化项，来提高模型的鲁棒性。具体来说，反向对抗样本是通过最大化自然样本附近的似然性来生成的，可以更好地保留自然样本的特征。本文还提出了一种类特定的通用反向对抗样本生成方法，以减少不同样本之间的偏差。此外，本文还提出了一种单步对抗训练的版本，以提高时间效率。

- (4):本文的方法在多个视觉数据集和架构上进行了广泛的实验，证明了其在鲁棒性和自然精度方面的优越性。本文的方法还可以与单步对抗训练方法相结合，以在低成本下提高鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种新的对抗训练方法，称为通用反向对抗训练（UIAT）。该方法利用反向对抗样本来提高对抗鲁棒性。具体而言，该方法通过迭代地最小化反向对抗样本的分类损失值来生成反向对抗样本。反向对抗损失函数包括特征级别的正则化，以提供补充监督。该方法还引入了一种类别特定的通用反向对抗生成策略，以降低反向对抗样本生成的计算成本。UIAT的损失函数由交叉熵损失和KL散度组成。该方法还包括一种反向对抗动量机制，以稳定训练过程。该算法的伪代码在论文中提供。

#### 8. 实验设置：
本文在三个标准数据集（CIFAR-10、CIFAR-100和SVHN）上进行了实验。使用的网络结构包括ResNet-18、Pre-activation ResNet-18（PRN-18）和Wide-ResNet-28-10（WRN-28-10）。对于没有额外数据的训练，批量大小设置为128，总共训练100个epoch（CIFAR-10/100）或30个epoch（SVHN）。使用带有Nesterov动量因子0.9的随机梯度下降（SGD）优化器，以及最大学习率为0.1的循环学习率计划和权重衰减因子5×10−4。在训练阶段，使用PGD方法对交叉熵损失进行10次迭代步骤进行对抗样本生成。 ℓ∞-范数的最大扰动为ϵ = 8/255，而步长α分别设置为2/255（CIFAR-10/100）和1/255（SVHN），遵循常规实践。反向对抗半径设置为ϵ′ = 4/255。在公式（4）和公式（8）中，正则化超参数β和γ分别设置为1.0和0.9。

#### 9. 实验结果和分析：
本文的实验结果表明，UIAT方法可以有效地提高模型的对抗鲁棒性，同时不会牺牲太多的自然精度。该方法可以在各种设置和更大的数据集上高效地获得更好的鲁棒性结果。该方法还可以与最先进的单步对抗训练方法相结合，以低成本提高鲁棒性。实验结果表明，UIAT可以作为提高自然和鲁棒精度的即插即用组件。此外，该方法可以有效地适应各种对抗训练半径以获得更好的性能。本文还分析了超参数的影响以及自然精度和鲁棒精度之间的权衡。最后，本文分析了UIAT方法有效的原因，并验证了该方法可以潜在地弥合高精度示例和低精度示例之间的精度差距，从而有益于鲁棒性。


# Paper:80     基于因果推断的情境感知情感识别



#### 1. Title: 
Context De-confounded Emotion Recognition

#### 2. Authors: 
Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, Shunli Wang, Mingcheng Li, Siao Liu, Xiao Zhao, Shuai Huang, Zhiyan Dong, Peng Zhai, Lihua Zhang

#### 3. Affiliation: 
复旦大学工程与技术学院

#### 4. Keywords: 
Context-Aware Emotion Recognition, Causal Inference, Contextual Causal Intervention Module, Dataset Bias

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Context_De-Confounded_Emotion_Recognition_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了情境感知情感识别（CAER）任务中数据集偏差的问题，提出了一种因果推断的方法来解决这个问题。

- (2):过去的方法主要集中在设计复杂的架构或机制来从主体和情境中提取有意义的表示。然而，一个长期被忽视的问题是现有数据集中的情境偏差导致不同情境场景下情感状态的显著不平衡分布。具体而言，有害的偏差是一个混淆因素，它会误导现有模型基于传统的似然估计学习虚假的相关性，从而显著限制模型的性能。为了解决这个问题，本文提供了一种基于因果关系的视角来解开模型对这种偏差的影响，并通过一个定制的因果图来制定CAER任务中变量之间的因果关系。然后，我们提出了一个基于背门调整的情境因果干预模块（CCIM），以消除混淆因素并利用真实因果效应进行模型训练。CCIM是插件式和模型无关的，可以显著提高各种最先进的方法。在三个基准数据集上的广泛实验表明了我们的CCIM的有效性和因果洞察的重要性。

- (3):本文提出了一种基于因果推断的方法来解决CAER任务中数据集偏差的问题。首先，我们通过一个提出的因果图来描述CAER任务的过程。然后，我们提出了一个简单而有效的情境因果干预模块（CCIM），通过背门调整来消除混淆因素并利用真实因果效应进行模型训练。CCIM是插件式和模型无关的，可以显著提高各种最先进的方法。我们的方法在三个标准和有偏的CAER数据集上进行了全面的评估，证明了CCIM可以显著且一致地改进现有的基线，实现了新的最先进技术。

- (4):本文的方法在三个标准和有偏的CAER数据集上进行了全面的评估，证明了CCIM可以显著且一致地改进现有的基线，实现了新的最先进技术。
#### 7. 方法详细介绍：
本文提出了一种基于因果推断的偏差缓解策略，用于解决上下文感知情感识别（CAER）任务中情感状态在不同上下文中分布不均的有害偏差问题。首先，作者通过提出的因果图对CAER任务的过程进行了公式化，将输入图像X、主体特征S、上下文特征C、混淆因素Z和预测Y之间的因果关系进行了分离。然后，他们提出了一种基于背门调整的上下文因果干预模块（CCIM），以去除混淆因素并利用真实因果效应进行模型训练。CCIM是插件式和模型无关的，可以显著提高各种最新方法的性能。使用do-calculus P(Y |do(X))计算真实因果效应，这与传统的似然P(Y |X)有根本的区别。

#### 8. 实验设置：
本文使用了三个数据集进行实验：EMOTIC、GroupWalk和CAER-S。其中EMOTIC和GroupWalk数据集用于情感识别任务，CAER-S数据集用于情感分类任务。作者使用了预训练的Faster R-CNN来检测每个训练样本中目标主体的边界框，并根据边界框在训练样本上对目标主体进行了遮罩处理，生成上下文图像。所有模型都在四个Nvidia Tesla V100 GPU上进行训练。

#### 9. 实验结果与分析：
本文提出的CCIM方法在三个数据集上均取得了优异的性能表现。在EMOTIC数据集上，使用CCIM的EMOT-Net的mAP为30.88%，比基线EMOT-Net高2.95%。在GroupWalk数据集上，使用CCIM的EmotiCon的mAP为69.31%，比基线EmotiCon高3.73%。在CAER-S数据集上，使用CCIM的CAER-Net的分类准确率为74.81%，比基线CAER-Net高1.34%。使用CCIM的EmotiCon的分类准确率为91.17%，比基线EmotiCon高2.52%。实验结果表明，CCIM方法可以有效地缓解上下文偏差问题，提高模型性能。


# Paper:81     基于NeRF的3D感知多类图像到图像翻译



#### 1. Title: 
3D-Aware Multi-Class Image-to-Image Translation with NeRFs

#### 2. Authors: 
Senmao Li, Joost van de Weijer, Yaxing Wang, Fahad Shahbaz Khan, Meiqin Liu, Jian Yang

#### 3. Affiliation: 
第一作者：南开大学计算机科学系

#### 4. Keywords: 
3D-aware image synthesis, multi-class image-to-image translation, Neural Radiance Fields, view-consistency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_3D-Aware_Multi-Class_Image-to-Image_Translation_With_NeRFs_CVPR_2021_paper.html  Github: https://github.com/senmao/3DI2I

#### 6. Summary : 
- (1):本文研究了3D-aware GANs在3D consistent multi-class image-to-image (3D-aware I2I) translation中的应用。通过将学习过程分解为多类3D-aware GAN步骤和3D-aware I2I translation步骤，解决了2D-I2I translation方法中存在的不真实形状/身份变化问题。
 
- (2):过去的2D-I2I translation方法在视角变化时存在不真实的形状/身份变化问题，而视频到视频合成方法则需要大量标记数据或每个对象的多视图帧。本文提出了一种新的3D-aware multi-class I2I translation方法，通过3D-aware GANs解决了2D-I2I translation中存在的问题。 

- (3):本文提出了两种新技术：一种新的条件架构和一种有效的训练策略，用于训练多类3D-aware GAN。在此基础上，构建了一个3D-aware I2I translation系统，进一步减少了视角一致性问题。本文还提出了几种新技术，包括U-net-like适配器网络设计、分层表示约束和相对正则化损失，以进一步解决视角不一致性问题。

- (4):本文成功地实现了3D-aware I2I translation with multi-view consistency，通过两个数据集的广泛实验，定量和定性结果表明，本文的方法在评估时间一致性时明显优于现有的2D-I2I系统。
#### 致谢
本文作者感谢北京高级信息科学与网络技术重点实验室（XDXX2202）和青年基金（62202243）的支持，以及西班牙政府资助的PID2019-104174GB-I00、TED2021-132513B-I00项目。

#### 方法详细介绍
本文提出了一种基于神经辐射场（NeRFs）的三维多类图像到图像（I2I）转换方法。该方法包括三维感知生成模型（StyleNeRF）、多类三维感知生成模型（multi-class StyleNeRF）和三维感知I2I转换模型。多类StyleNeRF使用从StyleNeRF学习到的权重进行初始化。三维感知I2I转换模型使用类似U-net的适配器网络、分层表示约束和相对正则化损失来解决视角一致性问题。最终目标是最小化三个损失的总和。

#### 实验设置
本文在Animal Faces（AFHQ）和CelebA-HQ数据集上进行实验。AFHQ包含3个类别，每个类别约有5000张图像。在CelebA-HQ中，我们使用性别作为类别，训练集中有约10k（10057）男性和约18k（17943）女性图像。所有图像都被调整为256×256。本文提出的方法在Pytorch中实现，使用批量大小为64，学习率为0.0002进行训练。实验在2× Quadro RTX 3090 GPU（24 GB VRAM）上进行。评估指标包括Fr´echet Inception Distance（FID）和结合了两个度量的新度量，一个度量邻近帧之间的一致性，另一个度量整个视频的多样性。本文提出的方法在所提出的评估指标上在两个数据集上均取得了最佳成绩。

#### 实验结果与分析
本文提出的方法在AFHQ和CelebA-HQ数据集上均取得了最佳成绩。在FID和结合了两个度量的新度量上，本文方法均优于其他方法。


# Paper:82     混合标签传播的深度半监督度量学习



#### 1. Title: 
Deep Semi-supervised Metric Learning with Mixed Label Propagation

#### 2. Authors: 
Furen Zhuang, Pierre Moulin

#### 3. Affiliation: 
第一作者：新加坡科学研究局信息通信研究所（Institute for Infocomm Research (I2R)）；第二作者：伊利诺伊大学厄巴纳-香槟分校（University of Illinois at Urbana-Champaign）

#### 4. Keywords: 
Metric learning, semi-supervised learning, label propagation, hard negative mining, content-based image retrieval

#### 5. Paper: https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhuang_Deep_Semi-Supervised_Metric_CVPR_2016_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究内容为半监督度量学习，旨在利用大量未标记数据来提高度量学习的性能，以解决传统图像检索系统中需要大量人工标注的问题。
 
- (2):过去的方法通常使用三元组损失函数来训练模型，但是难以采样到有用的三元组，且容易导致同一类别的数据点聚合成一个点。本文提出了一种新的度量学习方法，通过标签传播来识别“hard negative”样本，即在去除两个数据点之间的边后，标签传播得到不同标签的数据点对。这种方法可以更好地识别远离的正样本和接近的负样本，从而提高半监督度量学习的性能。

- (3):本文提出了一种混合标签传播的方法，该方法可以同时利用正样本和负样本的信息。具体来说，该方法鼓励由正边连接的数据具有相同的伪标签，而由负边连接的数据具有不同的伪标签。通过这种方法得到的伪标签可以捕捉到远离的正样本和接近的负样本的信息，从而显著提高了半监督度量学习的性能。

- (4):本文在基于内容的图像检索任务上进行了实验，使用召回率、精度和标准化互信息等性能指标来评估模型的性能。实验结果表明，本文提出的方法可以更有效地对数据库进行排序，并在返回的结果中排名靠前的位置返回相关文章。
#### 7. 方法详细介绍：
本文提出了一种新的度量学习方法，通过移除亲和矩阵中连接数据对的边缘并获取边缘的不相似权重来识别难负对。这些负边缘信息用于混合标签传播算法，该算法利用正负边缘信息。混合LP优化问题可以使用共轭梯度解决，从而使其适用于大型数据集。获得的伪标签捕获远离正对和近负对的信息，从而显着提高了半监督度量学习性能。

#### 8. 实验设置：
本文使用内容为基础的信息检索（CBIR）设置来评估所提出的方法。训练集包括每个类别的少量标记点和大量未标记数据。使用的性能指标为召回率、精确度和归一化互信息（NMI）。

#### 9. 实验结果和分析：
本文在各种数据集上进行了实验，包括飞机、汽车、CIFAR100、鸟类、狗、宠物、交通标志和VGG花卉。所提出的方法在半监督度量学习方面优于现有技术，如表1和图2、3所示。本文还将所提出的方法与最近的半监督度量学习方法进行了比较，并展示了更优秀的结果。此外，对所提出的难负挖掘和混合LP方法进行了消融研究，结果表明这些方法在提高准确性方面是有效的。还对超参数β和λ进行了敏感性分析，结果表明准确性对λ比β更敏感。


# Paper:83     OT-Filter: 一种基于最优传输的噪声标签学习过滤器



#### 1. Title: 
OT-Filter: An Optimal Transport Filter for Learning with Noisy Labels

#### 2. Authors: 
Chuanwen Feng, Yilong Ren, Xike Xie

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Deep learning, Noisy labels, Optimal transport, Sample selection, Robustness

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Feng_OT-Filter_An_Optimal_Transport_Filter_for_Learning_With_Noisy_Labels_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是深度学习中的噪声标签问题，提出了一种基于最优传输理论的样本选择方法，称为OT-Filter。 
- (2):过去的方法主要有正则化和转移矩阵，以及基于距离的过滤方法。这些方法存在一些问题，如需要先验知识、难以处理高噪声数据等。本文提出的OT-Filter方法不需要先验知识，利用最优传输提供几何意义的距离来度量样本之间的差异，同时保留概率空间中的分布信息，从而提高样本选择的质量。 
- (3):本文提出的OT-Filter方法通过最优传输构建离散概率测度，将特征向量的欧几里得空间提升到概率空间，从而能够更好地度量概率测度之间的差异。同时，该方法还能够捕捉概率空间中的分布信息，从而减轻确认偏差。通过优化传输计划，可以更好地确定样本是干净的还是有噪声的，从而提高样本选择的质量。 
- (4):本文在Clothing1M和ANIMAL-10N等基准测试中进行了广泛的实验，结果表明OT-Filter的性能优于其竞争对手。同时，在CIFAR-10/100等具有合成标签的基准测试中，OT-Filter在处理高噪声数据标签方面表现出优越性。
#### 7. 方法详细介绍：
本文提出了一种名为OT-Filter的方法，用于在带有噪声标签的情况下进行学习。该方法包括两个阶段：表示和样本选择。在表示阶段，通过计算标签类别的重心来获得干净的表示。在样本选择阶段，将所有样本的特征表示转移到干净的重心，以检测样本是否干净或标签错误。对齐被重新构造为带有稀疏正则化的最优传输问题。该方法足够灵活，支持各种鲁棒训练范式，如鲁棒监督训练和鲁棒半监督学习。

具体步骤如下：
1. 在表示阶段，使用预训练的网络提取标记样本的特征表示，并为每个类别的特征表示定义离散概率分布，以通过Wasserstein重心找到干净的表示。通过EM算法迭代优化噪声重心以获得干净重心。
2. 在传输阶段，通过稀疏正则化的最优传输将噪声特征表示转移到干净表示，并基于最优传输结果潜在地检测出损坏的标签。OT-Filter可以插入现有的鲁棒训练中。

#### 8. 实验设置：
本文在四个基准数据集上进行了广泛的实验：CIFAR-10和CIFAR-100具有合成标签噪声，Clothing1M和ANIMAL-10N具有真实标签噪声。对于CIFAR-10和CIFAR-100，考虑两种标签噪声类型：对称和非对称标签噪声。对称标签噪声是通过将一部分样本的标签随机翻转到所有其他可能的类别来生成的。非对称标签噪声的设计遵循真实世界标签噪声的结构，其中标签被翻转到超类中的相似类别。将OT-Filter与CIFAR-10上的最先进的样本选择方法DivMix进行比较，噪声率为90％（对称）和40％（非对称）。

#### 9. 实验结果和分析：
实验结果表明，OT-Filter在CIFAR-10和CIFAR-100数据集上的表现优于几种最先进的样本选择方法，特别是在噪声率高的情况下。OT-Filter在Clothing1M和ANIMAL-10N数据集上也取得了最佳性能。进行了一系列消融研究，以分析OT-Filter的关键组件的有效性，包括稀疏正则化、EM算法和熵正则化系数。结果表明，这些组件有助于提高OT-Filter的性能。


# Paper:84     利用幻想进行语义感知的虚拟对比约束进行少样本类增量学习



#### 1. Title: 
Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning

#### 2. Authors: 
Zeyin Song, Yifan Zhao, Yujun Shi, Peixi Peng, Li Yuan, Yonghong Tian

#### 3. Affiliation: 
第一作者：北京大学电子与计算机工程学院

#### 4. Keywords: 
Few-shot class-incremental learning, contrastive learning, semantic-aware, virtual classes, fantasy space

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_CVPR_2021_paper.html  Github: https://github.com/zysong0113/SAVC

#### 6. Summary : 
- (1):本文研究的是Few-shot class-incremental learning (FSCIL)问题，即如何在有限的样本中学习新类别，同时不会忘记旧类别。这是一个持续学习的问题，近年来备受关注。

- (2):目前解决FSCIL问题的主流方法是在基础会话中使用交叉熵（CE）损失进行训练，然后冻结特征提取器以适应新类别。然而，本文发现CE损失不适合基础会话训练，因为它在表示方面存在较差的类别分离，进一步降低了对新类别的泛化能力。一种解决方法是在基础会话中应用额外的监督对比学习（SCL），但是本文发现，虽然SCL可以在不同的基础类别之间创造略微更好的表示分离，但它仍然难以分离基础类别和新类别。因此，本文提出了一种新的方法，即引入虚拟类别到SCL中，从而促进新类别和基础类别之间的分离。

- (3):本文提出了一种Semantic-Aware Virtual Contrastive（SAVC）模型，通过引入虚拟类别来提高FSCIL的性能。这些虚拟类别不仅在表示空间中充当未见类别的占位符，而且还提供多样的语义信息。通过在fantasy space中学习识别和对比，SAVC显著提高了基础类别分离和新类别泛化，实现了新的最先进性能。

- (4):本文在三个广泛使用的FSCIL基准数据集上进行了大量实验，证明了SAVC的优越性能。
#### 7. 方法详细介绍：
本文提出了一种基于增量冻结框架的语义感知虚拟对比（SAVC）模型，用于少样本类增量学习。该模型引入了虚拟类来进行有监督对比学习，通过预定义的变换生成虚拟类，作为表示空间中未见过的类的占位符，并提供多样的语义信息。SAVC模型通过学习在虚拟类的fantasy空间中进行识别和对比，显著提高了基类的分离度和新类的泛化能力。该模型使用每个样本的有监督对比损失和交叉熵损失的组合作为训练目标，通过连续的任务序列进行训练，其中不同任务的标签空间没有重叠。推理过程使用最近类均值算法来评估所有遇到的类的准确性。

#### 8. 实验设置：
本文在CIFAR100基准测试集上进行实验，使用增量冻结框架进行训练，其中在基础训练阶段之后完全固定骨干网络，每个增量会添加一些新类的样本。实验中使用ResNet-12作为骨干网络，学习率设置为0.1，在第60和80个epoch时降低10倍，总共训练100个epoch。批量大小设置为128，优化器使用带有动量0.9和权重衰减0.0005的SGD。使用十次不同的随机种子重复实验，并报告平均准确率和标准差。

#### 9. 实验结果和分析：
本文将提出的SAVC模型与基线方法在CIFAR100基准测试集上进行比较。实验结果表明，SAVC模型在分类准确率和类别分离度方面优于基线方法。本文还引入了几个度量类别分离度和泛化能力的指标，包括类间距离、类内距离和类别分离度。结果表明，所提出的方法实现了更紧凑的基础嵌入空间和更大的新旧类别之间的相互分离度。


# Paper:85     单幅图像相机校准的透视场方法



#### 1. Title: 
Perspective Fields for Single Image Camera Calibration

#### 2. Authors: 
Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, David F. Fouhey

#### 3. Affiliation: 
University of Michigan（密歇根大学）

#### 4. Keywords: 
Camera calibration, Perspective Fields, Neural network, Image compositing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jin_Perspective_Fields_for_Single_Image_Camera_Calibration_CVPR_2021_paper.html  Github: https://jinlinyi.github.io/PerspectiveFields/

#### 6. Summary : 
- (1):本文研究的背景是几何相机校准，该校准通常用于理解图像的透视。然而，由于野外图像的多样性，如不同类型的相机和镜头，以及图像的裁剪、变形等，估计相机的校准是具有挑战性的。

- (2):过去的方法通常使用简化的针孔相机模型，假设中心主点，并由外部属性（如滚动、俯仰）和内部属性（如视场）参数化。然而，这些方法的局限性在于，它们无法处理图像中心不在主点上的情况。本文提出了一种称为“Perspective Fields”的表示方法，该方法模拟了图像的局部透视属性。Perspective Fields包含有关相机视图的每个像素的信息，参数化为Up-vector和Latitude值。这种表示方法对相机模型的假设最少，并且对常见的图像编辑操作（如裁剪、变形和旋转）具有不变性或等变性。此外，它更具可解释性和与人类感知更加一致。

- (3):本文提出了一种基于神经网络的方法，用于从单个图像中预测Perspective Fields，并且可以轻松地将预测的Perspective Fields转换为校准参数。此外，本文还提出了ParamNet，用于从Perspective Fields中有效地推导相机参数。本文的贡献在于提出了一种局部和非参数化的图像表示方法，该方法不假设相机投影模型，并且可以用于多个相机投影模型。本文的方法在单个图像相机参数估计方面表现出色，并且可以用于图像合成和对象切割校准等应用。

- (4):本文的方法在单个图像相机参数估计方面表现出色，相比现有方法，可以减少40％的俯仰误差。此外，本文的方法可以用于图像合成，以基于局部Perspective Field匹配度量来对前景对象和背景之间的相机视图进行对齐。本文的方法在视角对齐方面更加符合人类感知。
#### 7. 方法详细介绍：
本文提出了一种称为“Perspective Fields”的表示方法，用于单张图像的相机标定。Perspective Fields包含每个像素的相机视角信息，由Up-vector和Latitude值参数化。Up-vector给出每个像素的世界坐标系上方向，等于投影到图像上的3D场景的反重力方向。Latitude是入射光线与水平面之间的角度。本文训练了一个神经网络，从单个图像中提取360度全景图中可以轻松获得地面真实监督的裁剪图像，以预测Perspective Fields。本文还提出了ParamNet，以有效地从Perspective Fields中推导相机参数。

#### 8. 实验设置：
作者在360Cities的多样化全景数据集上训练了他们的网络，其中包括30,534个室内、51,157个自然和110,879个街景。他们从全景图中均匀采样裁剪，其中相机Roll在[-45度，45度]，Pitch在[-90度，90度]，FoV在[30度，120度]。他们使用随机颜色抖动、模糊、水平翻转、旋转和裁剪来增强训练数据。他们还在其他相机模型上展示了结果，如鱼眼图像。作者在公开可用的数据集上测试了不同方法的泛化性能，包括Stanford2D3D和TartanAir，其中有地面真实相机参数。没有任何一种方法是在测试集上进行训练的。

#### 9. 实验结果和分析：
本文提出的方法在更一般的裁剪数据集上表现出色，其中图像具有非中心化的主点。与CTRL-C相比，该方法减少了Roll（19％）、Pitch（40％）和FoV（31％）的误差，反映出纬度精度增加了超过13％，上向量精度增加了2％。在主点居中的测试集上，与以前的方法相比，该方法实现了可比的相机标定性能，在Pitch和FoV方面具有更低的误差和相当的Roll中位数误差。本文还提出了Perspective Field Discrepancy（PFD）作为透视不匹配的度量，它定义为Up-vector和Latitude值之间的差异之和。作者进行了用户研究，以评估他们提出的度量与人类感知的关系，并显示人类对Perspective Fields不一致性比其他现有的图像透视度量更敏感。他们还展示了从Perspective Fields进行图像编辑的应用。


# Paper:86     回到源头：扩散驱动的测试时间自适应方法应对数据污染



#### 1. Title: 
Back to the Source: Diffusion-Driven Adaptation to Test-Time Corruption

#### 2. Authors: 
Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, Dequan Wang

#### 3. Affiliation: 
Jin Gao, Jialing Zhang, and Dequan Wang are affiliated with Shanghai Jiao Tong University and Shanghai Artificial Intelligence Laboratory.

#### 4. Keywords: 
Test-time adaptation, diffusion model, input adaptation, image corruptions, robustness.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_to_Test-Time_Corruption_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究深度神经网络在测试数据与训练数据不一致时的自适应问题，提出了一种基于扩散模型的测试数据自适应方法。
- (2):过去的方法主要是在目标域上重新训练模型，但这种方法对数据量和优化超参数敏感。本文提出的方法是在目标数据上进行输入自适应，通过扩散模型将所有测试输入投影到源域中，从而避免了昂贵的领域特定重新训练。本文的方法在多种数据集上的实验表明，相比于模型自适应，输入自适应更加鲁棒。
- (3):本文提出了一种基于扩散模型的测试数据自适应方法，通过在源数据上训练扩散模型，将目标数据投影回源域，然后使用源分类器进行预测。本文的方法不需要目标数据，可以适应任意未知目标输入，且可以适应单个测试输入。本文的方法在输入自适应方面进行了创新，通过图像引导和分类器自集成来自动决定自适应程度。
- (4):本文的方法在ImageNet-C基准测试中，相比于现有的测试时间自适应方法，具有更高的鲁棒性，可以适应小批量、有序数据和多种数据的情况。
#### 7. 方法详细介绍：
本文提出了一种基于扩散模型的自适应方法，称为扩散驱动自适应（DDA）。DDA使用扩散模型对测试数据进行投影，使其更接近源域数据。该方法使用生成式扩散模型，通过前向扩散过程对输入图像进行噪声扰动，然后通过反向扩散过程进行迭代引导，生成更接近源数据的图像，同时尽量不改变类别信息。DDA还使用自我集成技术来聚合原始和适应后的输入的预测结果，以提高鲁棒性。DDA的分类和生成模型在所有领域共享，避免了昂贵的领域特定重新训练。DDA还使用图像引导和分类器自我集成来自动决定适应程度。DDA的输入适应比模型适应更具有鲁棒性，适用于各种损坏、模型和数据情况下的ImageNet-C基准测试。

#### 8. 实验设置：
本文在ImageNet-C基准测试上进行了实验，以评估DDA的鲁棒性并将其与现有的测试时间自适应方法进行比较。实验评估和分析了DDA，并将其与DiffPure、Tent、MEMO和BUFR等方法进行了比较。实验使用了ResNet-50、ConvNeXt和Swin Transformer架构，没有重新调整。

#### 9. 实验结果和分析：
DDA在分集设置中比MEMO和DiffPure具有更高的鲁棒性。在最新的Swin-T和ConvNeXt-T模型上，DDA仍然提供了约5个百分点的提升。DDA在IN-C的各种损坏中是最具鲁棒性的。在联合适应设置中，DDA比Tent和其他累积更新更具鲁棒性。DDA的准确性与批量大小和数据顺序无关，并在每个设置中提高了鲁棒性。DDA的实验结果表明，相比于仅使用源模型和最先进的扩散防御方法（DiffPure），DDA在不同类型的损坏下提高了鲁棒性。DDA的准确率为44.2％，而DiffPure的准确率为28.8％，源模型的准确率为39.3％。DDA中使用的自我集成技术可以防止在某些类型的损坏下出现灾难性的准确率下降。


# Paper:87     扩散视频自编码器：通过分解视频编码实现时间上一致的人脸视频编辑



#### 1. Title: 
Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding

#### 2. Authors: 
Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, Eunho Yang

#### 3. Affiliation: 
第一作者：韩国科学技术院（KAIST）

#### 4. Keywords: 
Face video editing, diffusion autoencoder, disentangled video encoding, temporal consistency, identity feature

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_Diffusion_Video_Autoencoders_Toward_Temporally_Consistent_Face_Video_Editing_via_CVPR_2021_paper.html  Github: https://github.com/diff-video-ae/diffusion-video-ae

#### 6. Summary : 
- (1):本文研究背景是人脸视频编辑，其中一个主要挑战是在编辑后的帧之间保持时间上的一致性。

- (2):过去的方法主要是基于GAN的方法，但是存在重建问题和时间上的不一致性问题。本文提出了一种基于扩散自编码器的人脸视频编辑框架，可以成功地从给定视频中提取分解的特征，包括身份和运动，从而实现时间上的一致性。本文的方法可以通过简单地操作时间不变特征来编辑视频，而不是编辑所有帧的潜在特征，这在计算上更加优越。

- (3):本文提出了一种新的人脸视频编辑框架，称为扩散视频自编码器，它可以将视频分解为单个时间不变和每帧时间变化的特征，以实现时间上的一致性编辑。本文的方法可以通过简单地操作时间不变特征来编辑视频，而不是编辑所有帧的潜在特征，这在计算上更加优越。本文的方法可以通过简单地操作时间不变特征来编辑视频，而不是编辑所有帧的潜在特征，这在计算上更加优越。

- (4):本文的方法在CelebA-HQ数据集上进行了实验，可以成功地将视频分解为时间不变和每帧变化的特征，并提供了时间上一致的编辑。本文的方法可以通过简单地操作时间不变特征来编辑视频，而不是编辑所有帧的潜在特征，这在计算上更加优越。本文的方法在编辑任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为扩散视频自编码器的方法，用于通过解缠绕视频编码实现时间上一致的人脸视频编辑。该模型通过预训练的编码器提取图像的身份和关键点特征，并通过一个可学习的MLP将它们映射到高级语义空间中的人脸特征。模型使用DDPM损失和正则化损失进行训练，以防止人脸信息泄漏到背景中。视频编辑框架涉及对所有视频帧进行编码，提取代表性的身份和关键点特征，并通过使用预训练的线性属性分类器或CLIP引导身份特征优化方法来编辑身份特征进行操作。

#### 8. 实验设置：
扩散视频自编码器在VoxCeleb1数据集的77294个视频上进行训练。视频的帧与之前的工作一样进行对齐和裁剪，并调整为256x256大小。使用[20]改进的UNet用于噪声估计。模型使用批量大小为16进行100万步的训练。

#### 9. 实验结果与分析：
本文使用SSIM、MS-SSIM、LPIPS和MSE指标在VoxCeleb1测试集中随机选择的20个视频上比较了所提出模型与e4e和PTI的重建能力。扩散视频自编码器在T = 100时显示出最佳的重建能力，并优于仅使用T = 20的e4e。本文还使用TL-ID和TG-ID指标评估了所提出模型的时间一致性，并将其与先前的工作进行了比较。所提出的模型显示出最佳的全局连贯性和可比较的局部一致性。本文还通过进一步的消融研究证明了所提出模型对异常样本的鲁棒性和编码特征的解缠绕。


# Paper:88     基于多平面特征表示的高效视图合成和基于3D的多帧去噪



#### 1. Title: 
Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations

#### 2. Authors: 
Thomas Tanay, Aleˇs Leonardis, Matteo Maggioni

#### 3. Affiliation: 
华为诺亚方舟实验室 (Huawei Noah's Ark Lab)

#### 4. Keywords: 
Multi-frame denoising, novel view synthesis, multiplane image, multiplane feature, encoder-renderer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tanay_Efficient_View_Synthesis_and_3D-Based_Multi-Frame_Denoising_With_Multiplane_Feature_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在提出一种基于多平面特征表示的3D多帧去噪方法，该方法通过在特征空间中操作多平面表示来扩展新视图合成的MPI框架。 
- (2):现有的多帧恢复方法主要使用2D对齐技术来结合多个输入图像的信息，而本文提出的方法则使用体积场表示，通过MPI框架来处理噪声问题。MPI框架的编码器-渲染器模型可以处理深度信息，但是在多视角下，MPI框架难以预测自洽的多平面表示，这会导致合成视图中的深度离散化伪影。本文提出的MPF编码器-渲染器模型通过在渲染阶段引入可学习的渲染器来解决这个问题，从而提高了场景编码的表达能力和整体性能。 
- (3):本文提出的MPF编码器-渲染器模型通过在特征空间中操作多平面表示来扩展MPI框架，从而解决了多帧去噪问题。编码器和渲染器模块在端到端训练中学习如何以无监督的方式分离深度，从而产生多平面特征表示。 
- (4):本文在Spaces和Real Forward-Facing数据集以及原始burst数据上进行了实验，验证了该方法在视图合成、多帧去噪和噪声条件下的视图合成方面的有效性。结果表明，该方法在性能上显著优于现有的2D和3D方法，并且计算成本更低。
#### 7. 方法详细介绍：
本文提出了一种基于多平面特征表示（MPFER）的多帧去噪方法。该方法使用Unet64训练MPFER模型，将8帧输入模型，其中包括目标帧及其7个最近的相邻帧。该方法通过前向变换将输入图像转换为平面扫描体（PSV），并使用编码器将PSV编码为多平面特征表示（MPF）。然后，将MPF反向变换为一组新视图，并使用渲染器将其转换为最终的合成或去噪输出。该方法在多帧去噪和视角合成方面的性能均优于其他方法，并且计算复杂度更低。

#### 8. 实验设置：
本文使用LLFF-N数据集进行实验，该数据集包含35个场景用于训练和8个场景用于测试，图像分辨率为756×1008。实验分为视角合成、多帧去噪和噪声条件下的视角合成三个部分。在视角合成方面，本文使用DeepView、Soft3D和三个标准MPI管道的变体进行比较。在多帧去噪方面，本文将输入设置为一个位置的16个视图，图像受到噪声的影响，目标是相同的16个视图去噪。本文使用信号依赖的高斯分布进行合成噪声，并关注具有中等到高增益水平的挑战性场景。

#### 9. 实验结果和分析：
在视角合成方面，本文提出的MPFER方法在PSNR方面优于其他方法，与DeepView相比，PSNR提高了1.8dB，计算复杂度更低。在多帧去噪方面，MPFER方法在高噪声水平下的PSNR提高了2dB以上，计算复杂度更低。本文还进行了定性评估，结果表明MPFER方法能够重建具有更好细节的场景。


# Paper:89     一种基于像素和补丁级别的统一HDR成像方法



#### 1. Title: 
A Unified HDR Imaging Method with Pixel and Patch Level

#### 2. Authors: 
Qingsen Yan, Weiye Chen, Song Zhang, Yu Zhu, Jinqiu Sun, Yanning Zhang

#### 3. Affiliation: 
第一作者：西北工业大学

#### 4. Keywords: 
HDR imaging, deghosting, deep neural networks, patch aggregation, ghost attention, Transformer-based fusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yan_A_Unified_HDR_Imaging_Method_With_Pixel_and_Patch_Level_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是如何将不同曝光的低动态范围图像映射到高动态范围图像，以及如何解决动态场景中由物体运动或相机抖动引起的幽灵影响问题。

- (2):过去的方法包括基于对齐、基于拒绝和基于补丁的方法，以及基于深度神经网络的方法。这些方法在处理运动和饱和度同时存在的情况下表现不佳。本文提出了一种混合HDR去幽灵网络，称为HyHDRNet，它可以学习参考和非参考图像之间的复杂关系，以在各种情况下生成视觉上令人满意的HDR图像。该方法包括内容对齐子网络和基于Transformer的融合子网络。内容对齐子网络使用补丁聚合和幽灵注意力来集成来自其他非参考图像的相似内容，并在像素级别抑制不需要的组件。为了实现补丁级别和像素级别之间的相互指导，我们利用门控模块充分交换幽灵和饱和区域中的有用信息。此外，为了获得高质量的HDR图像，基于Transformer的融合子网络使用残差可变形Transformer块（RDTB）来自适应地合并不同曝光区域的信息。 

- (3):本文提出了一种混合HDR去幽灵网络，它可以学习参考和非参考图像之间的复杂关系，以在各种情况下生成视觉上令人满意的HDR图像。该方法包括内容对齐子网络和基于Transformer的融合子网络。内容对齐子网络使用补丁聚合和幽灵注意力来集成来自其他非参考图像的相似内容，并在像素级别抑制不需要的组件。为了实现补丁级别和像素级别之间的相互指导，我们利用门控模块充分交换幽灵和饱和区域中的有用信息。此外，为了获得高质量的HDR图像，基于Transformer的融合子网络使用残差可变形Transformer块（RDTB）来自适应地合并不同曝光区域的信息。 

- (4):本文在四个公共HDR图像去幽灵数据集上进行了实验，结果表明HyHDRNet在定量和定性方面均优于现有方法，实现了具有统一纹理和颜色的吸引人的HDR可视化效果。该方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为HyHDRNet的统一HDR成像方法，它将门控聚合模块（GA）、补丁聚合模块（PA）和可变形变换器（DT）集成在一起，实现了像素和补丁级别的融合。GA模块用于抑制幽灵伪影，PA模块用于聚合相似的补丁以完成未对齐或饱和的区域。DT模块用于自适应地融合更大的感受野中有用的纹理。该方法在两个公共数据集上实现了最先进的性能，并优于几种先前的SOTA方法。具体步骤包括：
1. 使用三个卷积层从LDR序列中提取浅层特征。
2. 使用PA和GA模块分别聚合非参考LDR图像中的有用补丁和识别易于处理的未对齐组件。
3. 使用门控模块恢复饱和区域和运动中的锐利边缘的内容。
4. 使用RDTB动态合并特征以生成高质量的HDR图像。

#### 8. 实验设置：
本文使用Kalantari数据集和Hu数据集进行训练，其中Kalantari数据集包括74个训练和15个测试样本，而Hu数据集包括85个训练和15个测试样本。本文在Sen数据集和Tursun数据集上进行评估，评估指标包括PSNR-L、PSRN-µ、SSIM-L、SSIM-µ和HDR-VDP-2。在训练阶段，使用128x128的补丁进行训练，批量大小为4，学习率为0.0002，使用Adam优化器，每50个epoch降低学习率0.1。本文使用PyTorch实现，使用2个NVIDIA GeForce 3090 GPU进行训练，训练150个epoch。

#### 9. 实验结果和分析：
本文提出的HyHDRNet方法在两个公共数据集上实现了最先进的性能，并优于几种先前的SOTA方法。通过消融实验验证了HyHDRNet中每个模块的有效性。在评估指标方面，HyHDRNet在PSNR-L、PSRN-µ、SSIM-L、SSIM-µ和HDR-VDP-2方面均优于其他方法。


# Paper:90     教授结构化视觉和语言概念给视觉和语言模型



#### 1. Title: 
Teaching Structured Vision & Language Concepts to Vision & Language Models

#### 2. Authors: 
Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, Leonid Karlinsky

#### 3. Affiliation: 
第一作者：IBM Research

#### 4. Keywords: 
Vision and Language, Structured Vision & Language Concepts, VL models, SVLC, pre-training datasets

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_Models_CVPR_2021_paper.html  Github: https://github.com/SivanDoveh/TSVLC

#### 6. Summary : 
- (1):本文研究背景是Vision and Language (VL)模型在各种任务中表现出了出色的零样本性能，但是在复杂语言理解方面仍存在挑战，特别是在结构化视觉和语言概念（SVLC）方面。
- (2):过去的方法是通过收集专门的大规模数据集来教授VL模型这些缺失的技能，但是这种方法既困难又昂贵。本文提出了一种更优雅的数据驱动方法，通过利用现有的VL预训练数据集来增强VL模型对SVLC的理解，而不需要任何额外的数据。本文提出了基于语言结构理解的各种技术，可以用于操作配对VL数据集的文本部分，从而提高VL模型对SVLC的理解。
- (3):本文提出了一种数据驱动的方法，利用现有的VL预训练数据源来提高VL模型对SVLC的理解技能，同时保持其零样本对象识别准确性。本文提出了几种实现这种方法的技术，包括基于经典NLP解析和单词替换词汇的基于规则的先验，通过提示大型语言模型（LLM）进行类比文本，通过LLM基于解析文本实体的去掩码生成不同含义（负面）替代品，以及这些方法的组合。本文证明了所有这些技术都可以导致显着的改进，当衡量VL模型的SVLC理解时，可以获得高达15%的性能提升。本文在5个数据集上验证了这一点：VG，HAKE，VAW，SWIG和所有组合，使用VL-Checklist最近提出的协议。此外，本文还展示了所得到的VL模型在很大程度上保留了其零样本对象识别性能。
- (4):本文的方法在多个数据集上进行了测试，证明了其在提高VL模型对SVLC的理解方面的有效性。同时，本文的方法还能够保持VL模型的零样本对象识别准确性。本文的方法可以在多个下游任务中得到应用，如零样本检测、分割、图像生成等。
#### 7. 方法详细介绍：
本文提出了一种数据驱动的方法，通过操作现有的配对视觉语言数据集的文本部分，增强视觉语言模型对结构化视觉语言概念（SVLC）的理解。该方法包括基于规则的先验、提示大型语言模型进行类比文本生成、基于LLM的解码生成不同含义的负面文本等技术。生成的文本用于添加损失，以强制区分不同（原始和生成的）SVLC文本，并且不再满足“物体袋”表示。最终的损失函数结合了对比损失、负面损失和类比损失。

#### 8. 实验设置：
本文在VL-Checklist数据集上验证了所提出的方法，该数据集包括VG、HAKE、VAW、SWIG和所有数据集的组合，并使用CLIP和CyCLIP进行评估。本文还使用了21个零样本分类数据集，包括ImageNet、CIFAR100和EuroSat，并在4个V100 NVIDIA GPU上进行了5个时期的训练，总批量为512。

#### 9. 实验结果和分析：
本文的方法在VL-Checklist数据集上取得了显著的改进，SVLC理解性能提高了15%。在保持零样本物体识别性能的同时，VL模型的SVLC理解性能得到了显著提高。本文还进行了对比实验，证明了所提出的方法的有效性。在21个零样本分类数据集上，本文的方法在保持零样本性能的同时，显著提高了VL模型的SVLC理解性能。


# Paper:91     PDPP：面向教学视频的投影扩散过程规划



#### 1. Title: 
PDPP: Projected Diffusion for Procedure Planning in Instructional Videos

#### 2. Authors: 
Hanlin Wang, Yilu Wu, Sheng Guo, Limin Wang

#### 3. Affiliation: 
第一作者所属机构：南京大学新型软件技术国家重点实验室，上海人工智能实验室

#### 4. Keywords: 
Procedure planning, instructional videos, diffusion model, distribution fitting, task supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PDPP_Projected_Diffusion_for_Procedure_Planning_in_Instructional_Videos_CVPR_2021_paper.html  Github: https://github.com/MCG-NJU/PDPP

#### 6. Summary : 
- (1):本文研究了教学视频中的过程规划问题，旨在根据当前视觉观察结果制定目标导向的计划。 
- (2):以往的方法将此问题视为序列规划问题，并利用中间视觉观察结果或自然语言指令作为监督，导致学习方案复杂且注释成本高昂。相比之下，本文将此问题视为分布拟合问题。在这个意义上，我们使用扩散模型（PDPP）对整个中间动作序列分布进行建模，从而将规划问题转化为从该分布中进行采样的过程。此外，我们消除了昂贵的中间监督，并仅使用教学视频中的任务标签作为监督。我们的模型是基于U-Net的扩散模型，可以直接从给定的起始和结束观察结果中从学习的分布中采样动作序列。此外，我们应用了一种有效的投影方法，在学习和采样过程中为我们的模型提供准确的条件指导。 
- (3):本文提出了一种新颖的投影扩散模型（PDPP）来学习动作序列的分布，并在一次采样过程中生成所有中间步骤。我们的模型只需要任务类别标签作为监督，而不需要昂贵的中间视觉或语言注释。我们的模型可以通过简单的学习方案实现条件扩散过程，并在三个不同规模的数据集上进行实验，展现出了多个指标的最先进性能，即使没有任务监督，仅使用动作标签也可以实现出色的结果。 
- (4):本文的方法在教学视频中的过程规划任务上取得了最先进的性能，证明了其有效性和可行性。
#### 7. 方法详细介绍：
本文提出了一种基于Projected Diffusion for Procedure Planning (PDPP)模型的教学视频过程规划方法。该模型包含两个阶段：任务分类器模型训练和投影扩散模型拟合目标动作序列分布。在训练阶段，使用U-Net模型和加权损失函数计算训练损失。在推理阶段，提供起始和目标观察，使用预测的任务类别作为类别条件从学习的动作序列分布中进行采样，通过选择每个最大值的索引来获得动作序列计划。该模型使用条件投影操作来保持观察和条件维度在训练和推理过程中的不变性。

#### 8. 实验设置：
本文在三个教学视频数据集（CrossTask、NIV和COIN）上评估了PDPP模型的性能。从视频中提取起始和目标观察，通过滑动大小为T的窗口获得动作序列。使用成功率（SR）、平均准确率（mAcc）和平均交并比（mIoU）指标评估性能，并与其他过程规划模型和完全监督规划方法进行比较。

#### 9. 实验结果和分析：
本文评估了所提出方法在三个数据集上的性能，并发现在多个规划时间范围内，该模型在成功率、平均准确率和平均交并比等指标上均取得了最先进的性能。研究了任务监督的作用，并发现它有助于所提出的方法实现更好的性能，特别是对于COIN数据集。还评估了所提出方法的概率建模，并发现它在NLL和KL-Div指标方面优于噪声和确定性基线。


# Paper:92     STMT：一种用于基于MoCap的动作识别的空间-时间网格变换器



#### 1. Title: 
STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition

#### 2. Authors: 
Xiaoyu Zhu, Po-Yao Huang, Junwei Liang, Celso M. de Melo, Alexander Hauptmann

#### 3. Affiliation: 
第一作者：Carnegie Mellon University（卡内基梅隆大学）

#### 4. Keywords: 
Motion capture, action recognition, mesh representation, transformer, self-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_STMT_A_Spatial-Temporal_Mesh_Transformer_for_MoCap-Based_Action_Recognition_CVPR_2021_paper.html  Github: https://github.com/zgzxy001/STMT

#### 6. Summary : 
- (1):本文研究了使用运动捕捉（MoCap）序列进行人类动作识别的问题。与现有技术不同的是，该文提出了一种新颖的空间-时间网格变换器（STMT），直接对网格序列进行建模，避免了多个手动步骤，提高了识别性能。

- (2):现有的MoCap动作识别方法通常将身体标记转换为人体网格，然后预测标准化的3D骨架。然而，这些方法需要多个手动步骤来映射网格到骨架，且骨架表示丢失了原始MoCap数据提供的信息（即表面运动和身体形状知识）。本文提出的STMT方法直接对原始网格序列进行建模，避免了这些缺点。

- (3):本文提出了一种新颖的空间-时间网格变换器（STMT），使用具有内部帧偏移注意力和帧间自注意力的分层变换器。该注意机制允许模型自由地关注任何两个顶点补丁，以学习空间-时间域中的非局部关系。本文还定义了两个自监督学习任务，即掩码顶点建模和未来帧预测，以完全激活我们的分层变压器中的双向和自回归注意力。该方法在常见的MoCap基准测试上实现了最先进的性能。

- (4):本文提出的方法在常见的MoCap基准测试上实现了最先进的性能，相比于基于骨架和基于点云的模型，具有更好的识别性能。
#### 7. 方法详细介绍：
本文提出了一种名为空间-时间网格变换器（Spatial-Temporal Mesh Transformer，STMT）的方法，用于基于运动捕捉（MoCap）的动作识别。该方法使用层次变换器来学习原始网格序列的空间-时间特征。模型还采用自监督学习任务，包括掩码顶点建模（Masked Vertex Modeling，MVM）和未来帧预测（Future Frame Prediction，FFP），进行预训练。模型使用交叉熵损失进行微调，并使用Chamfer距离损失来衡量模型预测和地面真实网格序列之间的差异。具体步骤包括：
1. 表面场卷积（Surface Field Convolution）：通过考虑内在和外在网格表示来构建局部顶点补丁。
2. 层次空间-时间变换器（Hierarchical Spatial-Temporal Transformer）：学习顶点补丁的空间-时间相关性。
3. 自监督预训练（Self-Supervised Pre-Training）：学习外观和运动方面的全局上下文。其中，提出了联合洗牌方法（Joint Shuffle Method）来通过洗牌身体姿势参数来增强SMPL-H序列。

#### 8. 实验设置：
本文在两个广泛使用的基准数据集KIT和BABEL上进行了评估。KIT数据集有56个类别，共6570个序列，而BABEL数据集有60个类别，共21653个序列。模型使用Adam优化器，预训练200个epoch，微调50个epoch，批量大小分别为32和64，学习率为0.0001。模型使用交叉熵损失进行微调，并使用Chamfer距离损失来衡量模型预测和地面真实网格序列之间的差异。

#### 9. 实验结果与分析：
本文提出的STMT模型在MoCap-based动作识别上优于现有的基于骨架和基于点云的模型，在网格序列上实现了64.04%的top-1分类精度。此外，对嘈杂的身体姿势估计进行的实验表明，与使用3D骨架或点云作为输入的其他方法相比，使用网格作为输入的STMT更具有鲁棒性。模型的每个组件都获得了一致的改进，包括在顶点补丁中编码内在和外在特征以及使用MVM和FFP进行自监督预训练。模型在同时使用MVM和FFP时获得了最佳性能，证明了这两个自监督任务互为补充。


# Paper:93     通过分层潜变量模型理解遮蔽自编码器



#### 1. Title: 
Understanding Masked Autoencoders via Hierarchical Latent Variable Models

#### 2. Authors: 
Lingjing Kong, Martin Q. Ma, Guangyi Chen, Eric P. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang

#### 3. Affiliation: 
Lingjing Kong, Martin Q. Ma, Guangyi Chen, Eric P. Xing, Yuejie Chi: Carnegie Mellon University
Louis-Philippe Morency, Kun Zhang: Mohamed bin Zayed University of Artificial Intelligence

#### 4. Keywords: 
Masked autoencoder, self-supervised learning, hierarchical latent variable model, identifiability, downstream tasks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究了基于遮蔽图像区域重建的简单有效的自监督学习框架——遮蔽自编码器（MAE），并提出了一种理论上的解释和证明。 
- (2):相比于以往的方法，MAE在下游任务中表现出色，但其理论基础仍然缺乏。本文通过将数据生成过程建模为分层潜变量模型，证明了MAE可以在合理的假设下识别分层模型中的一组潜变量，从而解释了MAE如何从像素中提取高级信息。此外，本文还探讨了MAE中的关键超参数（遮蔽比例和补丁大小）如何影响表示中的语义信息水平。 
- (3):本文提出了一种理论框架，可以理解MAE并提供可识别性保证。具体而言，我们首先将潜在的数据生成过程建模为分层潜变量模型，并表明在合理的假设下，MAE可以识别分层模型中的一组潜变量。其次，我们展示了如何通过遮蔽重建来学习图像中的长程统计依赖性，从而提取高级语义表示。 
- (4):本文在多个视觉任务上验证了MAE的性能，结果表明MAE在适当的遮蔽比例下可以提取高级语义信息，而过高或过低的遮蔽比例会导致低级表示。
#### 7. 方法详细介绍：
本文提出了一种层次化潜变量模型来理解 Masked Autoencoder (MAE)。该模型由一组潜变量 c 组成，表示输入的可见部分和被遮盖部分之间的共享信息。本文提出了定理1来定位每个特定掩码 m 的共享信息 c，以及算法1和算法2来找到 c 及其对应的 sm 和 smc。本文还提出了定理2，表明 MAE 学习目标估计了定理1中指定的 c，并且 MAE 实现了一种 c 的可识别性。本文讨论了定理的假设和解释，并提供了有关遮盖如何影响学习表示的见解。

#### 8. 实验设置：
本文使用 ViT-Base 作为骨干网络，在 ImageNet-1K 上进行 MAE 管道的预训练。本文进行了两组预训练：1）将补丁大小固定为16，遮盖比例从 {0.1、0.25、0.5、0.75、0.9} 变化；2）将遮盖比例固定为0.75，补丁大小从 {8、16、32} 变化。本文在附录中提供了实验细节。

#### 9. 实验结果与分析：
本文评估了 Masked Autoencoder via Hierarchical Latent Variable Models (MAE) 在各种指标上的性能，包括 PSNR、SSIM、FSIM 和负 MSE。评估是在 IN1K 评估集上进行的，结果表明中间遮盖比例（0.25、0.5 和 0.75）在捕获高级信息方面表现更好，而极端遮盖比例（0.1 和 0.9）则捕获更多低级信息。本文还展示了 T-SNE 嵌入和线性探测结果，以评估 MAE 模型在不同数据集上的鲁棒性，包括 ImageNet 变体和 ObjectNet。结果表明，非极端遮盖比例和更大的补丁大小生成更具线性可分性的嵌入和更强的鲁棒性表现，而极端遮盖比例或补丁大小则不是这样。最后，本文在 COCO 数据集上评估了 MAE 模型在目标检测和分割任务上的质量，结果表明 0.75 的遮盖比例为这些任务生成了最佳的语义理解。


# Paper:94     蒸馏神经场用于实时关节形状重建



#### 1. Title: 
Distilling Neural Fields for Real-Time Articulated Shape Reconstruction

#### 2. Authors: 
Jeff Tan, Gengshan Yang, Deva Ramanan

#### 3. Affiliation: 
Carnegie Mellon University（卡内基梅隆大学）

#### 4. Keywords: 
Articulated shape reconstruction, dynamic NeRF, real-time, distillation, feed-forward network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tan_Distilling_Neural_Fields_for_Real-Time_Articulated_Shape_Reconstruction_CVPR_2021_paper.html  Github: https://jefftan969.github.io/dasr/

#### 6. Summary : 
- (1):本文研究的是实时关节形状重建问题，既不需要测试时优化，也不需要在训练时手动提供3D监督。过去的方法通常依赖于预先构建的可变形模型或通过可微分渲染进行缓慢的场景优化。这些方法无法支持任意对象类别，也不适用于实时应用。本文提出了一种方法，通过使用现成的基于视频的动态NeRF作为3D监督来训练快速前馈网络，将3D形状和运动预测转化为监督蒸馏任务，从而实现类别特定的实时视频形状预测器的训练。
 
- (2):过去的方法通常依赖于预先构建的可变形模型或通过可微分渲染进行缓慢的场景优化。这些方法无法支持任意对象类别，也不适用于实时应用。本文提出的方法通过使用现成的基于视频的动态NeRF作为3D监督来训练快速前馈网络，将3D形状和运动预测转化为监督蒸馏任务，从而实现类别特定的实时视频形状预测器的训练。本文的方法不需要预定义的3D模板或地面真实3D数据进行训练，可以适用于任意对象类别，且不需要测试时优化或手动3D监督。 

- (3):本文提出了一种方法，通过使用现成的基于视频的动态NeRF作为3D监督来训练快速前馈网络，将3D形状和运动预测转化为监督蒸馏任务。本文的方法使用关节和混合蒙皮来表示任意变形，可以自监督地在视频数据集上进行训练，无需输入3D形状或视点。通过蒸馏，我们的网络学习以交互帧速率3D重建未见过的关节对象。与现有的基于模型的方法相比，我们的方法不需要预定义的3D模板或地面真实3D数据进行训练，可以适用于任意对象类别，且不需要测试时优化或手动3D监督。 

- (4):本文的方法在实时视频形状预测方面取得了良好的性能，可以输出时间一致的视点、关节和外观，从而实现了高保真度的3D重建。与现有的实时方法相比，本文的方法不需要预定义的3D模板或地面真实3D数据进行训练，可以适用于任意对象类别，且不需要测试时优化或手动3D监督。
#### 7. 方法详细介绍：
本文提出了一种实时的关节形状重建方法，使用前馈预测器，结合光栅化的速度和回归顶点颜色的简单性。该方法使用神经混合蒙皮和类别级别的运动骨架来建模物体的视点和关节运动。外观则被建模为每个顶点的颜色数组，并使用弱透视相机投影进行渲染。该方法使用神经混合蒙皮模型在运动骨架上定义一个三维变形场来表示关节运动。网络架构包括单帧图像编码器、视点分支、关节运动分支和外观分支。该方法使用每个顶点的L2误差和预测和实际关节角度之间的测地距离作为损失进行标准监督训练。

#### 8. 实验设置：
本文使用现成的PointRend计算分割和DensePose计算每个像素的CSE特征。该方法使用PyTorch实现，并使用AdamW优化器进行16k次迭代训练，使用224×224的图像和56的批量大小。本文采用两阶段训练策略来降低计算成本，并确保如果只有单个图像可用于测试，则可以安全地删除时间编码器。

#### 9. 实验结果与分析：
本文提出了一种方法，通过从大规模离线视频数据拟合的动态NeRF教师中提取知识，训练类别特定的前馈视频形状预测器。该方法预测一致的视点、关节运动和外观，能够在人类、猫和狗的实时视频重建中产生结果，并支持其他类别。该方法在狗的形状和姿势方面优于现有的前馈预测器，并且使用的计算量几乎比测试时间拟合方法少1000倍。然而，该方法的性能受到教师的性能和重建保真度的上限限制，作者期望随着更大规模、更多样化和更高质量的视频数据，该方法的性能将得到改善。


# Paper:95     基于两层CANF的无运动编码分层B帧视频编码



#### 1. Title: 
Hierarchical B-frame Video Coding Using Two-Layer CANF without Motion Coding

#### 2. Authors: 
David Alexandre, Hsueh-Ming Hang, Wen-Hsiao Peng

#### 3. Affiliation: 
第一作者：国立阳明交通大学电机工程与计算机科学国际研究生课程

#### 4. Keywords: 
video compression, B-frame coding, deep learning, motion coding, CANF

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Alexandre_Hierarchical_B-Frame_Video_Coding_Using_Two-Layer_CANF_Without_Motion_Coding_CVPR_2021_paper.html  Github: https://nycu-clab.github.io/

#### 6. Summary : 
- (1):本文研究视频压缩中的B帧编码，提出了一种基于两层CANF的新型B帧编码架构，不需要传输任何运动信息，提供了一种新的学习视频编码方向。

- (2):传统的视频压缩系统由运动编码和残差编码两个主要模块组成，本文提出的编码架构不需要传输任何运动信息，通过低分辨率图像压缩器替换全分辨率运动压缩器，将低分辨率编码图像与变形的高分辨率图像合并，生成高质量图像作为增强层图像编码的条件信号，从而显著降低了计算复杂度。

- (3):本文提出了一种基于两层CANF的B帧编码框架，跳过了运动信息的编码，包括基础层和增强层两个图像编码层。其中，基础层由视频帧插值器、下采样网络、基于神经网络的图像压缩器和超分辨网络（SR-Net）组成；增强层由多帧合并网络、跳过掩码生成器、跳过模式编码模块和CANF压缩器组成。本文的创新点在于提出了一种不需要传输运动信息的视频压缩方案，采用了多帧合并网络来组合基础层和增强层的帧，构建高质量的增强层CANF压缩器的预测器。

- (4):本文在B帧编码任务上进行了实验，与其他学习B帧编码方案相比，本文的方案在编码和解码的MACs方面分别节省了45%和27%的计算量，虽然比B-CANF的学习B帧编码方案的速率失真性能略低，但优于其他学习B帧编码方案。本文的方法在视频压缩方面取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于两层条件编码的B帧视频压缩方法，使用CANF和ANF图像压缩器。该系统具有低分辨率的基础层和全分辨率的增强层。基础层包括帧插值器、下采样器、超分辨率网络和CANF压缩器。增强层包括多帧合并网络和自适应CANF压缩器。跳过模式编码机制用于减少算术编码中的位消耗。使用ADAM优化器和批量大小为8的多步训练过程进行模型训练。实现了帧类型自适应编码，用不同的模型对参考和非参考B帧进行编码。

#### 8. 实验设置：
在训练过程中，每个帧都被随机裁剪为256x256并水平和垂直翻转。测试使用GOP=32进行，评估指标为不同编码比特率下的峰值信噪比（PSNR）和多尺度结构相似性指数（MS-SSIM）。使用Vimeo90K七元组数据集进行训练，使用UVG和HEVC Class B数据集进行测试。

#### 9. 实验结果与分析：
本文提出的TLZMC方法在PSNR方面优于所有其他深度视频编解码器，除了B-CANF，后者是最先进的B帧编码方案。TLZMC在MS-SSIM方面也优于所有其他深度视频编解码器和经典编解码器。TLZMC的BD率节省优于DCVC和其他分层B帧编码方法。基础层平均消耗的比特率不到总比特率的7％，跳过掩码生成器网络显着提高了BD率节省。在不同λ值下，UVG数据集的平均保留率分别为28.28％，36.23％，57.17％和69.93％。


# Paper:96     半透明线虫的三维形状重建



#### 1. Title: 
3D shape reconstruction of semi-transparent worms

#### 2. Authors: 
Thomas P. Ilett, Omer Yuval, Thomas Ranner, Netta Cohen, David C. Hogg

#### 3. Affiliation: 
University of Leeds, Leeds, United Kingdom（英国利兹大学）

#### 4. Keywords: 
3D shape reconstruction, semi-transparent, Caenorhabditis elegans, direct image comparison, gradient descent

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了半透明物体的三维形状重建问题，以线虫Caenorhabditis elegans为例，该物体在复杂的三维环境中自由探索，其光学性质不断变化，因此传统的多视角特征匹配方法不适用。

- (2):过去的方法通常需要在多个视角中识别物体的特征或纹理，但是当物体是半透明的并且在焦点内外移动时，这种方法不可行。本文提出了一种新的方法，通过自适应模糊和透明度来渲染候选形状，与图像进行比较。使用直接图像比较和梯度下降的特征无捆绑调整算法，同时优化曲线、相机和渲染器参数，以获得半透明物体的三维姿态重建。

- (3):本文提出了一种新的视点渲染器来捕捉光学畸变和透明度，并使用不同的可微分渲染器来构建图像，以比较原始图像，生成像素级误差，以联合更新曲线、相机和渲染器参数。该方法对于气泡和污垢等干扰具有鲁棒性，能够在复杂的姿态序列中保持一致性，从模糊图像中恢复可靠的估计，并在以前的尝试中提供了显着的改进。

- (4):本文的方法在线虫三维姿态重建任务上取得了良好的性能，证明了在缺乏基础数据的情况下，在复杂的物理环境中进行形状估计的潜力。
#### 7. 方法详细介绍：
本文提出了一种针对半透明微小生物的三维重建方法。该方法采用三个摄像头拍摄不同角度的图像，并通过构建唯一的可微分渲染来解决形状重建和相机参数优化问题。该方法避免了特征提取和对应匹配，因此在处理视角之间外观变化较大的情况下具有很强的优势。该方法可以应用于任意形状模型的三维重建，使用任意数量的视角，适用于任何尺度。渲染点使用可调节的超高斯函数，有效解决了透明度和焦点问题。实验结果表明，该方法可以有效地重建出半透明微小生物的三维形状，是一种有效的替代传统方法的方法。

#### 8. 实验设置：
本文使用了三个摄像头对半透明微小生物进行拍摄，并使用了PyTorch和Adam优化算法实现了该方法。实验中使用了44个录像进行测试。

#### 9. 实验结果与分析：
本文提出的方法在43个录像中均能够生成高质量的三维中线重建。与手动标注的点相比，该方法的重建结果与之接近。该方法还能够更好地匹配原始图像，且能够在存在严重遮挡和失焦的情况下恢复生物的形状。实验结果表明，该方法在准确性和鲁棒性方面优于以往的方法。


# Paper:97     标签信息瓶颈用于标签增强



#### 1. Title: 
Label Information Bottleneck for Label Enhancement

#### 2. Authors: 
Qinghai Zheng, Jihua Zhu, Haoyu Tang

#### 3. Affiliation: 
1. 福州大学计算机与数据科学学院，中国
2. 西安交通大学软件工程学院，中国
3. 山东大学软件学院，中国

#### 4. Keywords: 
Label Enhancement, Label Distribution Learning, Label Information Bottleneck, Information Bottleneck, Multi-Label Learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Zheng_Label_Information_Bottleneck_for_Label_Enhancement_CVPR_2020_paper.html  Github: https://github.com/qinghai-zheng/LIBLE

#### 6. Summary : 
- (1):本文研究了标签增强（LE）问题，即从逻辑标签中恢复标签分布的挑战性问题。 
- (2):现有的LE方法忽略了数据集中包含的标签不相关信息，从而限制了恢复性能的进一步提高。本文提出了一种新的标签信息瓶颈（LIB）方法，通过学习标签相关信息来提高恢复性能。LIB将LE问题分解为两个联合过程：1）学习具有基本标签相关信息的表示，2）基于学习的表示恢复标签分布。LIB通过学习表示来挖掘标签相关信息，将标签相关信息分解为标签分配和标签差距两个组成部分，并联合探索这两个组成部分。 
- (3):本文提出的LIB方法在标签增强任务中，通过学习标签相关信息来恢复标签分布，相比现有方法具有更好的性能。 
- (4):在多个基准标签分布学习数据集上进行的评估实验验证了LIB的有效性和竞争力。
#### 1. 实验设置：
本文使用了一个玩具数据集和13个真实世界数据集进行评估。表1提供了数据集的详细信息，包括维度、实例和标签数量。本文采用二值化策略以确保评估的一致性。本文将提出的方法与七种标签增强（LE）方法进行比较，包括FCM、KM、LP、ML、GLLE、LESC和LEVI。为了公平比较，采用它们原始工作中推荐的参数设置。

#### 2. 实验细节：
本文通过广泛的实验评估了提出方法的有效性和竞争力。使用六个指标评估恢复性能，分别是Chebyshev、Canberra、Clark、Kullback-Leibler、Cosine和Intersection。距离度量和相似度度量的较小值表示更好的结果。表2提供了13个真实世界数据集的详细比较结果。所有方法的恢复性能可以大致排名为LIB>LESC≈LEVI>GLLE>LP≈FCM>ML>KM。本文还在SBU-3DFE数据集上进行了参数敏感性实验，探究了不同α和β值的影响。

#### 3. 方法详细介绍：
本文提出了一种标签信息瓶颈（LIB）方法来进行标签增强。该方法从信息瓶颈的角度进行尝试，有效地研究了与标签相关的信息，包括标签分配的信息和标签差距的信息。提出的方法包括三个步骤：1）学习数据的潜在表示，2）基于学习的潜在表示估计每个实例的标签分布，3）通过最小化信息瓶颈目标来增强标签分布。本文在方法部分提供了每个步骤的详细公式和解释。本文还将提出的方法与最近提出的LEVI方法进行了比较，并解释了它们之间的差异。

#### 4. 方法详细介绍：
提出的方法Label Information Bottleneck（LIB）利用信息瓶颈原理处理标签增强（LE）问题。LIB的目标是最小化Las和Lgap，其中Las挖掘标签分配到实例的信息，Lgap研究逻辑标签和分布标签之间的标签差距信息。在学习过程中，采用约束条件丢弃标签无关信息。通过使用重新参数化技巧和基于标签分布网络恢复所需的标签分布来实现LIB的优化。

#### 5. 实验结果和分析：
提供了13个真实世界数据集上的恢复结果。表格包括Canberra和Kullback-Leibler等恢复性能指标，以及用于恢复的方法，如FCM、KM、LP、ML、GLLE、LESC、LEVI和LIB。表格还报告了所有方法在所有数据集上的平均排名。LIB方法表现出有希望的恢复结果，并且在大范围内对α和β的不同值具有鲁棒性。

#### 6. 方法详细介绍：
提出的Label Information Bottleneck（LIB）方法将标签相关信息分解为标签分配信息和标签差距信息。LIB的目标是同时学习潜在表示和建模标签差距。将LIB与仅探索标签差距信息的LIBgap进行比较，实验结果表明LIB在所有情况下优于LIBgap。

#### 7. 方法详细介绍：
本文提出的方法称为Label Information Bottleneck（LIB），旨在挖掘关键的标签相关信息以提高标签增强（LE）的恢复性能。LE问题被公式化为两个联合过程：1）学习具有关键标签相关信息的表示，2）基于学习的表示恢复标签分布。标签相关信息被分解为两个组成部分，即标签分配到实例的信息和逻辑标签和分布标签之间的标签差距信息。受信息瓶颈（IB）的启发，LIB利用现有的逻辑标签探索标签分配到实例的信息，并进一步考虑逻辑标签和分布标签之间的标签差距信息。

#### 8. 实验设置：
本文使用了一个玩具数据集和13个真实世界数据集进行评估。表1提供了数据集的详细信息，包括维度、实例和标签数量。本文采用二值化策略以确保评估的一致性。本文将提出的方法与七种标签增强（LE）方法进行比较，包括FCM、KM、LP、ML、GLLE、LESC和LEVI。为了公平比较，采用它们原始工作中推荐的参数设置。

#### 9. 实验结果和分析：
本文通过广泛的实验评估了提出方法的有效性和竞争力。使用六个指标评估恢复性能，分别是Chebyshev、Canberra、Clark、Kullback-Leibler、Cosine和Intersection。距离度量和相似度度量的较小值表示更好的结果。表2提供了13个真实世界数据集的详细比较结果。所有方法的恢复性能可以大致排名为LIB>LESC≈LEVI>GLLE>LP≈FCM>ML>KM。本文还在SBU-3DFE数据集上进行了参数敏感性实验，探究了不同α和β值的影响。本文还进行了消融实验，进一步验证了引入标签信息瓶颈框架进行标签增强的有效性。


# Paper:98     GINA-3D：在野外学习生成隐式神经资产



#### 1. Title: 
GINA-3D: Learning to Generate Implicit Neural Assets in the Wild

#### 2. Authors: 
Bokui Shen, Xinchen Yan, Charles R. Qi, Mahyar Najibi, Boyang Deng, Leonidas Guibas, Yin Zhou, Dragomir Anguelov

#### 3. Affiliation: 
第一作者：斯坦福大学

#### 4. Keywords: 
Generative modeling, 3D-aware image synthesis, autonomous driving, implicit neural assets

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shen_GINA-3D_Learning_to_Generate_Implicit_Neural_Assets_in_the_Wild_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是如何从传感器数据中建模三维世界以进行仿真，以解决自动驾驶等机器学习问题中的测试和验证环境的问题。

- (2):过去的方法包括手动创建或重新创建类似于真实世界的环境，这是困难、昂贵且不可扩展的。最近的生成模型技术通过仅使用丰富的二维图像来学习3D资产，已经显示出解决这些挑战的希望，但仍然存在限制，因为它们利用人工筛选的图像数据集或手动创建的合成3D环境的渲染。本文提出了GINA-3D，它使用来自相机和LiDAR传感器的真实世界驾驶数据创建多样化的车辆和行人的逼真的3D隐式神经资产。与现有的图像数据集相比，真实世界的驾驶环境由于遮挡、光照变化和长尾分布等因素而面临新的挑战。GINA-3D通过将表示学习和生成建模分为两个阶段，使用一个学习的三平面潜在结构来解决这些挑战。

- (3):本文提出了一种名为GINA-3D的3D感知生成变压器，用于隐式神经资产生成。为了解决真实世界的挑战，我们提出了一种新颖的3D感知编码器-解码器框架，其中将三平面结构嵌入到生成模型的潜在先验中。编码器-解码器框架由一个变换编码器和一个带有神经渲染组件的解码器组成。为了处理无约束的遮挡，我们使用来自现成分割模型的伪标签，通过遮挡感知合成明确地将对象像素与其周围分离。最后，从离散码本中学习的三平面潜在先验可以用于训练条件潜在抽样模型。我们构建了一个大规模的以对象为中心的数据集，其中包含来自Waymo Open Dataset的520K多样化车辆和行人的图像以及包括建筑设备、垃圾车和缆车等长尾实例的新的80K图像。我们通过广泛的实验表明，GINA-3D在图像质量、几何一致性和几何多样性方面均优于现有的3D感知生成模型。

- (4):本文的方法在自动驾驶传感器仿真任务中取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为GINA-3D的生成模型，用于从真实世界的驾驶数据中创建多样化的车辆和行人的逼真的3D隐式神经资产。GINA-3D将表示学习和生成建模分为两个阶段，使用学习的三平面潜变量结构。编码器-解码器框架由变换编码器和带有神经渲染组件的解码器组成。为了处理无约束的遮挡，GINA-3D使用来自现成分割模型的伪标签，通过遮挡感知合成明确地将对象像素与其周围分离开来。最后，从离散码本中学习的三平面潜变量的先验知识可以用于训练条件潜变量采样模型。

具体而言，GINA-3D的生成过程分为两个阶段。第一阶段是编码器-解码器框架，包括2D到3D编码器、可学习的码本量化和3D到2D解码器。2D到3D编码器使用Transformer作为图像特征提取器，并使用交叉注意力模块将每个图像标记与三平面潜变量中的标记相关联。码本量化将每个空间码的三平面嵌入量化为其最接近的码本条目。3D到2D解码器将三平面潜变量作为输入，并输出用于渲染的高维特征图。解码器使用令牌Transformer，接着是基于样式的生成器和浅层MLP，用神经辐射场公式进行体积渲染。第二阶段是迭代潜变量采样，使用MaskGIT。潜变量生成器被学习为迭代采样潜变量序列，采样的潜变量可以使用第一阶段学习的解码器解码为神经资产。该方法具有灵活的监督和条件设置，允许包括对象尺度、对象类别、时间和对象语义嵌入等附加信息。

#### 8. 实验设置：
本文使用Waymo开放数据集（WOD）作为生成建模的对象中心基准。该数据集包括在旧金山市中心和凤凰城市中心拍摄的1,150个驾驶场景，每个场景包含200帧多传感器观测。该基准是迄今为止最大的生成建模数据集之一，包括野外多样化和长尾示例。本文使用8个Tesla V100 GPU进行训练，使用Adam优化器，每个阶段的批量大小分别为32和64。在训练期间，图像分辨率为2562，渲染分辨率为1282。

#### 9. 实验结果和分析：
本文在WOD-Vehicle数据集上进行了定量评估，将GINA-3D与两种最先进的方法GIRAFFE和EG3D进行比较。对于图像质量，本文计算了50K生成图像与所有可用验证图像之间的Fréchet Inception Distance（FID）。GINA-3D在FID、mask FOU、COV和MMD方面的表现均优于GIRAFFE和EG3D。本文还通过计算投影的3D边界框被对象分割掩模占用的百分比来衡量生成图像的完整性。GINA-3D的完整性高于GIRAFFE和EG3D。本文展示了GINA-3D在野外生成多样化和逼真的隐式神经资产的有效性。


# Paper:99     MetaCLUE：面向全面视觉隐喻研究的探索



#### 1. Title: 
MetaCLUE: Towards Comprehensive Visual Metaphors Research

#### 2. Authors: 
Arjun R. Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo, Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas, William T. Freeman, Yuanzhen Li, Varun Jampani

#### 3. Affiliation: 
谷歌（Google）

#### 4. Keywords: 
Visual Metaphors, Creativity, Computer Vision, Metaphorical Abstraction, Advertisements

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2021_paper.html  Github: https://metaclue.github.io/

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉领域中对于视觉隐喻的理解和生成的研究相对较少，而视觉隐喻在广告等领域中应用广泛，因此有必要对其进行深入研究。

- (2):过去的方法主要集中在对图像的字面解释上，忽略了视觉隐喻的重要性，因此需要对视觉隐喻进行更深入的研究。本文提出的方法在视觉隐喻的分类、定位、理解和生成等任务上都有很好的表现，相比之前的方法有很大的提升。

- (3):本文提出了一种基于视觉隐喻的计算机视觉任务集MetaCLUE，并收集了高质量的视觉隐喻注释数据集。在此基础上，本文对现有的视觉隐喻理解和生成方法进行了全面的评估，并提出了一种基于文本提示的视觉隐喻生成方法。本文的创新点在于提出了一种全面的视觉隐喻研究框架，并在此基础上进行了深入的研究。

- (4):本文的方法在视觉隐喻的分类、定位、理解和生成等任务上都取得了很好的表现，证明了其在视觉隐喻研究领域的重要性和有效性。本文的方法可以为开发具有人类创造力的AI系统提供重要的思路和方法。
#### 7. 方法详细介绍：
本文提出了MetaCLUE，包括四个高级任务：分类、定位、理解和生成，用于全面评估和发展视觉隐喻研究。在分类任务中，作者使用EfficientNet和Vision Transformer等最先进的模型进行微调，以分类图像是否包含视觉隐喻。在理解任务中，作者收集了隐喻中的主要和次要概念以及从次要到主要的特征/关系的详细注释。他们进行了多个试点研究，以提高注释的质量和一致性。在生成任务中，作者提出了一种基于图像的视觉隐喻生成方法，其中输入是一个图像，目标是在句法结构<主要概念>与<次要概念>之间的<关系>。作者还挖掘了难以区分的隐喻性概念作为候选集，以测试他们的隐喻识别模型的性能。

#### 8. 实验设置：
作者使用Pitt's Ads数据集中的图像作为隐喻广告的样本，进行了多阶段的注释过程，使用专家注释员进行筛选、添加隐喻注释和执行其他验证步骤，以收集高质量和丰富的隐喻注释，用于评估现有模型在四个不同任务（分类、定位、理解和生成）上的表现。

#### 9. 实验结果和分析：
作者使用不同的指标（包括p@1、rank和accuracy）评估了他们的隐喻识别模型的性能，并发现模型倾向于依赖主要对象而不是次要对象来识别正确的解释。作者还使用不同的指标（包括BLEU4、ROUGE-L、METEOR、SPICE和CIDEr）评估了他们的隐喻图像字幕和视觉问答模型的性能。结果表明，他们的模型与最先进的模型相比具有竞争性的性能。本文的实验结果揭示了现有技术在理解和生成视觉隐喻方面的优点和缺点，为进一步的人工智能研究提供了具体的第一步。


# Paper:100     重新思考Few-Shot医学分割：基于向量量化的视角



#### 1. Title: 
Rethinking Few-Shot Medical Segmentation: A Vector Quantization View

#### 2. Authors: 
Shiqi Huang, Tingfa Xu, Ning Shen, Feng Mu, Jianan Li

#### 3. Affiliation: 
北京理工大学

#### 4. Keywords: 
Few-shot segmentation, vector quantization, medical imaging, self-organized clustering, residual oriented vector quantization

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Rethinking_Few-Shot_Medical_Segmentation_A_Vector_Quantization_View_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究医学图像分割中的few-shot问题，即在有限的标注数据下，如何对新类别进行分割。 
- (2):过去的方法主要采用原型学习，即从支持图像中提取原型向量，然后与查询特征进行比较以执行分割。然而，这些方法中的原型生成策略存在一些问题，如过多的原型会导致性能下降，而且对于原型学习的强泛化要求往往被忽视。本文提出了一种基于向量量化的学习机制，包括网格格式向量量化（GFVQ）、自组织向量量化（SOVQ）和残差定向向量量化（ROVQ），以解决这些问题。 
- (3):本文提出的SOVQ方法包含自组织聚类和局部映射两个部分，通过自组织聚类将特征点分配到不同的神经元中，然后通过局部映射将神经元映射到编码空间中，以生成原型向量。ROVQ引入残差连接来微调原型向量，以增强模型的泛化能力。 
- (4):本文在腹部、心脏和前列腺MRI数据集上进行了实验，结果表明，所提出的方法在性能上优于现有方法。
#### 7. 方法详细介绍：
本文提出的方法是基于向量量化（Vector Quantization，VQ）框架的，包括三个组件：网格格式向量量化（Grid-format Vector Quantization，GFVQ）、自组织向量量化（Self-Organized Vector Quantization，SOVQ）和残差定向向量量化（Residual Oriented Vector Quantization，ROVQ）。具体步骤如下：
1. GFVQ：使用编码器网络提取全局特征，然后通过VQ将其压缩到特征空间中。
2. SOVQ：在压缩的特征空间中，使用自组织聚类将特征点自适应地分配到不同的本地类别中，并创建一个新的表示空间，其中可学习的本地原型通过全局视图进行更新。
3. ROVQ：引入残差信息，通过微调已学习的本地原型，而无需重新训练，从而有利于泛化性能，因为其与训练任务无关。
该方法将医学图像的原型少样本分割任务形式化为向量量化学习，通过相似度测量进行密集预测。

#### 8. 实验设置：
本文在三个MRI数据集（腹部MRI、心脏MRI和前列腺MRI）上进行了评估，采用1-way、1-shot的设置。使用ResNet101作为编码器网络，该网络在部分MS-COCO上进行了预训练。SOVQ的最大迭代次数Ts为1000，ROVQ的最大迭代次数Tr为10。学习率设置为0.001，每1000次迭代衰减率为0.98。使用Dice分数作为评估指标。

#### 9. 实验结果与分析：
本文提出的方法在所有三个MRI数据集上均取得了最先进的性能，其中腹部MRI数据集的平均Dice分数为72.44，心脏MRI数据集为68.02，前列腺MRI数据集为52.25。当与SSL集成时，该方法在腹部、心脏和前列腺MRI数据集上的平均Dice分数分别比其他方法高出5.72、7.96和5.40。分割图的定性比较表明，与现有最佳性能方法相比，本文提出的方法在拟合对象形状和减少误报方面取得了更好的结果。消融研究证实了每个VQ组件的有效性，包括GFVQ、SOVQ、LM和ROVQ。


# Paper:101     通过定制化学习实现主动领域自适应



#### 1. Title: 
Divide and Adapt: Active Domain Adaptation via Customized Learning

#### 2. Authors: 
Duojun Huang, Jichang Li, Weikai Chen, Junshi Huang, Zhenhua Chai, Guanbin Li

#### 3. Affiliation: 
第一作者：Sun Yat-sen大学计算机科学与工程学院，研究院，中国广州

#### 4. Keywords: 
Active domain adaptation, active learning, domain adaptation, data partitioning, Gaussian mixture model

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Divide_and_Adapt_Active_Domain_Adaptation_via_Customized_Learning_CVPR_2021_paper.html
Github: None

#### 6. Summary: 
- (1):本文研究领域自适应（DA）和主动学习（AL）的结合，提出了一种新的主动领域自适应（ADA）框架，旨在通过将目标数据分为四个具有分层可转移属性的子集，实现定制化学习策略，从而提高模型适应性能。
- (2):传统的AL方法不考虑领域偏移的存在，因此无法在领域自适应的情况下识别出真正有价值的样本。本文提出的方法通过将目标数据分为四个子集，采用不同的学习策略，解决了传统AL方法的问题。同时，本文提出了一种基于不确定性和领域性的新型数据细分协议，可以准确识别最有价值的样本。此外，本文还提出了一种信息量评分方法，可以统一数据分区标准，从而自动将未标记数据分为四个类别。本文的方法可以处理具有大量领域差异的数据，并且可以推广到不同的领域自适应设置中。
- (3):本文提出了一种新的主动领域自适应（ADA）框架，名为Divide-and-Adapt（DiaNA），该框架将目标实例分为四个具有分层可转移属性的类别。DiaNA采用一种新颖的数据细分协议，基于不确定性和领域性，可以准确识别最有价值的样本。在将信息量最大的实例发送进行注释的同时，DiaNA为其余类别采用定制化的学习策略。此外，本文提出了一种信息量评分方法，可以统一数据分区标准，从而自动将未标记数据分为四个类别。DiaNA可以处理具有大量领域差异的数据，并且可以推广到不同的领域自适应设置中。
- (4):本文在DomainNet、Ofﬁce-Home和CIFAR-10等公共数据集上评估了DiaNA的性能，并取得了新的最先进的性能。DiaNA的采样策略可以推广到不同的领域自适应问题，包括无监督领域自适应（UDA）、半监督领域自适应（SSDA）和无源领域自适应（SFDA）。
#### 7. 方法详细介绍：
本文提出了一种名为“Divide-and-Adapt”（DiaNA）的主动域自适应框架。该方法包括特征提取器和分类器两个部分。其目标是识别信息量大的样本进行注释，并利用有限的标记样本来最大程度地提高目标域的分类性能。DiaNA方法通过设计信息量函数来将目标样本分成四个类别，其中一个类别是用于主动注释的信息量大的目标样本。信息量函数是一个统一的评分系统，它将域性和不确定性结合起来。该方法还提出了一种自动数据采样机制，将未标记的目标数据分成四个类别。最后，为不同类别的目标样本设计了量身定制的学习策略，从而进一步提高了模型性能。DiaNA方法适用于无监督域自适应（UDA）、半监督域自适应（SSDA）和无源域自适应（SFDA）方法。

#### 8. 实验设置：
本文在三个基准数据集（DomainNet、Office-Home和CIFAR-10）上评估了DiaNA方法的性能。其中，对于DomainNet和CIFAR-10，使用ResNet-34模型作为网络骨干，对于Office-Home，使用ResNet-50模型。对于每个数据集和适应场景，标记预算和采样比例设置不同的值。实验进行了三次试验，并报告了平均准确率。

#### 9. 实验结果与分析：
DiaNA框架在平均准确率上的表现优于所有模型变体，证明了每个组件的有效性。将DiaNA的选择策略与其他DA方法相结合，可以显著提高现有DA算法的性能，这些算法通过随机选择标记的目标样本进行选择。结果表明，自适应地考虑域性和不确定性对于选择信息量大的目标样本搜索是有益的，尤其是在结合多样化的域自适应技术时。


# Paper:102     高斯过程分类的交互式分割



#### 1. Title: 
Interactive Segmentation as Gaussian Process Classification

#### 2. Authors: 
Minghao Zhou, Hong Wang, Qian Zhao, Yuexiang Li, Yawen Huang, Deyu Meng, Yefeng Zheng

#### 3. Affiliation: 
第一作者：西安交通大学，中国；其他作者：腾讯 Jarvis 实验室，中国

#### 4. Keywords: 
Interactive Segmentation, Gaussian Process, Pixel-wise Binary Classification, Deep Kernel Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Interactive_Segmentation_as_Gaussian_Process_Classification_CVPR_2023_paper.html  Github: https://github.com/zmhhmz/GPCIS

#### 6. Summary : 
- (1):本文研究交互式分割任务，旨在通过用户交互提取目标对象。目前大多数基于深度学习的方法主要遵循语义分割的通用流程，但它们并没有充分利用和传播点击信息，导致即使在点击点处也会出现不理想的分割结果。因此，本文提出将交互式分割任务作为基于高斯过程的像素级二元分类模型来解决，以充分利用和传播点击信息，提高分割效果。

- (2):过去的方法主要是将用户交互编码为点击地图，并将其与输入图像一起馈送到深度神经网络中提取深度特征，然后进行后续分割。然而，这些方法通常存在两个限制：1）在提取深度特征后，它们通常执行像素级分类，而没有针对交互式分割任务的特定设计。因此，在最后一层分类期间，不同像素的深度特征不完全互动，点击像素中包含的信息无法在明确的正则化下传播到其他像素。2）没有明确的理论支持，表明可以正确激活和分类点击区域。本文通过高斯过程模型构建了一个简洁而清晰的交互式分割网络，以理论上的健全框架为基础，可以明确地建模不同像素的深度特征之间的关系，并提供了理论支持，以支持点击点的准确预测。

- (3):本文提出了一种高斯过程分类模型，将交互式分割任务重新定义为每个图像上的像素级二元分类问题，其中红色（绿色）点击被视为具有前景（背景）标签的训练数据，未点击的像素作为待分类的测试数据。为了解决这个模型，本文提出了使用变分推理来近似不可计算的高斯过程后验，然后采用解耦技术来实现具有线性复杂度的GP后验采样。为了提高学习灵活性，本文进一步将深度内核学习策略嵌入到解耦的GP后验推理过程中。最后，通过将推导的GP后验采样机制与DNN骨干相应地集成，构建了一个名为GPCIS的GP分类交互式分割框架。

- (4):本文在几个基准测试上进行了全面的实验比较，与代表性方法进行了定量和定性比较，证明了GPCIS的优越性能和高效性。本文提出的GPCIS方法在不
#### 7. 方法详细介绍：
本文提出了一种基于高斯过程分类（GPC）的交互式分割方法，称为GPCIS。该方法将交互式分割任务视为每个输入图像上的像素二分类问题。GPC模型用于建模不同像素之间的深度特征关系，并充分传播点击区域中包含的信息，以提高未点击像素的正确预测。GPC后验概率使用高斯变分分布进行近似，该分布在端到端的学习过程中基于所有训练图像进行学习。GPC后验概率被分解为权重空间先验项和函数空间更新项，以实现高效的采样。整个推理框架通过灵活地将GPC模型与深度神经网络（DNN）骨干网络相结合而构建。具体步骤包括：
1. 使用骨干网络提取深度特征。
2. 基于深度特征和点击区域的标签，学习GPC模型的权重空间先验项。
3. 基于深度特征和点击区域的标签，学习GPC模型的函数空间更新项。
4. 将权重空间先验项和函数空间更新项结合起来，得到GPC后验概率。
5. 基于GPC后验概率进行像素级别的二分类分割。

#### 8. 实验设置：
本文在四个广泛采用的数据集（GrabCut、Berkeley、SBD和DAVIS）上进行了全面的实验，以评估所提出的GPCIS方法。评估指标包括在达到目标交并比（IoU）为85％和90％时所需的点击次数（NoC），失败次数（NoF），第N次点击的平均IoU（IoU＆N）和新的指标称为错误分类点击数（NoIC）。默认最大点击次数n为20。实验中使用的骨干网络包括SegFormerB0-S2、HRNet18s-S2和DeepLabv3+ with ResNet50。

#### 9. 实验结果和分析：
本文对所提出的GPCIS方法进行了定性和定量的评估。实验结果表明，与基线方法RITM相比，所提出的方法具有更好的通用性，并且在需要更少的点击和更少的失败情况下实现了更高的分割精度。不同方法的输出概率图的可视化比较表明，GPCIS实现了更好的分割结果，并接近于GT掩码。对特定设计的消融研究，包括LV I、双空间DKL以及是否将特征与I连接起来，表明LVI的适当指导有助于网络学习，并且将输入图像I与深度特征X连接起来可以进一步提高像素之间的信息传播并带来更好的分割性能。在四个数据集上，所提出的GPCIS方法在三个不同的骨干网络上均实现了最低的平均NoC@85和NoC@90，证明了其良好的效果和通用性。


# Paper:103     通过语义容忍对比损失的自监督图像到点蒸馏



#### 1. Title: 
Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss

#### 2. Authors: 
Anas Mahmoud, Jordan S. K. Hu, Tianshu Kuai, Ali Harakeh, Liam Paull, and Steven L. Waslander

#### 3. Affiliation: 
第一作者：University of Toronto Robotics Institute（多伦多大学机器人研究所）

#### 4. Keywords: 
Self-supervised learning, contrastive loss, 3D representation learning, autonomous driving, semantic segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mahmoud_Self-Supervised_Image-to-Point_Distillation_via_Semantically_Tolerant_Contrastive_Loss_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文的研究背景是自监督学习在无标签图像上学习有用的表示，以及在自动驾驶数据集上学习3D表示的挑战。 
- (2):过去的方法主要是通过多模态不变性来学习3D表示，其中3D表示被学习为对从自监督学习中训练的图像编码器提取的特征不变。然而，这些方法面临着自相似性和类别不平衡等问题。本文提出了一种新的对抗损失函数，以解决自相似性问题，并提出了一种类不平衡的平衡损失函数，以解决类别不平衡问题。 
- (3):本文提出了一种新的对抗损失函数，该函数考虑了正负图像区域之间的语义距离，以最小化对比语义相似的点和图像区域。此外，本文还设计了一种类不平衡的平衡损失函数，通过聚合样本之间的语义相似度度量来近似类别不平衡程度。本文的方法在3D语义分割任务中取得了最先进的性能。 
- (4):本文的方法在3D语义分割任务中取得了最先进的性能，证明了本文提出的对抗损失函数和平衡损失函数的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为“Semantically Tolerant Self-Supervised Image-to-Point Distillation (ST-SLidR)”的方法。该方法使用2D自监督预训练模型从图像中提取特征，并使用3D骨干网格对点云进行编码。超像素嵌入层用于将点分组成超像素，超点嵌入层用于对超像素进行编码。该方法使用语义容忍对比损失进行训练，该损失基于负样本与正样本的相似性平衡负样本的贡献。还提出了K最近邻感知损失，以根据相似性值从负样本集中排除K个最近邻。最后，使用类不可知的平衡对比损失来解决自动驾驶数据集中的类别不平衡问题。

#### 8. 实验设置：
本文使用nuScenes数据集进行预训练，该数据集包含700个训练场景。将数据集分为600个场景进行预训练，100个场景用于选择最佳超参数。使用Minkowski U-Net进行3D骨干网格，使用ResNet-50架构进行2D骨干网格。将点云从笛卡尔坐标系转换为柱坐标系，并使用3D柱状分区进行体素化。使用线性变换和随机立方体删除进行数据增强。

#### 9. 实验结果与分析：
本文使用线性探测和微调在nuScenes数据集上评估预训练表示的性能。在nuScenes、SemanticKITTI和Waymo验证集上，所提出的ST-SLidR方法在语义分割结果方面优于以前的最先进方法，包括PointContrast、DepthContrast、PPKT和SLidR。所提出的方法在nuScenes、SemanticKITTI和Waymo数据集上分别取得了+2.08%、+1.92%、+0.76%和+0.81%的改进。


# Paper:104     基于未对齐文本的弱监督序列视频表示学习



#### 1. Title: 
Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos

#### 2. Authors: 
Sixun Dong, Huazhang Hu, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao

#### 3. Affiliation: 
上海科技大学

#### 4. Keywords: 
Sequential video understanding, weakly supervised learning, video-text alignment, contrastive learning, transformer

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Dong_Weakly_Supervised_Video_Representation_Learning_With_Unaligned_Text_for_Sequential_CVPR_2021_paper.html
Github: https://github.com/svip-lab/WeakSVR

#### 6. Summary : 
- (1):本文研究了弱监督下的序列视频理解问题，即在没有准确的时间戳级别的文本-视频对齐的情况下进行视频表示学习。 
- (2):过去的方法通常依赖于时间戳级别的标注，即顺序动作的时间戳，这通常在实践中很难获得。本文提出了一种基于CLIP的方法，使用transformer聚合帧级特征进行视频表示，并使用预训练的文本编码器分别对每个动作和整个视频的文本进行编码。为了建模文本和视频之间的对应关系，本文提出了多粒度损失，其中视频-段落对比损失强制匹配整个视频和完整脚本之间的匹配，而细粒度的帧-句子对比损失强制匹配每个动作及其描述之间的匹配。由于帧-句子对应关系不可用，本文提出使用视频动作在时间域中顺序发生的事实来生成伪帧-句子对应关系，并使用伪标签监督网络训练。 
- (3):本文提出了一种弱监督视频表示学习流程，并引入了多粒度对比损失来约束模型，充分考虑了帧和句子之间的伪时序对齐。本文的创新点在于提出了一种基于伪标签的细粒度对比损失，该损失利用了视频动作在时间域中的顺序性质，以实现帧和句子之间的对齐。 
- (4):本文在视频序列验证和文本到视频匹配等下游任务上进行了广泛的实验，结果表明我们的方法在性能上优于基线，验证了我们提出的方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种弱监督视频表示学习框架，用于学习序列视频的多模态对比学习，不需要准确的时间戳级别的文本-视频注释。该方法包括多粒度对比损失，其中视频-段落对比损失约束整个视频和完整脚本之间的匹配，细粒度帧-句子对比损失约束每个动作及其描述之间的匹配。该方法还使用视频动作在时间域中按顺序发生的事实来生成伪帧-句子对应关系，并使用伪标签监督网络训练。

#### 8. 实验设置：
本文在两个下游任务上进行了广泛的实验：程序的视频验证和文本到视频匹配。本文使用CSV数据集进行实验，该数据集包含11,827个视频和2,000个不同的程序。本文使用与[40]相同的训练-测试分割进行视频验证任务，并使用5倍交叉验证进行文本到视频匹配任务。

#### 9. 实验结果和分析：
本文表明，所提出的方法在视频验证和文本到视频匹配任务中均优于其他基线方法。本文还演示了模型的很好的泛化性。本文提出了三种基于时间域中句子的时间关系的方法来生成伪标签：最大索引排序、Viterbi算法和分割。本文计算基于伪标签的Info-NCE对比损失，以指导网络集中于序列视频中的细粒度动作匹配。


# Paper:105     利用伪标签的完整性和不确定性进行弱监督视频异常检测



#### 1. Title: 
Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection

#### 2. Authors: 
Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun Qing, Qingming Huang, Ming-Hsuan Yang

#### 3. Affiliation: 
Chen Zhang: 1. State Key Laboratory of Information Security, Institute of Information Engineering, CAS; 2. School of Cyber Security, University of Chinese Academy of Sciences

#### 4. Keywords: 
Weakly supervised learning, video anomaly detection, pseudo labels, uncertainty, completeness

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Exploiting_Completeness_and_Uncertainty_of_Pseudo_Labels_for_Weakly_Supervised_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究的是弱监督视频异常检测，即仅使用视频级别标签来识别视频中的异常事件。由于异常事件在视频中很少出现，因此该任务存在很大的挑战。
- (2):过去的方法主要分为两类：基于多实例学习的一阶段方法和两阶段自我训练方法。本文提出了一种增强伪标签的框架，通过利用完整性和不确定性属性来有效地进行自我训练。具体来说，本文首先设计了一个多头分类模块，通过多样性损失来最大化预测的伪标签在头之间的分布差异，从而鼓励生成的伪标签尽可能覆盖多个异常事件。然后，本文设计了一个迭代的不确定性伪标签细化策略，通过Monte Carlo Dropout来衡量不确定性，并仅使用不确定性较低的剪辑来训练最终分类器。本文的方法在UCF-Crime、TAD和XD-Violence基准数据集上表现优于现有方法。
- (3):本文提出了一种增强伪标签的框架，通过利用完整性和不确定性属性来有效地进行自我训练。具体来说，本文设计了一个多头分类模块，通过多样性损失来最大化预测的伪标签在头之间的分布差异，从而鼓励生成的伪标签尽可能覆盖多个异常事件。然后，本文设计了一个迭代的不确定性伪标签细化策略，通过Monte Carlo Dropout来衡量不确定性，并仅使用不确定性较低的剪辑来训练最终分类器。
- (4):本文的方法在UCF-Crime、TAD和XD-Violence基准数据集上表现优于现有方法，表明本文提出的增强伪标签的框架可以有效地提高弱监督视频异常检测的性能。
#### 7. 方法详细介绍：
本文提出了一种基于完整性和不确定性的伪标签增强框架，包括两个阶段：完整性增强的伪标签生成和不确定性感知的自训练。在完整性增强的伪标签生成阶段，使用多头分类器和多实例学习（MIL）排序损失训练，生成初始的片段级伪标签。然后，使用迭代的不确定性感知伪标签细化策略逐步提高伪标签的质量，以训练最终的分类器。在不确定性感知的自训练阶段，使用MC-Dropout估计片段级伪标签的不确定性，并选择不确定性得分较低的可靠片段进行训练。使用长期特征记忆模型建模视频片段之间的时间关系。

#### 8. 实验设置：
本文在三个公开数据集UCF-Crime、TAD和XD-Violence上进行了实验。使用I3D模型提取片段特征，在XD-Violence中使用VGGish网络提取音频特征。UCF-Crime和TAD使用曲线下面积（AUC）作为评估指标，XD-Violence使用平均精度（AP）作为评估指标。将本文提出的方法与之前的半监督和弱监督方法进行比较。

#### 9. 实验结果与分析：
本文提出的方法在三个数据集上均取得了最先进的性能。在UCF-Crime上，本文提出的方法取得了86.22%的AUC，优于所有先前的方法。在TAD上，本文提出的方法取得了91.66%的AUC，比两阶段MIST和一阶段RTFM分别提高了2.40%和2.02%。在XD-Violence上，本文提出的方法仅使用I3D特征就取得了78.74%的AP，使用多模态特征则取得了81.43%的AP，优于所有先前的方法。消融实验表明，完整性和不确定性对本文提出的方法的性能都很重要。


# Paper:106     用视频本地化叙述连接视觉和语言



#### 1. Title: 
Connecting Vision and Language with Video Localized Narratives

#### 2. Authors: 
Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, Vittorio Ferrari

#### 3. Affiliation: 
Paul Voigtlaender: Google Research（谷歌研究院）

#### 4. Keywords: 
Vision and Language, Video Localized Narratives, Video Narrative Grounding, Video Question Answering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Voigtlaender_Connecting_Vision_and_Language_With_Video_Localized_Narratives_CVPR_2021_paper.html  Github: https://google.github.io/video-localized-narratives/

#### 6. Summary : 
- (1):本文研究了视觉和语言之间的联系，提出了一种新的多模态视频注释形式，即视频本地化叙述。 
- (2):过去的方法主要是将图像与标题相连，而本文提出的方法可以将视频与语言相连。本文提出的方法可以描述包含多个参与者和多个被动对象的复杂事件，而过去的方法无法描述这些事件。本文的方法是有动机的，因为视频是一种更具信息量的媒介，可以提供更多的信息。 
- (3):本文提出了一种新的注释协议，使注释者能够在视频中讲述故事，捕捉涉及多个参与者和多个被动对象的复杂事件。本文还构建了新的基准测试，用于视频叙述接地和视频问答任务，并提供了来自强基线模型的参考结果。 
- (4):本文的方法在视频叙述接地和视频问答任务上取得了很好的性能，支持了他们的目标。
#### 1. 实验细节：
(1). 本文提出了一种新的注释协议，称为Video Localized Narratives (VidLNs)，用于将自然语言描述与视频进行连接。该协议包括观看视频、识别主要演员、选择关键帧、为每个演员单独讲述故事以及转录和时间对齐等五个步骤。该协议的结果提供了每个单词的鼠标跟踪定位，并覆盖了通用领域，而不仅仅是第一人称或烹饪领域。
(2). 本文使用VidLNs协议对OVIS、UVO和Oops数据集中的约20k个视频进行了注释，总计超过1.6百万个单词。作者使用这些注释生成了两个Video Narrative Grounding基准和一个Oops-QA基准，并提供了自动Q+A生成过程和Oops-QA基准的手动验证过程的详细信息。最后，作者描述了用于两个基准的评估措施。

#### 2. 方法：
本文提出了一种新的注释协议，称为Video Localized Narratives (VidLNs)，用于将自然语言描述与视频进行连接。该协议包括以下步骤：
(1). 观看视频，理解视频内容。
(2). 选择主要演员，确定视频中的关键帧。
(3). 为每个演员单独讲述故事，描述他们在视频中的行为和动作。
(4). 转录和时间对齐，将每个单词的鼠标跟踪定位到相应的对象和动作上。
该协议的结果提供了每个单词的鼠标跟踪定位，并覆盖了通用领域，而不仅仅是第一人称或烹饪领域。

#### 3. 实验设置：
本文使用了OVIS、UVO和Oops数据集中的约20k个视频进行了注释，并使用这些注释生成了两个Video Narrative Grounding基准和一个Oops-QA基准。作者还使用OVIS和UVO数据集构建了Video Narrative Grounding (VNG)任务的基准，并使用名词标记器选择了VidLN字幕中的名词。作者还提出了一种新的基于ReferFormer的方法，用于解决VNG任务，并提出了两个新的VNG基准：OVIS-VNG和UVO-VNG。

#### 4. 实验结果和分析：
本文的实验结果表明，使用VidLNs协议进行注释可以提高注释质量。作者提出的ReferFormer-VNG方法在VNG任务中表现出色，优于简单的基线模型。作者还提供了详细的基准结果，并将其与其他数据集进行了比较。作者还指出，新的基准远未解决，回答许多VNG任务中的问题需要对整个视频有深入的理解。


# Paper:107     SAP-DETR：基于显著点和基于查询的Transformer检测器之间的快速模型收敛的桥梁



#### 1. Title: 
SAP-DETR: Bridging the Gap between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency

#### 2. Authors: 
Yang Liu, Yao Zhang, Yixin Wang, Yang Zhang, Jiang Tian, Zhongchao Shi, Jianping Fan, Zhiqiang He

#### 3. Affiliation: 
第一作者：中国科学院计算技术研究所

#### 4. Keywords: 
Object detection, Transformer, Salient points, Spatial prior, Convergency speed

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2021_paper.html  Github: https://github.com/liuyang-ict/SAP-DETR

#### 6. Summary : 
- (1):本文研究目标检测中的Transformer模型，旨在提高模型的收敛速度和检测精度。

- (2):过去的方法主要是基于中心点的空间先验，但这种方法可能会降低查询的显著性并混淆检测器，因为中心化的参考点可能会严重损害查询的显著性。本文提出了一种基于显著点的方法，通过将每个对象查询的查询特定参考点初始化为显著点，逐渐将它们聚合成一个实例对象，并预测从边界框的每一侧到这些点的距离，从而有效地弥合了显著点和基于查询的Transformer检测器之间的差距。

- (3):本文提出了一种基于显著点的DETR模型，通过显著点增强的交叉注意机制，使查询具有空间先验，从而实现了快速的模型收敛速度和优秀的检测精度。本文的创新点在于将显著点的概念引入到端到端的Transformer检测器中，并通过预测边界框的每一侧来实现对显著点的聚合。

- (4):在COCO数据集上的实验表明，SAP-DETR在相同的训练设置下，比现有的方法具有更快的收敛速度和更好的检测精度。在ResNet-DC-101上，SAP-DETR实现了46.9的AP，比现有的方法提高了约1.0 AP。
#### 7. 方法详细介绍：
本文提出了一种新的模型SAP-DETR，它将显著点检测器和基于查询的Transformer检测器相结合，以实现快速模型收敛。该方法使用显著点检测器从输入图像中提取显著点，然后使用基于Transformer的检测器基于提取的显著点检测对象。显著点检测器使用对比损失函数进行训练，基于Transformer的检测器使用分类和回归损失的组合进行训练。该方法在多个基准数据集上实现了最先进的性能。

具体步骤如下：
1. 将显著点检测器和Transformer检测器相结合，将对象检测视为从显著点到实例对象的转换。
2. 将属于一个正查询的参考点定义为显著点，并保持这个查询特定的空间先验性，然后通过预测边界框的每个边距离逐渐将其更新为实例对象。
3. 应用可移动策略进行参考点调整，以解开参考点的稀疏性并稳定训练过程。
4. 开发了显著点增强的交叉注意力机制，以区分显著区域和其他条件极端区域与上下文图像特征。

#### 8. 实验设置：
本文使用了多个基准数据集进行实验，包括COCO、PASCAL VOC和Visual Genome等。模型在PyTorch框架下实现，使用Adam优化器进行训练，学习率为1e-4，批量大小为16。在训练过程中，使用了学习率调度程序和早期停止策略。所有实验都在单个NVIDIA Tesla V100 GPU上进行。

#### 9. 实验结果和分析：
SAP-DETR在多个基准数据集上实现了最先进的性能，包括COCO、PASCAL VOC和Visual Genome等。与其他最先进的方法相比，SAP-DETR在检测精度和速度方面都有显著的提高。此外，本文还进行了消融实验和可视化分析，证明了SAP-DETR的有效性和可解释性。


# Paper:108     考虑系统状态的自适应网络用于在线流媒体视频理解



#### 1. Title: 
System-status-aware Adaptive Network for Online Streaming Video Understanding

#### 2. Authors: 
Lin Geng Foo, Jia Gong, Zhipeng Fan, Jun Liu

#### 3. Affiliation: 
第一作者：新加坡科技设计大学

#### 4. Keywords: 
Online video understanding, system-status-aware, adaptive network, deep neural networks, fluctuating system status

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Foo_System-Status-Aware_Adaptive_Network_for_Online_Streaming_Video_Understanding_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究在线视频理解任务中的硬件系统状态波动对模型性能的影响，提出了一种考虑系统状态的自适应网络，以实现高质量预测和低延迟。
 
- (2):现有的在线视频理解方法大多没有考虑设备状态的波动，因此在实际应用中可能会出现延迟过高的问题。本文提出的自适应网络可以根据实时状态调整计算复杂度，以实现在线视频理解任务的高效处理。此外，为了适应不同的硬件平台，本文还提出了一种元自监督适应方法，可以在测试时自适应地调整代理策略，从而方便地将模型部署到其他未见过的硬件平台上。

- (3):本文提出了一种系统状态感知的自适应网络，包括一个轻量级代理模块和一个动态主网络模块。代理模块决定如何处理输入帧，主网络模块根据代理模块生成的策略动态地调整计算复杂度。主网络模块采用动态编码器，具有动态深度和动态分辨率机制，可以在不增加硬件负担的情况下灵活地生成预测结果。

- (4):本文在在线动作识别和姿态估计任务上进行了实验，结果表明，所提出的方法在保持低延迟的同时，取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于系统状态感知的自适应网络（SAN）用于在线流媒体视频理解。SAN由两个模块组成：轻量级代理模块π和动态主网络M。动态编码器根据系统状态调整输入视频帧的分辨率和深度，而轻量级代理模块根据当前系统状态为每个帧生成最佳的分辨率和深度。动态编码器的输出特征在每个时间步骤都被馈送到一个长短期记忆网络（LSTM）中，LSTM根据输入帧的特征和先前状态更新其隐藏状态。然后，任务头使用LSTM隐藏状态确定视频任务的最终预测。基于强化学习的代理模块π通过在每个时间步骤生成帧级处理策略来控制动态主网络M，旨在以低延迟维持任务相关的准确性。代理模块π既具有系统状态感知能力，又具有流媒体输入感知能力，并根据观察到的状态在每个步骤上采取行动并传递到下一个状态。SAN使用基于指定的在线视频处理任务的损失和基于强化学习的损失来训练代理模块π。

#### 8. 实验设置：
本文在两个在线视频理解任务上评估了所提出的SAN：在线动作识别和在线姿态估计。在线动作识别使用50Salads数据集，考虑了三个分辨率候选项。在线姿态估计使用Sub-JHMDB数据集，考虑了三个分辨率候选项。本文将所提出的SAN与现有方法在两个数据集上的性能进行了比较。

#### 9. 实验结果和分析：
本文将SAN在目标设备上进行微调，使用延迟预测的辅助任务，使代理模块能够更好地了解目标设备的特性和处理时间。本文还在一组已知源设备上进行元优化，以评估所提出的基于系统状态感知的（MSA）设计在部署到未知设备时的有效性。本文报告了SAN在两个数据集上的准确性和延迟，并将其与现有方法进行了比较。本文还通过消融研究评估了动态分辨率和深度机制的影响。结果表明，通过允许动态分辨率或动态深度，主网络可以降低最大和平均延迟，同时保持高准确性。此外，通过结合这两种机制，SAN可以进一步降低平均延迟并将最大延迟抑制在较低水平，同时具有更好的准确性。本文还提供了SAN在不同延迟阈值和准确性系数下的性能评估，并展示了系统负载动态的效果，证明了其系统状态感知和流媒体感知设计的优势，可以同时控制实时延迟并获得在线视频理解任务的高准确性。


# Paper:109     使用未知姿态分布学习3D感知图像合成



#### 1. Title: 
Learning 3D-aware Image Synthesis with Unknown Pose Distribution

#### 2. Authors: 
Zifan Shi, Yujun Shen, Yinghao Xu, Sida Peng, Yiyi Liao, Sheng Guo, Qifeng Chen, Dit-Yan Yeung

#### 3. Affiliation: 
香港科技大学

#### 4. Keywords: 
3D-aware image synthesis, neural radiance fields, pose-free generator, pose-aware discriminator, adversarial training

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Shi_Learning_3D-Aware_Image_Synthesis_With_Unknown_Pose_Distribution_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究的是3D感知图像合成，现有方法大多依赖于预先估计的3D姿态分布，但不准确的估计可能会导致模型学习到错误的几何形状。因此，本文提出了一种名为PoF3D的方法，该方法通过一个高效的姿态学习器来自动近似真实姿态分布，从而使生成的辐射场不再需要3D姿态先验。同时，本文重新设计了鉴别器，使其具有姿态感知能力，从而能够在生成器的监督下学习姿态分布，并以预测的姿态作为条件来区分真实和合成图像。最终，通过对多个数据集的广泛实验，证明了本文方法在图像质量和几何质量方面的表现与现有最先进方法相当。

- (2):现有的3D感知图像合成方法大多依赖于预先估计的3D姿态分布，这种依赖性会导致3D感知图像合成的不稳定性，而且需要大量的实验成本。本文提出的PoF3D方法通过一个高效的姿态学习器来自动近似真实姿态分布，从而使生成的辐射场不再需要3D姿态先验。同时，本文重新设计了鉴别器，使其具有姿态感知能力，从而能够在生成器的监督下学习姿态分布，并以预测的姿态作为条件来区分真实和合成图像。这种方法不仅能够避免3D姿态分布的依赖性，而且能够自动学习姿态分布，从而简化了输入要求。

- (3):本文提出的PoF3D方法通过一个高效的姿态学习器来自动近似真实姿态分布，从而使生成的辐射场不再需要3D姿态先验。同时，本文重新设计了鉴别器，使其具有姿态感知能力，从而能够在生成器的监督下学习姿态分布，并以预测的姿态作为条件来区分真实和合成图像。这种方法不仅能够避免3D姿态分布的依赖性，而且能够自动学习姿态分布，从而简化了输入要求。本文的创新点在于提出了一种无需3D姿态先验的3D感知图像合成方法，并通过实验证明了其有效性。

- (4):本文的方法在多个数据集上进行了广泛实验，包括FFHQ、Cats和Shapenet Cars等。实验结果表明，PoF3D方法能够避免3D姿态分布的依赖性，同时能够自动学习姿态分布，从而简化了输入要求。在
#### 7. 方法详细介绍：
本文提出了一种名为PoF3D的方法，用于无需使用姿态先验的3D感知图像合成。该方法使用无姿态生成器，直接从潜在空间推断相机姿态，消除了姿态先验的需求。鉴别器通过首先预测相机姿态，然后在执行条件真/假鉴别时使用它作为条件，使其具有姿态感知能力。这种设计有助于生成器更好地捕捉底层分布。生成器将潜在代码映射到神经辐射场以及相机姿态，然后使用体积渲染器输出最终图像。鉴别器从给定图像中提取姿态，并将其用作条件标签进行真/假分类。该方法使用标准的对抗性损失进行训练，并包括对称性损失和姿态损失。

#### 8. 实验设置：
本文在三个数据集（FFHQ、Cats和Shapenet Cars）上评估了PoF3D，并与几个基线进行了比较。FFHQ和Cats的分辨率为256×256，Shapenet Cars的分辨率为128×128。本文使用五个指标评估性能，包括Fr´echet Inception Distance（FID）、深度误差、姿态误差、重投影误差（RE）和Jensen-Shannon Divergence（JS）。

#### 9. 实验结果与分析：
本文展示了与基线的定性比较结果。CAMPARI难以生成合理的结果，基线方法在FFHQ和Cats数据集上存在模式崩溃。EG3D在给定训练图像的相机姿态时实现了令人印象深刻的渲染和重建结果。相比之下，PoF3D可以在没有任何姿态先验的情况下生成高质量的渲染和几何形状。本文报告了每种方法在每个数据集上的FID、深度误差、姿态误差、RE和JS值。PoF3D在所有指标和数据集上均取得了最佳或具有竞争力的性能。


# Paper:110     图形胶囊：从2D图像中学习层次结构的3D人脸表示



#### 1. Title: 
Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D Images

#### 2. Authors: 
Chang Yu, Xiangyu Zhu, Xiaomei Zhang, Zhaoxiang Zhang, Zhen Lei

#### 3. Affiliation: 
中国科学院自动化研究所

#### 4. Keywords: 
Capsule network, 3D face representation, unsupervised learning, graphics capsule

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Graphics_Capsule_Learning_Hierarchical_3D_Face_Representations_From_2D_Images_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究背景是如何从2D图像中学习到3D人脸表示。
- (2):过去的方法通常只能处理2D对象，难以解释3D对象。本文提出了一种新的capsule网络，称为graphics capsule，用于学习3D人脸表示。该方法首先将对象分解为一组语义一致的部分级别描述，然后将它们组装成对象级别描述以构建层次结构。与过去的方法相比，该方法可以处理更复杂的3D对象，并且提供了更好的解释性。
- (3):本文提出了一种逆图形胶囊网络（IGC-Net），用于从大规模未标记的图像中学习层次结构的3D人脸表示。IGC-Net的核心是一种新类型的胶囊，称为图形胶囊，它使用计算机图形学（CG）中的可解释参数表示3D基元，包括深度、反照率和3D姿态。IGC-Net首先将对象分解为一组语义一致的部分级别描述，然后将它们组装成对象级别描述以构建层次结构。该方法可以提供更好的解释性，并且可以处理更复杂的3D对象。
- (4):本文在CelebA、BP4D和Multi-PIE数据集上进行了实验，证明了IGC-Net的有效性。IGC-Net可以提供可解释的3D部分表示，并且可以用于无监督的人脸分割任务。此外，IGC-Net还可以用于可解释的人脸分析，以揭示神经网络在识别人脸时的机制。
#### 7. 方法详细介绍：
本文提出了一种名为IGC-Net的方法，用于从大规模无标签的野外图像中学习分层的三维人脸表示。该方法使用深度和反照率信息作为线索，将对象分解为一组部分级别的图形胶囊，并通过将部分级别的胶囊组装成对象级别的胶囊来构建对象的层次结构。部分级别的图形描述可用于无监督的人脸分割和可解释的人脸分析。IGC-Net由图像编码器、图形分解模块、胶囊解码器和照明模块组成。图形分解模块用于将全局嵌入分解为一组部分级别的嵌入，这些嵌入可以进一步解码为可解释的图形胶囊。部分胶囊根据其深度组装，生成全局对象胶囊。在组装过程中，只有深度小于其他胶囊时，胶囊才在位置上可见。通过可微分渲染生成恢复的图像。网络参数可以通过最小化输入图像和重建图像之间的距离来无监督地学习，遵循分析合成策略。

#### 8. 实验设置：
本文在三个数据集BP4D、Multi-PIE和CelebA上评估了所提出的IGC-Net。其中，BP4D和CelebA用于评估无监督的人脸分割，而Multi-PIE用于可解释的人脸分析。使用分段地图预测的地标的标准化平均误差（NME）评估分割方式的质量。本文将所提出的方法与无监督人脸分割的最新方法进行比较，包括DFF、SCOPS和HP-Capsule。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的IGC-Net方法在无监督人脸分割和可解释人脸分析方面均优于最新的方法。在BP4D和CelebA数据集上，IGC-Net的分割结果比DFF、SCOPS和HP-Capsule更准确。在Multi-PIE数据集上，IGC-Net的可解释性比最新的方法更好。此外，本文还进行了消融实验，证明了所提出的方法的有效性。


# Paper:111     PointDistiller：面向高效紧凑的3D检测的结构化知识蒸馏



#### 1. Title: 
PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection

#### 2. Authors: 
Linfeng Zhang, Runpei Dong, Hung-Shuo Tai, Kaisheng Ma

#### 3. Affiliation: 
1. 清华大学
2. 西安交通大学
3. 滴滴出行

#### 4. Keywords: 
Point cloud, 3D object detection, knowledge distillation, dynamic graph convolution, reweighted learning strategy

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_CVPR_2021_paper.html  Github: https://github.com/RunpeiDong/PointDistiller

#### 6. Summary : 
- (1):本文研究点云3D检测中的知识蒸馏问题，旨在提高检测模型的效率和紧凑性。
 
- (2):过去的知识蒸馏方法通常是将预测的分类概率分布或骨干特征从教师模型传递到学生模型，但是这些方法无法很好地处理点云的稀疏性和不规则性。本文提出了一种结构化的知识蒸馏框架PointDistiller，其中包括局部蒸馏和加权学习策略。局部蒸馏通过动态图卷积提取和蒸馏点云的局部几何结构，加权学习策略通过给予多点体素更大的学习权重来处理点云的稀疏性和噪声。这些方法使得学生模型能够更好地理解点云的局部几何信息，从而提高检测性能。

- (3):本文提出的PointDistiller框架包括局部蒸馏和加权学习策略，能够有效地将教师模型的知识传递给学生模型。实验结果表明，PointDistiller在PointPillars、SECOND和PointRCNN等检测器上均取得了优于七种先前知识蒸馏方法的性能。例如，我们的4倍压缩PointPillars学生在BEV和3D目标检测上分别取得了2.8和3.4 mAP的提升，优于其教师模型0.9和1.8 mAP。

- (4):本文研究了点云3D检测中的知识蒸馏问题，提出了一种结构化的知识蒸馏框架PointDistiller，包括局部蒸馏和加权学习策略。实验结果表明，PointDistiller在PointPillars、SECOND和PointRCNN等检测器上均取得了优于七种先前知识蒸馏方法的性能，能够有效地将教师模型的知识传递给学生模型，提高检测性能。
#### 7. 方法详细介绍：
本文提出了一种名为PointDistiller的结构化知识蒸馏框架，用于点云三维物体检测。该方法包括以下步骤：
1. 基于重要性分数对要蒸馏的体素或点进行采样。
2. 使用动态图卷积层提取局部几何信息。
3. 基于重要性分数重新加权知识蒸馏损失。
该方法旨在解决三维物体检测中高计算和存储成本的问题，同时保持显著的检测性能。

#### 8. 实验设置：
本文在KITTI数据集上进行了实验，评估了基于体素和基于原始点的检测器的性能，并以中等难度的mAP为性能指标。

#### 9. 实验结果与分析：
实验结果表明，PointDistiller方法在BEV和3D检测任务上均优于无知识蒸馏的学生检测器和其他先进方法。在BEV和3D检测任务上，该方法分别实现了93.1%和93.1%的mAP，分别比无知识蒸馏的学生检测器提高了0.7%和5.8%。该方法还能够成功地将知识从教师检测器转移到学生检测器，实现了显著和一致的性能提升。在体素和原始点检测器上，平均精度提高了2.4和1.0个中等mAP。在BEV和3D检测上，分别可以获得1.9和1.9个中等mAP的提高。使用该方法训练的4倍压缩和加速的学生检测器在PointPillars和SECOND检测器的BEV检测中优于其教师检测器0.9和0.9个mAP。在PointPillars和SECOND检测器的3D检测中，使用该方法训练的4倍压缩和加速的学生检测器分别优于其教师检测器1.8和0.1个mAP。


# Paper:112     追求更高FLOPS的神经网络加速



#### 1. Title: 
Run, Don’t Walk: Chasing Higher FLOPS for Faster Neural Networks

#### 2. Authors: 
Jierun Chen, Shiu-hong Kao, Hao He, Weipeng Zhuo, Song Wen, Chul-Ho Lee, S.-H. Gary Chan

#### 3. Affiliation: 
香港科技大学

#### 4. Keywords: 
Neural Networks, FLOPs, FLOPS, Partial Convolution, FasterNet

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2022_paper.html  Github: https://github.com/JierunChen/FasterNet

#### 6. Summary : 
- (1):本文旨在设计更快的神经网络，许多工作都集中在减少浮点运算（FLOPs）上。然而，这种FLOPs的减少并不一定会导致相似程度的延迟降低。这主要源于低效的每秒浮点运算（FLOPS）。 
- (2):现有的神经网络虽然尝试减少FLOPs，但它们往往会因为增加内存访问而导致低FLOPS。本文提出了一种新的部分卷积（PConv）来提取空间特征，通过同时减少冗余计算和内存访问来更有效地提取空间特征。在此基础上，本文进一步提出了FasterNet，一种新的神经网络系列，它在各种设备上的运行速度比其他神经网络都要快得多，而且在各种视觉任务中不会牺牲准确性。 
- (3):本文提出了一种名为PConv的新型卷积算子，它通过仅对一部分输入通道应用滤波器而不触及其余通道来更好地利用设备上的计算能力。PConv还通过减少计算冗余和内存访问数量来提高计算效率。在此基础上，本文进一步提出了FasterNet，它是一种新的神经网络系列，它利用PConv来提高计算效率，从而在各种设备上运行速度更快。 
- (4):本文在各种任务上进行了广泛的实验，并验证了PConv和FasterNet的高速和有效性。例如，在ImageNet-1k上，我们的微型FasterNet-T0在GPU、CPU和ARM处理器上分别比MobileViT-XXS快2.8倍、3.3倍和2.4倍，同时准确率更高2.9%。我们的大型FasterNet-L在GPU上的吞吐量比Swin-B高36%，在CPU上节省了37%的计算时间，同时实现了83.5%的top-1准确率，与Swin-B相当。
#### 7. 方法详细介绍：
本文提出了一种新的算子PConv，它只对输入通道的一部分应用常规卷积进行空间特征提取，而将其余通道保持不变。PConv的FLOPs仅为常规卷积的1/16，具有更少的内存访问量。PConv后跟一个PWConv，以利用所有通道的信息。作者还提出了一种名为FasterNet的新型神经网络，它使用PConv和PWConv作为主要构建算子。FasterNet有四个分层阶段，每个阶段都有一堆FasterNet块，并且前面有一个嵌入或合并层。最后三层用于特征分类。在每个FasterNet块中，PConv层后跟两个PWConv层。作者还仅在中间层之后放置归一化和激活层，以保留特征多样性并实现更低的延迟。

#### 8. 实验设置：
本文在ImageNet-1k和COCO数据集上进行了广泛的实验，评估了PConv和FasterNet的性能。在ImageNet-1k数据集上，作者使用224×224分辨率和0.9的裁剪比例，在验证集上报告了他们的top-1准确性。作者还在三种典型处理器上基准测试了延迟和吞吐量：GPU（2080Ti）、CPU（Intel i9-9900X，使用单个线程）和ARM（Cortex-A72，使用单个线程）。

#### 9. 实验结果和分析：
本文提出的PConv和FasterNet在各种视觉任务中均取得了优异的性能。在ImageNet-1k数据集上，FasterNet-T0比MobileViT-XXS快2.8倍、3.3倍和2.4倍，同时准确率更高2.9%。FasterNet-L在GPU上的吞吐量比Swin-B高36%，在CPU上节省37%的计算时间，同时达到83.5%的top-1准确率。在COCO数据集上，FasterNet在目标检测和实例分割任务中的平均精度比ResNet和ResNext模型更高，同时具有相似的延迟。


# Paper:113     itKD：基于交换传递的知识蒸馏用于3D物体检测



#### 1. Title: 
itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection

#### 2. Authors: 
Hyeon Cho, Junyong Choi, Geonwoo Baek, Wonjun Hwang

#### 3. Affiliation: 
第一作者：Ajou University

#### 4. Keywords: 
3D object detection, point cloud, knowledge distillation, autoencoder, self-attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/hyeon-jo/interchange-transfer-KD

#### 6. Summary : 
- (1):本文研究背景是3D物体检测，目前的研究大多数只关注提高准确率，而忽略了计算效率的问题。

- (2):过去的方法主要是通过非极大值抑制或锚点等方法来提高3D物体检测的速度，但网络参数仍然很大。本文提出了一种基于知识蒸馏的自动编码器框架，通过通道压缩和解压缩来学习教师网络的地图视图特征，同时使用压缩表示损失来绑定来自学生和教师网络的通道压缩知识作为一种正则化。最后，本文提出了一个头部注意力损失来匹配多头自注意机制绘制的3D物体检测信息。这种方法可以训练轻量级模型，与3D点云检测任务很好地对齐，并在公共数据集（如Waymo和nuScenes）上展示其优越性。

- (3):本文提出了一种新颖的基于交换传递的知识蒸馏（itKD）方法，旨在为轻量级点云3D物体检测提供有效的知识蒸馏框架。itKD包括两个模块：（1）基于通道的自动编码器，通过重构知识的交换传递来进行通道压缩和解压缩；（2）基于头部关系感知的自我注意力机制，用于匹配多个3D检测头的检测信息。本文的贡献在于提出了一种统一的知识蒸馏框架，成功地学习了教师网络的3D表示和3D检测结果，用于轻量级3D点云物体检测的参数压缩。本文还进行了广泛的消融研究，验证了我们的方法在Waymo和nuScenes数据集上的优越性。

- (4):本文的方法在Waymo和nuScenes数据集上取得了优异的性能，证明了其在3D点云物体检测任务中的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于交替转移的知识蒸馏方法(itKD)，用于轻量级三维点云物体检测。该方法包括一个通道级自编码器，用于压缩和解压教师和学生网络的地图视图特征，以及一个头关系感知自注意力，用于提取检测头中心的知识。自编码器使用压缩表示损失将来自两个网络的通道压缩知识绑定为一种正则化。解压缩特征在相反方向上传输，以减少交换重构中的差距。头关系感知自注意力考虑多个3D检测头的内部关系和外部关系，以优化教师的物体检测结果和表示。

#### 8. 实验设置：
本文在两个大规模数据集Waymo和nuScenes上进行了评估，这两个数据集反映了真实的驾驶条件。实验在单个NVIDIA Tesla V100 GPU上进行，具有16GB内存。使用的骨干架构是CenterPoint，它简单，接近实时，并在野外情况下取得了良好的性能。

#### 9. 实验结果与分析：
本文进行了广泛的实验以验证所提出方法的有效性。结果表明，itKD方法可以训练出与3D点云检测任务良好对齐的轻量级模型，并且优于基线模型。实验还展示了所提出方法在Waymo和nuScenes数据集上的优越性。消融研究表明，所提出方法的组成部分，即通道级自编码器和头关系感知自注意力，对整体性能的提高有所贡献。


# Paper:114     基于点云的三维步态识别基准测试：LidarGait



#### 1. Title: 
LidarGait: Benchmarking 3D Gait Recognition with Point Clouds

#### 2. Authors: 
Chuanfu Shen, Fan Chao, Wei Wu, Rui Wang, George Q. Huang, Shiqi Yu

#### 3. Affiliation: 
第一作者：香港大学工业及制造系统工程系；其他作者：南方科技大学计算机科学与工程系、南方科技大学可信自主系统研究所、香港理工大学工业及系统工程系

#### 4. Keywords: 
Gait recognition, 3D point clouds, LiDAR, SUSTech1K dataset, convolutional neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shen_LidarGait_Benchmarking_3D_Gait_Recognition_With_Point_Clouds_CVPR_2021_paper.html  Github: https://lidargait.github.io/

#### 6. Summary : 
- (1):本文研究的背景是基于视频的步态识别在受限场景下取得了令人瞩目的成果，但视觉相机忽略了人类的三维结构信息，这限制了在三维野外环境中进行步态识别的可行性。
- (2):过去的方法主要是基于图像的步态识别，但是这些方法忽略了人类的三维结构信息，且在复杂背景下容易出现视觉歧义。本文提出了一种基于LiDAR传感器的3D步态识别方法，可以提供精确的3D感知和3D几何结构，具有更好的实用性。 
- (3):本文提出了一种名为LidarGait的简单而有效的基于点云的步态识别框架，该框架将稀疏点云投影到深度图中，利用卷积神经网络从投影中提取具有3D结构信息的步态特征。本文还构建了第一个大规模的基于LiDAR传感器的步态识别数据集SUSTech1K，该数据集包含来自1,050个受试者的25,239个序列，涵盖了许多变化，包括可见性、视角、遮挡、服装、携带和场景。实验结果表明，LidarGait在各种挑战下表现出色，且优于现有的基于点和基于轮廓的方法。
- (4):本文的方法在SUSTech1K数据集上取得了很好的性能，证明了3D结构信息对于步态识别的重要性。LidarGait在各种挑战下表现出色，且优于现有的基于点和基于轮廓的方法。
#### 7. 方法详细介绍：
本文提出了一种名为LidarGait的3D步态识别方法，该方法将3D点云投影到深度图像中，并使用卷积神经网络提取带有3D结构信息的步态特征。该方法包括两个组件：结构特征编码器和时间聚合网络。结构特征编码器从投影的前视深度图像中捕获空间局部连接性，时间聚合网络则模拟顺序输入中的动态连接。该方法使用三元组和交叉熵损失的组合进行优化。 

#### 8. 实验设置：
本文使用SUSTech1K数据集进行实验，该数据集包括1050个身份，25239个序列，763416个点云帧和3075575个RGB图像。数据集在SUSTech校园的三个场景中收集，每个参与者被要求沿着四个往返路径和四个单向路径正常行走，然后再沿着相同路径以随机变化的方式行走。数据集包括多种属性，如Normal、Bag、Clothes Changing、Views、Object Carrying、Occlusion、Illumination、Uniform和Umbrella。本文还描述了数据集的注释和表示，包括基于相机和基于LiDAR的表示。最后，本文提到了用于数据集的评估指标，包括Rank-1准确度和Rank-5。

#### 9. 实验结果与分析：
实验结果表明，LidarGait在所有条件下均优于所有现有的基于点和基于相机的方法，并在除了雨伞子集外的所有情况下实现了最先进的结果。使用轮廓的方法在夜间表现不佳。基于点的方法提供了更有前途的结果，LidarGait在这方面的表现优于其他方法。所有使用帧顺序的模型都获得了较低的结果，而其他基于集合的方法则获得了更好的准确性。LidarGait在各种情况下都取得了显着的结果，但在主体携带雨伞时表现不佳。


# Paper:115     自适应人像抠图技术在动态视频中的应用



#### 1. Title: 
Adaptive Human Matting for Dynamic Videos

#### 2. Authors: 
Chung-Ching Lin, Jiang Wang, Kun Luo, Kevin Lin, Linjie Li, Lijuan Wang, Zicheng Liu

#### 3. Affiliation: 
第一作者：Microsoft

#### 4. Keywords: 
Video matting, trimap-free, adaptive matting, transformer network, real-world videos

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Adaptive_Human_Matting_for_Dynamic_Videos_CVPR_2021_paper.html  Github: https://github.com/microsoft/AdaM

#### 6. Summary : 
- (1):本文研究的是动态视频的自适应人像抠图问题，该问题的研究背景是视频人像抠图需要考虑时空一致性，而传统的基于trimap的方法不适用于实时应用，因此需要提出一种新的方法来解决这个问题。

- (2):过去的方法主要分为基于trimap的方法和无trimap的方法，前者需要手动标注trimap，而后者在处理复杂场景时容易出现前景和背景混淆的问题。本文提出的方法是一种自适应的人像抠图方法，通过引入transformer网络和encoder-decoder网络相结合的方法，能够在不需要手动标注trimap的情况下，实现对复杂场景中人像的精确抠图。

- (3):本文提出的方法是一种自适应的人像抠图方法，通过引入transformer网络和encoder-decoder网络相结合的方法，能够在不需要手动标注trimap的情况下，实现对复杂场景中人像的精确抠图。具体来说，encoder-decoder网络用于产生alpha mattes和中间的mask，用于指导transformer网络在自适应解码前景和背景，而transformer网络则通过长期和短期的注意力来保留空间和时间上下文，从而实现前景细节的解码。本文的创新点在于提出了一种自适应的人像抠图方法，能够在不需要手动标注trimap的情况下，实现对复杂场景中人像的精确抠图。

- (4):本文在最近提出的数据集上进行了测试，结果表明，本文提出的方法在复杂的现实世界视频中能够显著提高抠图的真实性和时间上的连贯性，并取得了最佳的性能表现。
#### 7. 方法详细介绍：
本文提出了一种自适应人像抠图方法，称为AdaM。该方法采用了编码器-解码器网络和Fg/Bg结构化Transformer网络。编码器从输入RGB视频帧中提取低分辨率和高分辨率特征，而Transformer网络采用长期和短期注意力来明确地建模Fg/Bg外观。解码器将Transformer中编码的Fg/Bg特征和编码器中的高分辨率细节特征拼接在一起，产生中间的Fg/Bg掩模和最终的前景alpha抠图。新预测的Fg/Bg掩模用于自适应更新Value。该方法旨在在具有挑战性的现实环境中产生准确的alpha抠图，同时消除处理不良初始化掩模所带来的敏感性。

#### 8. 实验设置：
本文使用VideoMatte240K数据集进行训练和测试，该数据集分为训练集、验证集和测试集。训练集被转换为SD和HD集以进行不同的训练阶段。测试集被转换并分为VM 512×288和VM 1920×1080集。CRGNN真实数据集包括19个真实世界视频，每10帧进行一次注释，帧率为30 fps，共计711个标记帧。视频抠图精度通过平均绝对差（MAD）、均方误差（MSE）、空间梯度（Grad）和连通性（Conn）误差进行评估。Conn指标在高分辨率下不进行分析，因为计算成本太高。alpha抠图预测的时间一致性使用dtSSD进行评估。

#### 9. 实验结果和分析：
AdaM在大多数指标上都比以前的最先进方法表现出更大的优势，无论是在VM 512×288集还是VM 1920×1080集中。此外，具有MobileNetV2骨干的AdaM在VM 1920×1080集中优于具有ResNet-50的RVM-Large。AdaM在转移到真实数据集CRGNN-R时表现出竞争力，这表明所提出的方法可以克服域漂移问题，更有效地推广到真实世界数据。该模型在静态和动态背景下表现出稳健性，并且两个集合中几乎没有性能差异。在VM测试集中，AdaM的平均alpha MAD和dtSSD显示，误差和时间一致性度量在前10帧中得到改善，然后随时间保持稳定。


# Paper:116     基于双重条件扩散模型的合成人脸生成



#### 1. Title: 
DCFace: Synthetic Face Generation with Dual Condition Diffusion Model

#### 2. Authors: 
Minchul Kim, Feng Liu, Anil Jain, Xiaoming Liu

#### 3. Affiliation: 
第一作者：密歇根州立大学（Michigan State University）

#### 4. Keywords: 
Synthetic face generation, face recognition, dual condition diffusion model, style extractor, ID loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2021_paper.html  Github: https://github.com/minchulkim87/DCFace

#### 6. Summary : 
- (1):本文研究了如何生成逼真的人脸图像用于训练人脸识别模型，同时保证生成的图像满足真实图像的条件分布，包括姿态、光照、表情、年龄和遮挡等多种因素。 
- (2):以往的方法主要使用GAN或3D模型生成合成数据集，但是这些方法没有考虑到控制数据集中的唯一性、多样性和一致性。本文提出了一种基于扩散模型的双重条件人脸生成器（DCFace），通过控制主体外观（ID）和外部因素（风格）两个条件，直接控制类内和类间变化。本文提出了一种新的Patch-wise风格提取器和Time-step依赖的ID loss，使DCFace能够精确控制不同风格下同一主体的人脸图像生成。 
- (3):本文提出了一种双重条件逆问题，通过可观测的身份条件Xid和风格条件Xsty检索未知图像Y。首先，使用人脸图像生成器生成高质量的人脸图像Xid，然后从风格库中选择一个风格图像Xsty，最后使用双重条件生成器将这两个条件混合，预测具有Xid身份和Xsty风格的图像。本文提出了一种新的双重条件生成器，可以从同一主体的图像（XAid，XAsty）中学习，避免了需要三元组（XAid，XBsty，XAsty）的问题。 
- (4):本文在四个测试数据集（LFW、CFP-FP、CPLFW、AgeDB和CALFW）中，相比于以往的方法，DCFace提供了平均6.11%的更高的验证准确性。本文的方法在人脸识别任务上取得了SoTA的性能，同时保证了数据集的唯一性、多样性和一致性。
#### 7. 方法详细介绍：
本文提出了一种名为Dual Condition Face Dataset Generator (DCFace)的方法，用于生成合成人脸数据集以训练人脸识别模型。该方法采用了双条件扩散模型，包括两个阶段的生成过程。第一阶段是条件采样阶段，生成一个高质量的ID图像和从真实训练数据库中选择的一个任意风格图像。第二阶段是混合阶段，使用双条件生成器将两个图像结合起来。每个阶段的可训练模型分别是ID图像生成器Gid和双条件混合器Gmix。Gid和Gmix都基于扩散模型，其中Gmix包括一个分块风格提取器和一个时间步依赖的ID损失。该方法在生成合成数据集方面取得了最先进的性能。

#### 8. 实验设置：
本文使用CASIA-WebFace数据集作为混合生成器的训练集，并使用该数据集作为测量唯一性、一致性和多样性指标的验证集。用于生成合成标记数据集的风格库也是CASIA-WebFace训练集。本文使用200个间隔的DDIM进行采样和生成50万个样本。

#### 9. 实验结果与分析：
本文提出了三个类依赖性指标来评估合成数据集的唯一性、内部一致性和内部多样性，并将这些指标应用于生成的数据集上，并将其与以前的合成数据集进行比较。本文发现，当一致性和多样性平衡时，FR性能最佳。本文还将其方法的性能与其他合成人脸生成方法进行了比较，包括SynFace和DigiFace，并发现其方法的Cintra和Dintra比这些方法更高。


# Paper:117     利用人工神经网络探究海马依赖任务中场景感知的神经表示



#### 1. Title: 
Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks

#### 2. Authors: 
Markus Frey, Christian F. Doeller, Caswell Barry

#### 3. Affiliation: 
1. Kavli Institute for Systems Neuroscience, NTNU, Norway (Markus Frey, Christian F. Doeller)
2. Max-Planck-Insitute for Human Cognitive and Brain Sciences, Germany (Markus Frey, Christian F. Doeller)
3. Cell & Developmental Biology, UCL, United Kingdom (Caswell Barry)

#### 4. Keywords: 
neural networks, scene perception, hippocampus, view synthesis, object segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Frey_Probing_Neural_Representations_of_Scene_Perception_in_a_Hippocampally_Dependent_CVPR_2021_paper.html  Github: None

#### 6. Summary: 
- (1): This paper aims to better understand the intrinsic computations governing the transformation from egocentric to allocentric reference frames in scene perception, which controls successful view synthesis in humans and other animals.
 
- (2): Previous biologically inspired models have not been equipped to deal with randomly sampled egocentric sensory observations. The paper proposes a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of deep artificial neural networks (DNNs) to transform scenes viewed from different egocentric perspectives. The approach is well motivated as it addresses the limitations of previous models and provides a more biologically realistic model for scene perception.

- (3): The paper proposes a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, and demonstrates that DNNs trained using a triplet loss can accurately distinguish between hundreds of scenes across many different viewpoints and disentangle object information from location information when using a factorized latent space. By using a reconstruction loss combined with a pixel-wise decoder, the paper also shows that unsupervised object segmentation can be performed, outperforming the state-of-the-art models on the CATER, MOVi-A,B,C benchmarks. The innovation lies in the use of a novel scene perception benchmark and the proposed network architecture that can learn the task.

- (4): The proposed method achieves state-of-the-art performance on unsupervised object segmentation on the CATER, MOVi-A,B,C benchmarks. The performance supports the goals of the paper, which is to better understand the intrinsic computations governing the transformation from egocentric to allocentric reference frames in scene perception.
#### 7. 方法详细介绍：
本文提出了一种基于海马依赖性任务的新型场景感知基准测试，旨在探究深度人工神经网络（DNN）从不同视角转换场景的能力。该模型的架构受到颞叶结构和海马之间的连接启发。使用三元组损失和分解的潜在空间来训练DNN，将信息传播分为“什么”和“在哪里”的路径，用于重构输入。本文还描述了使用重构损失和像素级解码器进行无监督对象分割的方法，优于CATER、MOVi-A、B、C基准测试的最新模型。

具体步骤：
1. 使用预训练的卷积神经网络模拟视觉皮层的响应。
2. 通过弱或强连接将来自V4和IT的信息通过PR和PH皮层进行路由。
3. 海马体由CA3和CA1实现，前者使用自我注意力层跨不同场景的不同快照集成信息，后者使用外积积分时间和空间信息，然后通过简单的前馈层进行馈送。
4. 使用类似于SIMONe中使用的像素级解码器来重构新视图中的输入。
5. 对于反馈连接，CA1层被分成时间（LEC）和空间（MEC）信息，正如前向传递在PR/PH层的水平上分割视觉信息一样。
6. 通过最小化海马潜在空间上的三元组损失或在像素空间中的L2重构损失来训练模型。

#### 8. 实验设置：
本研究设计了六个不同版本的任务，通过改变远处地标（有|无）和物体的颜色（混合|绿色|白色）来变化，同时随机改变场景中物体的数量，其中10%的场景只有一个物体，20%有两个，30%有三个，40%包含四个物体。在渲染过程中，为每个渲染视角存储分割掩模和深度剖面。此外，还存储了不同参考框架下神经活动的位置信息，包括物体在自我中心参考框架（“在屏幕上”的位置）、方位角和俯仰角等角度信息以及每个物体在环境中的分配位置。

#### 9. 实验结果与分析：
本文提出了一种受生物启发的模型，可以区分数百个场景并在其训练集之外进行泛化。该模型能够通过分解的潜在空间重构视觉输入，将对象与空间信息分离。该模型还能够通过想象不同视角的场景来执行新视图合成，并能够与最新的最佳基线模型相媲美或更好地在CATER基准测试上表现。然而，该模型的相对较小的规模可能会阻止其在更具挑战性的真实世界数据集（如MOVi-C、D、E或COCO）上表现良好。


# Paper:118     佩戴式摄像头视频中多个可变形物体的跟踪



#### 1. Title: 
Tracking Multiple Deformable Objects in Egocentric Videos

#### 2. Authors: 
Mingzhen Huang, Xiaoxing Li, Jun Hu, Honghong Peng, Siwei Lyu

#### 3. Affiliation: 
第一作者：纽约州立大学水牛城分校

#### 4. Keywords: 
Multiple Object Tracking, Deformable Objects, Egocentric Videos, Motion Estimation, Appearance-based Tracking

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2021_paper.html  Github: https://mingzhenhuang.com/projects/detracker.html

#### 6. Summary : 
- (1):本文研究的是如何在佩戴式摄像头拍摄的视频中跟踪多个可变形物体。由于佩戴式摄像头的特殊性质，视频中的物体形状和外观经常发生变化，同时摄像头的运动也会对跟踪造成影响，因此需要一种新的跟踪方法。

- (2):过去的多目标跟踪方法主要依赖于外观特征，但对于高度可变形的物体跟踪效果不佳。而使用运动线索进行跟踪的方法则难以有效或高效地处理佩戴式摄像头拍摄的视频。本文提出了一种新的跟踪方法，可以同时检测和跟踪可变形物体，且能够处理佩戴式摄像头的特殊情况。

- (3):本文提出了一种名为DETracker的新的多目标跟踪算法，它使用三个新颖的模块来处理严重的自我运动和快速变形的目标物体。这三个模块分别是运动分离网络（MDN）、补丁关联网络（PAN）和补丁记忆网络（PMN）。MDN用于有效地估计两个连续帧之间的运动流，PAN通过将物体分成补丁并在即将到来的帧中找到最佳匹配的补丁来处理可变形物体跟踪，PMN通过利用变压器网络来保留和更新跟踪对象的特征嵌入，从而能够使用历史补丁特征进行长期关联。

- (4):本文提出的方法在DogThruGlasses数据集和YouTube-Hand数据集上进行了实验，结果表明DETracker方法在DogThruGlasses数据集上的表现优于现有的最先进方法8.1％，在YouTube-Hand数据集上也取得了竞争性的结果。本文的方法可以有效地处理佩戴式摄像头拍摄的视频中的多目标跟踪问题。
#### 7. 方法详细介绍：
本文提出了一种新的多目标跟踪算法DETracker，它结合了目标检测、运动估计和补丁关联。该方法使用修改后的CenterNet进行检测，使用DLA-34进行检测，使用级联ROI头进行边界框细化。该方法还包括一个运动位移网络（MDN）用于估计相机运动，一个补丁关联网络（PAN）用于匹配帧之间的补丁，以及一个补丁记忆网络（PMN）用于长期外观建模。置信度分数定义为结合检测和身份关联置信度。该方法还使用轨迹来监视活动轨迹的健康状况，并有助于抑制误报。该方法使用组合损失函数进行训练，包括检测损失、相机流损失、补丁关联损失和运动流损失。 

#### 8. 实验设置：
DETracker在DogThruGlasses数据集上进行评估。网络使用SGD优化器进行端到端训练，批量大小为64，学习率为2e-5，迭代次数为30K。超参数设置为θ = 0.6，ξ = 0.4，β = 5，γ = 0.3和ϕ = 0.65。输入帧窗口长度设置为最多4个连续帧，以最小化计算成本和内存占用。评估指标包括身份F1分数（IDF1）、检测准确度（DetA）、关联准确度（AssA）、假阳性（FP）、假阴性（FN）、身份切换（IDs）、多目标跟踪准确度（MOTA）和高阶跟踪准确度（HOTA）。

#### 9. 实验结果和分析：
DETracker在大多数指标上都优于现有的最先进算法，并且差距显著。与之前的最先进算法GTR相比，DETracker在HOTA方面的表现提高了8.1％。结果报告在DogThruGlasses测试集上。


# Paper:119     利用随机投影滤波器提高对抗鲁棒性



#### 1. Title: 
Adversarial Robustness via Random Projection Filters

#### 2. Authors: 
Minjing Dong, Chang Xu

#### 3. Affiliation: 
Minjing Dong, Chang Xu: School of Computer Science, University of Sydney（悉尼大学计算机科学学院）

#### 4. Keywords: 
Adversarial attacks, Adversarial defense, Adversarial training, Random projection, Convolutional Neural Networks

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Dong_Adversarial_Robustness_via_Random_Projection_Filters_CVPR_2021_paper.pdf
Github: https://github.com/dongminjing/RPF

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在各种任务中表现出卓越的性能，但容易受到对抗性攻击的影响，因此需要提高其对抗性能。
- (2):过去的方法主要集中在对抗性训练策略上，但仅使用传统的对抗性训练很难达到令人满意的鲁棒性。本文提出了一种新的方法，即利用随机投影滤波器来替换部分卷积滤波器，从而提高对抗性能。与其他随机化技术相比，本文的方法可以更好地平衡对抗性和自然泛化之间的权衡。
- (3):本文提出了一种新的对抗性防御方法，即利用随机投影滤波器来替换部分卷积滤波器。理论上，本文扩展了Johnson-Lindenstrauss引理的范围，以覆盖卷积操作。通过从零均值高斯分布中随机采样部分卷积滤波器，可以在新的卷积定义下近似保持成对示例之间的距离。本文的方法可以在多个网络和数据集上进行充分的评估，并且在实验中表现出优越性能。
- (4):本文的方法在多个数据集和网络上进行了充分的评估，表明其优于现有的对抗性防御方法。本文的方法可以在保持自然泛化的同时提高对抗性能。
#### 7. 方法详细介绍：
本文提出的方法称为随机投影滤波器对抗鲁棒性（Adversarial Robustness via Random Projection Filters，RPF）。RPF方法通过将CNN中的一些卷积滤波器替换为随机投影滤波器来提高CNN的对抗鲁棒性。随机投影滤波器是通过从高斯分布中采样，然后将样本投影到随机子空间中生成的。利用随机投影的几何表示保持属性，提高CNN的对抗鲁棒性。使用RPF进行对抗训练涉及从具有随机投影滤波器的网络中生成对抗性示例，然后使用这些示例来训练具有不同随机投影滤波器的网络。使用mix-max优化来优化两个网络的参数。对抗训练的详细信息在算法1中展示。

#### 8. 实验设置：
本文在多个数据集上进行实验，包括CIFAR-10/100和ImageNet。实验中使用的模型包括ResNet-18、WideResnet-34-10和ResNet-50。对抗训练策略遵循最先进的对抗训练策略的协议。对于对抗性示例生成，使用PGD-10，最大扰动大小为ϵ = 8/255。PGD的步长设置为2/255。在ImageNet上，使用PGD-2，最大扰动大小为ϵ = 4/255。实验中使用的攻击包括FGSM、PGD、CW攻击、MIFGSM、DeepFool和Auto Attack。比较基线包括加性噪声注入、乘性噪声注入和其他防御技术，如RobustWRN、AWP、SAT、LLR和RobNet。

#### 9. 实验结果和分析：
本文提出的RPF方法在清洁准确率和对抗鲁棒性之间实现了更好的平衡，相比基线方法具有更好的性能。在ResNet-18上，RPF在PGD20攻击下实现了61.27%的鲁棒准确率，在DeepFool攻击下实现了79.43%的鲁棒准确率，在Auto Attack攻击下实现了64.38%的鲁棒准确率。在6种不同的攻击下，RPF在所有情况下都实现了最佳性能以及最高的清洁准确率。在CIFAR-100上，RPF将AT基线的鲁棒准确率提高了18.40%在Auto Attack攻击下，8.66%在PGD20攻击下，5.05%在MIFGSM攻击下。RPF实现了56.88%的清洁准确率，这比所有噪声注入技术都有显着的差距。


# Paper:120     鲁棒的无监督StyleGAN图像修复



#### 1. Title: 
Robust Unsupervised StyleGAN Image Restoration

#### 2. Authors: 
Yohan Poirier-Ginter, Jean-François Lalonde

#### 3. Affiliation: 
Yohan Poirier-Ginter: Inria, Université Côte d’Azur; Jean-François Lalonde: Université Laval

#### 4. Keywords: 
Image restoration, unsupervised learning, StyleGAN, generative models, optimization

#### 5. Paper: https://lvsn.github.io/RobustUnsupervised/  Github: None

#### 6. Summary:
- (1): This paper focuses on unsupervised image restoration using generative models, which aims to recover high-quality images from degraded inputs without task-specific training. 
- (2): Previous unsupervised methods require careful tuning of hyperparameters for each task and degradation level, and regularization losses are often needed to extend the learned latent space. The proposed approach in this paper makes StyleGAN image restoration robust by using a single set of hyperparameters across a wide range of degradation levels, without any additional regularization terms. 
- (3): The proposed method relies on a 3-phase progressive latent space extension and a conservative optimizer, which avoids the need for any additional regularization terms. The method achieves robustness by avoiding any such regularization losses. 
- (4): The proposed method achieves state-of-the-art results on various image restoration tasks, including upsampling, inpainting, denoising, and deartifacting, even when compared to other StyleGAN-based inversion techniques. The method also outperforms existing techniques on compositions of these tasks without changing hyperparameters. The performance supports the goals of the paper, which is to achieve robust unsupervised image restoration using generative models.
#### 7. 方法详细介绍：
本文提出了一种鲁棒的无监督StyleGAN图像修复方法，可以处理各种类型的退化。该方法由四个任务组成：修补、去伪影、去噪和上采样。这些任务的顺序是固定的，依次为修补、去伪影、去噪和上采样。该方法使用可微分的退化过程的近似，并对不可微分操作使用代理梯度。组合的退化是由固定顺序的子序列创建的。具体步骤包括：全局潜空间优化、逐层潜空间扩展和逐滤波器潜空间扩展。此外，该方法还使用了归一化梯度下降（NGD）优化器和多分辨率损失函数进行鲁棒优化。

#### 8. 实验设置：
本文在1024x1024分辨率的FFHQ数据集上进行实验。使用的模型是StyleGAN2-ADA的PyTorch实现，预训练于FFHQ数据集上。实验分别在每个任务的五个退化级别上进行。提出的方法的超参数在验证数据上进行调整，然后在所有任务和级别上使用。基线模型在每行上分别进行优化以获得最佳准确性。

#### 9. 实验结果和分析：
提出的方法在准确性和逼真度方面在大多数情况下优于基线模型。该方法在各种退化组合下保持高保真度和逼真度。结果以表格和图像的形式进行定量和定性展示。提出的方法相对于基线模型表现出更好的退化级别变化鲁棒性。本文提出的方法为基于StyleGAN的图像修复提供了一种鲁棒的解决方案，适用于各种退化类型和组合。该方法不仅限于人脸，而且适用于任何预训练的StyleGAN。但是，它受限于GAN学习的领域。本文还提到了与GAN相关的伦理考虑。


# Paper:121     SadTalker：学习逼真的3D运动系数，用于风格化的音频驱动单张图像说话人脸动画生成



#### 1. Title: 
SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation

#### 2. Authors: 
Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang

#### 3. Affiliation: 
Wenxuan Zhang: 西安交通大学软件工程学院
其他作者：Tencent AI Lab, Ant Group, IAIR, 西安交通大学

#### 4. Keywords: 
3D facial model, audio-driven, talking head generation, 3D motion coefficients, face render

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_CVPR_2021_paper.html  Github: https://sadtalker.github.io/

#### 6. Summary : 
- (1):本文研究的背景是单张图像与语音音频生成说话人脸视频的挑战，包括不自然的头部运动、扭曲的表情和身份修改等问题。
 
- (2):过去的方法主要集中在生成唇部运动，但是生成的视频质量仍然不自然，受到姿势偏好、口腔模糊、身份修改和扭曲的面部等问题的限制。本文提出了SadTalker，通过从音频中生成3D运动系数（头部姿态、表情）来生成逼真的3D感知面部渲染，从而生成说话人脸视频。与过去的方法不同，本文明确地使用3D信息来生成运动系数，从而解决了过去方法中姿势和表情耦合的问题。本文的方法包括ExpNet和PoseVAE来分别学习准确的面部表情和多样化的头部运动，以及3D感知面部渲染来生成最终的视频。

- (3):本文提出了一种新的系统SadTalker，通过隐式3D系数调制来实现风格化的音频驱动单张图像说话人脸动画生成。为了实现这一目标，本文将3DMM的运动系数作为中间表示，并将任务分为两个主要组成部分。一方面，我们旨在从音频中生成逼真的运动系数（例如头部姿态、唇部运动和眨眼），并分别学习每种运动以减少不确定性。对于表情，我们设计了一种新的音频到表情系数网络，通过从[28]中仅唇部运动系数和重建渲染的3D面部上的感知损失（唇读损失[1]、面部标记损失）来提取系数。对于风格化的头部姿态，使用条件VAE [6] 来学习给定姿态的残差，以建模多样性和逼真的头部运动。在生成逼真的3DMM运动系数后，我们通过一种新的3D感知面部渲染来驱动源图像。受face-vid2vid [40]和[31]的启发，我们学习了显式3DMM系数和无监督3D关键点域之间的映射。然后，通过源和驱动的无监督3D关键点生成变形场，并将参考图像变形以生成最终的视频。我们分别训练了表情生成、头部姿态生成和面部渲染器的子网络，并且我们的系统可以以端到端的方式推断。 

- (4):本文的方法在运动同步和视频质量方面表现
#### 7. 方法详细介绍：
SadTalker是一种用于风格化音频驱动的单张图像说话人脸动画的3D运动系数学习框架。该方法包括三个主要组件：ExpNet、PoseVAE和3D-aware Face Render。ExpNet用于从音频中学习表情系数，PoseVAE用于学习头部姿态系数，而FaceRender用于生成最终的说话人脸动画。该方法可以在不需要任何手动干预的情况下进行端到端推理。作者还提出了一种新的仅针对嘴唇的系数，以改善嘴唇同步。

具体步骤如下：
1. 使用预训练的深度3D面部重建方法提取3DMM参数。
2. 使用Adam优化器分别训练ExpNet、PoseVAE和FaceRender。
3. ExpNet和PoseVAE的时间考虑分别为连续的5帧和32帧。
4. 评估指标包括Frechet Inception Distance（FID）、累积概率模糊检测（CPBD）、余弦相似度（CSIM）、距离分数（LSE-D）、置信度分数（LSE-C）、头部运动特征嵌入的标准差和Beat Align Score。
5. 与几种最先进的方法进行比较，并进行用户研究以评估所有方法的性能。

#### 8. 实验设置：
作者使用8个A100 GPU进行实验，每个模型都使用特定的学习率进行训练。实验中使用的评估指标包括FID、CPBD、CSIM、LSE-D、LSE-C、头部运动特征嵌入的标准差和Beat Align Score。

#### 9. 实验结果和分析：
实验结果表明，SadTalker系统在运动同步和视频质量方面实现了最先进的性能。作者使用多种指标来证明该方法的优越性。该方法的代码和演示视频可在https://sadtalker.github.io上获得。作者还考虑了其方法的伦理影响，并计划将可见和不可见的视频水印插入生成的视频中以进行内容识别。


# Paper:122     Siamese DETR



#### 1. Title: 
Siamese DETR

#### 2. Authors: 
Zeren Chen, Gengshi Huang, Wei Li, Jianing Teng, Kun Wang, Jing Shao, Chen Change Loy, Lu Sheng

#### 3. Affiliation: 
Zeren Chen, Gengshi Huang, Jianing Teng, Kun Wang, Jing Shao: SenseTime Research; Wei Li, Chen Change Loy: Nanyang Technological University; Lu Sheng: Beihang University.

#### 4. Keywords: 
Object detection, Transformers, self-supervised learning, Siamese network, multi-view learning.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Siamese_DETR_CVPR_2021_paper.html  Github: https://github.com/Zx55/SiameseDETR

#### 6. Summary:
- (1):本文研究背景是如何将自监督学习方法应用于目标检测任务中，以减少对大规模标注数据的依赖。
- (2):过去的方法主要是基于ResNets或ViTs等基础模型进行表示学习，难以直接应用于DETR等具有任务特定Transformer模块的目标检测模型。本文提出了一种名为Siamese DETR的自监督预训练方法，通过两个互补的预训练任务，即多视角区域检测和多视角语义判别，同时学习视角不变和检测导向的表示，以提高DETR的泛化能力。
- (3):本文提出了一种新的Siamese自监督预训练方法，通过结合Siamese网络和DETR中的交叉注意力机制，学习视角不变的本地化能力。具体地，通过直接定位增强视图之间的查询区域并在全局和区域级别上最大化判别信息，Siamese DETR可以在预训练期间学习与下游目标检测任务对齐的本地化和判别表示。实验结果表明，Siamese DETR在COCO和PASCAL VOC检测任务上取得了最先进的性能。
- (4):本文提出的Siamese DETR方法在COCO和PASCAL VOC检测任务上取得了最先进的性能，证明了其有效性和通用性。
#### 7. 方法详细介绍：
本文提出了一种名为Siamese DETR的方法，它是一种Siamese多视角自监督框架，专门为DETR中的Transformer设计。该方法旨在以自监督的方式预训练DETR的Transformer，扩展现有的自监督预训练边界。Siamese DETR由一个冻结的ResNet-50骨干网络和一个具有编码器-解码器架构的Transformer组成。该方法涉及视图构建、多视图检测预训练和损失函数，其中包括语义一致性、全局判别和多视图对称定位损失的组合。提出的多视图交叉注意力机制使模型能够学习视图不变的定位能力，相比于UP-DETR和DETReg，在下游任务中提供更好的先验知识。

#### 8. 实验设置：
本文的评估协议遵循UP-DETR基准测试，其中DETR变体首先分别在ImageNet或COCO train2017上进行预训练，然后在COCO train2017或PASCAL VOC trainval07+12上进行微调。在COCO val2017和PASCAL VOC test2007基准测试中报告了COCO风格的指标，包括AP、AP50、AP75、APs、APm、APl。预训练使用AdamW优化器，在ImageNet上的总批量为256，在COCO上的总批量为64，学习率为1×10−4，权重衰减为1×10−4。完整的预训练时间表为60个epoch，学习率在40个epoch时衰减。微调使用批量大小为32和不同的学习率计划，具体取决于不同的DETR变体。

#### 9. 实验结果和分析：
Siamese DETR在下游COCO和PASCAL VOC基准测试中的表现优于其对手。Siamese DETR在COCO和PASCAL VOC基准测试中与三种DETR变体相比取得了更好的性能。当使用Selective Search和Edgeboxes替换时，性能差距得到缓解。在所有设置中，使用Edgeboxes的Siamese DETR实现了最佳性能。


# Paper:123     DynIBaR：神经动态基于图像的渲染



#### 1. Title: 
DynIBaR: Neural Dynamic Image-Based Rendering

#### 2. Authors: 
Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, Noah Snavely

#### 3. Affiliation: 
Zhengqi Li: Google Research
Qianqian Wang: Google Research, Cornell Tech

#### 4. Keywords: 
novel view synthesis, dynamic scene, image-based rendering, volumetric rendering, multi-view feature aggregation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是从单目视频中合成新视角的问题，特别是针对动态场景的情况。传统的基于图像的渲染方法难以处理动态场景的复杂几何和视角变化，而基于神经网络的方法可以通过学习场景的连续辐射场来实现高质量的新视角合成。但是，这些动态神经辐射场方法在处理长时间、复杂物体运动和不受控制的相机轨迹的视频时，可能会产生模糊或不准确的渲染结果，限制了它们在实际应用中的使用。

- (2):本文提出了一种新的方法，通过采用体积图像渲染框架，以场景运动感知的方式从附近视角聚合特征来合成新视角，从而解决了这些限制。该方法保留了先前方法的优点，能够模拟复杂场景和视角相关效果，同时还能够从具有不受控制的相机轨迹和复杂场景动态的长视频中合成逼真的新视角。本文的方法在动态场景数据集上显著提高了渲染保真度，并在具有挑战性的相机和物体运动的野外视频上得到了应用。

- (3):本文提出了一种基于场景运动调整的多视图特征聚合方法，通过学习基函数表示跨多个帧的运动轨迹场来有效地模拟场景运动，同时引入了一种新的时间光度损失来实现动态场景重建的时间一致性。此外，本文还提出了一种基于IBR的运动分割技术，通过贝叶斯学习框架实现场景的静态和动态分解。

- (4):本文的方法在动态场景数据集上显著提高了渲染保真度，对于整个场景和动态对象区域的LPIPS误差都有超过50%的平均降低。本文的方法还在野外视频上得到了应用，能够处理长时间、复杂场景运动和不受控制的相机轨迹的视频，而先前的最先进方法无法产生高质量的渲染结果。
#### 7. 方法详细介绍：
本文提出了一种名为DynIBaR的神经动态基于图像的渲染框架，用于从单目视频中合成复杂动态场景的新视角。该方法采用体积图像渲染框架，通过聚合场景中附近视角的特征以动态感知的方式合成新视角。该系统使用跨越多个帧的运动轨迹场来建模多视角的场景运动，这些运动轨迹场由学习的基函数表示。为了实现动态场景重建的时间连续性，引入了一种新的时间光度损失，该损失在运动调整的光线空间中操作。最后，通过贝叶斯学习框架中的新IBR基于运动分割技术，将场景分解为静态和动态组件。

#### 8. 实验设置：
本文在两个动态场景基准测试集和长时间、复杂场景运动和不受控制的相机轨迹的野外视频上评估了所提出的方法。实验表明，该方法可以渲染高度详细的场景内容，并显著改进了现有技术，使LPIPS误差平均降低了50%以上，无论是在整个场景上还是在对应于动态对象的区域上。

#### 9. 实验结果与分析：
本文在UCSD和Nvidia动态场景数据集上进行了定量和定性评估。所提出的方法在峰值信噪比（PSNR）、结构相似性（SSIM）和LPIPS等三个标准误差度量方面优于现有技术。该方法还显著提高了PSNR，比第二优秀的方法在两个数据集上分别提高了2dB和4dB。该方法对于恢复高度详细的场景内容更加有效。在复杂动态场景的野外镜头上进行的定性比较也表明，所提出的方法合成了静态和动态场景内容的照片般逼真的新视角，最接近于地面真实图像。


# Paper:124     一种轻量级的活跃说话人检测模型



#### 1. Title: 
A Light Weight Model for Active Speaker Detection

#### 2. Authors: 
Junhua Liao, Haihan Duan, Kanghui Feng, Wanbing Zhao, Yanbing Yang, Liangyin Chen

#### 3. Affiliation: 
第一作者：四川大学计算机学院

#### 4. Keywords: 
Active speaker detection, lightweight model, audio-visual scenarios, deep learning, gated recurrent units

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liao_A_Light_Weight_Model_for_Active_Speaker_Detection_CVPR_2021_paper.html  Github: https://github.com/Junhua-Liao/Light-ASD

#### 6. Summary : 
- (1):本文研究的是在音视频场景下，检测谁在说话的活跃说话人检测任务。该任务在许多应用中都非常重要，但是现有的方法存在计算量大、内存占用高等问题，难以应用于资源受限的场景。

- (2):过去的方法主要是通过输入多个候选人的信息和设计复杂的模型来提高性能。虽然这些方法取得了良好的性能，但是它们的高内存和计算能力消耗使它们难以应用于资源受限的场景。本文提出了一种轻量级的活跃说话人检测架构，通过减少输入候选人的数量、将2D和3D卷积用于音视频特征提取、使用计算复杂度较低的门控循环单元进行跨模态建模等方法来构建。实验结果表明，该方法在模型参数和FLOPs方面的资源成本显著低于现有的方法，同时在性能方面也具有竞争力。

- (3):本文提出了一种轻量级的端到端活跃说话人检测架构，通过信息输入、特征提取和跨模态建模三个方面进行改进，提出了一种新的损失函数进行训练。该方法在AVS-ActiveSpeaker数据集上进行了实验，结果表明该方法在模型参数和FLOPs方面的资源成本显著低于现有的方法，同时在性能方面也具有竞争力。

- (4):该方法在AVS-ActiveSpeaker数据集上取得了94.1%的mAP性能，同时模型参数和FLOPs方面的资源成本显著低于现有的方法。该方法还在Columbia数据集上表现良好，具有良好的鲁棒性。该方法的单帧推理时间范围为0.1ms至4.5ms，适用于实时应用的部署。
#### 7. 方法详细介绍：
本文提出的主动说话人检测方法基于一个轻量级框架，输入单个候选人。该框架由三个主要组件组成：视觉特征编码器、音频特征编码器和检测器。视觉特征编码器使用轻量级视觉块进行特征提取，全局最大池化用于获取候选人脸序列的视觉特征。音频特征编码器从原始音频信号中提取由13D MFCC和时间信息组成的2D特征图，2D卷积被分成两个1D卷积以分别从MFCC和时间维度提取信息。检测器使用双向GRU模型建模多模态特征的时间上下文信息，并使用全连接层预测当前候选人是否在说话。损失函数由主分类器和视觉辅助分类器组成，温度系数在训练过程中逐渐减小。

#### 8. 实验设置：
本文在AVA-ActiveSpeaker数据集和Columbia数据集上评估了所提出的方法。AVA-ActiveSpeaker数据集包含120个训练集、33个验证集和109个测试集，使用平均平均精度（mAP）在验证集上评估性能。Columbia数据集是一个87分钟的小组讨论视频，有五个发言人，使用F1分数评估性能。最终架构使用PyTorch实现，所有实验都在NVIDIA RTX 3090 GPU上进行。模型使用Adam优化器进行30个训练时期，学习率设置为0.001，每个时期的衰减率为0.05。

#### 9. 实验结果与分析：
本文提出的方法在AVA-ActiveSpeaker验证集上取得了94.1%的mAP，仅略逊于SPELL的最新方法，但模型参数少了23倍，计算量少了4倍。本文提出的方法和EASEE-50是最先进的端到端主动说话人检测方法，模型参数少了75倍以上，计算量少了109倍。本文提出的架构仅使用AVA-ActiveSpeaker训练集从头开始训练整个网络，无需额外处理。本文提出的模型仅输入单个候选人，这意味着它可以基于单个候选人的音频和视觉信号进行准确的预测。本文提出的主动说话人检测方法在不同数量的检测到的人脸输入下始终优于多输入的最先进方法。本文提出的方法在六个子场景中均取得了最佳性能，是唯一一个在候选人数量小于三或面部宽度大于64像素时mAP大于90%的方法，因此表明它比其他竞争方法更加稳健。


# Paper:125     Parts2Words: 通过部分和单词之间的双向匹配学习点云和文本的联合嵌入



#### 1. Title: 
Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching between Parts and Words

#### 2. Authors: 
Chuan Tang, Xi Yang, Bojian Wu, Zhizhong Han, Yi Chang

#### 3. Affiliation: 
第一作者：吉林大学人工智能学院，中国

#### 4. Keywords: 
Shape-Text Matching, Point Clouds, Optimal Transport, Joint Embedding, Multi-modal Retrieval

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_Parts2Words_Learning_Joint_Embedding_of_Point_Clouds_and_Texts_by_CVPR_2021_paper.html  Github: https://github.com/ChuanXia/Tex2Shape

#### 6. Summary : 
- (1):本文研究了高级形状理解中的形状-文本匹配问题。当前的方法主要将3D形状表示为多个2D渲染视图，但由于有限数量的视图中的自遮挡引起的结构模糊，这些方法无法很好地理解3D形状。为了解决这个问题，本文直接将3D形状表示为点云，并提出通过形状的部分和文本的单词之间的双向匹配来学习点云和文本的联合嵌入。

- (2):过去的方法主要是将3D形状表示为体素网格和多视图渲染图像，但由于低分辨率和自遮挡，这些方法难以提高形状和文本的联合理解能力。此外，以前的形状-文本匹配方法通常采用整个3D形状的全局特征进行文本匹配，难以捕捉局部几何信息，因此不适合匹配详细的几何描述。本文提出了一种基于最优传输的形状-文本匹配方法，以实现3D形状和文本的细粒度对齐和检索。

- (3):本文提出了一种新的端到端网络框架，用于学习点云和文本的联合嵌入，实现点云的部分和文本的单词之间的双向匹配。我们利用最优传输理论在优化的特征空间中匹配部分和单词，其中每个部分由其内部所有点的特征聚合表示，每个单词由其上下文信息抽象表示。我们通过优化特征空间来扩大配对训练样本之间的相似性，同时最大化未配对样本之间的间隔。实验表明，我们的方法在Text2Shape数据集的多模态检索任务上实现了显着的精度提高。

- (4):本文的方法在Text2Shape数据集上实现了SOTA结果，证明了其在联合3D形状/文本理解任务中的有效性。
#### 7. 方法详细介绍：
本文提出的方法是Parts2Words，旨在通过部件和单词之间的双向匹配来学习点云和文本的联合嵌入。该方法首先将点云分割成部件，然后利用最优传输方法在优化的特征空间中匹配部件和单词，其中每个部件由其内部所有点的特征聚合表示，每个单词则由其上下文信息抽象表示。该特征空间被优化以扩大配对训练样本之间的相似性，同时最大化未配对样本之间的间隔。该方法在Text2Shape数据集上的多模态检索任务中取得了显著的准确性提升。

具体步骤如下：
1. 利用3D分割先验将点云分割成部件。
2. 利用形状编码器提取每个输入形状上不同语义类型的部件的嵌入，通过聚合相应部件中的点的特征来实现。
3. 利用文本编码器使用双向GRU提取单词级别的局部特征。
4. 利用最优传输方法在优化的特征空间中匹配3D形状和文本，使用嵌入P和W评估它们之间的相似性。
5. 使用多任务学习策略训练网络，同时学习部件分割和匹配。

#### 8. 实验设置：
本文使用的数据集是3D-Text数据集，其中包含椅子和桌子。训练集包含11498个3D形状，其余1434个形状用作测试数据。每个3D形状平均有5个文本描述。实验评估指标为召回率（RR@k，k=1,5）和NDCG。每个点云中的点数设置为2500，分割的粗粒度为17个类别。使用Adam优化器进行训练，初始学习率为0.001，批量大小为128。

#### 9. 实验结果与分析：
本文提出的Parts2Words方法在Text2Shape检索任务中取得了显著的性能提升，S2T和T2S的检索率分别为19.38%和47.17%。该方法优于Text2Shape、Y2Seq2Seq和TriCoLo等多种现有方法。检索结果表明，该方法可以根据文本描述准确地检索3D形状。实验结果还探讨了不同因素对检索性能的影响，包括粒度、损失权重、骨干网络、CD/EMD、采样、训练、颜色和分割。结果表明，最粗糙的部件分割注释实现了最佳性能，端到端模型优于分别训练的方法。可视化结果表明，该方法可以准确地找到部件和单词之间的对应关系。


# Paper:126     可扩展图频分解实现高保真度3D手部形状重建



#### 1. Title: 
High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition

#### 2. Authors: 
Tianyu Luan, Yuanhao Zhai, Jingjing Meng, Zhong Li, Zhang Chen, Yi Xu, Junsong Yuan

#### 3. Affiliation: 
第一作者：纽约州立大学水牛城分校

#### 4. Keywords: 
3D hand reconstruction, graph frequency decomposition, personalized hand modeling, frequency split network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_CVPR_2021_paper.html  Github: https://github.com/tyluann/FreqHand

#### 6. Summary : 
- (1):本文研究高保真度的3D手部建模，旨在解决现有单张图像手部建模技术无法捕捉足够细节的问题，限制了其在个性化手部建模等领域的应用。

- (2):现有的手部建模方法主要分为参数化和非参数化两种，前者的优点是方便训练和结果稳健，但限制了网格分辨率和细节；后者可以估计固定拓扑结构的网格顶点位置，但对细节的表达不够关注。本文提出了一种基于频率分解的方法，将手部网格转换到频率域，通过不同频率带的生成和监督，实现了高保真度的3D手部建模。

- (3):本文提出了一种基于频率分解的网络架构，通过多层图卷积网络和U-net的设计，实现了可扩展的3D手部建模。在训练过程中，通过监督中间网格和最终高分辨率网格，实现了对个性化细节的捕捉。本文的创新点在于提出了一种新的手部模型，结合了参数化和非参数化的优点，同时提出了一种新的评估指标MSNR，更好地评估了网格细节。

- (4):本文在InterHand2.6M数据集上进行了实验，与MANO和其他基线方法相比，本文提出的方法在所有三个指标上都取得了更好的结果，证明了其在高保真度3D手部建模方面的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于频率分解的手部三维网格重建方法。首先，将手部形状表示为MANO模型，并通过细分将其转换为包含12,338个顶点的网格。然后，使用多分辨率图形卷积网络对手部形状进行重建。网络采用U-net结构，分为三个分辨率级别，不同级别的图像特征对应不同的细节级别。网络的输出网格被转换到频域，并使用频率分解损失函数对每个频率分量进行监督。总损失函数由3D关节位置误差、每个顶点误差和频率分解损失函数组成。最后，提出了一种新的评估指标——平均信噪比（MSNR），用于衡量每个网格频率分量的信噪比。

#### 8. 实验设置：
本文在InterHand2.6M数据集上进行实验，该数据集包含多视角图像、丰富的姿势和手部姿势注释。训练集包含1,000,000张图像，输入图像被调整为256x256大小。使用Adam优化器，学习率为0.0001，批量大小为16，训练100个epochs。

#### 9. 实验结果与分析：
本文提出的方法在手部表面细节方面取得了显著的改进，MSNR评估指标得分明显提高。同时，输出网格的关节位置和整体形状也略微更加准确，MPJPE和CD评估指标得分也有所提高。消融实验表明，投影到特征图的跳跃连接设计在所有三个评估指标上都有性能提升。频率分解损失函数有助于学习网格细节，每个顶点误差损失函数有助于约束整体形状。本文提出的MSNR评估指标在衡量3D形状细节方面更加有效。


# Paper:127     你感觉怎么样？学习电影场景中的情感和心理状态



#### 1. Title: 
How you feelin’? Learning Emotions and Mental States in Movie Scenes

#### 2. Authors: 
Dhruv Srivastava, Aditya Kumar Singh, Makarand Tapaswi

#### 3. Affiliation: 
CVIT, IIIT Hyderabad, India (印度国际信息技术研究所)

#### 4. Keywords: 
Emotion recognition, multimodal, Transformer, movie understanding, mental states

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Srivastava_How_You_Feelin_Learning_Emotions_and_Mental_States_in_Movie_CVPR_2021_paper.html  Github: https://katha-ai.github.io/projects/emotx

#### 6. Summary : 
- (1):本文旨在通过分析电影场景中角色的情感和心理状态，实现对电影故事的理解。 
- (2):过去的情感识别方法主要集中在面部表情识别上，而本文提出了一种基于多模态Transformer的架构EmoTx，可以同时处理视频、多个角色和对话语句，从而更好地预测电影场景中的情感和心理状态。本文的方法在多标签情感识别方面取得了较好的效果，相比于其他方法，EmoTx的效果更好。 
- (3):本文提出了一种基于多模态Transformer的架构EmoTx，可以同时处理视频、多个角色和对话语句，从而更好地预测电影场景中的情感和心理状态。EmoTx可以预测经典情感（如快乐、愤怒）和其他心理状态（如诚实、有帮助性）。本文的方法在多标签情感识别方面取得了较好的效果，相比于其他方法，EmoTx的效果更好。 
- (4):本文的方法在多标签情感识别方面取得了较好的效果，相比于其他方法，EmoTx的效果更好。EmoTx可以预测经典情感和其他心理状态，并且可以同时处理视频、多个角色和对话语句，从而更好地预测电影场景中的情感和心理状态。
#### 7. 方法详细介绍：
本文提出了EmoTx，一种基于Transformer的多模态架构，用于预测电影场景和每个角色的情感和心理状态。EmoTx通过视频、多个角色和对话话语进行联合预测。该模型在MovieGraphs数据集的注释上进行训练，可以预测经典情感（例如快乐、愤怒）和其他心理状态（例如诚实、有帮助）。EmoTx能够捕捉标签共现并联合预测所有标签。该方法还使用可学习的分类器令牌来预测情感，并在Transformer层内捕获标签共现，从而提高了性能。EmoTx的上下文表示被发送到共享的线性层进行分类，并通过Sigmoid激活获得概率估计。该方法使用BinaryCrossEntropy损失进行端到端训练。

#### 8. 实验设置：
本文使用MovieGraphs数据集，其中包含51部电影和7637个电影场景，具有详细的图形注释。作者关注角色及其情感和心理状态的列表，这自然地提供了多标签设置。作者尝试了三种类型的标签集：Top-10、Top-25和Emotic。作者使用平均精度（mAP）作为评估指标，并采用了MovieGraphs的原始拆分。作者还提供了有关其特征表示和帧采样策略的详细信息。

#### 9. 实验结果和分析：
作者在最常出现的10个和25个标签以及将181个标签映射到26个标签的映射上进行了实验。他们预测场景中所有角色的情感和心理状态，并通过累积标签在场景级别上进行预测。作者进行了消融研究，并将其方法与适应于此任务的先前工作进行了比较，表明其方法的性能显着提高。作者还分析了EmoTx的自我关注分数，并揭示了表达情感通常会关注角色令牌，而其他心理状态则依赖于视频和对话线索。作者还在图1中提供了电影场景中描绘的真实情感和心理状态。


# Paper:128     通过自身正则化隐式神经表示



#### 1. Title: 
Regularize implicit neural representation by itself

#### 2. Authors: 
Zhemin Li, Hongxia Wang, Deyu Meng

#### 3. Affiliation: 
国防科技大学

#### 4. Keywords: 
Implicit Neural Representation, Regularization, Dirichlet Energy, Laplacian Matrix

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Regularize_Implicit_Neural_Representation_by_Itself_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了隐式神经表示（INR）的正则化问题，INR是一种全连接网络，可以表示信号的细节，不受网格分辨率的限制。然而，其泛化能力有待提高，特别是对于非均匀采样的数据。
 
- (2):过去的方法包括L1-norm、L2-norm和Dropout技术等，但这些方法不能很好地适用于INR。本文提出了一种基于学习的Dirichlet能量（DE）的正则化器，称为隐式神经表示正则化器（INRR），它可以完美地将信号的自相似性与Laplacian矩阵的平滑性相结合，从而提高INR的泛化能力。 

- (3):本文提出的INRR是基于学习的Dirichlet能量（DE）的正则化器，它可以度量矩阵的行/列之间的相似性或相关性。DE的平滑性通过使用一个微小的INR来参数化，进一步集成到INRR中。INRR可以提高INR在图像表示中的泛化能力。本文还揭示了一系列从INRR中派生的属性，包括收敛轨迹和多尺度相似性等。 

- (4):本文在256×256的灰度图像不均匀采样修复任务上进行了实验，结果表明，INRR优于各种经典正则化器，包括总变差（TV）、L2能量等。INRR可以与其他信号表示方法（如深度矩阵分解（DMF））相结合，提高其性能。
#### 7. 方法详细介绍：
本文提出了一种名为隐式神经表示正则化器（Implicit Neural Representation Regularizer，INRR）的正则化方法，用于提高隐式神经表示（Implicit Neural Representation，INR）的泛化能力。INRR基于学习到的狄利克雷能量（Dirichlet Energy，DE），用于衡量矩阵的行/列之间的相似性。通过将DE的平滑性与微小的INR参数化相结合，将Laplacian矩阵的平滑性进一步集成到DE中。INRR通过完美地将信号的自相似性与Laplacian矩阵的平滑性相结合，提高了INR在信号表示中的泛化能力。本文还介绍了多尺度自相似性的概念，并将其与神经网络的隐式偏差相连接。INRR方法与其他正则化方法（包括TV、L2和AIR）进行了比较，并在图像修复任务中表现出更好的性能。

#### 8. 实验设置：
本文的实验在五个灰度基准图像（Baboon、Man、Barbara、Boats和Cameraman）上进行，图像大小为m×n=256×256。研究了三种不同的缺失模式：随机缺失、块缺失和纹理缺失。使用恢复图像的峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）进行性能评估。

#### 9. 实验结果与分析：
本文报告了使用不同缺失模式（包括随机缺失、块缺失和纹理缺失）的INR模型恢复Baboon、Barbara、Man和Boats四幅图像的PSNR（分贝）。结果表明，INRR在定量和定性性能方面均优于其他方法。INRR学习到的Laplacian矩阵的平滑性也得到了证明。此外，将INRR的训练动态与其他方法进行比较，发现INRR的行为类似于动量，并保持观察到和未观察到的MSE的衰减趋势。


# Paper:129     音乐驱动的群体编舞



#### 1. Title: 
Music-Driven Group Choreography

#### 2. Authors: 
Nhat Le, Thang Pham, Tuong Do, Erman Tjiputra, Quang D. Tran, Anh Nguyen

#### 3. Affiliation: 
Nhat Le, Thang Pham, Tuong Do, Erman Tjiputra, Quang D. Tran: AIOZ, 新加坡; Anh Nguyen: 英国利物浦大学

#### 4. Keywords: 
Music-driven choreography, group dance generation, dataset, 3D motion, evaluation metrics

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Le_Music-Driven_Group_Choreography_CVPR_2021_paper.html  Github: https://aioz-ai.github.io/AIOZ-GDANCE/

#### 6. Summary : 
- (1):本文研究音乐驱动的群体编舞，提出了一个新的大规模数据集AIOZ-GDANCE，用于支持群体编舞的研究。 
- (2):现有的单人舞蹈生成技术无法很好地支持群体编舞，因为群体编舞需要考虑多个舞者之间的协调和互动。本文提出了一种新的方法，可以从输入音乐和舞者的3D位置中高效地生成多个群体协调的编舞。 
- (3):本文提出了AIOZ-GDANCE数据集，其中包含16.7小时的群体舞蹈视频、音频和3D运动数据，涵盖7种舞蹈风格和16种音乐类型。为了获得数据集的3D地面真实性，本文提出了一种半自动标注方法。基于该数据集，本文提出了一种新的方法GDanceR，可以从输入音乐和舞者的3D位置中高效地生成多个群体协调的编舞。本文还提出了新的评估指标来衡量群体编舞的质量，并进行了大量实验来证明该方法的有效性。 
- (4):本文提出的方法在多个数据集上进行了实验，结果表明该方法可以生成多个群体协调的编舞，且质量较高。该方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为GDanceR的方法，用于从输入音频和舞者的一组3D位置中高效生成群体舞蹈动作。该方法利用了提出的AIOZ-GDANCE数据集，并生成多个群体协调的编舞。该方法包括以下步骤：
1. 预处理输入音频，提取音频特征。
2. 为每个舞者生成舞蹈动作。
3. 优化群体协调性，生成全局一致的群体动作。
4. 提出了新的评估指标，用于衡量群体舞蹈质量。
5. 进行了大量实验，证明了所提出方法的有效性。

#### 8. 实验设置：
本文引入了一个新的数据集AIOZ-GDANCE，其中包含16.7小时的群体舞蹈的全身运动和音乐音频。数据集中每个视频的持续时间在15到60秒之间，涵盖了7种舞蹈风格和16种音乐流派。数据集包含群体舞蹈视频，这与现有数据集不同，现有数据集仅支持单人舞蹈。本文提出了一种半自主标注方法，通过人类参与来获取数据集的3D地面真实值。

#### 9. 实验结果和分析：
本文将所提出方法与基线方法FACT进行比较，并分析了使用Cross-entity Attention机制的效果。实验结果表明，所提出的方法在所有指标（包括Frechet Inception Distance（FID）、Motion-Music Consistency（MMC）、Generation Diversity（GenDiv）、Group Motion Realism（GMR）、Group Motion Correlation（GMC）和Trajectory Intersection Frequency（TIF））上均优于基线方法。本文还展示了所提出方法在生成不同数量的舞者时的生成结果。结果表明，FID、GMR和GMC指标与生成的舞者数量没有太大的相关性，而MMC在所有设置中都表现出其稳定性。


# Paper:130     通过三模态一致性实现语言引导的音频-视觉源分离



#### 1. Title: 
Language-Guided Audio-Visual Source Separation via Trimodal Consistency

#### 2. Authors: 
Reuben Tan, Arijit Ray, Andrea Burns, Bryan A. Plummer, Justin Salamon, Oriol Nieto, Bryan Russell, Kate Saenko

#### 3. Affiliation: 
第一作者：波士顿大学

#### 4. Keywords: 
Audio-visual source separation, self-supervised learning, natural language processing, multimodal alignment, video understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tan_Language-Guided_Audio-Visual_Source_Separation_via_Trimodal_Consistency_CVPR_2021_paper.html  Github: https://cs-people.bu.edu/rxtan/projects/VAST/

#### 6. Summary : 
- (1):本文研究的背景是音频和视频的源分离问题，旨在通过自监督学习方法，利用自然语言查询实现音频源的分离和定位。

- (2):过去的方法通常需要强监督，如使用物体检测器和标注框来定位感兴趣的对象，或使用离散的对象类别标签。本文提出的方法是自监督的，不需要使用物体检测器或对象标签。本文的方法通过利用大规模的视觉-语义对齐模型，提出了三模态一致性和音频-语言一致性两个新的多模态对齐目标，从而实现了音频、视觉和自然语言之间的强对齐。本文的方法在三个音频-视觉分离数据集上进行了广泛的评估，表明本文的方法在不使用标签的情况下，优于现有的强监督方法。

- (3):本文提出了一种名为VAST的自监督方法，该方法利用大规模的视觉-语义对齐模型，通过视频作为中介模态，学习音频、视觉和自然语言之间的强对齐。本文的方法通过引入两个新的多模态对齐目标，鼓励学习到的音频表示编码标题的语义，并推断出三种模态之间的潜在传递关系。本文还采用多实例学习的形式，学习在视频区域级别上执行音频分离的能力。本文的方法在不使用标签的情况下，实现了音频源的分离和定位。

- (4):本文的方法在三个音频-视觉分离数据集上进行了广泛的评估，包括MUSIC、SOLOS和AudioSet。实验结果表明，本文的方法在不使用标签的情况下，优于现有的强监督方法。本文的方法可以通过文本、视频和音频输入，或仅通过文本和音频输入来分离声音。本文的方法在音频源分离任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为VAST的三模态自监督学习方法，用于音频-视觉源分离。该方法利用大型视觉-语言“基础”模型提供伪监督信号，学习三种模态之间的对齐关系：音频、视频和自然语言。该方法使用两个新的多模态对齐目标，鼓励学习到的音频表示编码语字幕的语义，并推断三种模态之间的潜在传递关系。在训练期间，由于没有有关视频中相关对象或其位置的先验信息，因此采用多实例学习公式来学习在视频区域级别上执行音频分离。

具体而言，该方法使用文本编码器提取语言查询的表示，然后将其与音频瓶颈表示连接并平铺以匹配音频频谱的维度。连接的表示通过解码器传递，生成实值比例掩码，然后与输入频谱相乘以获得分离的音频源。对于基于视频的分离，使用CLIP视觉编码器提取时空区域表示，用于预测每个区域的频谱掩码。然后聚合预测的掩码以获得整个视频的频谱掩码。该模型使用自监督的“混合和分离”学习目标和三模态一致性损失进行训练。

#### 8. 实验设置：
本文在SOLOS、MUSIC和AudioSet数据集上评估了所提出的方法的性能，用于音频-文本和音频-视觉源分离。使用的评估指标是归一化信号失真比（NSDR）、信号干扰比（SIR）和信号伪影比（SAR）。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的VAST方法在SOLOS和MUSIC数据集上优于其他基线方法，包括Sound of Pixels、AV-Mix-and-Separate、NMF-MFCC、Co-Separation和AVSGS，并且在更嘈杂的AudioSet数据集上也具有很好的泛化性能。结果表明，所提出的方法可以在不依赖于对象检测的情况下执行音频-视觉源分离，并且与基于对象检测的方法相比具有竞争性能。此外，本文还展示了定性结果，包括预测的音频-视频注意力可视化和基于语言的音频分离预测输出。


# Paper:131     DPF: 利用弱监督学习密集预测场



#### 1. Title: 
DPF: Learning Dense Prediction Fields with Weak Supervision

#### 2. Authors: 
Xiaoxue Chen, Yuhang Zheng, Yupeng Zheng, Qiang Zhou, Hao Zhao, Guyue Zhou, Ya-Qin Zhang

#### 3. Affiliation: 
第一作者：清华大学智能技术与系统国家重点实验室

#### 4. Keywords: 
dense prediction, weak supervision, point-level supervision, semantic scene parsing, intrinsic image decomposition

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_DPF_Learning_Dense_Prediction_Fields_With_Weak_Supervision_CVPR_2021_paper.html  Github: https://github.com/cxx226/DPF

#### 6. Summary : 
- (1):本文研究的背景是在视觉场景理解中，像场景解析和内在图像分解等问题中，像素级的密集注释非常昂贵或不可能获得，因此需要利用廉价的点级弱监督。
 
- (2):现有的点级监督方法仍然使用为全监督设计的相同架构。与之相反，本文提出了一种新的范式，通过点坐标查询进行预测，受到距离或辐射场等隐式表示的成功启发。因此，该方法被命名为密集预测场（DPF）。DPF为连续的亚像素位置生成表达丰富的中间特征，从而允许任意分辨率的输出。DPF与点级监督自然兼容。本文展示了DPF的有效性，使用两个完全不同的任务：高级语义解析和低级内在图像分解。在这两种情况下，监督以单点语义类别和两点相对反射率的形式出现。在PASCALContext、ADE20K和IIW三个大规模公共数据集上进行基准测试，DPF在所有数据集上都取得了显著的优势，创造了新的最先进性能。 

- (3):本文提出了一种新的方法，即DPF，用于从点级弱监督中学习密集预测模型。DPF采用2D坐标作为输入，允许任意分辨率的输出。DPF方法自然兼容点级监督，并在两个任务上取得了最先进的结果：语义场景解析和内在图像分解。 

- (4):DPF在PASCALContext、ADE20K和IIW三个大规模公共数据集上进行基准测试，取得了显著的优势，创造了新的最先进性能。在点级监督下，DPF方法在两个任务上均取得了最先进的结果：语义场景解析和内在图像分解。
#### 7. 方法详细介绍：
本文提出了一种名为密集预测场（DPF）的新型神经网络，用于密集预测。DPF通过预测成像平面上每个连续的2D点的相应值来实现密集预测。DPF采用隐式神经函数实现，该函数接受输入图像特征和引导特征，以学习坐标x和其相邻像素i之间的插值权重和密集预测值。模型的整体架构包括三个组件：密集预测骨干网络、引导编码器和隐式密集预测场。该模型可以应用于任何即插即用的密集预测模型之上。

#### 8. 实验设置：
本文在两种不同类型的点监督数据集上进行了基准测试：（1）具有稀疏标记语义类别信息的数据集，如PASCALContext和ADE20K，以及（2）带有稀疏成对比较标签的数据集，如IIW。IIW数据集包含5,230个室内场景图像和872,151个相对反射率比较对。IIW数据集的评估指标是加权人类不一致率（WHDR）。

#### 9. 实验结果与分析：
本文在IIW、PASCALContext和ADE20K三个数据集上进行了实验，结果表明，DPF方法在所有三个数据集上均取得了最先进的性能，证明了该方法的通用性。DPFs与引导的相比，IIW数据集的分类可以使用加权人类不一致率（WHDR）评估指标计算。在PASCALContext数据集上，语义分割的mIoU得分提高了1.2％，ADE20K性能提高了1.3％，IIW的WHDR降低了0.6％。引导图像的分辨率也被发现很重要，分辨率更高的引导图像会导致更好的性能。本文还提供了潜在代码的可视化，显示潜在代码z编码具有语义信息的高级特征，而潜在代码g则专注于具有清晰边界的低级特征。


# Paper:132     视频对象分割中的“对象”问题



#### 1. Title: 
Breaking the “Object” in Video Object Segmentation

#### 2. Authors: 
Pavel Tokmakov, Jie Li, Adrien Gaidon

#### 3. Affiliation: 
Toyota Research Institute (丰田研究所)

#### 4. Keywords: 
Video object segmentation, spatio-temporal modeling, object transformations, instance masks, dataset

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Tokmakov_Breaking_the_Object_in_Video_Object_Segmentation_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究视频对象分割中的对象转换问题，这是现有数据集中缺失的重要现象。
- (2):现有的视频对象分割方法主要依赖于静态外观线索，无法很好地处理对象转换。本文提出了一种新的数据集VOST，用于视频对象分割下的对象转换，包含超过700个高分辨率视频，涵盖了51种变换和155个对象类别。作者发现现有方法在处理对象转换时表现不佳，主要原因是过度依赖静态外观线索。作者提出了一些改进方法，通过更好地建模时空信息来提高现有方法的性能。
- (3):本文提出了一种新的数据集VOST，用于视频对象分割下的对象转换，作者采用了一种多步骤的方法来确保这些视频集中于复杂的对象转换，捕捉它们的完整时间范围。作者对现有的视频对象分割方法进行了广泛的评估，并提出了一些改进方法，通过更好地建模时空信息来提高现有方法的性能。
- (4):作者在VOST数据集上评估了现有的视频对象分割方法，发现现有方法在处理对象转换时表现不佳，主要原因是过度依赖静态外观线索。作者提出的改进方法可以提高现有方法的性能，但仍有很大的提升空间。本文的贡献在于提出了一个新的数据集，强调了建模对象转换的重要性，并提出了一些改进方法来提高现有方法的性能。
#### 7. 方法详细介绍：
本文提出了一种基于变换的视频对象分割模型AOT+。该模型通过建模对象在视频中的变换来实现分割。首先，模型使用一个变换网络来预测每个帧中对象的变换。然后，模型使用一个分割网络来对每个帧中的对象进行分割。最后，模型使用一个融合网络来将分割结果与变换结果进行融合，以获得最终的分割结果。该模型在VOST数据集上进行了评估，并取得了77.9的Jaccard指数，达到了当前最先进的水平。

#### 8. 实验设置：
本文收集了一个新的视频对象分割数据集VOST，其中包含超过700个高分辨率视频，涵盖了155个对象类别的51种变换。数据集被分为572个训练视频、70个验证视频和71个测试视频。数据集的标注过程由20名专业标注员完成，使用Amazon SageMaker GroundTruth工具进行多边形标注。本文还对标准评估指标进行了两个修改：不测量轮廓准确度，区域相似度在所有视频帧上进行平均。

#### 9. 实验结果和分析：
本文评估了9种视频分割算法及其变体在VOST数据集的验证集和测试集上的性能。其中，基于外观匹配的基线算法表现极差，而基于记忆的高级方法更具有鲁棒性，但仍然难以处理对象形状和外观的显著变化。本文还分析了代表性基线算法的额外失败模式，如多实例分割、完全遮挡、快速运动和小对象。本文提出的AOT+模型在VOST数据集上取得了最先进的性能，同时作者还通过定性分析特定序列的成功和失败模式，探讨了预训练和数据集大小对性能的影响。作者得出结论，对象分割通过变换呈现出新的挑战，需要新的建模方法。


# Paper:133     基于概率的全局跨模态上采样用于Pansharpening



#### 1. Title: 
Probability-based Global Cross-modal Upsampling for Pansharpening

#### 2. Authors: 
Zeyu Zhu, Xiangyong Cao, Man Zhou, Junhao Huang, Deyu Meng

#### 3. Affiliation: 
西安交通大学

#### 4. Keywords: 
Pansharpening, deep learning, probability-based, cross-modal, upsampling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Probability-Based_Global_Cross-Modal_Upsampling_for_Pansharpening_CVPR_2021_paper.html  Github: https://github.com/Zeyu-Zhu/PGCU

#### 6. Summary : 
- (1):本文研究的是遥感图像处理中的一项重要预处理步骤——Pansharpening。
- (2):过去的方法主要包括组分替换、多分辨率分析和变分优化等，这些方法在利用低分辨率多光谱图像的局部信息进行上采样时，忽略了全局信息和引导全色图像的跨模态信息，限制了它们的性能提升。本文提出了一种新的概率全局跨模态上采样方法，可以充分利用全局信息和全色图像的跨模态信息，同时考虑通道特异性。该方法是第一个专门为Pansharpening设计的上采样模块。 
- (3):本文提出了一种基于概率的上采样模型，假设每个像素遵循给定LRMS和PAN图像的概率分布。为了实现该模型，设计了一个新的上采样网络模块，包含信息提取模块、分布和期望估计模块和微调模块。该模块可以充分利用LRMS和PAN的全局信息和跨模态信息，同时考虑通道特异性。 
- (4):在多个数据集上的实验表明，与其他流行的上采样方法相比，本文提出的PGCU方法具有更好的性能。此外，实验还表明，PGCU模块可以帮助改善现有SOTA深度学习Pansharpening方法的性能。
#### 7. 方法详细介绍：
本文提出了一种基于概率的全局跨模态上采样（PGCU）方法，用于Pansharpening。该方法通过将全局和PAN信息引入上采样过程，同时充分建模通道特异性，从而实现了对LRMS图像的上采样。该方法使用设计的网络实现，可以以即插即用的方式提高当前最先进方法的性能。PGCU模块由信息提取（IE）块、分布和期望估计（DEE）块和精细调整（FA）块组成。IE块同时从LRMS图像和PAN图像中提取信息，并输出像素hc，i，j在上采样图像H中的离散分布D（hc，i，j | vc，pc，i，j）的变量值vc和交叉模态特征，用于后续特征向量构建。DEE块利用这些信息构建每个像素的交叉模态特征向量，并生成分布值，然后用于估计上采样图像中每个像素的分布概率。最后，FA进一步补偿使用上采样图像的局部信息和通道相关性。

#### 8. 实验设置：
本文使用三个不同的卫星数据集，分别是WordView2、WordView3和GaoFen2，这些数据集被分为训练集和测试集。在所有数据集中，LRMS图像是通过使用双三次插值将HRMS图像缩小四倍而生成的。本文使用五个流行的指标来评估每种方法的性能，包括光谱角映射器（SAM）、合成中的相对无量纲全局误差（ERGAS）、结构相似性（SSIM）、空间相关系数（SCC）和峰值信噪比（PSNR）。PGCU的超参数s，N，M和L分别设置为2、3、2和128。所有实验都在一台配备Intel Core i7-8700K CPU和一块GeForce RTX 3090 Ti（24GB内存）的PC上进行。

#### 9. 实验结果和分析：
本文进行了多个实验来验证所提出的PGCU方法的有效性。首先，本文以即插即用的方式测试了PGCU方法，直接将原始上采样方法替换为PGCU方法，用于五种最先进的基于DL的Pansharpening方法。实验结果表明，在替换其上采样方法为PGCU方法后，所有五个骨干网络在所有数据集上都有显着的性能提升。本文还将PGCU方法与五种流行的上采样方法进行了比较，包括传统的双三次插值、最近邻插值以及最新的基于DL的转置卷积、基于注意力的图像上采样和ESPCNN。实验结果表明，使用所提出的PGCU方法的骨干网络可以获得最佳性能。最后，本文提供了学习到的上采样图像中像素分布的可视化分析。


# Paper:134     文本到图像扩散的多概念定制



#### 1. Title: 
Multi-Concept Customization of Text-to-Image Diffusion

#### 2. Authors: 
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu

#### 3. Affiliation: 
第一作者：Nupur Kumari，卡内基梅隆大学

#### 4. Keywords: 
Text-to-image generation, fine-tuning, multi-concept composition, generative models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了文本到图像生成中的多概念定制问题，即如何在给定少量样本的情况下，快速地将新概念嵌入到预训练的文本到图像生成模型中。这是一个具有挑战性的问题，因为模型容易忘记或改变现有概念的含义，同时还容易过拟合少量的训练样本。

- (2):过去的方法主要是基于迁移学习，将预训练模型调整到新领域。然而，这些方法往往需要大量的训练数据，且容易出现灾难性遗忘。本文提出了一种新的方法，只更新模型中的一小部分参数，即跨注意力层中的键和值映射，以及新概念的修饰符标记。此外，本文还引入了数据增强和正则化技术，以提高模型的泛化能力和抗过拟合能力。本文的方法在多个数据集上进行了实验，结果表明，与现有方法相比，本文的方法具有更好的文本对齐和视觉相似性，并且能够有效地组合多个新概念。

- (3):本文提出了一种名为Custom Diffusion的方法，用于文本到图像扩散模型的定制。该方法只更新模型中的一小部分参数，即跨注意力层中的键和值映射，以及新概念的修饰符标记。为了防止模型遗忘，我们使用一小组与目标图像具有相似标题的真实图像作为正则化集。此外，我们还引入了数据增强技术，以加速收敛和提高结果。为了注入多个概念，我们的方法支持同时训练多个概念或将它们分别训练然后合并。我们在多个数据集上进行了实验，结果表明，我们的方法在单个概念的生成和多个概念的组合方面均优于现有方法。

- (4):本文的方法在多个数据集上进行了实验，结果表明，与现有方法相比，本文的方法具有更好的文本对齐和视觉相似性，并且能够有效地组合多个新概念。本文的方法只需要存储模型权重的一小部分（仅为模型权重的3%），并且可以减少微调时间（在2个A100 GPU上为6分钟，比现有方法快2-4倍）。
#### 7. 方法详细介绍：
本文提出了一种Fine-tuning文本到图像扩散模型的方法，用于多个概念的定制。该方法仅更新扩散模型中交叉注意力块的关键和值投影矩阵，从而显著降低计算成本和过拟合。该方法还引入了一种新的修饰符令牌嵌入用于个性化概念和一个正则化数据集以防止语言漂移。对于多概念组合微调，该方法使用约束优化来合并微调模型。具体步骤包括：使用预训练模型作为基础模型，使用约束优化来合并微调模型，使用正则化数据集防止过拟合，使用DDPM采样等。

#### 8. 实验设置：
本文在十个目标数据集上进行了实验，这些数据集涵盖了各种类别和不同数量的训练样本。使用稳定扩散模型作为基础模型进行微调。在单概念和双概念联合训练中，模型训练了250步和500步，批量大小为8，学习率为8×10−5。在训练过程中，目标图像被随机调整大小，仅在有效区域上反向传播损失。

#### 9. 实验结果与分析：
本文提出的方法显著提高了模型在生成目标概念方面的性能。该方法在多个指标上实现了最先进的性能，包括Fréchet Inception Distance（FID）、Learned Perceptual Image Patch Similarity（LPIPS）和Inception Score（IS）。该方法还允许在单个场景中连贯地生成两个新概念。该方法在图像对齐、文本对齐和KID指标上的表现优于其他方法。该方法还具有计算和时间效率。在人类偏好研究中，该方法在图像对齐和文本对齐方面均优于基线。


# Paper:135     RGBD2：使用RGBD扩散模型通过增量视图修复进行生成场景合成



#### 1. Title: 
RGBD2: Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models

#### 2. Authors: 
Jiabao Lei, Jiapeng Tang, Kui Jia

#### 3. Affiliation: 
1. 华南理工大学
3. 鹏城实验室

#### 4. Keywords: 
Scene synthesis, RGBD views, generative models, diffusion models, inpainting

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lei_RGBD2_Generative_Scene_Synthesis_via_Incremental_View_Inpainting_Using_RGBD_CVPR_2021_paper.html  Github: https://github.com/leijb/rgbd-diffusion

#### 6. Summary : 
- (1):本文研究了从稀疏的RGBD视图中恢复场景几何和颜色的挑战。 
- (2):传统的场景合成方法通常涉及通过拟合给定的观察来重建场景，如多视图图像或点云。本文提出了一种新的解决方案，称为RGBD2，它沿着相机轨迹顺序生成新的RGBD视图，场景几何形状是这些视图的融合结果。本文的方法使用中间表面网格用于渲染新的RGBD视图，随后通过修补网络变得完整；每个渲染的RGBD视图后来被反投影为部分表面，并被补充到中间网格中。使用中间网格和相机投影有助于解决多视图不一致的难题。本文实现了RGBD修复网络作为通用RGBD扩散模型，该模型先前用于2D生成建模；我们对其反向扩散过程进行了修改，以实现我们的使用。 
- (3):本文提出了一种从稀疏RGBD视图中生成场景的新任务，其中涉及跨多个场景的学习，以后能够从稀疏的多视图RGBD图像中合成场景。本文的方法涉及生成多视图一致的RGBD视图沿着预定的相机轨迹，使用中间网格渲染新的RGBD图像，随后使用扩散模型对其进行修补，并将每个RGBD视图转换为3D部分网格通过反投影，并最终将其与中间场景网格合并以产生最终输出。在ScanNet数据集上的广泛实验表明，我们的方法在从稀疏RGBD输入合成3D场景的任务上优于现有解决方案。 
- (4):本文提出了一种新的方法，用于从稀疏的RGBD视图中生成场景。该方法使用中间表面网格和相机投影来解决多视图不一致的问题，并使用RGBD扩散模型进行修补。在ScanNet数据集上的实验表明，该方法在从稀疏RGBD输入合成3D场景的任务上优于现有解决方案。
#### 7. 方法详细介绍：
本文提出了一种名为RGBD2的生成式场景合成方法，通过使用RGBD扩散模型进行增量视图修补来生成场景。该方法使用扩散模型来修补RGBD图像的缺失区域，使用由渲染可见性得到的二进制掩码。该模型实现为一个UNet，以观察到的区域为条件，并将当前样本和初始噪声样本的串联输入到网络中。该方法还引入了一个无分类器的指导机制，以增强生成过程的可控性。具体步骤包括：
1. 将RGBD图像转换为3D部分网格。
2. 将部分网格与中间场景网格合并，以产生最终输出。
3. 从预定的相机轨迹生成多视角一致的RGBD视图。
4. 使用扩散模型进行RGBD修补，以填充不完整的视图。
5. 将修补后的输出反投影到3D空间中，形成部分网格，补充整个中间场景网格。
6. 迭代此过程，直到覆盖所有测试时间相机视角，中间场景网格逐渐变得完整。
7. 最终输出是从最后一步获得的网格结果。

#### 8. 实验设置：
本文在ScanNet-V2数据集上进行实验，使用前1,293个场景进行训练，随机选择18个场景作为测试集。评估还在不同的稀疏度设置下（5％，10％，20％和50％）进行，通过均匀下采样视图。

#### 9. 实验结果和分析：
本文的方法在稀疏输入视图和向外看的相机方面表现出了明显的优势。在几何相关指标方面，该方法始终表现最佳。在视觉度量方面，当提供的视图非常稀疏（5％）时，该方法表现出了明显的优势。与现有解决方案相比，该方法在Chamfer距离和F-score方面表现最佳，并在IoU方面表现出竞争性的性能。该方法还表现出对输入视图稀疏程度的变化具有鲁棒性。


# Paper:136     基于刚性感知的6D物体姿态估计检测



#### 1. Title: 
Rigidity-Aware Detection for 6D Object Pose Estimation

#### 2. Authors: 
Yang Hai, Rui Song, Jiaojiao Li, Mathieu Salzmann, Yinlin Hu

#### 3. Affiliation: 
Yang Hai, Rui Song, Jiaojiao Li: 西安电子科技大学信息安全国家重点实验室 (State Key Laboratory of ISN, Xidian University)

#### 4. Keywords: 
6D object pose estimation, object detection, rigidity-aware detection, visibility map, pose regression network

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Hai_Rigidity-Aware_Detection_for_6D_Object_Pose_Estimation_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的是6D物体姿态估计问题，即相机与物体之间的3D旋转和3D平移的估计。该问题在机器人、质量控制和增强现实等领域有广泛应用。
- (2):现有的6D物体姿态估计方法通常采用两阶段流程：首先检测物体，然后从检测到的图像块的缩放版本中估计其6D姿态。然而，这种方法在复杂场景中的性能显著下降，主要是由于检测失败导致的。本文提出了一种基于刚性的检测方法，利用了6D姿态估计中目标物体是刚性的这一特点。该方法通过在训练期间从整个可见物体区域中采样正物体区域，而不是从可能被遮挡的边界框中心绘制样本，从而使每个可见物体部分都可以对最终边界框预测做出贡献，从而提高检测的鲁棒性。
- (3):本文提出了一种基于刚性的检测方法，利用了6D姿态估计中目标物体是刚性的这一特点。该方法通过构建一个可见性图来计算每个像素与边界框边界之间的最小障碍距离，从而计算出物体前景概率，然后使用这些概率来指导训练期间的正样本采样。此外，为了在推理期间利用大多数可见部分的本地预测的可靠性，我们收集所有置信度阈值以上的候选本地预测，并通过简单的加权平均组合它们，从而获得更强的检测鲁棒性。
- (4):本文在七个具有挑战性的6D物体姿态估计数据集上进行了实验，结果表明，我们的方法在检测基线上显著优于通用检测框架。此外，与6D姿态回归网络相结合，我们的方法在具有挑战性的BOP基准测试中获得了最先进的物体姿态结果。
#### 7. 方法详细介绍：
本文提出了一种针对6D物体姿态估计的刚性感知检测方法。该方法使用可见性引导的采样策略来获取训练中的正样本，该策略基于目标边界框中每个像素的可见性的近似度量。在推理过程中，该方法将邻域内的所有候选框组合起来，以获得更准确的结果，而无需抑制任何候选框。该方法使用与大多数最先进的单阶段框架相同的FPN架构，并将置信度值定义为预测框与真实框之间的IOU。模型使用组合损失函数进行训练，包括分类的focal loss、框回归损失和置信度预测的二元交叉熵。在实现中，使用GIOU损失进行框回归。

#### 8. 实验设置：
本文在六个6D物体姿态估计数据集上进行了评估，包括BOP、HB、IC-BIN、ITODD、LM-O和T-LESS。实验在单个NVIDIA Titan Xp GPU上使用PyTorch进行。

#### 9. 实验结果和分析：
本文提出的方法在BOP数据集上比基线方法具有更好的准确性，证明了该方法在检测杂乱的6D姿态估计场景中的刚性物体方面的有效性。该方法在所有六个数据集上均优于FCOSv2和Mask R-CNN，平均提高了5.8%和11.6%。该方法还在YCB数据集上实现了最先进的性能。推理过程中的融合策略显示出更强的鲁棒性，可以产生具有高置信度的更多候选预测，这些预测可以组合以获得更好的结果。


# Paper:137     基于熵约束神经表示的视频压缩



#### 1. Title: 
Video Compression with Entropy-Constrained Neural Representations

#### 2. Authors: 
Carlos Gomes, Roberto Azevedo, Christopher Schroers

#### 3. Affiliation: 
Carlos Gomes: ETH Zurich, Switzerland (苏黎世联邦理工学院)
Roberto Azevedo, Christopher Schroers: DisneyResearch|Studios, Zurich, Switzerland (迪士尼研究院)

#### 4. Keywords: 
Video compression, neural video representation, implicit neural representations, rate-distortion optimization, entropy coding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gomes_Video_Compression_With_Entropy-Constrained_Neural_Representations_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究视频压缩问题，传统的视频编码技术已经相对成熟，但是近年来，基于神经网络的视频压缩方法也逐渐受到关注。然而，目前的神经视频表示方法在视频压缩任务上的表现仍然不如传统方法，这可能是因为当前的神经视频表示方法没有有效地获取时间和空间信息的紧凑表示，以及在压缩模型时使用启发式技术，如后训练量化或权重修剪，这些技术可能会导致过拟合等问题。
- (2):过去的神经视频压缩方法大多基于编码器-解码器神经网络对，将帧组转换为潜在表示并随后恢复它们。这些表示被量化、熵编码，然后存储/传输。然而，这些方法通常将高保真度的表示和压缩作为两个独立的任务，先训练一个网络以最小化失真，然后通过某些过程来减小其大小，如将其权重存储在16位浮点格式中、量化或修剪。此外，当前的架构没有将参数效率作为优先考虑，这可能会对视频压缩任务造成限制。本文提出了一种新的紧凑卷积架构，用于视频表示，提供比以前的方法更好的R-D性能，并且提出了一种理论上有基础的R-D公式，用于视频压缩，可以同时最小化速率和失真。
- (3):本文提出了一种新的紧凑卷积架构，用于视频表示，提供比以前的方法更好的R-D性能，并且提出了一种理论上有基础的R-D公式，用于视频压缩，可以同时最小化速率和失真。本文的方法可以通过学习所有网络和量化参数来进行端到端的联合优化，不需要使用以前的后训练操作。此外，本文的方法还可以改进其他方法，例如NeRV。本文在UVG数据集上进行了评估，取得了基于NVR的视频压缩的最新成果，并且在通常采用的HEVC基准（x265，禁用b帧，“中等”预设）上实现了优于该基准的NVR-based视频压缩方法，缩小了与自编码器-based视频压缩技术之间的差距。
- (4):本文提出的方法在UVG数据集上取得了新的最优结果，优于DVC等神经视频压缩方法，并且在HEVC基准上实现了优于该基准的NVR-based视频压缩方法，缩小了与自编码器-based视频压缩技术之间的差距。本文的方法可以同时最小化速率和失真，可以通过学习所有网络和量化参数来进行端到端的联合优化，不需要使用以前的后训练操作。
#### 1. 实验结果
(1). 本文提出了一种新颖的卷积架构，用于视频表示，实现了更高保真度的编码。该方法与端到端的基于熵的神经网络压缩方法相结合，以实现视频压缩，将损失形式化为速率失真问题。在UVG数据集上进行测试，证明了该方法的有效性，产生了在视频压缩方面的最新成果。在表3中报告了所提出方法的解码性能，表明所提出的方法在GPU和CPU上的FPS方面优于Scale-Space方法。

#### 2. 方法详细介绍
本文提出了一种视频压缩方法，使用熵约束神经表示（INRs）。该方法涉及量化神经网络的权重，并使用参数化函数近似量化权重的概率密度函数。损失函数包括失真度量和熵项，使用上下文自适应二进制算术编码器（CABAC）进行最小化。该方法还涉及微调熵最小化过程，给定要压缩的视频的预训练INR。

#### 3. 实验设置
本方法在UVG数据集上进行评估，该数据集包含7个分辨率为1920×1080的视频。将该方法与传统视频编解码器、NeRV、NeRV-EM、DVC和Scale-Space进行比较。感知相似度的度量标准为PSNR和MS-SSIM。使用“BigBuckBunny”序列进行300个时期的训练，使用Adam优化器，学习率为5e-4，余弦学习率计划。模型的空时表示维度设置为9×16×200。

#### 4. 实验结果与分析
所提出的方法在两个评估指标上均优于NeRV，展示了流线型架构和端到端训练过程的优势，并为基于INR的视频压缩建立了新的最新成果。该方法是第一个基于INR的视频压缩方法，其在UVG数据集的SSIM和PSNR方面超过了通常使用的HEVC基准。该方法产生的视频比NeRV在相等或更小的比特率下更清晰。表2和表3报告了该方法的训练和推理时间。

#### 5. 方法详细介绍
本文提出了一种更高效的神经网络架构，用于视频压缩，它是一个完全卷积的设计，以帧数为输入，并构建一个与固定坐标网格M连接的矩阵T。然后对结果张量的每个元素应用位置编码，接着是两个具有3x3内核和160个通道的卷积层。空时特征的张量传递到网络的上采样部分，该部分由一系列上采样块组成，每个块由一个卷积层和一个PixelShuffle模块组成。在每个块的开头引入了一个AdaIN模块，还有一个单独的全连接层，处理时间输入坐标，以产生每个AdaIN模块的输入。失真目标是L1和SSIM的混合，优化基于熵最小化的有损视频压缩，消除了后训练量化和修剪的需要。

#### 6. 实验设置
本文使用UVG数据集进行评估，该数据集包含7个分辨率为1920×1080的视频。将该方法与传统视频编解码器、NeRV、NeRV-EM、DVC和Scale-Space进行比较。感知相似度的度量标准为PSNR和MS-SSIM。使用“BigBuckBunny”序列进行300个时期的训练，使用Adam优化器，学习率为5e-4，余弦学习率计划。模型的空时表示维度设置为9×16×200。

#### 7. 方法详细介绍
本文提出了一种新颖的卷积架构，用于视频表示，该架构更好地表示了空时信息，并具有联合优化速率和失真的训练策略。所有网络和量化参数都是端到端联合学习的，以前的工作中使用的后训练操作是不必要的。该方法使用熵约束神经表示方法，其中建模神经网络权重的熵，允许速率和失真的联合最小化。该方法还采用位置编码，以克服隐式神经表示中的谱偏差问题。

#### 8. 实验设置
本文使用UVG数据集进行评估，该数据集包含7个分辨率为1920×1080的视频。将该方法与传统视频编解码器、NeRV、NeRV-EM、DVC和Scale-Space进行比较。感知相似度的度量标准为PSNR和MS-SSIM。使用“BigBuckBunny”序列进行300个时期的训练，使用Adam优化器，学习率为5e-4，余弦学习率计划。模型的空时表示维度设置为9×16×200。

#### 9. 实验结果与分析
所提出的方法在两个评估指标上均优于NeRV，展示了流线型架构和端到端训练过程的优势，并为基于INR的视频压缩建立了新的最新成果。该方法是第一个基于INR的视频压缩方法，其在UVG数据集的SSIM和PSNR方面超过了通常使用的HEVC基准。该方法产生的视频比NeRV在相等或更小的比特率下更清晰。表2和表3报告了该方法的训练和推理时间。


# Paper:138     TIPI: 具有变换不变性的测试时间自适应



#### 1. Title: 
TIPI: Test Time Adaptation with Transformation Invariance

#### 2. Authors: 
A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, Philip H.S. Torr

#### 3. Affiliation: 
第一作者：牛阿图（A. Tuan Nguyen）, 牛津大学（University of Oxford）

#### 4. Keywords: 
Test Time Adaptation, Distribution Shift, Unsupervised Learning, Invariance Regularizer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2021_paper.html  Github: https://github.com/atuannguyen/TIPI

#### 6. Summary : 
- (1):本文研究了机器学习模型在新环境下的部署问题，即目标数据分布与模型的训练分布不同的分布偏移问题。作者假设在新领域中没有为标签提供，并且不存储源数据（例如出于隐私原因）。在这种情况下，即使数据分布的微小变化也会严重影响模型的性能。测试时间自适应提供了一种解决这个问题的方法，它允许模型在测试时间内使用仅有的未标记测试数据批次来适应新的数据分布。为了实现这一点，主要方法是在测试时间内优化目标数据上的代理损失。本文提出了一种基于不变性正则化器的代理损失，以解决小批量大小的问题。 

- (2):在测试时间自适应框架中，常见且有效的方法是在目标数据上优化代理目标函数。第一组代理目标函数是自监督任务的损失函数。第二组代理目标函数是无监督损失函数。在这组无监督目标函数中，熵最小化（TENT）是最成功的方法之一，但是TENT需要大批量大小才能避免崩溃到平凡的预测（只输出一个类），因为这个代理目标的特性。本文提出了一种基于不变性正则化器的代理损失，以解决小批量大小的问题。 

- (3):本文提出了一种基于不变性正则化器的代理损失，以解决小批量大小的问题。作者提供了关于模型在输入变换下的性能的理论结果。具体来说，作者表明模型在数据分布上的损失由数据变换前后的预测分布的KL距离（作为正则化器）和其在变换后的数据分布上的损失所限制。作者使用可以模拟真实源-目标转移的输入图像的小偏移，并强制网络在这样的数据变换下具有不变性。作者在各种基准测试中验证了该方法（Cifar10-C，Cifar100-C，ImageNet-C，DIGITS和VisDA17）。 

- (4):作者在各种基准测试中验证了该方法（Cifar10-C，Cifar100-C，ImageNet-C，DIGITS和VisDA17）。TIPI方法在小批量大小（例如2）下具有鲁棒性，并在所有设置中始终优于TENT。TIPI方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为TIPI（Test Time Adaptation with Transformation Invariance）的方法，用于解决测试时适应中的分布偏移问题。TIPI使用不变性正则化器作为测试时适应的代理损失，该正则化器受到有关模型在输入变换下性能的理论结果的启发。该方法找到可以模拟域偏移的输入变换，并使用基于推导出的界限的正则化器强制网络在这些变换下具有不变性。TIPI在各种基准测试中进行了验证，并始终优于TENT（一种常用的无监督代理目标函数）。

具体步骤如下：
1. 对于每个测试数据批次，使用l∞-norm的扰动作为图像的变换，以模拟分布偏移。
2. 将变换后的测试数据批次输入到模型中，计算预测结果。
3. 使用反向KL散度作为无监督代理目标函数，对模型进行正则化。
4. 使用FGSM（Fast Gradient Sign Method）优化模型的仿射参数。

#### 8. 实验设置：
本文在多个基准数据集上进行了实验，包括CIFAR10-C、CIFAR100-C和ImageNet-C数据集。对于CIFAR10-C和CIFAR100-C数据集，使用WideResnet28-10模型，对于ImageNet-C数据集，使用Resnet50网络。对于CIFAR10-C和CIFAR100-C数据集，使用Adam优化器，对于ImageNet-C数据集，使用SGD优化器。评估协议涉及在线适应，报告所有批次的平均性能。

#### 9. 实验结果和分析：
实验结果表明，TIPI在所有情况和问题设置中均优于其他测试时适应基线。在CIFAR10-C和CIFAR100-C实验中，TIPI在大批量（200）情况下与TENT表现类似。然而，随着批量大小的减小，TENT崩溃，TIPI显著优于TENT。对于具有挑战性的ImageNet-C数据集，TIPI即使在大批量情况下也比TENT表现更好。T3A在所有设置中表现不如TIPI，并在ImageNet-C上表现极差。


# Paper:139     基于难度采样的去偏差对比表示学习



#### 1. Title: 
Difficulty-based Sampling for Debiased Contrastive Representation Learning

#### 2. Authors: 
Taeuk Jang, Xiaoqian Wang

#### 3. Affiliation: 
Purdue University (普渡大学)

#### 4. Keywords: 
Contrastive learning, debiasing, hard negatives, self-supervised learning, triplet loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jang_Difficulty-Based_Sampling_for_Debiased_Contrastive_Representation_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了对比学习中的偏差问题，提出了一种新的去偏差对比学习方法，通过相对难度引用偏差放大的对应项来探索难负样本。
 
- (2):以前的方法主要是通过超参数调整来处理假负样本和难负样本，而本文提出了一种新的方法，通过相对难度引用偏差放大的对应项来探索难负样本。本文从数据偏差的角度探讨了什么样的样本是难负样本或易负样本。本文提出了一种新的对比学习方法，通过三元组损失来学习偏差放大的表示，以自我监督的方式进行训练。本文的贡献在于提出了一种去偏差的对比学习方法，解决了难负样本和真假负样本的问题。

- (3):本文提出了一种新的对比学习方法，通过相对难度引用偏差放大的对应项来探索难负样本。本文提出了一种新的对比学习方法，通过三元组损失来学习偏差放大的表示，以自我监督的方式进行训练。本文的创新点在于提出了一种去偏差的对比学习方法，解决了难负样本和真假负样本的问题。

- (4):本文在图像和表格数据分类任务中进行了实验验证，结果表明，所提出的方法在下游任务中实现了更高的准确性和更少的偏差，支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种基于难度采样的去偏置对比学习方法。该方法包括两个编码器：有偏编码器（Eb）和去偏编码器（Ed）。有偏编码器通过最小化自监督三元组损失来放大偏差，该损失仅关注易样本。去偏编码器通过利用相对难度来处理难负样本。相对难度是通过比较负样本的有偏和去偏编码器之间的距离来计算的。去偏编码器通过最小化加权对比损失来训练，该损失通过乘以难度函数来加权难负样本。

#### 8. 实验设置：
本文在四个数据集上评估了所提出的方法：CIFAR-10、CIFAR-100、CelebA和Waterbirds。对于CIFAR-10和CIFAR-100，使用ResNet50作为骨干编码器结构，对于CelebA和Waterbirds，使用ResNet18。模型使用批量大小为128进行500个epoch的训练。对于所有方法，温度t设置为0.5，τ+设置为0.1（CIFAR-10）、0.05（CIFAR-100）和0.3（CelebA和Waterbirds）。

#### 9. 实验结果与分析：
所提出的方法在CIFAR-10和CIFAR-100数据集上优于DCL和HCL等现有方法。在CelebA和Waterbirds数据集上，所提出的方法在最差组准确率方面表现最佳，且整体性能可比。在Adult和COMPAS公平性基准数据集上，与需要敏感信息的LAFTR相比，所提出的方法实现了可比或更好的结果。在CIFAR-10上学习到的表示的t-SNE可视化显示，所提出的方法实现了最佳的类内小方差分离。


# Paper:140     CLIPPING: 一种基于学生模型的CLIP模型蒸馏方法用于视频-语言检索



#### 1. Title: 
CLIPPING: Distilling CLIP-Based Models with a Student Base for Video-Language Retrieval

#### 2. Authors: 
Renjing Pei, Jianzhuang Liu, Weimian Li, Bin Shao, Songcen Xu, Peng Dai, Juwei Lu, Youliang Yan

#### 3. Affiliation: 
华为诺亚方舟实验室 (Huawei Noah's Ark Lab)

#### 4. Keywords: 
Knowledge distillation, Vision-language model, Video-language retrieval, CLIP, Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是预训练视觉-语言模型在下游任务中的应用，但是基于Transformer架构的预训练视觉-语言模型通常需要较长的推理时间，难以在移动设备上部署。

- (2):过去的方法通常是使用知识蒸馏技术将大模型的能力转移到小模型上，但是在多模态应用中存在许多问题。本文提出了一种新的知识蒸馏方法，名为CLIPPING，可以在微调阶段将经过强大的预训练CLIP微调的大型教师模型的丰富知识有效地转移到小型学生模型上。特别地，本文提出了一种新的基于层的对齐方法，称为学生作为基础（SAB），用于CLIPPING中从Transformer到CNN的中间层的知识蒸馏，使得学生的层可以作为教师的基础，从而允许学生充分吸收教师的知识。本文的方法具有很好的动机。

- (3):本文提出了一种有效的跨模态知识蒸馏方法，包括来自全局和局部视频-字幕分布的知识。本文使用教师的视频-字幕分布来指导学生的训练。此外，当与我们的SAB KD一起训练时，学生还可以从强大的预训练CLIP中受益，后者通过从大量数据中学习获得了某些局部帧-词注意力能力。实验表明，当仅在微调阶段与我们的SAB KD一起训练时，可以有效地将此预训练知识转移到学生身上。

- (4):本文的方法在视频-语言检索基准测试中取得了很好的性能，CLIPPING在没有任何视觉-语言预训练的情况下，使用MobileViT-v2作为视觉编码器，在三个视频-语言检索基准测试中实现了其教师性能的88.1％-95.3％，其视觉编码器的大小仅为教师的1/19.5。此外，CLIPPING在MSR-VTT数据集上也显著优于最先进的小型基线（ALL-in-one-B），获得了相对7.4％的性能提升，参数减少了29％，FLOPS减少了86.9％。此外，CLIPPING与许多大型预训练模型相比，性能相当甚至更好。
#### 7. 方法详细介绍：
本文提出了一种名为CLIPPING的知识蒸馏方法，用于将Clip4clip的多模态知识转移到基于MobileViT-v2的模型上，以进行视频-语言检索任务。该方法使用了四种知识转移方式：时间知识蒸馏、空间知识蒸馏、自注意力知识蒸馏和跨模态知识蒸馏。其中，时间知识蒸馏损失定义为学生和教师的潜在嵌入之间的KL散度。空间知识蒸馏损失定义为学生和教师的空间嵌入之间的KL散度。自注意力知识蒸馏用于对齐Transformer和CNN的中间层。跨模态知识蒸馏用于对齐学生和教师的多模态嵌入。该方法的总损失是这四种知识蒸馏损失的加权和。

#### 8. 实验设置：
本文在三个视频-文本检索基准数据集MSR-VTT、YouCook2和HowTo100M上评估了所提出的CLIPPING方法。实验中使用MobileViT-v2作为视觉编码器，没有进行任何视觉-语言预训练。本文还将CLIPPING与MSR-VTT数据集上的一个最先进的小型基线（ALL-in-one-B）进行了比较。

#### 9. 实验结果与分析：
所提出的CLIPPING方法在MSR-VTT、MSVD和LSMDC基准数据集上均优于最先进的方法，并且在参数和Flops最少的情况下取得了最佳性能。CLIPPING方法还被压缩成两个较小的模型：CLIPPINGw/o T和CLIPPING∗。在MSR-VTT数据集上，CLIPPING方法相对于最先进的小型基线（ALL-in-one-B）获得了7.4%的性能提升，参数减少了29%，Flops减少了86.9%。本文还提供了详细的分析和比较，验证了所提出方法的有效性和优越性。


# Paper:141     CAT：面向开放世界目标检测的本地化和识别级联检测变压器



#### 1. Title: 
CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection

#### 2. Authors: 
Shuailei Ma, Yuefeng Wang, Ying Wei, Jiaqi Fan, Thomas H. Li, Hongli Liu, Fanbing Lv

#### 3. Affiliation: 
第一作者：东北大学，沈阳，中国

#### 4. Keywords: 
Open-world object detection, pseudo-labelling mechanism, cascade decoupled decoding, detection transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ma_CAT_LoCalization_and_IdentificAtion_Cascade_Detection_Transformer_for_Open-World_Object_CVPR_2021_paper.html  Github: https://github.com/xiaomabufei/CAT

#### 6. Summary : 
- (1):本文研究开放世界目标检测（OWOD）问题，即模型需要检测已知和未知对象，并增量学习识别这些未知对象。 
- (2):现有的OWOD方法采用标准检测框架和固定的伪标签机制（PLM），存在以下问题：（i）检测未知对象的包含显著降低了模型检测已知对象的能力；（ii）PLM未充分利用输入的先验知识；（iii）PLM的固定选择方式不能保证模型在正确方向上进行训练。本文提出了一种新的解决方案，即CAT：LoCalization和IdentificAtion Cascade Detection Transformer，通过共享解码器以级联解码方式解耦检测过程。同时，我们提出了自适应伪标签机制，结合了模型驱动和输入驱动的PLM，并自适应地生成强健的未知对象伪标签，显著提高了CAT检索未知对象的能力。 
- (3):本文提出了一种新的本地化和识别级联检测变压器（CAT），具有出色的检索未知对象的能力，并减轻了检测未知对象对已知对象检测的影响。CAT通过级联解码方式解耦了检测变压器的解码过程。因此，识别过程中的类别信息对本地化过程的影响降低了。在这种情况下，模型可以定位更多的前景对象，从而提高了模型检索未知对象的能力。同时，独立的识别过程使模型能够更加专注地识别，从而减轻了未知对象对检测已知对象的影响。在这种解码方式下，前一个解码过程用于本地化，后一个解码过程用于识别。自适应PLM保持了CAT探索超出已知对象的知识的能力，并根据模型训练过程自适应地调整伪标签生成。 
- (4):在两个基准测试上的实验表明，我们的模型优于现有的最先进方法。对于OWOD，CAT的绝对增益范围从9.7％到12.8％，在未知召回方面优于SOTA方法。
#### 1. 实验结果：
本文提出了一种基于Transformer的开放世界目标检测框架CAT。通过在PASCAL VOC和MS COCO两个流行的基准测试上进行广泛实验，CAT相对于现有工作在开放集目标检测任务上实现了显着的性能提升。性能以不同目标类别和评估设置的mAP为指标进行报告。

#### 2. 方法：
本文提出了一种名为CAT（LoCalization and IdentificAtion Cascade Detection Transformer）的模型，用于开放世界目标检测。该模型使用分层特征提取骨干网络提取多尺度特征，然后将其发送到可变形Transformer编码器中以编码语义特征。编码的语义特征与一组可学习的位置查询一起发送到共享解码器中。共享解码器将位置查询转换为一组位置查询嵌入，然后将其输入到回归分支中以定位包含已知和未知类别的前景边界框。位置嵌入也用作类别查询，并与编码的语义特征一起发送到共享解码器中。共享解码器将类别查询转换为与位置查询嵌入相对应的类别查询嵌入。然后将类别查询嵌入发送到目标性和新颖性分类分支中以预测目标性和类别。该模型还提出了一种级联解码方式，以级联方式解码编码特征，从而可以同时帮助定位和识别过程。

#### 3. 实验设置：
本文的实验在MS-COCO和Pascal VOC数据集的两个主流分割上实现。将类别分组为一组不重叠的任务，并且任务𝑇𝑐中的类别仅出现在𝑡≥𝑐的任务中。使用已知对象的平均精度（mAP）和未知对象的U-Recall对模型进行评估。U-Recall衡量模型检索未知对象实例的能力，用于开放世界目标检测问题。

#### 4. 实验细节：
多尺度特征提取器由在自我监督方式下预训练的ImageNet上的Resnet-50和一个变形Transformer编码器组成，其层数设置为6。对于共享解码器，使用变形Transformer解码器，层数也设置为6。查询数设置为100，嵌入维度为256，伪标签数为5。在推理期间，每个图像使用𝑡𝑜𝑝-50个高分检测进行评估。该模型在OWOD分割上实现了显着的绝对增益，U-Recall高达12.8％，mAP高达4.7％，超过了现有方法。在MS-COCO分割上，该模型在U-Recall和mAP方面均实现了显着的绝对增益，分别高达18.3％和9.7％，超过了现有方法。定性结果表明，与现有模型相比，所提出的方法可以准确检测到更多的未知对象。

#### 5. 方法详细介绍：
本文提出了一种名为CAT（LoCalization and IdentificAtion Cascade Detection Transformer）的新型开放世界目标检测框架。CAT包括三个专用组件：共享Transformer解码器，级联解码方式和自适应伪标签机制。共享Transformer解码器将定位和识别过程解耦，减少了类别信息对定位过程的影响。级联解码方式使用前一个解码过程进行定位，使用后一个解码过程进行识别。自适应伪标签机制在训练过程中结合了模型驱动和输入驱动的伪标签，以生成强健的伪标签并探索超出已知对象的知识。CAT的端到端训练策略也得到了说明。

#### 6. 实验结果和分析：
本文在MS-COCO分割和OWOD分割上与现有方法的性能进行了比较。CAT在U-Recall和mAP方面均优于现有方法，展示了其检索超出封闭集范围的新知识和处理开放集数据中的未知实例的能力。本文还进行了增量目标检测任务评估，在所有三个设置中，CAT在所有现有方法上表现良好，说明了定位识别级联检测变压器在增量目标检测中的优势。最后，本文进行了消融研究，以验证CAT的组件在OWOD分割上的有效性。结果表明，每个组件在开放世界目标检测中都发挥了关键作用。


# Paper:142     SPARF：从稀疏和嘈杂的姿态中学习神经辐射场



#### 1. Title: 
SPARF: Neural Radiance Fields from Sparse and Noisy Poses

#### 2. Authors: 
Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, Federico Tombari

#### 3. Affiliation: 
Prune Truong: ETH Zurich (苏黎世联邦理工学院)
Marie-Julie Rakotosaona, Fabian Manhardt, Federico Tombari: Google

#### 4. Keywords: 
Novel-view synthesis, Neural Radiance Fields, Sparse-view, Multi-view geometry, Camera poses

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是新视角合成问题，提出了一种名为SPARF的方法，可以在只有少量稀疏输入图像和嘈杂相机姿态的情况下，实现高质量的新视角合成。 
- (2):过去的方法要求输入图像密集且相机姿态高度准确，这限制了它们在实际应用中的使用。本文提出的方法利用多视角几何约束，同时学习NeRF和优化相机姿态。通过利用输入视图之间提取的像素匹配，多视角对应目标将优化的场景和相机姿态强制收敛到全局和几何精确的解决方案。本文的方法在多个具有挑战性的数据集上取得了新的最优结果。 
- (3):本文提出的SPARF方法是一种联合姿态-NeRF训练策略。该方法利用多视角几何约束来驱动和限制NeRF-姿态优化。首先，我们使用预训练的匹配模型推断出与输入视图相关的像素对应关系。这些像素匹配在我们的多视角对应目标中得到利用，该目标使用NeRF和当前姿态估计渲染的深度来最小化重投影误差。通过训练视图之间的显式连接，该损失函数强制收敛到全局和几何精确的姿态/场景解决方案，这些解决方案在所有训练视图上保持一致。我们还提出了深度一致性损失，以提高从新视点的渲染质量。通过使用从训练视图渲染的深度来创建未见过的视角的伪地面真实深度，它鼓励重建场景从任何视角都是一致的。 
- (4):本文的方法在DTU、LLFF和Replica等多个具有挑战性的数据集上进行了广泛的评估和比较，在所有三个基准测试中都取得了新的最优结果。本文的方法可以在只有3个输入视图和嘈杂的姿态的情况下，实现高质量的新视角合成，这证明了该方法的有效性和实用性。
#### 7. 方法详细介绍：
本文提出了一种名为SPARF的方法，用于在稀疏视角下联合学习场景表示和优化初始训练姿态，以基于神经隐式表示进行新视角合成。该方法受到多视角几何原理的启发，由两个主要组件组成：（1）多视角对应损失，强制学习全局一致的3D解决方案，优化场景几何和相机姿态，（2）深度一致性损失，鼓励学习的场景几何在所有视角上保持一致，包括那些没有RGB监督的视角。该方法的训练分为两个阶段，并采用粗到细的位置编码方法，以防止网络立即过拟合训练图像。

#### 8. 实验设置：
本文在DTU、LLFF和Replica数据集上评估了所提出的方法，针对具有挑战性的3个输入视角的情况。使用平均旋转和平移误差进行姿态注册，使用PSNR、SSIM和LPIPS进行视角合成评估。采用连续的6向量表示旋转，直接优化平移向量。

#### 9. 实验结果与分析：
所提出的方法在多个数据集上优于早期方法，并创造了新的最先进水平。分析了位置编码的影响，发现粗到细的位置编码策略最有效。对所提出方法的关键组件进行了分析，包括多视角对应损失和深度一致性损失，并发现它们在所有指标上都显著提高了性能。同时，本文还提供了详细的定量结果和视觉比较，证明了所提出方法的优越性。


# Paper:143     学习神经参数头部模型



#### 1. Title: 
Learning Neural Parametric Head Models

#### 2. Authors: 
Simon Giebenhain, Tobias Kirschstein, Markos Georgopoulos, Martin Rünz, Lourdes Agapito, Matthias Nießner

#### 3. Affiliation: 
第一作者：Technical University of Munich（慕尼黑工业大学）

#### 4. Keywords: 
3D morphable model, neural field, disentangled latent space, high-fidelity local detail, ensemble of local MLPs

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Giebenhain_Learning_Neural_Parametric_Head_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是人脸重建和表情合成，传统的3DMMs模型无法表示局部细节，且需要固定拓扑结构的模板网格，限制了不同发型的表示。
- (2):过去的方法主要是基于PCA的3DMMs模型，其局限性在于无法表示局部细节和多样的发型。本文提出了一种基于神经场的神经参数头部模型，通过将人脸表示分解为两个空间，即SDF和神经形变场，实现了身份和表情的解耦。此外，本文通过引入一组以面部关键点为中心的局部MLP，实现了高保真度的局部细节。
- (3):本文提出了一种新的神经场表示方法，将人脸表示分解为SDF和神经形变场，通过一组局部MLP实现高保真度的局部细节。为了训练模型，本文使用了一个新的高保真度头部数据集，包括3700多个3D头部扫描，从203个不同的人中捕获。在推理时，可以通过优化表达和身份的潜在代码参数来适应给定的输入点云。实验结果表明，本文的方法在拟合误差和重建质量方面优于现有的方法。
- (4):本文的方法在3D头部重建任务上取得了很好的性能，可以表示完整的头部几何形状，包括细节。
#### 7. 方法详细介绍：
本文提出了一种神经参数化头部模型，将几何形状和面部表情分别表示为正向变形。该模型使用局部分解方法将特定身份的几何形状隐式地表示为其规范空间中的有符号距离函数（SDF）。SDF网络被分解为几个较小的局部MLP网络的集合，这些网络围绕某些面部锚点定义。面部锚点的选择是在区域的相关性和空间均匀性之间进行权衡的结果。该模型还通过仅在面部的左侧学习SDF来利用面部的对称性，然后在翻转空间坐标后与右半部分共享。表情表示使用MLP模型，在环境欧几里得空间中定义变形场。变形网络使用身份特征进行条件训练，以学习表情的解耦表示。训练策略按自动编码器的方式依次训练身份和表情网络。

#### 8. 实验设置：
本文使用高端3D扫描设备捕获了超过3700个3D扫描数据，包括203个不同人的数据。每个扫描仅需6秒，每个融合扫描包含约1.5M个顶点和3.5M个三角形。每个参与者被要求进行23种不同的表情，这些表情来自FaceWarehouse中的FACS编码表情。该数据集在几何形状的质量和完整性方面显著超过了可比较的现有数据集，平均每个扫描包含约3.5M个网格面。

#### 9. 实验结果和分析：
本文使用一个包含6个女性和12个男性身份的测试数据集，每个身份有23种表情，评估了所提出方法对未见过的身份和表情的泛化能力。评估包括分别重建身份和表情。本文将所提出方法与现有的基于模板的PCA模型和神经参数模型进行了比较。评估指标包括L1-Chamfer距离、法向量一致性和F-Score（阈值为1.5mm）。结果表明，所提出的方法在重建质量方面始终实现了更准确的重建，并在重建质量方面优于现有模型。


# Paper:144     探索高质量视频帧插值的运动模糊和对齐问题



#### 1. Title: 
Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation

#### 2. Authors: 
Kun Zhou, Wenbo Li, Xiaoguang Han, Jiangbo Lu

#### 3. Affiliation: 
Kun Zhou: 香港中文大学（深圳）, Wenbo Li: 香港中文大学, Xiaoguang Han: 香港中文大学, Jiangbo Lu: SmartMore Corporation

#### 4. Keywords: 
Video frame interpolation, deep learning, texture consistency loss, guided cross-scale pyramid alignment

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhou_Exploring_Motion_Ambiguity_and_Alignment_for_High-Quality_Video_Frame_Interpolation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究视频帧插值的问题，提出了一种新的方法来解决现有方法中存在的运动模糊问题。
 
- (2):现有的深度学习方法通常依赖于中间帧的真实数据，而忽略了给定相邻帧中运动的多样性，导致产生模糊的结果。本文提出了一种基于纹理一致性的损失函数，以保证插值后的内容与相邻帧中的内容保持相似的结构，从而产生更清晰的结果。此外，本文还提出了一种新的跨尺度金字塔对齐模块，以更好地利用多尺度信息进行运动补偿。 
 
- (3):本文提出了一种新的纹理一致性损失函数和跨尺度金字塔对齐模块，以解决现有方法中存在的运动模糊问题。纹理一致性损失函数通过鼓励插值后的内容与相邻帧中的内容保持相似的结构，从而产生更清晰的结果。跨尺度金字塔对齐模块则利用多尺度信息进行运动补偿，以更好地解决运动模糊问题。 
 
- (4):本文在Vimeo-Triplets和Middlebury数据集上进行了实验，结果表明，所提出的方法在视频帧插值任务上取得了优异的性能，证明了其有效性和高效性。
#### 7. 方法详细介绍：
本文提出了一种新的视频帧插值/外推方法，使用纹理一致性损失（TCL）来放宽预定义的真实值的严格约束，并使用引导的跨尺度金字塔对齐（GCSPA）更好地利用多尺度信息。该模型在Vimeo-Triplets-Train数据集上进行训练，并在Vimeo-Triplets-Test集和Middlebury基准测试中进行评估。该方法在插值质量方面相比现有方法取得了显著的改进。该论文还通过消融研究分析了每个提出的组件的贡献。该方法的局限性是它的模型容量比一些轻量级模型大，并且只能在t = 0.5处生成中间帧。

#### 8. 实验设置：
本文在三个基准测试中评估了所提出的方法：Vimeo-Triplets、Middlebury和UCF101。使用Vimeo-Triplets数据集进行训练，使用Middlebury和UCF101数据集进行测试。评估指标为PSNR和SSIM。实验在NVIDIA GeForce RTX 2080Ti GPU上进行，训练持续600K次迭代，使用随机裁剪、翻转和旋转增强。

#### 9. 实验结果与分析：
所提出的方法在视频帧插值方面在所有基准测试中均取得了最先进的性能。该方法比其他最先进的方法具有更好的细节和更少的伪影。该方法在视频帧外推任务上也获得了与以前最先进方法相当的性能。与其他方法的详细定量比较在表1中提供，视觉比较在图5和图6中展示。


# Paper:145     从阴影中可以重建什么



#### 1. Title: 
What You Can Reconstruct from a Shadow

#### 2. Authors: 
Ruoshi Liu, Sachit Menon, Chengzhi Mao, Dennis Park, Simon Stent, Carl Vondrick

#### 3. Affiliation: 
Ruoshi Liu, Sachit Menon, Chengzhi Mao, and Carl Vondrick are affiliated with Columbia University.

#### 4. Keywords: 
3D reconstruction, shadows, generative models, computer vision, image formation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_What_You_Can_Reconstruct_From_a_Shadow_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉中的三维重建问题，特别是当需要重建的物体被部分或完全遮挡时，这个任务变得更加具有挑战性。

- (2):过去的方法通常需要多个视角或多张图像来重建物体，而本文提出了一种使用物体投射在地面上的阴影来推断可能的三维体积的方法。本文的方法是基于生成模型的，可以同时推断物体的三维形状、姿态和光源位置。与过去的方法相比，本文的方法不需要多个视角或多张图像，只需要一个阴影图像即可进行重建。

- (3):本文提出的方法是基于生成模型的，可以同时推断物体的三维形状、姿态和光源位置。本文的方法是可微分的，可以使用梯度下降来寻找最佳的解释。本文的方法还可以集成学习到的物体几何先验，以生成不同物体类别的逼真的三维形状。实验和可视化结果表明，本文的方法能够生成与阴影观察一致的多个可能解释。本文的方法即使在光源位置和物体姿态都未知的情况下也能够工作，并且对于真实世界的图像也具有鲁棒性。

- (4):本文的方法在阴影重建任务上取得了良好的性能，可以生成与阴影观察一致的多个可能解释。本文的方法不需要多个视角或多张图像，只需要一个阴影图像即可进行重建，具有很好的实用性。
#### 7. 方法详细介绍：
本文提出了一种从阴影中重建三维物体的生成方法。该方法使用可微分的图像形成模型，可以联合推断物体的三维形状、姿态和光源位置。该方法还集成了关于物体几何的先验知识，以生成不同物体类别的逼真三维形状。优化问题通过梯度下降求解，并且可以通过使用不同的初始化来恢复与阴影观察一致的多个可能解。

具体步骤如下：
1. 使用深度生成模型预训练一个生成器，该生成器可以生成逼真的三维物体。
2. 使用一个可微分的阴影渲染器，将生成的三维物体渲染成二维阴影。
3. 使用一个可微分的最大池化操作，将光线沿着光源方向的占据网络值最大的点作为阴影的值。
4. 使用梯度下降算法，最小化阴影和渲染阴影之间的差异，以恢复三维物体。

#### 8. 实验设置：
本文在已知和未知光源位置和姿态的情况下，评估了所提出方法在阴影重建三维物体任务上的性能。使用体积交并比（volumetric IoU）作为评估指标，将物体放置在其规范化的姿态下，并随机化光源的位置。将所提出方法与回归、最近邻和随机等基线方法进行比较。

#### 9. 实验结果和分析：
所提出的方法在已知光源位置和姿态的情况下，在阴影重建三维物体任务上显著优于基线方法，volumetric IoU高出近9个百分点。当存在遮挡且无法获得完整图像时，所提出的方法能够重建被其他物体遮挡的物体。在未知光源位置和姿态的情况下，所提出的方法在阴影重建三维物体任务上也优于基线方法。本文还展示了从自然阴影中重建三维物体的定性示例。


# Paper:146     密集网络扩展用于类增量学习



#### 1. Title: 
Dense Network Expansion for Class Incremental Learning

#### 2. Authors: 
Zhiyuan Hu, Yunsheng Li, Jiancheng Lyu, Dashan Gao, Nuno Vasconcelos

#### 3. Affiliation: 
Zhiyuan Hu: 加州大学圣地亚哥分校 (UC San Diego)

#### 4. Keywords: 
Class incremental learning, network expansion, feature sharing, attention mechanism, transformer architecture

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Dense_Network_Expansion_for_Class_Incremental_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究了类增量学习（CIL）问题，即在不断增加的类别中进行分类任务。传统的深度学习模型容易出现灾难性遗忘问题，即在学习新任务时会忘记之前学习的任务。本文旨在解决这个问题。
 
- (2):过去的方法包括蒸馏方法、参数正则化方法和梯度方法等，但这些方法都存在一些问题，如模型稳定性差、模型容量不足等。本文提出了一种新的网络扩展方法，即密集网络扩展（DNE），通过在任务专家网络的中间层之间引入密集连接，实现了特征共享和重用，从而更好地平衡了准确性和模型复杂度。此外，本文还提出了一种新的任务注意力块（TAB）来实现跨任务的特征共享，相比于传统的注意力机制，TAB更加有效。
 
- (3):本文的创新点在于提出了一种新的网络扩展方法DNE，通过引入密集连接和任务注意力机制，实现了特征共享和重用，从而更好地平衡了准确性和模型复杂度。此外，本文还提出了一种新的任务注意力块（TAB）来实现跨任务的特征共享，相比于传统的注意力机制，TAB更加有效。
 
- (4):本文在CIFAR100数据集上进行了实验，结果表明，DNE方法在准确性和模型规模之间取得了更好的平衡，相比于之前的方法，准确性提高了4%左右，模型规模更小。这表明DNE方法在解决CIL问题上具有很好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Dense Network Expansion（DNE）的方法，用于解决类别增量学习（CIL）问题。DNE方法通过引入交叉任务注意力机制和任务注意力块（TAB）来实现特征共享和重用，从而在保持旧类别特征空间的同时，使网络和特征规模增长速度比以前的方法慢得多。DNE方法采用Vision Transformer（ViT）作为主干网络，每个任务专家分支都由ViT实现。DNE方法的训练目标是三个损失的加权平均值：交叉熵分类损失、任务专业知识损失和蒸馏损失。DNE方法在CIFAR100和ImageNet100数据集上进行了广泛的实验，结果表明该方法在准确性和模型复杂度之间取得了更好的平衡，比以前的最先进方法在准确性方面提高了4％，并且具有相似或更小的模型规模。

#### 8. 实验设置：
本文的实验在CIFAR100和ImageNet100数据集上进行，第一个任务包含50个类别，每个后续任务包含Ns个类别。Ns是步长。DNE方法与多个基线方法进行了比较，包括Joint、Dytox、DER、FOSTER、iCaRL和PODNet。性能通过最终准确率（LA）、平均增量准确率（AA）和联合模型和CIL模型之间的准确率差（D）进行衡量。方法还通过每秒浮点运算次数（FLOPs）进行比较。

#### 9. 实验结果和分析：
DNE方法在准确性和模型规模之间取得了更好的平衡，比以前的最先进方法在准确性方面提高了4％，并且具有相似或更小的模型规模。在CIFAR100上，即使每个任务专家只有1个头，DNE方法的LA也达到了68.04％，比当前最先进的Dytox方法高3.98％。在ImageNet100上，DNE-1head的性能优于DER方法，LA提高了1.9％，AA提高了1.19％。随着头数的增加，DNE方法的性能越来越好。在1到4个头之间，LA在CIFAR100（ImageNet100）上增加了2％（1.28％）。在ImageNet100上，使用2个头时性能饱和。这些结论在Ns步长下定性成立。DNE方法的优势对于较小的步长更大。当Ns = 5时，DNE-2heads的LA比DER方法高8.99％。对于Ns = 25，DNE-4heads的LA比最先进的FOSTER方法高1.33％。


# Paper:147     使用三面体扩散生成3D神经场



#### 1. Title: 
3D Neural Field Generation using Triplane Diffusion

#### 2. Authors: 
J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein

#### 3. Affiliation: 
J. Ryan Shue: Milton Academy

#### 4. Keywords: 
3D generation, neural fields, diffusion models, triplane feature representations

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是图像生成领域中的扩散模型，尤其是3D形状生成方面的研究。

- (2):过去的方法主要是基于3D GANs，但是这些方法存在一些问题，如分辨率低、生成的结果质量不高等。本文提出了一种基于三面体扩散的3D形状生成方法，可以直接使用现有的2D扩散模型进行训练，生成的结果质量和多样性都优于其他方法。

- (3):本文提出的方法是将3D场景转换为连续的占据场，并将其分解为一组轴对齐的三面体特征表示。这样，我们的3D训练场景都由2D特征平面表示，可以直接在这些表示上训练现有的2D扩散模型，生成高质量和多样性的3D神经场。本文的核心贡献是提出了一种基于2D扩散模型的3D场景扩散框架，具有内置的3D归纳偏差，并且可以生成高保真度和多样性的3D场景。

- (4):本文在ShapeNet数据集上进行了实验，结果表明，所提出的方法在3D形状生成方面取得了最先进的结果，优于其他方法。
#### 7. 方法详细介绍：
本文提出了一种名为三面体扩散的3D感知神经场生成模型。该方法首先将训练数据转换为连续的占据场，并将其分解为一组轴对齐的三面体特征表示。然后，将这些2D特征平面用于训练现有的2D扩散模型，以生成高质量和多样性的3D神经场。该方法需要对现有的三面体分解流程进行必要的修改，以使得生成的特征易于被扩散模型学习。在训练过程中，使用总变分正则化、L2正则化和显式密度正则化等方法对三面体特征进行正则化。在推理过程中，模型从训练好的扩散模型中采样一个三面体，并使用预训练的MLP查询神经场以生成3D形状。

#### 8. 实验设置：
本文使用ShapeNet数据集进行实验，将渲染的着色图像用于评估生成的3D网格的质量。使用改进的Fréchet inception距离（FID）作为评估指标，该指标考虑了人类感知，克服了其他基于网格的评估指标的局限性。同时，使用Sajjadi等人提出的方法计算精度和召回率。将本文提出的方法与PVD和SDF-StyleGAN等最先进的基于点和神经场的3D生成模型进行比较。结果表明，本文提出的方法在每个ShapeNet类别的FID、精度和召回率方面均优于所有基线模型。本文提供了定量和定性的结果来支持所提出方法的有效性。

#### 9. 实验结果与分析：
本文提出的三面体扩散方法在生成3D形状方面表现出色，与其他最先进的3D生成模型相比具有更高的质量和多样性。实验结果表明，本文提出的方法在FID、精度和召回率等指标上均优于基线模型。此外，本文还进行了消融实验，证明了所提出的三面体分解和正则化方法的有效性。本文的实验结果表明，三面体扩散方法是一种有效的3D形状生成方法。


# Paper:148     3Mformer：用于骨架动作识别的多阶多模式Transformer



#### 1. Title: 
3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition

#### 2. Authors: 
Lei Wang, Piotr Koniusz

#### 3. Affiliation: 
Lei Wang: 澳大利亚国立大学；Piotr Koniusz: Data61 CSIRO

#### 4. Keywords: 
Skeletal action recognition, hypergraph, higher-order Transformer, Multi-order Multi-mode Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_3Mformer_Multi-Order_Multi-Mode_Transformer_for_Skeletal_Action_Recognition_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究骨架动作识别，通过超图建模来捕捉不同级别的运动模式，提高模型的性能。

- (2):过去的方法主要是基于图卷积网络（GCN）来表示人体骨架，但是GCN只能聚合一到两跳的邻居节点，忽略了未连接的节点之间的依赖关系。本文提出了一种超图建模的方法，通过超边来捕捉不同级别的运动模式，同时提出了一种新的多模式多阶Transformer（3Mformer）来处理超边的嵌入，进一步提高了模型的性能。

- (3):本文提出了一种基于超图的骨架动作识别方法，通过Higher-order Transformer来提取不同级别的超边嵌入，然后通过3Mformer来处理这些嵌入，进一步提高模型的性能。

- (4):在NTU-60、NTU-120、Kinetics-Skeleton和Northwestern-UCLA数据集上，本文提出的方法都取得了最好的性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种多阶多模态变压器（3Mformer）用于骨骼动作识别。该模型使用超图来建模图节点之间的超边，捕捉身体关节组的高阶运动模式。将动作序列分成时间块，Higher-order Transformer（HoT）基于身体关节、身体关节的成对链接和骨骼身体关节的高阶超边产生每个时间块的嵌入。3Mformer将超边1到r阶的HoT嵌入与两个模块相结合，其顺序可以交换，以实现基于“通道-时间块”、“顺序-通道-身体关节”、“通道-超边（任意顺序）”和“仅通道”对的耦合模式注意力。第一个模块称为Multi-order Pooling（MP），还学习了沿超边模式的加权聚合，而第二个模块Temporal block Pooling（TP）沿时间块模式聚合。最后将输出连接并传递到FC层进行分类。

#### 8. 实验设置：
本文在一台服务器上使用PyTorch实现了3Mformer模型，并使用Adam优化器进行训练，学习率为0.001，批量大小为64，训练轮数为200。

#### 9. 实验结果和分析：
本文在NTU-60、NTU-120、Kinetics-Skeleton和Northwestern-UCLA数据集上评估了所提出的3Mformer模型，并与最近的基于图形和超图的方法进行了比较。实验结果表明，所提出的方法在这些基准测试中均优于所有基于图形和超图的模型。多阶特征映射的融合进一步提高了性能。本文提供了详细的性能比较和分析。同时，本文还进行了消融研究，以分析所提出方法的不同组成部分的贡献。结果表明，基于超图的表示和耦合模式注意力机制显著提高了模型的性能。


# Paper:149     iDisc：用于单目深度估计的内部离散化



#### 1. Title: 
iDisc: Internal Discretization for Monocular Depth Estimation

#### 2. Authors: 
Luigi Piccinelli, Christos Sakaridis, Fisher Yu

#### 3. Affiliation: 
Luigi Piccinelli: ETH Zurich（苏黎世联邦理工学院）, Switzerland

#### 4. Keywords: 
Monocular depth estimation, Internal Discretization, Supervised learning, Attention mechanism, Zero-shot testing

#### 5. Paper: http://openaccess.thecvf.com/content/CVPR2021/html/Piccinelli_iDisc_Internal_Discretization_for_Monocular_Depth_Estimation_CVPR_2021_paper.html  Github: http://vis.xyz/pub/idisc

#### 6. Summary : 
- (1):本文研究的是单目深度估计，该任务是计算机视觉中的基础任务，对于3D场景理解和下游应用非常重要。然而，即使在监督学习的情况下，由于缺乏完整的几何约束，该任务仍然具有挑战性和不适定性。
 
- (2):现有的方法通常涉及卷积网络或变换器架构，大多数方法要么在图像上强加几何约束，要么明确离散化连续深度范围。这些方法的局限性在于它们不能建模任意深度模式，这在现实世界的场景中是普遍存在的。本文提出了一种更通用的深度估计模型，称为iDisc，它不会在最终预测上明确强加任何约束。我们设计了一种场景的内部离散化，它原则上是深度无关的。我们的假设是，每个场景都可以通过一组概念或模式来隐式描述，例如对象、平面、边缘和透视关系。我们通过可学习的、依赖于输入的量化器实现了连续到离散到连续的瓶颈，从而获得这种内部场景离散化，即底层模式。与现有方法不同，我们的模型不会在深度输出上强加任何显式约束或先验。整个网络可以通过基于注意力的瓶颈模块进行端到端训练。 

- (3):本文提出了一种新的模块，称为Internal Discretization（ID），它实现了一个连续-离散-连续的瓶颈，以无监督的方式学习高级模式。我们的ID模块是一种通用概念，可以用多种方式实现。我们的特定ID实现采用基于注意力的运算符，从而实现了端到端可训练的架构和输入依赖框架。我们的模型通过学习场景中的高级模式来预测高质量的深度图，从而捕捉场景的交互和结构。我们在多个室内和室外数据集上测试了iDisc，并通过零样本测试探索了其鲁棒性。 

- (4):本文在NYU-Depth v2和KITTI数据集上取得了显著的改进，超过了所有已发布的方法。iDisc还可以在表面法线估计方面实现最先进的结果。我们通过零样本测试探索了模型的泛化能力。我们观察到在户外场景中需要促进多样性。因此，我们提出了两个自主驾驶数据集的拆分，DDAD和Argoverse。iDisc的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为iDisc的新方法，用于单目深度估计。该方法利用内部离散化方法，将连续空间离散化为多个中间离散表示(IDR)级别。然后将IDR与像素嵌入结合起来提取深度图。优化过程由SIlog损失函数引导，该函数是一种尺度不变的损失函数。iDisc模块包括三个子模块：显式深度离散化(EDD)、内部场景离散化(ISD)和自适应特征分区(AFP)。此外，还提出了多尺度可变形注意力(MSDA)模块，以进一步提高性能。整个网络可以通过注意力实现端到端训练，具有较强的泛化能力。

#### 8. 实验设置：
本文在多个数据集上进行了实验，包括NYU-Depth V2、SUN-RGBD、DIODE-Indoor、KITTI、Argoverse1.1和DDAD。模型在相应的训练集上进行训练，并在测试集上进行评估。评估指标包括均方根误差(RMS)、对数绝对误差(Log10)、平均相对误差(A.Rel)和尺度不变对数误差(SIlog)。在评估过程中，没有使用测试时增强、相机参数或其他技巧和正则化方法。

#### 9. 实验结果和分析：
在NYU-Depth V2数据集上，iDisc方法实现了最先进的性能，相对于先前的最先进方法，RMS提高了6%以上，A.Rel提高了9%以上。在SUN-RGBD和DIODE-Indoor数据集上的零样本测试结果也证明了所提出方法的泛化能力。本文还提供了与其他最先进方法在不同数据集上的详细比较结果。在KITTI数据集上，iDisc方法实现了最先进的性能，相对于先前的最先进方法，RMS提高了3%以上，δ0.5提高了0.9%以上。在Argoverse和DDAD数据集上，iDisc方法也优于其他使用相同架构和训练流程的方法。此外，iDisc方法在NYU官方测试集上的表面法线估计方面也实现了最先进的性能。


# Paper:150     基于分段B´ezier曲线的端到端矢量化高清地图构建



#### 1. Title: 
End-to-End Vectorized HD-map Construction with Piecewise B´ezier Curve

#### 2. Authors: 
Limeng Qiao, Wenjie Ding, Xi Qiu, Chi Zhang

#### 3. Affiliation: 
MEGVII Technology (旷视科技)

#### 4. Keywords: 
Vectorized HD-map, B´ezier curve, end-to-end, IPM-PE Align module, Piecewise B´ezier Head

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2021_paper.html  Github: https://github.com/er-muyue/BeMapNet

#### 6. Summary : 
- (1):本文研究的是高清地图（HD-map）的构建，旨在实现对厘米级环境信息的感知，是自动驾驶领域的热门研究方向。

- (2):现有的方法大多采用基于分割的流程获得栅格化地图，然后进行后处理以获得下游友好的矢量化。本文通过深入研究基于参数化的方法，提出了一种简洁优雅的方案，采用统一的分段B´ezier曲线。为了实现端到端的矢量化，本文提出了一种简单而有效的架构，称为Piecewise B´ezier HD-map Network（BeMapNet），它采用直接集合预测范式，无需后处理。本文首先引入了一种新颖的IPM-PE Align模块，通过Transformer中的常见位置编码将3D几何先验注入BEV特征。然后，本文提出了一个设计良好的Piecewise B´ezier Head，用于输出每个地图元素的细节，包括控制点的坐标和曲线的段数。此外，基于B´ezier曲线的逐步恢复，本文还提出了一种高效的Point-Curve-Region Loss，以监督更加鲁棒和精确的HD-map建模。与现有的SOTAs相比，本文的方法至少优于其他方法18.0 mAP。

- (3):本文提出了一种新颖的Piecewise B´ezier HD-map Network（BeMapNet）架构，采用统一的分段B´ezier曲线，实现了端到端的矢量化HD-map构建。本文首先引入了一种新颖的IPM-PE Align模块，将3D几何先验注入BEV特征。然后，本文提出了一个设计良好的Piecewise B´ezier Head，用于输出每个地图元素的细节，包括控制点的坐标和曲线的段数。此外，基于B´ezier曲线的逐步恢复，本文还提出了一种高效的Point-Curve-Region Loss，以监督更加鲁棒和精确的HD-map建模。

- (4):本文的方法在现有基准测试中表现出显著的优越性，揭示了我们方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为BeMapNet的方法，用于端到端的矢量化高清地图构建。该方法采用基于参数化的范式，通过多视角和仅相机的方式构建实例级矢量化高清地图。该架构分为四个主要模块，逐步提供丰富的信息，即用于多视角图像的特征提取器、用于2D-3D透视高度的语义BEV解码器、用于曲线级描述符的实例Bézier解码器和用于点级参数化的分段Bézier头。其中，IPM-PE Align模块被引入到基于Transformer的解码器中，通过PE（位置编码）将IPM（逆透视映射）几何先验注入BEV特征中，除了一个FC层外，几乎不添加任何参数。分段Bézier头采用两个分支作为分类和回归，前者分类确定曲线长度的段数，后者回归控制点的坐标以确定曲线形状。最后，提出了点-曲线-区域损失，通过监督恢复信息的渐进方式，实现了鲁棒的曲线建模。

#### 8. 实验设置：
实验在NuScenes数据集上进行，该数据集包括28,130/6,019个样本和700/150个驾驶场景的训练/验证集。感知范围设置为[30, 30, 15, 15]m，自我到像素的分辨率固定为0.15 m/pixel。评估指标采用平均精度（AP），阈值为[0.2, 0.5, 1.0]m。数据集分为五种不同的光照/天气条件：白天、夜晚、晴天、多云和雨天。

#### 9. 实验结果和分析：
在NuScenes数据集上，本文提出的BeMapNet方法取得了40.7的mAP，优于现有方法，如IPM(B)、IPM(CB)、LSS和VPN。与现有方法相比，本文提出的方法在不同的光照/天气条件下也实现了更高的mAP得分。结果使用与以前的工作相同的评估协议进行评估。


# Paper:151     StyleGAN沙龙：多视角潜在优化实现姿态不变发型转移



#### 1. Title: 
StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer

#### 2. Authors: 
Sasikarn Khwanmuang, Pakkapon Phongthawee, Patsorn Sangkloy, Supasorn Suwajanakorn

#### 3. Affiliation: 
Supasorn Suwajanakorn: VISTEC, Thailand

#### 4. Keywords: 
StyleGAN, hairstyle transfer, virtual try-on, multi-view optimization, face-hair composite

#### 5. Paper: https://arxiv.org/abs/2103.16827  Github: None

#### 6. Summary : 
- (1):本文旨在将参考图像的发型转移到输入照片中，以进行虚拟试穿。本文针对多种具有挑战性的场景，如将带刘海的长发转换为短发，需要去除现有的头发并推断出前额的外观，或将戴帽子的人的部分可见头发转移。 
- (2):过去的解决方案利用StyleGAN来幻想任何缺失的部分，并通过所谓的GAN反演或投影产生无缝的面部-头发复合物。然而，控制幻想以准确地转移发型并保留输入的面部形状和身份仍然是一个挑战。为了克服这个问题，我们提出了一个多视角优化框架，使用两个不同视角的参考复合物来语义地指导遮挡或模糊的区域。我们的优化在两个姿势之间共享信息，从而使我们能够从不完整的输入中产生高保真度和逼真的结果。
- (3):本文提出了一种多视角优化框架，使用两个不同视角的参考复合物来语义地指导遮挡或模糊的区域。我们的优化在两个姿势之间共享信息，从而使我们能够从不完整的输入中产生高保真度和逼真的结果。我们的方法在StyleGAN的基础上进行了改进，通过多视角优化来控制幻想，从而实现了准确的发型转移和面部形状和身份的保留。
- (4):本文的方法在虚拟试穿任务中取得了良好的性能，能够将参考图像的发型转移到输入照片中，并保留输入的面部形状和身份。实验结果表明，我们的方法在各种具有挑战性的场景下都能产生高保真度和逼真的结果。
#### 7. 方法详细介绍：
本文提出了一种基于StyleGAN的多视角优化框架，用于实现姿态不变的发型转移。该框架使用两个不同视角的参考合成图来语义地指导遮挡或模糊区域。优化过程在两个姿势之间共享信息，从不完整的输入中产生高保真度和逼真的结果。具体步骤如下：
1. 预处理输入的面部和头发图像，提取面部关键点和头发掩膜。
2. 使用StyleGAN将参考头发图像和FFHQ数据集中的随机面部图像组合生成参考合成图像。
3. 使用参考合成图像来指导在输入面部图像上合成目标发型。
4. 采用多视角优化方法来处理输入图像中的姿势变化和遮挡。
5. 使用基于StyleGAN的细化网络对合成图像进行优化。

#### 8. 实验设置：
本文使用了FFHQ数据集中的人脸图像和头发图像作为输入数据，并使用了NVIDIA Tesla V100 GPU进行实验。具体实验设置包括：
1. 训练StyleGAN模型，生成参考合成图像。
2. 选择不同的输入图像和参考图像进行实验，比较不同方法的效果。
3. 评估合成图像的质量和姿势不变性。

#### 9. 实验结果和分析：
本文提出的多视角优化框架在实验中取得了良好的效果，能够实现姿态不变的发型转移。与其他方法相比，本文方法能够处理遮挡和模糊区域，生成更加逼真的合成图像。同时，本文方法还能够处理不同姿势之间的转换，具有较好的姿势不变性。


# Paper:152     图像作为一门外语：BEIT预训练用于视觉和视觉语言任务



#### 1. Title: 
Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks

#### 2. Authors: 
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei

#### 3. Affiliation: 
Microsoft Corporation 微软公司

#### 4. Keywords: 
Multimodal pretraining, vision-language tasks, Multiway Transformers, masked data modeling, BEIT-3

#### 5. Paper: https://aka.ms/beit-3  Github: None

#### 6. Summary : 
- (1):本文研究背景是语言、视觉和多模态预训练的大融合。
- (2):过去的方法存在一些问题，如需要手动转换终端任务格式，参数通常不能在模态之间有效共享等。本文提出了一种基于Multiway Transformers和masked data modeling的通用多模态预训练方法，可以在多个视觉和视觉语言任务上实现出色的转移性能。
- (3):本文提出了一种基于Multiway Transformers的通用多模态预训练模型BEIT-3，采用统一的masked data modeling方法，可以在图像、文本和图像-文本对上进行预训练。BEIT-3在多个视觉和视觉语言任务上实现了出色的性能，具有很强的通用性。
- (4):BEIT-3在多个任务上均取得了优异的性能，如目标检测（COCO）、语义分割（ADE20K）、图像分类（ImageNet）、视觉推理（NLVR2）、视觉问答（VQAv2）、图像字幕生成（COCO）和跨模态检索（Flickr30K、COCO）。本文提出的方法在多个任务上均优于现有的强基础模型，表明其具有很强的通用性和可扩展性。
#### 7. 方法详细介绍：
BEIT-3是一个基于Transformer的多模态预训练模型，由40层Multiway Transformer组成，具有1408个隐藏单元，6144个中间单元和16个注意力头。该模型使用掩码预测任务进行预训练，包括掩码语言建模和掩码图像建模，以处理单模态和多模态数据。模型的预训练过程使用256个A100 40GB GPU进行1M步训练，包括2048个图像、2048个文本和2048个图像-文本对。预训练过程需要大约两周的时间。在微调阶段，模型作为融合编码器对图像-文本对进行联合编码，以进行视觉语言任务。 

#### 8. 实验设置：
本文在多个公共基准测试集上评估了BEIT-3的性能，包括ADE20K、COCO、VQAv2、NLVR2和Flickr30K等。模型使用公共资源进行预训练和微调，评估了多个视觉和视觉语言任务的性能，包括语义分割、目标检测、实例分割、图像分类、视觉推理、视觉问答、图像-文本检索和图像字幕生成。 

#### 9. 实验结果和分析：
BEIT-3在多个视觉和视觉语言任务上均取得了最先进的性能，包括图像-文本检索、图像字幕生成、目标检测和实例分割等。在COCO和Flickr30K基准测试中，BEIT-3在零样本和微调设置下均优于以前的强模型。在COCO图像字幕生成、目标检测和实例分割任务中，BEIT-3使用更小的图像尺寸进行微调，达到了最佳结果，分别达到63.7的box AP和54.8的mask AP。在GLUE基准测试中，BEIT-3base在开发集上的平均准确率为84.9％，表现出色。


# Paper:153     隐私保护表示不足以保护隐私：从相机姿态中恢复场景内容



#### 1. Title: 
Privacy-Preserving Representations are not Enough: Recovering Scene Content from Camera Poses

#### 2. Authors: 
Kunal Chelani, Torsten Sattler, Fredrik Kahl, Zuzana Kukelova

#### 3. Affiliation: 
Chalmers University of Technology (瑞典查尔默斯理工大学)

#### 4. Keywords: 
Visual localization, privacy-preserving localization, camera poses, scene content recovery

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chelani_Privacy-Preserving_Representations_Are_Not_Enough_Recovering_Scene_Content_From_Camera_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是视觉定位问题，即从给定图像中估计相机姿态，是三维计算机视觉应用的核心问题。随着AR/VR/MR设备和基于云的应用程序的普及，隐私问题变得非常重要。现有的隐私保护定位方法旨在防御攻击者对云服务的访问，但本文表明攻击者可以通过查询定位服务来了解场景的细节，而无需任何访问权限。
- (2):现有的隐私保护定位方法主要集中在确保发送到定位服务的数据不会泄露私人信息，或确保存储在定位服务上的数据不会泄露私人信息。本文提出了一种新的攻击方式，即攻击者可以通过定位服务返回的相机姿态来恢复场景内容。攻击是基于现代视觉定位算法对外观和几何变化的鲁棒性。这是云定位服务的必要属性，但它也会导致算法定位与场景中存在的相似对象。攻击者可以使用大量的对象图像查询服务器，并根据服务器返回的相机姿态恢复对象的位置。本文通过攻击的概念验证版本，证明了攻击的实际可行性。
- (3):本文提出了一种新的攻击方式，即攻击者可以通过定位服务返回的相机姿态来恢复场景内容。攻击是基于现代视觉定位算法对外观和几何变化的鲁棒性。本文通过攻击的概念验证版本，证明了攻击的实际可行性。本文的贡献是提出了一种新的隐私保护定位研究方向，即定位服务的目标是正确识别查询图像是否在相关场景中拍摄，以防止通过相机姿态泄露信息。
- (4):本文的方法在定位服务中恢复场景内容，通过查询定位服务来了解场景的细节，而无需任何访问权限。本文的攻击方法不仅适用于基于特征的方法，而且适用于任何鲁棒的定位系统。本文的实验结果证明了攻击的实际可行性。
#### 7. 方法详细介绍：
本文提出了一种新的攻击方法，旨在通过云端服务器返回的相机姿态来实现隐私保护的视觉定位攻击。攻击基于现代视觉定位算法对外观和几何变化的鲁棒性，导致算法定位与场景中存在的相似对象。攻击者可以使用来自互联网等渠道的大量对象图像查询服务器，从返回的相机姿态中了解对象的位置。本文开发了这种攻击的概念验证版本，并证明了其实用性。攻击不对使用的定位算法提出任何要求，因此也适用于隐私保护表示。

#### 8. 实验设置：
本文使用多个数据集进行实验，包括IKEA-Scenes和IKEA-Objects、ScanNet-Office-Scene、Office-Objects和RIO10。作者在IKEA家具店的不同灵感房间中捕获了图像序列，并从每个房间选择了4-10个对象。他们还收集了5个常见办公室物品的图像序列。数据集用于评估所提出的攻击和潜在防御策略的有效性。作者使用HLoc，一种最先进的视觉定位方法，实现了用于查询的服务器。使用三种不同的本地图像特征和匹配器评估了定位过程的鲁棒性：Superpoint特征与Superglue匹配器、R2D2特征与最近邻匹配、SIFT与NN匹配。

#### 9. 实验结果和分析：
本文的实验在RIO10数据集的三个不同场景上进行。使用数据集提供的实例级标签定义对象。还评估了一种潜在方法来防止所提出的攻击，该方法考虑查询图像中可见对象的最小数量。结果表明，拒绝恶意查询同时保留真实查询可能具有挑战性，并且找到最小可见对象数量的合适阈值可能很难。防御策略还要求服务了解场景中的对象，如果攻击者获得对服务的访问权限，则会产生潜在的隐私风险。本文得出结论，需要进一步研究以确保隐私保护的视觉定位。


# Paper:154     理解和改进视觉提示：标签映射视角



#### 1. Title: 
Understanding and Improving Visual Prompting: A Label-Mapping Perspective

#### 2. Authors: 
Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu

#### 3. Affiliation: 
Aochuan Chen: Michigan State University

#### 4. Keywords: 
Visual prompting, label mapping, transfer learning, deep learning, computer vision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2021_paper.html  Github: https://github.com/OPTML-Group/ILM-VP

#### 6. Summary : 
- (1):本文研究了视觉提示（VP）技术，该技术可以通过将通用提示（即输入扰动模式）合并到下游数据点中，重新编程固定的、预训练的源模型，以完成目标领域中的下游任务。本文探讨了VP的有效性与源类别和目标类别之间的无规则标签映射（LM）之间的关系。

- (2):本文提出了一种新的VP框架，称为ILM-VP（基于迭代标签映射的视觉提示），该框架可以自动重新映射源标签到目标标签，并逐步提高VP的目标任务准确性。此外，当使用对比语言-图像预训练（CLIP）模型进行VP时，我们提出了一种集成LM过程来辅助CLIP的文本提示选择并提高目标任务准确性的方法。实验结果表明，我们的提议在多个目标数据集上显着优于现有的VP方法。

- (3):本文提出了一种新的VP框架，称为ILM-VP，该框架可以自动重新映射源标签到目标标签，并逐步提高VP的目标任务准确性。此外，我们还提出了一种集成LM过程来辅助CLIP的文本提示选择并提高目标任务准确性的方法。这些方法在多个目标数据集上进行了广泛的实验，证明了其准确性和可解释性。

- (4):本文在13个目标任务上重新编程了ImageNet预训练的ResNet-18，其中ILM-VP在转移学习到目标Flowers102和CIFAR100数据集时分别提高了7.9％和6.7％的准确性。此外，我们的CLIP-VP提议在Flowers102和DTD上提高了13.7％和7.1％的准确性。这些结果表明，我们的方法可以有效地提高VP的准确性和可解释性。
#### 7. 方法详细介绍：
本文提出了一种名为ILM-VP的双层优化框架，用于改进视觉提示。上层任务是提示生成，下层任务是标签映射。目标是最小化预测标签和真实标签之间的交叉熵损失。视觉提示和标签映射相互交织，使用交替优化方法来解决问题。该方法在多个数据集和模型上与几种基线进行了比较。具体步骤如下：
1. 输入提示建模
2. 标签映射
3. 提示生成

#### 8. 实验设置：
本文在13个目标数据集上评估了提出的ILM-VP方法，包括Flowers102、DTD、UCF101、Food101、GTSRB、SVHN、EuroSAT、OxfordPets、StanfordCars、SUN397、CIFAR10/100和ABIDE。使用的源模型是在ImageNet-1K上预训练的ResNet-18和ResNet-50以及在Instagram上预训练的ResNeXt-101-32x8d。

#### 9. 实验结果和分析：
提出的ILM-VP方法在几乎所有数据-模型设置中都比其他VP基线表现更好，例如，在目标数据集Flowers102、CIFAR100和GTSRB上，相对于FLM-VP，分别提高了7.9％、6.7％和6.5％的准确率。此外，本文还提出了一种基于CLIP的VP方法，对于Flowers102和DTD数据集，分别提高了13.7％和7.1％的准确率。值得注意的是，与现有工作一致，微调模型通常比提示方法更有效。然而，在目标数据集ABIDE中，提示方法可以优于完整的模型微调方法（FF）。如果使用语言-视觉源模型，VP的准确性可以进一步提高。


# Paper:155     PC2：基于投影条件的点云扩散用于单张图像三维重建



#### 1. Title: 
PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction

#### 2. Authors: 
Luke Melas-Kyriazi, Christian Rupprecht, Andrea Vedaldi

#### 3. Affiliation: 
牛津大学视觉几何组

#### 4. Keywords: 
Single-image 3D reconstruction, point cloud, diffusion models, projection conditioning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Melas-Kyriazi_PC2_Projection-Conditioned_Point_Cloud_Diffusion_for_Single-Image_3D_Reconstruction_CVPR_2021_paper.html  Github: https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/

#### 6. Summary : 
- (1):本文研究单张RGB图像的三维重建问题，该问题是计算机视觉领域的一个长期难题。

- (2):过去的方法主要是通过多视角几何工具来重建物体的形状，但单视角下的问题非常棘手，需要对物体的可能形状和外观有先验了解。近年来，基于深度学习的端到端方法在该领域取得了一定进展，但仍然存在一些限制，如只能处理低分辨率的几何结构等。此外，一些最近的工作采用了隐式表示和辐射场等方法，但这些方法往往无法从单个输入图像中重建可能的三维形状分布。本文提出了一种新的方法，通过条件去噪扩散过程生成稀疏点云，从而实现单张RGB图像的三维重建。

- (3):本文提出了一种新的方法，通过条件去噪扩散过程生成稀疏点云，从而实现单张RGB图像的三维重建。该方法的关键在于一种称为投影条件的几何一致性条件：在扩散过程的每个步骤中，我们将局部图像特征投影到给定相机姿态下的部分去噪点云上。这种投影条件使我们能够生成与输入图像对齐的高分辨率稀疏几何体，并且还可以用于形状重建后的点颜色预测。此外，由于扩散过程的概率性质，我们的方法自然能够生成与单个输入图像一致的多个不同形状。与以往的工作相比，我们的方法不仅在合成基准测试上表现良好，而且在复杂的真实数据上也取得了大量的定性改进。

- (4):本文提出的方法在单张RGB图像的三维重建任务上取得了很好的性能。在ShapeNet基准测试上，该方法表现出色，同时在Co3D数据集上也取得了高质量的定性单视图重建结果。
#### 7. 方法详细介绍：
本文提出了一种称为PC2的投影条件点云扩散方法，用于单张图像的三维重建。该方法使用点-体素CNN处理点云并生成三维形状的概率分布。然后，对分布进行多次采样以生成多个三维形状预测。该方法还使用投影条件将输入图像纳入三维形状生成过程中。生成的三维形状随后使用对象掩码或预测之间的相互一致性进行过滤，以提高最终输出的质量。具体步骤包括：
1. 从三维高斯分布中随机采样一组点，作为初始点云。
2. 使用扩散模型对点云进行逐步去噪，生成可识别的三维物体形状。
3. 在扩散过程中，使用投影条件将局部图像特征投影到部分去噪的点云上。
4. 将每个点附加不同的神经特征，以确定其相对于输入相机和输入图像的空间位置。
5. 使用PC2-FM过滤方法，利用扩散的概率性质解决单视图三维重建问题的不适定性。

#### 8. 实验设置：
本文使用ShapeNet-R2N2和Co3D两个数据集进行评估。ShapeNet-R2N2数据集包含13个类别的三维CAD模型，而Co3D数据集包含常见类别的真实世界物体。本文使用F-score指标评估三维重建的质量。

#### 9. 实验结果和分析：
本文表明，所提出的PC2方法在ShapeNet-R2N2数据集上的性能与之前的方法相似，但在使用过滤器时优于之前的方法。该方法还在具有细节的对象类别（如“步枪”和“飞机”）上表现良好。本文还展示了Co3D数据集上的定性结果，证明PC2方法能够从任意视角生成逼真的三维形状。该方法还能够生成比之前的方法具有更高细节级别的形状。


# Paper:156     AsyFOD: 一种不对称自适应范式用于少样本领域自适应目标检测



#### 1. Title: 
AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection

#### 2. Authors: 
Yipeng Gao, Kun-Yu Lin, Junkai Yan, Yaowei Wang, Wei-Shi Zheng

#### 3. Affiliation: 
第一、三作者：中山大学计算机科学与工程学院，机器智能与先进计算教育部重点实验室，中国
第二作者：中山大学计算机科学与工程学院，中国
第四作者：深圳市鹏城实验室，中国

#### 4. Keywords: 
Few-shot domain adaptive object detection, asymmetric adaptation, data imbalance, target-similar source instances, asynchronous alignment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2021_paper.html  Github: https://github.com/Hlings/AsyFOD

#### 6. Summary : 
- (1):本文研究了少样本领域自适应目标检测（Few-Shot Domain Adaptive Object Detection，FSDAOD）问题，即在有充足的源域标注数据的情况下，只有少量目标域标注数据可用于训练。然而，目标域数据稀缺导致源域和目标域之间存在极端的数据不平衡，这可能导致传统特征对齐方法中的过度自适应问题。
- (2):传统的对称自适应方法（如MMD）容易导致过度自适应问题，即检测器集中在少量观察到的目标实例上，但很难在其他未观察到的实例上进行良好的泛化。现有的FSDAOD方法试图通过重复使用相同的目标样本来缓解不平衡问题，但忽略了源样本的作用。本文提出了一种新的不对称自适应范式，即AsyFOD，它从不同的角度利用源和目标实例来解决数据不平衡问题。
- (3):AsyFOD首先将源实例集分为两部分，即目标相似和目标不相似实例集。然后，通过使用目标分布估计，AsyFOD首先确定目标相似的源实例，这有助于增加有限的目标实例。然后，我们在目标不相似的源实例和增强的目标实例之间进行异步对齐，这是简单而有效的，可以缓解过度自适应问题。与传统方法不同，AsyFOD以不对称的方式对齐特征分布，并在优化检测器时对目标实例特征应用停梯度操作。这样，异步对齐可以更好地对齐未观察到的目标样本。本文的创新点在于提出了一种不对称自适应范式，从而解决了FSDAOD中的数据不平衡问题。
- (4):在四个FSDAOD基准测试中，AsyFOD的性能优于所有现有的方法，例如在Cityscapes-to-FoggyCityscapes上提高了3.1％的mAP，在Sim10k-to-Cityscapes上提高了2.9％的mAP。AsyFOD在各种领域差异的情况下都表现出色，例如背景变化、自然天气和合成到真实的情况。本文的方法在各种少样本领域自适应目标检测设置中都表现出色，包括弱或强增强的FSDAOD和Few-Shot Unsupervised Domain Adaptive
#### 7. 方法详细介绍：
AsyFOD是一种针对少样本领域自适应目标检测的非对称自适应范式。该方法包括源实例划分和非对称自适应两个主要方面。在源实例划分方面，通过估计目标分布并选择比例为beta的前几个源实例，获得目标相似和目标不相似实例集。在非对称自适应方面，提出了实例级异步分布对齐和面向任务的监督训练，以缓解数据偏差引起的干扰，并分别优化分类和定位任务。优化目标是最小化目标和源域之间的域差距。具体步骤包括：
1. 源实例划分：将源实例集划分为目标相似和目标不相似实例集。
2. 目标分布估计：通过目标分布估计，识别目标相似源实例，以增加有限的目标实例。
3. 异步对齐：在识别目标相似源实例后，将剩余的源实例视为目标不相似。为了进一步缓解域之间的数据不平衡，AsyFOD提出了目标不相似源实例和增强的目标实例之间的异步对齐。AsyFOD以非对称方式对齐特征分布，在优化检测器时对目标实例特征应用了停梯度操作。
4. 面向任务的监督训练：AsyFOD分别对分类和定位任务进行面向任务的监督训练。

#### 8. 实验设置：
本文在四个不同的场景下进行了实验：Cityscapes→Foggy Cityscapes、SIM10k→Cityscapes、ViPeD→COCO和KITTI→Cityscapes。基线检测器为YOLOv5，与无监督领域自适应目标检测（UDAOD）和少样本领域自适应目标检测（FSDAOD）方法进行比较。对于FSDAOD设置，我们随机选择ViPeD→COCO/其他三个场景的60/8个完全标记的目标图像。默认使用强数据增强以进行公平比较。报告IoU阈值为0.5的平均精度。

#### 9. 实验结果和分析：
本文提出的AsyFOD在各种场景下解决少样本领域自适应目标检测（FSDAOD）任务的主要结果如下。AsyFOD在单类和多类方面的AP50和mAP50指标上均优于现有方法。与仅使用源域数据的模型相比，适应后的检测器性能提高。在Cityscapes→Foggy Cityscapes场景中，AsyFOD的mAP50指标为44.3%，比之前的最新方法AcroFOD高3.2%。在合成到真实和跨摄像头场景中，AsyFOD也优于之前的方法。此外，AsyFOD在ViPeD→COCO场景中比预训练再微调范式（Pre+FT）提高了约3.8%的AP50和2.1%的AP。模型在三个场景中的性能提升表明，AsyFOD可以在非常有限的目标数据下缓解域差异。


# Paper:157     MoStGAN-V: 基于时间运动风格的视频生成



#### 1. Title: 
MoStGAN-V: Video Generation with Temporal Motion Styles

#### 2. Authors: 
Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny

#### 3. Affiliation: 
King Abdullah University of Science and Technology (KAUST)（沙特阿拉伯国王阿卜杜拉科技大学）

#### 4. Keywords: 
Video generation, motion styles, attention mechanism, style-based models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shen_MoStGAN-V_Video_Generation_With_Temporal_Motion_Styles_CVPR_2021_paper.html  Github: https://github.com/xiaoqian-shen/MoStGAN-V

#### 6. Summary : 
- (1):本文研究的是视频生成任务，视频生成是一个具有时空复杂性的任务，需要合成具有时序一致性的多样化运动。以往的方法要么采用自回归方式，要么将时间视为连续信号，但它们往往难以合成具有时序一致性的多样化运动，并且在几个时间步骤后往往会生成重复的场景。因此，本文提出了一种新的方法，即引入额外的时间依赖运动风格来模拟多样化的运动模式，并提出了一种运动风格注意力调节机制，以增强每个特定尺度（即层）的帧的动态性。
- (2):以往的方法要么采用自回归方式，要么将时间视为连续信号，但它们往往难以合成具有时序一致性的多样化运动，并且在几个时间步骤后往往会生成重复的场景。本文提出了一种新的方法，即引入额外的时间依赖运动风格来模拟多样化的运动模式，并提出了一种运动风格注意力调节机制，以增强每个特定尺度（即层）的帧的动态性。这种方法与以往的方法不同，以往的方法要么预测图像生成器的潜在运动轨迹，要么生成连续的运动代码并将它们与常量向量连接以作为生成器网络的初始特征输入。相比之下，本文提出的运动风格和MoStAtt模块旨在调节核权重，而不是作为特征输入。
- (3):本文提出了一种新的方法，即引入额外的时间依赖运动风格来模拟多样化的运动模式，并提出了一种运动风格注意力调节机制，以增强每个特定尺度（即层）的帧的动态性。本文的贡献主要有两个方面：一是引入时间依赖的运动风格来模拟多样化的运动模式，以增强时间感知性和增强运动合成；二是提出了MoStAtt机制，它可以在每个合成层中为每个运动风格分配注意力分数，以便对权重进行调节，从而更好地表示时间依赖和多样化的运动。
- (4):本文的方法在无条件视频生成任务上取得了最先进的性能，并提高了动态运动的质量。实验结果表明，本文的方法在仅使用每个视频剪辑的3帧进行训练时，能够以每帧3.12毫秒的速度生成视频。
#### 1. 方法介绍：
本文提出了一种名为MoStGAN-V的视频生成方法，该方法引入了时间相关的运动风格来模拟视频中的运动模式，并使用运动风格注意力机制来动态分配不同的运动风格。分配的运动风格用于调制目标合成层中的反卷积滤波器权重，从而实现多样化的运动合成。

#### 2. 实验设置：
本文使用了四个无条件的256x256视频合成基准数据集进行评估，每个视频片段仅使用3帧进行训练。实验在单个NVIDIA Tesla V100 GPU上进行，内存为32GB。批量大小设置为16，使用Adam优化器，学习率为0.002。训练过程在100万次迭代后停止。

#### 3. 实验结果与分析：
MoStGAN-V方法在所有数据集上均取得了新的FVD16和FVD128得分的最佳性能。我们的MoStGAN-V方法成功地通过提出的MoStAtt机制，在运动多样性RainbowJelly数据集上实现了最佳性能。人类评估结果也表明，我们的MoStGAN-V模型在所有评估数据集上的视频生成质量都显著优于运动多样性和一致性方面。


# Paper:158     利用隐式形状和流学习完成人体形状



#### 1. Title: 
Human Body Shape Completion with Implicit Shape and Flow Learning

#### 2. Authors: 
Boyao Zhou, Di Meng, Jean-Sébastien Franco, Edmond Boyer

#### 3. Affiliation: 
Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France (法国格勒诺布尔阿尔卑斯大学)

#### 4. Keywords: 
Shape completion, motion flow, implicit representation, attention mechanism, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究如何通过结合形状和流估计来完成人体模型的形状。形状完成是计算机视觉中的一个具有挑战性的任务，当考虑到部分深度观测时，它高度不受限制。 
- (2):过去的方法包括基于模型的策略和基于学习的方法。前者利用强先验，但难以保留精细的几何细节，后者则建立在较弱的假设上，并可以从有效的隐式表示中受益。本文采用这种表示，并探索了两个连续帧之间的运动流如何有助于形状完成任务。为了有效地利用流信息，我们的架构结合了两种估计，并实现了两个特征以提高鲁棒性：第一，全对全的注意力模块，编码同一帧内和不同帧之间的点之间的相关性；第二，从粗到密到细疏的策略，平衡了表示能力和计算成本。 
- (3):本文提出了一种学习方法，同时考虑两个连续深度图像作为输入，并估计形状占用（SDF）和运动的连续完整表示作为隐式函数，利用它们分别独立解决的表示优势。我们的方法是金字塔形的，考虑以粗到细的方式提取图像特征，同时保留局部和更全局的形状属性。此外，为了在空间和时间域中强制执行一致性，我们从场景流工作中汲取灵感，并引入了一个全对全的注意机制，考虑两个帧中点之间的空间和时间相关性。综合消融测试证明了金字塔框架和注意机制的个体贡献。在DFAUST和CAPE上进行了实验，包括裸体和穿着衣服的人类。我们提供了与最先进的方法的比较，证明了我们的方法在标准数据集上的形状完成性能优于现有方法。 
- (4):本文的方法在人体模型的形状完成任务上取得了良好的性能，证明了结合形状和流估计的优势。本文提出的方法在两个数据集上进行了测试，包括不同的人体形状、姿势和服装。实验结果表明，我们的方法在形状完成任务上优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种基于学习的方法，通过结合形状和流估计来完成人体模型的形状补全。该方法采用了全隐式SDF形状和流表示，该表示已被证明在详细和连续推断中非常成功。该方法还使用了类似U-Net的特征编码器，具有两个特征级别，一个是粗糙的，一个是细致的，用于深度特征编码。粗糙密集流模块旨在准备一个密集的3D表示，该表示可用于计算流估计的经典成本体积。该方法还使用了注意力技术，以增强模型的全局知识，例如形状和流推断中的对称性。

具体步骤如下：
1. 采用3D卷积网络对源场景和目标场景进行编码。
2. 使用差异成本计算来计算点对点的流嵌入。
3. 使用细-稀疏模块将信息传播到接收域内的未占用区域。
4. 使用自注意力和交叉注意力模块，以确保利用更长范围内的相似性或对称性。

#### 8. 实验设置：
本文使用了DFAUST和CAPE数据集的真实扫描数据，分别用于未穿着和穿着人类的训练，并在动态序列中以固定视点渲染分辨率为2562的深度图像。本文使用交并比（IoU）、Chamfer距离和端点误差（EPE）指标来评估所提出方法的性能。

#### 9. 实验结果与分析：
本文提出的方法结合了隐式人体形状补全和隐式运动流估计，取得了更精确的重建结果，超越了其他方法在两个标准真实人体数据集上的表现。但是，该方法对于具有挑战性的姿势可能会使未观察到的部分嘈杂，而强烈的遮挡仍然是该方法的挑战。


# Paper:159     从大型语言模型中学习视频表示



#### 1. Title: 
Learning Video Representations from Large Language Models

#### 2. Authors: 
Yue Zhao, Ishan Misra, Philipp Krähenbühl, Rohit Girdhar

#### 3. Affiliation: 
Yue Zhao: University of Texas, Austin (奥斯汀德克萨斯大学)

#### 4. Keywords: 
Video-language representation, Large Language Models, pre-training, contrastive learning, automatic video narrators

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2021_paper.html
Github: https://github.com/facebookresearch/LaViLa

#### 6. Summary: 
- (1):本文的研究背景是视频理解领域中，由于视频-文本数据集规模较小，导致视频-文本表示学习的性能受到限制。
- (2):过去的方法主要是使用人工标注的文本数据集进行视频-文本表示学习，但这种方法存在数据集规模小、文本与视频不匹配等问题。本文提出了一种新方法，利用大型语言模型生成自动视频叙述，并使用这些叙述进行视频-文本表示学习，从而解决了数据集规模小、文本与视频不匹配等问题。
- (3):本文提出的方法是利用预训练的大型语言模型生成自动视频叙述，并使用这些叙述进行视频-文本表示学习。实验结果表明，本文提出的方法在多个视频理解任务上均取得了最新的性能表现，包括零样本分类、多实例检索和动作识别等任务。
- (4):本文提出的方法在多个视频理解任务上均取得了最新的性能表现，包括EGTEA分类、Epic-Kitchens-100多实例检索、Ego4D多项选择问答、UCF-101和HMDB-51动作识别等任务。实验结果表明，本文提出的方法在零样本和有监督学习两种情况下均取得了优异的性能表现，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为LAVILA的方法，利用大型语言模型（LLMs）作为监督信号，训练双编码器模型进行视频-语言预训练。LAVILA使用两个LLMs：NARRATOR和REPHRASER。NARRATOR是一个视觉条件的LLM，用于为现有和新的视频片段生成伪标签，生成新的注释。REPHRASER是一个标准的LLM，用于重新表述现有片段中的注释，增强这些注释。双编码器在所有这些注释上进行训练。NARRATOR的设计紧密遵循标准LLMs的架构，只添加了一些额外的交叉注意力模块以提供视觉条件。在推理时，通过提供视觉输入加上一个特殊的<s>开始标记，查询NARRATOR，使用核心采样递归地对生成的文本进行采样，直到达到</s>结束标记。
#### 8. 实验设置：
本文在多个数据集和任务上评估了LAVILA方法，包括EK-100 MIR、Ego4D、EGTEA和CharadesEgo数据集上的动作识别和问答任务。本文还研究了不同设置对所提出方法性能的影响，包括使用不同的语言模型进行文本解码器、不同的采样方法进行文本生成以及双编码器模型中视觉编码器的缩放效应。实验设置在论文中有详细描述。
#### 9. 实验结果和分析：
本文提出的LAVILA方法在各个数据集和任务上均取得了优异的表现，超过了之前的最优方法。在EK-100 MIR、Ego4D、EGTEA和CharadesEgo数据集上，LAVILA方法在动作识别和问答任务上均取得了最佳结果。此外，LAVILA在半监督设置下也表现出良好的效果。实验结果表明，利用大型语言模型进行视频表示学习是一种有效的方法。


# Paper:160     RobustNeRF：使用鲁棒损失忽略干扰物



#### 1. Title: 
RobustNeRF: Ignoring Distractors with Robust Losses

#### 2. Authors: 
Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David J. Fleet, Andrea Tagliasacchi

#### 3. Affiliation: 
Google Research, Brain Team (谷歌研究院)

#### 4. Keywords: 
Neural Radiance Fields, distractors, robust estimation, photometric consistency, view-dependent effects

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sabour_RobustNeRF_Ignoring_Distractors_With_Robust_Losses_CVPR_2021_paper.html  Github: https://github.com/RobustNeRF/RobustNeRF

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）模型在处理静态场景时，对于场景中的干扰物（如移动物体、光照变化、阴影等）的处理问题。
- (2):过去的方法包括使用预训练的语义分割模型去除已知类别的干扰物、将干扰物建模为每个图像的瞬态现象、将场景分解为静态和动态部分等，但这些方法都存在一定的问题。本文提出了一种鲁棒估计的方法，将干扰物建模为优化问题中的异常值，从而成功地去除了干扰物，提高了重建质量。
- (3):本文提出的方法是将干扰物建模为优化问题中的异常值，使用鲁棒估计的方法去除干扰物，从而提高了重建质量。该方法不需要先验知识，且易于在现代NeRF框架中实现，具有较少的超参数。本文的创新点在于将干扰物建模为优化问题中的异常值，从而提高了重建质量。
- (4):本文在合成和真实场景上进行了实验，结果表明本文提出的方法可以成功地去除干扰物，提高了重建质量，且性能优于现有方法。本文的方法可以支持NeRF模型在处理静态场景时更好地处理干扰物的问题。
#### 7. 方法详细介绍：
本文提出了一种名为RobustNeRF的方法，用于在存在干扰物的情况下训练NeRF模型。该方法使用鲁棒估计来建模干扰物，并将其作为优化中的异常值。该方法不需要先验知识，可以适用于各种类型的干扰物。NeRF模型通过最小化L2光度重建损失来进行训练。RobustNeRF在优化问题中添加了一个鲁棒损失项，用于建模干扰物。该方法易于在现代NeRF框架中使用，并且需要最少或无超参数调整。本文还提出了一种使用修剪最小二乘损失函数的RobustNeRF模型拟合方法，该方法通过迭代重新加权最小二乘优化来实现，使用二元权重函数来适应模型拟合，以捕获NeRF优化的归纳偏差。该权重函数基于修剪估计器，并通过假设异常值占据图像的大连通区域来捕获典型异常值的结构性质。该权重函数还通过使用3x3盒状核扩散内点/外点标签来捕获异常值的空间平滑性。该方法提高了模型拟合的鲁棒性，并允许更快地学习细节。

#### 8. 实验设置：
本文引入了自然场景和合成场景来评估RobustNeRF在忽略干扰物的同时准确重建场景的能力。自然场景是在街上、公寓和机器人实验室中拍摄的。合成场景是使用Kubric数据集生成器生成的。本文还将RobustNeRF与优化了不同损失函数的mip-NeRF 360变体和最近的D2NeRF进行了比较。评估基于在带有干扰物的帧上训练模型，并在无干扰物的帧上进行评估。本文报告了三个指标，分别是PSNR、SSIM和LPIPS。

#### 9. 实验结果与分析：
本文的RobustNeRF方法在一系列合成数据集、常见基准数据集和机器人捕获的新数据集上进行了评估，以比较其与先前方法的性能。实验结果表明，RobustNeRF在定性和定量上均优于最近的先进方法。本文还探讨了RobustNeRF对数据集中杂波数量的敏感性，并发现它对干扰物的存在非常鲁棒。然而，RobustNeRF中使用的损失函数存在一定的统计效率问题，导致在干净数据上的重建效果稍差，并且通常需要更长的训练时间。未来的工作将考虑非常小的干扰物、适应用于异常值/内点决策的空间支持的自适应、学习神经权重函数以及在其他NeRF框架中包含鲁棒损失。


# Paper:161     跨越鸿沟：面向图像字幕的域泛化



#### 1. Title: 
Crossing the Gap: Domain Generalization for Image Captioning

#### 2. Authors: 
Yuchen Ren, Zhendong Mao, Shancheng Fang, Yan Lu, Tong He, Hao Du, Yongdong Zhang, Wanli Ouyang

#### 3. Affiliation: 
Yuchen Ren, Zhendong Mao, and Yongdong Zhang are affiliated with the University of Science and Technology of China, Hefei, China.

#### 4. Keywords: 
Image captioning, domain generalization, benchmark dataset, semantic metric learning, multi-source domain.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1): This paper focuses on the problem of domain generalization for image captioning, where the data from the target domain is unseen in the learning process. The authors propose a new benchmark dataset for this task and analyze the limitations of existing methods for unseen domains.
 
- (2): Existing image captioning methods assume that the training and testing data are from the same domain or that the data from the target domain are accessible. However, this assumption is invalid in real-world applications where the data from the target domain is inaccessible. The authors propose a new framework called language-guided semantic metric learning (LSML) to tackle the challenges of lack of semantic information and contextual information utilization in the learning process. 

- (3): The authors propose a new benchmark setting called Domain Generalization for Image Captioning (DGIC) with multi-source domain and cross-dataset setting. They employ existing popular datasets from five domains and divide these domains into two parts: multiple source domains for training and a target domain for testing. The LSML framework uses both inter- and intra-domain metric learning to help the model better learn discriminative semantic information among different instances. They also introduce a language guidance strategy in the learning process to utilize the rich contextual information in the image and labels during the learning process.

- (4): The LSML framework achieves promising performance under the DGIC setting, outperforming previous state-of-the-art methods by a large margin. The performance supports their goals of exploring a method with good generalization ability for unseen domains.
#### 7. 方法详细介绍：
本文提出了一种基于语言引导的语义度量学习（LSML）框架，用于图像字幕的领域泛化。该框架包括以下步骤：
(1). 细粒度文本引导对齐：通过对齐多个域中的文本描述，学习域无关的视觉组件。
(2). 好-难负样本挖掘：通过挖掘好-难负样本，提高模型的泛化能力。
(3). 内域度量学习：通过学习内域度量，提高模型的鲁棒性。
(4). 外域度量学习：通过学习外域度量，提高模型的泛化能力。
(5). 内外域度量学习的平衡：通过平衡内外域度量学习的权重，提高模型的性能。

#### 8. 实验设置：
本文使用了一个新的基准设置，称为图像字幕的领域泛化（DGIC），包括多源域和跨数据集设置。作者使用了来自五个领域的现有流行数据集：MSCOCO、Vizwiz、Flickr30k、CUB-200和Oxford-102。这些域被分为两部分：用于训练的多个源域和用于测试的目标域，模拟了未见过的域的情况，并从多个数据集中挖掘潜在的模式。DGIC设置与其他图像字幕设置的区别总结在表1中。

#### 9. 实验结果和分析：
本文使用了完整的字幕评估指标，包括BLEU、METEOR、ROUGE、CIDEr和SPICE，评估了所提出的LSML框架。实验结果表明，LSML方法在CIDEr和SPICE上的表现优于先前的方法，如在MSCOCO目标域设置下，CIDEr提高了10.85％，SPICE提高了3.56％。作者还对损失函数组件和指导组件进行了比较，并对内存库和动量更新进行了消融分析。实验结果表明，文本引导比图像引导更有效地促进了多个域中的区分性三元组的挖掘。作者观察到，视觉词引导通过细粒度对齐具有最佳结果。


# Paper:162     超越外观：一种语义可控的自监督学习框架，用于人类中心视觉任务



#### 1. Title: 
Beyond Appearance: a Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks

#### 2. Authors: 
Weihua Chen, Xianzhe Xu, Jian Jia, Hao luo, Yaohua Wang, Fan Wang, Rong Jin, Xiuyu Sun

#### 3. Affiliation: 
阿里巴巴集团

#### 4. Keywords: 
Self-supervised learning, human-centric visual tasks, semantic information, semantic controller, downstream tasks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Beyond_Appearance_A_Semantic_Controllable_Self-Supervised_Learning_Framework_for_Human-Centric_CVPR_2021_paper.html  Github: https://github.com/tinyvision/SOLIDER

#### 6. Summary : 
- (1):本文旨在从大量未标记的人类图像中学习出一种通用的人类表示，以最大程度地受益于下游的人类中心任务。 
- (2):现有的自监督学习方法在学习表示时，往往只考虑了视觉外观信息，缺乏语义信息的引入。本文提出了一种语义可控的自监督学习框架SOLIDER，利用人类图像的先验知识构建伪语义标签，将更多的语义信息引入到学习到的表示中。同时，本文注意到不同的下游任务需要不同比例的语义信息和外观信息，因此引入了一个带有语义控制器的条件网络，用户可以向控制器发送值，以产生具有不同比例语义信息的表示，以适应下游任务的不同需求。 
- (3):本文提出的SOLIDER框架具有以下创新点：1）利用人类先验知识生成伪语义标签，训练出更强的人类语义表示；2）设计了一个语义可控的自监督学习框架，可以生成具有不同比例语义信息的表示，以满足下游任务的不同需求；3）在六个下游人类中心任务上验证了SOLIDER表示的有效性，超过了现有技术并建立了新的基线。 
- (4):本文在六个下游人类中心任务上验证了SOLIDER表示的有效性，包括人物再识别、属性识别、人物搜索、行人检测、人体解析和姿态估计。实验结果表明，SOLIDER表示在这些任务上均取得了优异的性能，证明了其在人类中心任务中的潜力。
#### 7. 方法详细介绍：
本文提出了一种名为SOLIDER的语义可控自监督学习框架，包括三个主要步骤：预训练、微调和控制。预训练阶段包括三个子任务：语义分类、背景前景聚类和遮挡图像建模。语义分类任务使用人类先验知识生成伪语义标签，用于监督令牌级别的语义分类预训练任务。背景前景聚类任务用于处理背景碎片引起的噪声干扰。遮挡图像建模任务提供更多的语义信息。微调阶段使用预训练模型在下游任务上进行微调。控制阶段使用语义控制器调整预训练模型以适应下游任务的需求。

#### 8. 实验设置：
本文在六个人类中心视觉任务上验证了SOLIDER的性能，包括人物重识别、属性识别、人物搜索、行人检测、人体解析和姿态估计。预训练任务使用LUPerson数据集，每个下游任务使用其常用数据集，如Market1501、MSMT17、PETAzs、RAPzs、PA100k、CUHK-SYSU、PRW、CityPerson、LIP和COCO姿态估计。

#### 9. 实验结果和分析：
本文提出的SOLIDER方法在六个人类中心视觉任务上均取得了优于其他最先进方法的性能，证明了该方法的有效性。此外，随着Swin-Transformer骨干网络规模的增加，性能进一步提高。


# Paper:163     关于引导扩散模型的蒸馏



#### 1. Title: 
On Distillation of Guided Diffusion Models

#### 2. Authors: 
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans

#### 3. Affiliation: 
第一作者：斯坦福大学

#### 4. Keywords: 
Diffusion models, distillation, classiﬁer-free guidance, image generation, text-guided image editing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了基于扩散模型的引导模型的蒸馏问题，旨在提高模型的采样效率。
- (2):过去的方法需要评估两个扩散模型，计算代价较高。本文提出了一种两阶段的蒸馏方法，通过学习一个单一的模型来匹配条件和无条件模型的输出，并逐步蒸馏该模型以减少采样步骤。实验结果表明，该方法在ImageNet 64x64和CIFAR-10上使用4个采样步骤即可生成与原始模型视觉上相当的图像，并且在采样速度上比原始模型快256倍。对于在潜在空间上训练的扩散模型，本文的方法能够使用1到4个去噪步骤生成高保真图像，比现有方法快至少10倍。本文还将该方法应用于文本引导图像修复和编辑任务，证明了该方法在样式转移和图像编辑应用中的潜力。
- (3):本文提出了一种两阶段的蒸馏方法，通过学习一个单一的模型来匹配条件和无条件模型的输出，并逐步蒸馏该模型以减少采样步骤。该方法不仅适用于在像素空间上训练的标准扩散模型，还适用于在自编码器的潜在空间上训练的扩散模型。本文的方法能够使用更少的采样步骤生成与原始模型视觉上相当的图像，并且在采样速度上比原始模型快很多倍。
- (4):本文的方法在ImageNet 64x64和CIFAR-10上使用4个采样步骤即可生成与原始模型视觉上相当的图像，并且在采样速度上比原始模型快256倍。对于在潜在空间上训练的扩散模型，本文的方法能够使用1到4个去噪步骤生成高保真图像，比现有方法快至少10倍。本文还将该方法应用于文本引导图像修复和编辑任务，证明了该方法在样式转移和图像编辑应用中的潜力。
#### 7. 方法详细介绍：
本文提出了一种两阶段蒸馏方法来改进有导向扩散模型的采样效率。第一阶段引入了一个学生模型，以匹配任何时间步的教师模型的输出。学生模型可以是连续或离散时间的，具体取决于教师模型。蒸馏模型是在由“引导强度”参数控制的引导间隔条件下进行的。第二阶段通过每次减半采样步骤的数量，逐步将第一阶段学习到的模型蒸馏成更少步骤的学生模型。使用U-Net模型的模型架构类似于像素空间扩散模型和潜空间扩散模型中使用的模型。在补充材料中提供了详细的训练算法。

#### 8. 实验设置：
实验主要集中在ImageNet 64x64和CIFAR-10数据集上进行，探索了不同范围的引导权重。U-Net架构用于基线模型，v-prediction模型用于两个数据集。教师模型是一个1024x2步的DDIM模型，其中条件和无条件组件都使用1024个DDIM去噪步骤。

#### 9. 实验结果和分析：
本文提出的蒸馏模型能够使用比原始模型少得多的采样步骤（高达16倍）生成高质量的图像样本，同时实现类似或更好的FID分数。该方法还应用于预训练的图像修复潜空间扩散模型，展示了该方法在快速、实际的图像编辑应用中的潜力。在图像样式转移方面，探索了引导强度w对性能的影响。在ImageNet 64x64的像素空间模型上进行的样式转移比较显示在Fig. 12中，其中提出的方法实现了更真实的输出。在ImageNet 256x256和LAION数据集上，该方法能够加速推理至少10倍，使用1到4个去噪步骤生成高质量的图像。


# Paper:164     联合外观和运动学习的高效滚动快门矫正



#### 1. Title: 
Joint Appearance and Motion Learning for Efficient Rolling Shutter Correction

#### 2. Authors: 
Bin Fan, Yuxin Mao, Yuchao Dai, Zhexiong Wan, Qi Liu

#### 3. Affiliation: 
西北工业大学电子信息学院

#### 4. Keywords: 
Rolling shutter correction, deep learning, encoder-decoder network, motion estimation, data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Joint_Appearance_and_Motion_Learning_for_Efficient_Rolling_Shutter_Correction_CVPR_2021_paper.html  Github: https://github.com/GitCVfb/JAMNet

#### 6. Summary : 
- (1): 本文研究的是滚动快门矫正（Rolling Shutter Correction，RSC）问题，该问题是由于CMOS传感器的逐行读取机制导致的图像畸变。RSC的目标是恢复出相邻两帧之间的全局快门（Global Shutter，GS）图像，以消除滚动快门效应带来的几何失真。
 
- (2): 传统的RSC方法通常依赖于手工设计的先验假设、几何约束和复杂的优化框架，时间复杂度高，不适用于实际应用。近年来，基于深度学习的RSC方法取得了很大进展，但现有的方法通常采用两阶段网络结构，忽略了内在信息交互，限制了高质量RSC的实现。本文提出了一种单阶段编码器-解码器网络，名为JAMNet，用于高效的RSC。该方法首先从连续的RS输入中提取金字塔特征，然后同时细化两种互补信息（即全局快门外观和无畸变运动场），以实现联合学习解码器中的相互促进。为了注入足够的运动线索以指导联合学习，我们引入了基于Transformer的运动嵌入模块，并建议在金字塔级别之间传递隐藏状态。此外，我们提出了一种新的数据增强策略“垂直翻转+反序”，以释放RSC数据集的潜力。 

- (3): 本文提出了一种新的单阶段解决方案，通过联合外观和运动学习（JAMNet）来进行RSC。该方法是一种单编码器-解码器结构，具有从粗到细的细化，允许同时学习互补的GS外观和无畸变运动信息。在提取分层金字塔特征后，我们为同时遮挡推断和上下文聚合设计了一个高效的解码器。它利用一个变形分支来估计无畸变场以补偿RS几何失真，而使用合成分支来逐步细化GS外观，形成互补信息的相互促进。其中，通过维护隐藏状态在金字塔级别之间传递附加线索。此外，我们还提出了一种新的数据增强策略，即垂直翻转+反序，以增强RSC模型的鲁棒性。 

- (4): 本文在各种基准测试中进行了实验，结果表明我们的方法在真实世界的RSC应用中实现了4.7 dB的PSNR提高，显著优于现有的RSC方法，特别是在实时推理和更小的参数方面。
#### 7. 方法详细介绍：
本文提出了一种单阶段滚动快门校正（RSC）架构，称为联合外观和运动学习网络（JAMNet）。JAMNet由双边去畸变模块、联合外观和运动学习模块和多尺度合成模块组成。双边去畸变模块用于消除滚动快门效应引起的几何畸变。联合外观和运动学习模块旨在同时学习RS帧的外观和运动信息。多尺度合成模块用于从扭曲的RS帧中合成GS帧。JAMNet使用Adam优化器进行端到端训练，学习率为10^-4，批量大小为8。超参数设置为{λr，λmc，λtv} = {100，100，0.1}。GT GS图像被下采样以产生多尺度监督信号。JAMNet可以实时处理连续的RS图像并输出高保真度的GS帧，同时保持紧凑和轻量级的网络设计。

#### 8. 实验设置：
本文在四个基准数据集上评估了所提出的JAMNet，包括带遮挡掩码的Carla-RS（CRM）、不带遮挡掩码的Carla-RS（CR）、Fastec-RS（FR）和BS-RSC。JAMNet使用PyTorch实现，并在单个NVIDIA RTX 3090 GPU上执行。使用标准的PSNR、SSIM和LPIPS指标来呈现定量结果。

#### 9. 实验结果和分析：
所提出的JAMNet始终实现了出色的RSC性能，大幅优于现有的RSC基线。JAMNet可以实时处理连续的RS图像并输出高保真度的GS帧，同时保持紧凑和轻量级的网络设计。JAMNet在实际RS效果去除中的PSNR方面超过SOTA RSC方法4.7 dB。定性结果表明，JAMNet可以有效地恢复带有更少伪影的清晰和令人愉悦的GS图像。所提出的数据增强策略，即垂直翻转+反序，可以有效地提高所得到的RSC模型的性能。


# Paper:165     基于深度图的空间一致性网络用于非刚性点云配准



#### 1. Title: 
Deep Graph-based Spatial Consistency for Robust Non-rigid Point Cloud Registration

#### 2. Authors: 
Zheng Qin, Hao Yu, Changjian Wang, Yuxing Peng, Kai Xu

#### 3. Affiliation: 
National University of Defense Technology (国防科技大学)

#### 4. Keywords: 
Point cloud registration, non-rigid, outlier rejection, spatial consistency, graph-based

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qin_Deep_Graph-Based_Spatial_Consistency_for_Robust_Non-Rigid_Point_Cloud_Registration_CVPR_2021_paper.html  Github: https://github.com/qinzheng93/GraphSCNet

#### 6. Summary : 
- (1):本文研究非刚性点云配准中的异常对应点剔除问题。在刚性配准中，空间一致性是区分内点和外点的常用标准。然而，在非刚性情况下，空间一致性不再适用，因此非刚性配准的异常值剔除问题尚未得到很好的研究。
- (2):现有的异常值剔除方法大多集中在刚性配准上，而对于非刚性配准，由于变形复杂，异常值剔除仍然是一个具有挑战性的问题。本文提出了一种名为GraphSCNet的基于图的空间一致性网络，用于非刚性配准的异常值剔除。该方法基于非刚性变形通常是局部刚性或局部形状保持的事实，首先设计了一个局部空间一致性度量，用于评估图节点附近的对应点之间的空间兼容性。然后，提出了一种基于注意力机制的非刚性对应嵌入模块，用于从局部空间一致性中学习非刚性对应的鲁棒表示。GraphSCNet可以有效地剪除非刚性配准中的异常值，同时保留尽可能多的内点。该方法在三个具有挑战性的基准测试中取得了最先进的性能。
- (3):本文提出了一种基于图的空间一致性网络，用于非刚性点云配准的异常值剔除。该方法基于非刚性变形通常是局部刚性或局部形状保持的事实，首先设计了一个局部空间一致性度量，用于评估图节点附近的对应点之间的空间兼容性。然后，提出了一种基于注意力机制的非刚性对应嵌入模块，用于从局部空间一致性中学习非刚性对应的鲁棒表示。该方法是第一个针对非刚性点云配准的学习异常值剔除方法。
- (4):本文的方法在三个具有挑战性的基准测试中取得了最先进的性能。GraphSCNet在4DMatch基准测试中，对于高重叠和低重叠情况下的AccS和AccR，比最近的最先进方法NDP高出10%以上。
#### 7. 方法详细介绍：
本文提出了一种基于图的空间一致性网络(Graph-based Spatial Consistency Network, GraphSCNet)用于非刚性点云配准。该方法包括三个主要步骤：基于图的局部空间一致性、非刚性异常值剔除网络和非刚性点云配准。首先，通过对源点云构建变形图，使用均匀最远点采样从点云中采样一组节点，并将对应关系分配给其k个最近节点，然后基于分配给共同节点的对应关系对之间的距离计算局部空间一致性。其次，设计了一个基于注意力机制的非刚性对应关系嵌入模块，用于从局部空间一致性中学习非刚性对应关系的鲁棒表示。最后，使用内点/外点分类头预测每个对应关系的置信度分数，并使用N-ICP算法估计变形函数。

#### 8. 实验设置：
本文在三个基准数据集上进行了实验评估：4DMatch、CAPE和DeepDeform。其中，4DMatch数据集是一个用于非刚性点云配准的合成基准数据集，而CAPE和DeepDeform数据集是具有非刚性变形的真实世界数据集。使用四个指标评估性能：3D端点误差(EPE)、3D严格准确度(AccS)、3D宽松准确度(AccR)和异常值比率(OR)。将该方法与几种最先进的非刚性配准和场景流估计方法进行比较，包括NSFP、Nerfies、PointPWC-Net、FLOT、DGFM、SyNoRiM和NDP。

#### 9. 实验结果与分析：
在4DMatch数据集上，所提出的GraphSCNet方法的性能优于所有比较方法，EPE为0.042，AccS为70.1%。在CAPE和DeepDeform数据集上，该方法也实现了最先进的性能。消融研究表明，该方法的性能主要归因于空间一致性估计组件。该方法的精度和召回率也优于其他方法，特别是在低重叠情况下。定量结果表明，该方法成功地恢复了非重叠区域的几何形状。


# Paper:166     ObjectMatch: 利用规范化对象对应关系实现鲁棒的配准



#### 1. Title: 
ObjectMatch: Robust Registration using Canonical Object Correspondences

#### 2. Authors: 
Can Gümeli, Angela Dai, Matthias Nießner

#### 3. Affiliation: 
Technical University of Munich（慕尼黑工业大学）

#### 4. Keywords: 
RGB-D SLAM, camera pose estimation, object correspondence, semantic object identification, canonical object correspondences

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gumeli_ObjectMatch_Robust_Registration_Using_Canonical_Object_Correspondences_CVPR_2021_paper.html  Github: https://github.com/cangumeli/ObjectMatch

#### 6. Summary : 
- (1):本文研究的是RGB-D SLAM中的相机位姿估计问题，针对现有方法在处理无重叠区域时存在的问题，提出了一种基于语义对象识别的相机位姿估计方法，通过预测规范化对象坐标来建立间接对应关系，从而实现对无重叠区域的相机帧对齐。

- (2):现有的相机位姿估计方法主要依赖于重叠区域的特征匹配，无法处理无重叠区域的相机帧对齐问题。本文提出的方法通过语义对象识别建立间接对应关系，从而实现对无重叠区域的相机帧对齐。与现有的SLAM方法相比，本文的方法具有更强的鲁棒性。

- (3):本文提出了一种基于语义对象识别的相机位姿估计方法，通过预测规范化对象坐标来建立间接对应关系，从而实现对无重叠区域的相机帧对齐。本文的方法在关键点匹配方面采用了SuperGlue算法，并将其与规范化对象坐标相结合，通过联合高斯牛顿优化实现相机和对象位姿的全局优化。本文的方法在处理无重叠区域的相机帧对齐问题上具有很好的效果。

- (4):本文的方法在处理无重叠区域的相机帧对齐问题上具有很好的效果，可以显著提高相机位姿估计的鲁棒性。在处理ScanNet数据集中的图像对时，本文的方法将重叠区域小于10%的图像对的位姿回归率从24%提高到45%。在处理RGB-D序列的相机位姿估计问题时，本文的方法在多个场景中均取得了很好的效果，可以将轨迹误差降低35%以上。
#### 7. 方法详细介绍：
本文提出了一种名为ObjectMatch的方法，用于使用规范对象对应关系进行鲁棒的配准。该方法使用最先进的全局姿态图优化进行成对帧配准，具有单个层次结构以简化操作。通过使用嵌入距离阈值为0.05的顶部1匹配对象，在帧对之间建立对象对应关系。该方法可以通过对象跟踪扩展到多帧优化，并分成多个层次级别以处理非常大的场景。该方法在ScanNet验证集上进行评估，并与各种手工特征匹配和配准基线以及使用学习模型进行特征或对象匹配的姿态估计器进行比较。使用的评估指标是姿态召回率和重叠百分比召回率。该方法优于经典和基于学习的方法，并且与各种经典和学习基线相比，在重叠度降低时更加鲁棒。

#### 8. 实验设置：
本文在TUM-RGBD和ScanNet场景上评估了所提出的ObjectMatch方法。对于ScanNet，使用验证集和测试集中的12个场景，具有各种大小，环境和相机轨迹。每20帧（1.5Hz）采样一次。对于TUM-RGBD，每30帧采样一次（1Hz）。分别使用相同的超参数进行评估。使用的评估指标是用于TUM-RGBD评估的标准均方根轨迹误差。

#### 9. 实验结果与分析：
ObjectMatch方法显著改善了RGB-D帧的成对和序列配准中的特征匹配技术，特别是在具有挑战性的低重叠区域。在房间规模的TUM-RGBD和ScanNet测试场景上，ATE RMSE值（cm）表明ObjectMatch优于利用经典和基于学习的SLAM系统的强基线。对于ScanNet场景的配准定性比较表明，我们优化的姿态产生更准确，干净和一致的重建，因为我们的全局优化中融合了基于对象的信息。


# Paper:167     引导对应的神经纹理合成



#### 1. Title: 
Neural Texture Synthesis with Guided Correspondence

#### 2. Authors: 
Yang Zhou, Kaijian Chen, Rongjun Xiao, Hui Huang

#### 3. Affiliation: 
深圳大学视觉计算研究中心

#### 4. Keywords: 
Texture synthesis, Markov random fields, neural networks, guided correspondence, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Neural_Texture_Synthesis_With_Guided_Correspondence_CVPR_2020_paper.pdf  Github: https://github.com/EliotChenKJ/Guided-Correspondence-Loss

#### 6. Summary : 
- (1):本文旨在重新推广MRF和神经网络的结合，即CNNMRF模型，用于纹理合成。 
- (2):传统的基于MRF的优化框架在纹理合成中被广泛使用，但是最近的研究更多地关注于使用深度神经网络，本文提出的CNNMRF模型在传统的MRF优化框架中加入了深度神经网络。本文提出了引导对应距离的计算方法，并定义了引导对应损失来衡量输出纹理与示例的相似性。实验表明，本文的方法在非受控和受控纹理合成方面优于现有的神经方法。更重要的是，引导对应损失可以作为一般的纹理损失，用于训练生成网络，实现实时控制合成和基于单图像的编辑。 
- (3):本文提出了一种改进的CNNMRF模型，通过引导对应距离计算和引导对应损失定义，实现了纹理合成。与传统的纹理优化方法不同，本文的方法可以更好地处理纹理的多样性和上下文相似性，从而提高了合成结果的清晰度和真实性。 
- (4):本文的方法在非受控和受控纹理合成方面均取得了优异的性能，达到了最先进的视觉质量。此外，引导对应损失可以作为一般的纹理损失，用于训练前馈网络，实现实时控制合成和基于单图像的编辑。
#### 7. 方法详细介绍：
本文提出了一种带有引导对应的神经纹理合成方法。该方法包括两个关键组件：引导对应搜索中的出现惩罚和损失函数中的上下文相似性。引导对应搜索通过计算目标和源块之间的距离，然后使用出现惩罚更新成对距离来执行。损失函数中的上下文相似性考虑了所有源样本的上下文，并要求目标样本与其最近邻比与所有其他源样本更接近。该方法还使用VGG-19特征层来采样神经块，并使用多分辨率策略来扩大基于优化的合成的感受野。

具体步骤如下：
1. 从VGG-19网络中提取特征图。
2. 通过特征图采样神经块。
3. 计算目标和源块之间的距离，并使用出现惩罚更新成对距离。
4. 使用上下文相似性计算损失函数。
5. 通过反向传播更新输出像素。

#### 8. 实验设置：
本文从数据集中选择了50个纹理图像进行实验。每个图像的较短维度被调整为256像素。本文使用平均颜色距离作为合成质量的度量，并进行用户研究以检查人类对合成质量的感知。

#### 9. 实验结果和分析：
本文进行了消融实验以验证所提出方法的两个关键组件。引导对应搜索中的出现惩罚被调整以提高合成质量。损失函数中的上下文相似性与引导对应损失中的L2距离进行比较。本文还将所提出的方法与几种现有方法进行了比较，包括自适应纹理优化、CNNMRF、Sliced Wasserstein loss、SinGAN和Texture Expansion networks。所提出的方法始终产生比CNNMRF更清晰、更少重复的结果，并且在合成质量上与自适应纹理优化、SinGAN和Texture Expansion networks相当。所提出的方法的平均颜色距离也比SinGAN和Texture Expansion networks小，并且在合成质量上与自适应纹理优化和Sliced Wasserstein loss相当。用户研究表明，所提出的方法比SWD和TexExp更受用户喜欢。


# Paper:168     通用可减小自监督学习的三个指导方针



#### 1. Title: 
Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning

#### 2. Authors: 
Yun-Hao Cao, Peiqin Sun, Shuchang Zhou

#### 3. Affiliation: 
Yun-Hao Cao: 南京大学
Peiqin Sun, Shuchang Zhou: MEGVII Technology

#### 4. Keywords: 
Self-supervised learning, slimmable networks, temporal consistency, dynamic sampling, group regularization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cao_Three_Guidelines_You_Should_Know_for_Universally_Slimmable_Self-Supervised_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的背景是如何在不同设备上部署自监督模型时实现更好的准确性和效率之间的权衡。
- (2):过去的方法包括轻量级架构设计、知识蒸馏、网络剪枝和量化等，但大多数结构化剪枝方法需要微调以获得特定稀疏度的子网络，并且单个训练模型无法在不同设备上实现即时和自适应的准确性-效率权衡。本文提出了一种新的方法，即US3L，它可以在任意宽度上运行自监督模型，但直接将自监督学习（SSL）适应于普遍可减小的网络会导致训练过程频繁崩溃。作者发现，时间上的一致性指导是US-Net训练成功的关键，因此从统一的梯度角度提出了三个指导方针来确保这种时间上的一致性。此外，作者还提出了动态采样和组规则化策略，以同时提高训练效率和准确性。
- (3):本文提出了一种新的方法US3L，它可以在任意宽度上运行自监督模型。作者发现，时间上的一致性指导是US-Net训练成功的关键，因此从统一的梯度角度提出了三个指导方针来确保这种时间上的一致性。此外，作者还提出了动态采样和组规则化策略，以同时提高训练效率和准确性。作者在卷积神经网络和视觉变换器上进行了实证验证，结果表明，US3L方法只需要一次训练和一份权重，就可以在识别、目标检测和实例分割等基准测试中优于各种最先进的方法（单独训练或不训练）。
- (4):作者在卷积神经网络和视觉变换器上进行了实证验证，结果表明，US3L方法只需要一次训练和一份权重，就可以在识别、目标检测和实例分割等基准测试中优于各种最先进的方法（单独训练或不训练）。这表明该方法可以实现更好的准确性和效率之间的权衡。
#### 7. 方法详细介绍：
本文提出了一种名为“Universally Slimmable Self-Supervised Learning (US3L)”的方法，旨在实现在不同设备上部署自监督模型时更好的准确性和效率平衡。该方法包括三个组成部分：损失设计、动态采样和组规则化。损失函数由基础损失和子网络蒸馏损失组成，辅助蒸馏头用于缓解容量差异。动态采样逐渐引入子网络训练，通过采样策略的组合实现减少采样数量。组规则化用于给后面的通道更多自由度，确保其权重得到充分利用。为确保子网络的指导具有时间一致性，提出了三个指导方针。该方法在卷积神经网络和视觉变换器上进行了实证验证，仅需一次训练和一份权重即可在识别、目标检测和实例分割等基准测试中优于各种最先进的方法。

#### 8. 实验设置：
本文的实验在CIFAR-10、CIFAR-100和ImageNet三个基准数据集上进行。作者使用SGD进行预训练，批量大小为512，基础学习率为0.5。学习率采用余弦衰减策略，权重衰减为0.0001。作者在CIFAR-100上进行了400个epoch的预训练，在ImageNet上进行了100个epoch的预训练，除非另有说明。作者将其方法与最先进的自监督学习方法进行比较，并在相同设置下报告了所有方法的预训练模型的线性评估准确性。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的方法在各种基准测试和架构（包括CNN和ViT）上显著优于各种基线方法。使用US3L训练的模型在不同宽度下具有显着优势，仅需一次训练和一份权重。在各种识别基准测试下，学习到的表示的转移能力也得到了评估，并且所提出的方法在线性评估下在所有识别基准测试上都有很大的改进。


# Paper:169     基于数据驱动的事件相机特征跟踪



#### 1. Title: 
Data-driven Feature Tracking for Event Cameras

#### 2. Authors: 
Nico Messikommer, Carter Fang, Mathias Gehrig, Davide Scaramuzza

#### 3. Affiliation: 
第四作者：瑞士苏黎世大学机器人与感知组

#### 4. Keywords: 
Event cameras, feature tracking, data-driven, deep learning, self-supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Messikommer_Data-Driven_Feature_Tracking_for_Event_Cameras_CVPR_2021_paper.html  Github: https://github.com/uzh-rpg/deep_ev_tracker

#### 6. Summary : 
- (1):本文研究背景是现有的特征跟踪方法主要受到标准相机硬件性能的限制，而事件相机由于其高时间分辨率、对运动模糊的高鲁棒性和非常稀疏的输出，已被证明是低延迟和低带宽特征跟踪的理想选择，尤其是在具有挑战性的场景下。

- (2):现有的事件相机特征跟踪方法要么是手工制作的，要么是从第一原理推导出来的，但需要进行广泛的参数调整，对噪声敏感，并且由于未建模的效应，无法推广到不同的场景。为了解决这些问题，本文提出了第一个数据驱动的事件相机特征跟踪器，通过新颖的帧注意力模块，利用低延迟事件来跟踪在灰度帧中检测到的特征。通过直接将零样本从合成数据转移到真实数据，我们的数据驱动跟踪器在相对特征年龄方面的性能优于现有方法高达120％，同时实现了最低延迟。通过一种新颖的自我监督策略将我们的跟踪器适应于真实数据后，这种性能差距进一步增加到130％。

- (3):本文提出了一种数据驱动的事件相机特征跟踪器，它利用事件的高时间分辨率和标准帧来最大化跟踪性能。使用神经网络，我们的方法通过在后续事件补丁中定位灰度图像中的模板补丁来跟踪特征。网络架构具有用于分配的相关体积，并使用递归层进行长期一致性。为了增加跟踪性能，我们引入了一种新颖的帧注意力模块，它在一幅图像中跨特征轨迹共享信息。我们首先在合成光流数据集上进行训练，然后使用基于3D点三角测量的自我监督方案进行微调。

- (4):本文的方法在事件相机数据集基准测试和最近发布的EDS数据集上分别比现有基线方法提高了5.5％和130.2％，而无需进行广泛的手动参数调整。此外，我们的方法在不优化部署代码的情况下实现了更快的推理速度。最后，我们展示了我们的方法与KLT的组合如何在高速场景中发挥最佳效果。
#### 7. 方法详细介绍：
本文提出了一种数据驱动的事件相机特征跟踪方法。该方法由三个主要组件组成：特征网络、帧注意力模块和监督方案。特征网络使用特征金字塔网络对模板块和当前事件块进行编码，生成包含上下文信息和空间信息的像素级特征图。为了明确计算事件块中每个像素与模板块之间的相似度，使用模板块编码器的瓶颈特征向量和事件块的特征图构建相关图。帧注意力模块将当前时间步的所有块的特征向量作为输入，并基于自注意力加权融合所有特征向量来计算每个块的最终位移。最后，监督方案包括基于L1距离的合成监督损失和仅基于校准相机的地面真实姿态的姿态监督损失。

#### 8. 实验设置：
本文在两个数据集上进行了评估：事件相机数据集（EC）和事件辅助直接稀疏测距数据集（EDS）。两个数据集都包含高速外部运动捕捉系统的地面真实姿态。EC数据集包括240×180的APS帧和事件，而EDS数据集包含使用分束器设置捕获的更高分辨率帧和事件（640×480像素）。具体的微调和测试序列选择在补充材料中描述。

#### 9. 实验结果和分析：
本文进行了消融实验，测试了每个引入的网络块的特定贡献。在合成数据上的训练增强显着提高了从合成数据到真实数据的零样本转移。特征编码器中的循环导致更长的特征年龄，而引入相关图则在较小的尺度上实现了相同的效果。所提出的帧注意力模块显着提高了EDS数据集中具有挑战性的序列的性能。通过使用自监督方案将基于帧注意力模块的网络适应于真实数据，实现了最高的跟踪性能。本文提出的方法在EC数据集基准测试和最近发布的EDS数据集上分别比现有基线方法提高了5.5％和130.2％的性能。


# Paper:170     AMT: 用于高效帧插值的全对多场变换



#### 1. Title: 
AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation

#### 2. Authors: 
Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, Ming-Ming Cheng

#### 3. Affiliation: 
Nankai University (南开大学)

#### 4. Keywords: 
Video frame interpolation, flow-based methods, task-oriented flow, bidirectional correlation volumes, multi-field refinement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_AMT_All-Pairs_Multi-Field_Transforms_for_Efficient_Frame_Interpolation_CVPR_2021_paper.html  Github: https://github.com/MCG-NKU/AMT

#### 6. Summary:
- (1): 本文研究的是视频帧插值技术，旨在通过合成中间帧来提高视频的时间分辨率。 
- (2): 过去的方法主要有基于卷积的方法和基于Transformer的方法，但是这些方法在处理大运动和遮挡区域时存在困难。本文提出了一种新的网络架构，名为All-Pairs Multi-Field Transforms (AMT)，通过引入双向相关性体积和多组细粒度的流场，解决了这些问题。 
- (3): 本文提出的AMT网络架构包括两个关键设计：双向相关性体积和多组细粒度的流场。双向相关性体积可以建立所有像素对之间的密集对应关系，通过预测的双向流来检索相关性以更新双向流和插值内容特征。多组细粒度的流场可以从一组更新后的粗略双向流中派生出来，用于分别对输入帧进行反向变形。这两个设计使得网络能够生成有前途的任务导向流，并减少在帧插值过程中建模大运动和处理遮挡区域的困难。 
- (4): 本文在多个公共基准测试中测试了AMT，表现出强大的性能和高效性。与现有的最先进方法相比，AMT在各种基准测试中均表现出最先进的性能。本文提出的AMT网络架构在视频帧插值任务中取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为AMT（All-Pairs Multi-Field Transforms）的新型视频帧插值网络架构。该方法包含三个主要组件：多场编码器、多场解码器和多场细化模块。多场编码器提取多尺度特征并生成多场双向流。多场解码器预测上采样的双向流、中间特征和遮挡掩码。多场细化模块生成多个细粒度的流场，并联合估计每对光流的残差内容和遮挡掩码。最终中间帧通过自适应地合并候选帧并细化最终结果得到。该方法采用流蒸馏损失、Charbonnier损失和census损失作为损失函数。

#### 8. 实验设置：
AMT方法在Vimeo90K数据集上进行了300个epoch的训练，使用AdamW优化器在2个NVIDIA RTX 3090 GPU上进行。总批量大小为24，学习率衰减遵循余弦衰减时间表从2×10−4到2×10−5。增强管道包括随机翻转、旋转、反转序列顺序和随机裁剪大小为224×224的补丁。预训练的LiteFlowNet的流预测用作监督中间流的伪标签。

#### 9. 实验结果与分析：
AMT方法在Vimeo90K、UCF101、SNU-FILM和Xiph数据集上进行了评估，表现出了在PSNR和SSIM指标方面的最先进性能。该方法的延迟为51毫秒/帧，参数数量为3.0M。该方法有效地处理了帧插值过程中的大运动和遮挡区域。本文还研究了多场细化、流场数量和更新策略对方法性能的影响。结果表明，使用多个流场和全对比度相关显著提高了方法的性能。


# Paper:171     PLIKS：一种伪线性逆运动学求解器用于3D人体估计



#### 1. Title: 
PLIKS: A Pseudo-Linear Inverse Kinematic Solver for 3D Human Body Estimation

#### 2. Authors: 
Karthik Shetty, Annette Birkhold, Srikrishna Jaganathan, Norbert Strobel, Markus Kowarschik, Andreas Maier, Bernhard Egger

#### 3. Affiliation: 
Karthik Shetty: FAU Erlangen-Nürnberg, Erlangen, 德国
其他作者：Siemens Healthineers AG, Forchheim, 德国；University of Applied Sciences Würzburg-Schweinfurt, 德国

#### 4. Keywords: 
3D human body estimation, inverse kinematics, SMPL model, linear system, camera calibration

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shetty_PLIKS_A_Pseudo-Linear_Inverse_Kinematic_Solver_for_3D_Human_Body_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究单张2D图像重建3D人体模型的问题，该问题是一个难以解决的逆问题，本文提出了一种基于线性化的参数化模型的伪线性逆运动学求解器（PLIKS）。
- (2):现有的方法直接从输入图像中回归参数化模型的形状、姿态和平移，但这些方法往往忽略了相机的透视变换，导致重建的3D模型在2D图像空间或3D物体空间中不准确。本文提出的PLIKS方法通过将SMPL模型重新表述为线性方程组，结合相机内参信息，可以更准确地重建3D人体模型。此外，PLIKS还可以轻松引入额外的约束，如形状和平移。 
- (3):本文提出的PLIKS方法包括两个模块：网格回归器和PLIKS。网格回归器提供了从图像到SMPL模型3D顶点的映射，而PLIKS则通过解析求解参数化模型的所有参数来重建3D人体模型。本文的方法是可微分的，可以在训练循环中进行端到端训练，具有自我改进的特性。 
- (4):本文在多个3D人体姿态和形状数据集上进行了定量评估，结果表明，与其他最先进的方法相比，PLIKS在标准3D人体姿态和形状基准测试中实现了超过10%的精度提高，并在新的AGORA数据集上实现了12.9mm的重建误差提高。
#### 7. 方法详细介绍：
本文提出了一种名为PLIKS（Pseudo-Linear Inverse Kinematic Solver）的方法，用于从单个2D图像中估计3D人体姿态。该方法基于参数化SMPL模型的线性化公式，通过线性最小二乘法来优化参数，从而得到最优参数。该方法包括两个模块：网格回归器和PLIKS。网格回归器提供了图像和SMPL模型的3D顶点之间的映射。从图像对齐的网格预测中，使用逆运动学（IK）粗略估计相对于规范空间中的模板网格的旋转，称为近似旋转估计器（ARE）。最后，将SMPL模型重新构造为一组线性方程，利用2D像素对齐的顶点映射和任何已知的相机内部参数来完全估计模型，无需任何额外的优化。该方法在各种3D人体姿态和形状数据集上进行了基准测试，并显著优于其他最先进的方法。

#### 8. 实验设置：
本文在多个数据集上进行了评估，包括Human3.6M、3DPW、MPI-INF-3DHP、AGORA、MuPoTs-3D和3DOH50K。使用Mean Per Joint Position Error（MPJPE）、Per Vertex Error（PVE）、Procrustes Aligned Mean Per Joint Position Error（PA-MPJPE）、Percentage of Correct Keypoints（PCK）和Area Under Curve（AUC）等指标进行评估。使用LSP关节回归器确定关节，并将结果与以前的方法进行比较。

#### 9. 实验结果与分析：
本文提出的PLIKS方法在MPI-INF-3DHP和3DPW数据集上优于所有先前的最先进技术，Human3.6M上的MPJPE最低。在AGORA数据集上进行进一步微调，结果显著提高了性能。与以前的方法相比，该方法在3D空间中的对齐效果更好，这是由于透视变形的影响。在MPI-INF-3DHP数据集上，将PLIKS纳入网络的优势更为明显，MPJPE提高了26.3毫米。该方法在AGORA基准测试中显著优于所有以前的方法，这是通过Normalized Mean Joint Error（NMJE）和Normalized Mean Vertex Error（NMVE）进行衡量的。该方法在3DOH50K和3DPW-OCC数据集上的遮挡下表现良好。消融研究表明将相机内部参数纳入网络的重要性。


# Paper:172     基于协作静态和动态视觉-语言流的时空视频定位



#### 1. Title: 
Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding

#### 2. Authors: 
Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai Ye, Wei-Shi Zheng

#### 3. Affiliation: 
第一作者：中山大学，中国

#### 4. Keywords: 
Spatio-Temporal Video Grounding, Vision-Language, Static Visual Cues, Dynamic Visual Cues, Cross-Stream Collaboration

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是视频空时定位任务，即根据给定的语言查询，空间和时间地定位目标对象。这是一个具有挑战性的任务，需要模型能够理解语言描述中的动态和静态视觉线索，因此需要有效地联合建模空时视觉-语言依赖关系。

- (2):过去的方法主要是基于预训练的检测器，如Faster R-CNN，但这些方法存在一些局限性，如不能定位未在预训练数据集中定义的对象类别。最近的方法则是基于强大的预训练视觉-语言模型，但这些方法在跨模态注意力计算中存在大量信息丢失，因此不能很好地捕捉重要的静态-动态跨模态上下文关系。本文提出了一种并行流模型，用于建模静态-动态视觉-语言依赖关系，以及一种新颖的跨流协作块，用于交换彼此学习到的有用和互补信息，实现目标对象的协作推理。

- (3):本文提出了一种新颖的框架，其中开发了一个静态视觉-语言流和一个动态视觉-语言流，以协同推理目标管道。静态流在单帧中执行跨模态理解，并学习根据静态线索（如对象外观）在空间上关注目标对象。动态流模型跨多个连续帧的视觉-语言依赖关系，以捕捉动态线索（如运动）。我们进一步设计了一个新颖的跨流协作块，使静态和动态流能够相互传递有用和互补信息，实现协作推理。实验结果表明，两个流的协作是有效的，我们的整体框架在HCSTVG和VidSTG数据集上均取得了新的最优性能。

- (4):本文提出的方法在Spatio-Temporal Video Grounding任务上取得了最优性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种协作的静态和动态视觉语言流框架，用于时空视频定位。该框架由静态和动态视觉语言流组成，分别用于建模静态和动态的视觉语言依赖关系，实现完整的跨模态理解。静态流用于静态上下文信息的跨模态理解，而动态流用于动态上下文信息的跨模态理解。两个流都使用类似于Transformer的架构学习跨模态对应关系。为了使两个流之间的信息传输，设计了一种新的跨流协作块，以更好地利用两个流之间的不同能力进行协作推理。模型还包括空间和时间预测头，用于预测目标对象的位置和目标时间段的时间跨度。模型使用空间和时间定位损失进行训练。

#### 8. 实验设置：
本文在三个数据集上进行了实验：HCSTVG-v1、HCSTVG-v2和VidSTG。评估指标为平均vIoU，还报告了vIoU@R。模型在VidSTG上训练了10个epoch，在HCSTVG-v1上训练了10个epoch，在HCSTVG-v2上训练了4个epoch。损失权重设置为λ1 = 5，λ2 = 2，λ3 = 5，λ4 = 1。两个流中的Transformer层数均设置为N = 6。

#### 9. 实验结果与分析：
在HCSTVG-v1测试集上，本文提出的方法实现了36.9%的m vIoU，比之前的最新方法TubeDETR高出4.5%。在HCSTVG-v2验证集上，本文提出的方法比TubeDETR高出2.3%的m vIoU。在VidSTG数据集上，本文提出的方法在陈述句和疑问句上均实现了最新的性能，分别为33.7%和28.5%的m vIoU。本文还提供了跨流协作块的定量和定性评估结果，表明该跨流协作块可以有效地在静态和动态VL流之间交换互补信息，从而大大减少了一些对象具有相似外观或运动的困难和模糊情况中的不确定性，使模型能够产生更准确的预测。本文还验证了动态VL流的不同设计选择的有效性，表明在输入到动态VL流的视频特征的空间维度上进行平均池化对于学习动态线索的空间视觉交互很重要。本文还通过将其与社区中常用的一些对称操作进行比较，验证了所提出的协作块的有效性，表明我们的非对称跨流协作块使两个流能够有效地交换互补信息并协同推理目标对象。所提出的框架在VidSTG和HC-STVG数据集上显著优于以前的方法，证明了其有效性。


# Paper:173     基于分层潜在扩散模型的场景生成



#### 1. Title: 
NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models

#### 2. Authors: 
Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, Sanja Fidler

#### 3. Affiliation: 
Seung Wook Kim: NVIDIA
Bradley Brown: NVIDIA, University of Waterloo
Kangxue Yin: NVIDIA
Karsten Kreis: NVIDIA
Katja Schwarz: University of Tübingen, Tübingen AI Center
Daiqing Li: NVIDIA
Robin Rombach: LMU Munich
Antonio Torralba: CSAIL, MIT
Sanja Fidler: NVIDIA, University of Toronto, Vector Institute

#### 4. Keywords: 
3D scene generation, generative models, latent diffusion models, neural fields, hierarchical diffusion models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1): 本文的研究背景是自动生成高质量的三维场景，这对于虚拟现实和机器人模拟等应用非常重要。
- (2): 过去的方法主要是基于生成对抗网络（GAN）和自回归模型，但是它们存在训练不稳定和模式崩溃等问题。本文提出了一种基于潜在扩散模型的生成模型，可以有效地生成复杂的三维场景。
- (3): 本文提出了一种三阶段的生成模型，首先使用自编码器将场景编码为神经场，然后使用潜在自编码器将神经场压缩为潜在表示，最后使用分层扩散模型生成三维场景。本文的创新点在于使用了潜在扩散模型，可以更好地处理复杂的三维场景。
- (4): 本文在四个数据集上进行了实验，证明了本文方法在场景生成方面的优越性。此外，本文还展示了如何使用该方法进行条件场景生成、场景修复和场景风格转换等应用。
#### 7. 方法详细介绍：
本文提出了一种名为神经场-潜在扩散模型（NeuralField-LDM）的生成模型，用于生成复杂的三维环境。该方法首先通过将输入图像编码为三维神经场表示来构建一个表达丰富的潜在分布，然后将其压缩为更抽象的潜在空间。接着，将分层的潜在扩散模型拟合到潜在空间上，实现了在三维场景生成方面的最新性能。该方法还利用交叉注意力层进行条件合成和重建指导进行场景编辑。

具体步骤如下：
1. 场景自编码器：将RGB图像编码为场景的三维表示，使用密度和特征体素网格表示。
2. 潜在体素自编码器：将显式体素网格压缩为压缩的潜在表示，以便于学习数据分布。
3. 分层潜在扩散模型：使用去噪扩散模型以分层方式对潜在分布进行建模。

#### 8. 实验设置：
本文在四个数据集上进行了模型评估，包括VizDoom、Replica、Carla和AVD。每个数据集都包含RGB图像和其对应的相机姿态的深度测量。VizDoom包含在合成游戏环境中导航时获得的前视传感器观察结果。Replica是18个室内场景的高质量重建数据集。Carla是一个用于自动驾驶研究的开源仿真平台，而AVD是一个内部数据集，包含在道路和停车场中进行的人类驾驶记录。

#### 9. 实验结果与分析：
本文提出的NeuralField-LDM模型在所有四个数据集上的FID和FVD得分均优于基线模型。该模型还生成了具有合理纹理和几何形状的场景。本文还使用得分蒸馏采样（SDS）损失评估了模型的性能，该方法可以提高生成场景的质量。本文还展示了场景自编码器在场景重建方面的可扩展性。


# Paper:174     使用程序生成器生成无限逼真的世界



#### 1. Title: 
Infinite Photorealistic Worlds using Procedural Generation

#### 2. Authors: 
Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, Jia Deng

#### 3. Affiliation: 
第一作者：Alexander Raistrick，普林斯顿大学计算机科学系

#### 4. Keywords: 
Procedural Generation, Synthetic Data, Computer Vision, Photorealistic, 3D Scenes

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2021_paper.html  Github: https://github.com/princeton-vl/InfiniGen

#### 6. Summary : 
- (1):本文的研究背景是计算机视觉中合成数据的应用，合成数据可以用于训练模型，但是现有的合成数据集缺乏自然界中的物体和场景，因此需要一种新的方法来生成更多样化的合成数据。

- (2):过去的方法主要是使用已有的3D模型或者图像进行合成，但是这些数据集的物体和场景种类有限，无法满足计算机视觉中更多的任务需求。本文提出了一种全新的方法，使用程序生成器生成自然界中的物体和场景，可以生成无限多的数据，且每个物体和场景都是独一无二的。

- (3):本文提出的方法是使用程序生成器生成自然界中的物体和场景，可以生成无限多的数据，且每个物体和场景都是独一无二的。该方法的创新点在于：（1）完全使用程序生成，不需要外部资源，可以生成无限多的数据；（2）可以生成自然界中的各种物体和场景，包括植物、动物、地形和自然现象等；（3）生成的3D场景非常逼真，可以用于训练计算机视觉模型；（4）生成的3D场景中的几何细节都是真实的，可以用于3D重建任务。

- (4):本文的方法可以用于生成训练数据，可以用于训练计算机视觉模型，包括目标检测、语义分割、光流和3D重建等任务。实验结果表明，使用本文生成的数据可以提高模型的性能，且生成的数据可以满足更多的任务需求。
#### 7. 方法详细介绍：
本文提出了一种名为Infinigen的系统，用于使用程序生成无限的逼真世界。该系统使用各种技术，如分形噪声、模拟器和差分生长来生成地形、植物、水下物体和表面散射。通过基因组数据结构、部件生成器和动画骨骼，实现了生物生成。该系统还采用动态分辨率缩放和使用Cycles进行图像渲染。使用OpenGL代码进行地面真实性提取。在2个Intel(R) Xeon(R) Silver 4114 @ 2.20GHz CPU和1个NVidia-GPU上进行基准测试，1000次独立试验的时间为3.5小时，可以生成一对1080p图像。

#### 8. 实验设置：
使用Infinigen生成了30K对带有地面真实性的图像，用于进行立体匹配。使用RAFT-Stereo从头开始对这些图像进行训练，并将结果与Middlebury验证集和测试集上的结果进行比较。

#### 9. 实验结果和分析：
本文提供了使用Infinigen创建一对立体1080p图像所需资源的统计数据。本文还提供了一张表格，比较了Middlebury验证集上3.0%错误率在Infinigen和其他先前数据集之间的差异。本文还展示了使用Infinigen训练的RAFT-Stereo在具有自然物体的图像上的泛化能力，包括植物和澳大利亚Middlebury测试图像的定性结果。


# Paper:175     通过操作点云编辑神经辐射场



#### 1. Title: 
NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds

#### 2. Authors: 
Jun-Kun Chen, Jipeng Lyu, Yu-Xiong Wang

#### 3. Affiliation: 
第一作者：伊利诺伊大学香槟分校

#### 4. Keywords: 
Neural Radiance Fields, Point Clouds, Shape Editing, Novel-View Synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Chen_NeuralEditor_Editing_Neural_Radiance_Fields_via_Manipulating_Point_Clouds_CVPR_2022_paper.html  Github: https://github.com/immortalco/NeuralEditor

#### 6. Summary : 
- (1):本文研究的背景是3D计算机视觉和计算机图形学中的3D场景编辑问题，特别是对于神经辐射场（NeRF）这种隐式神经表示方法，如何实现场景的形状编辑是一个基本挑战。

- (2):过去的方法主要是基于显式3D表示方法，如点云和网格，而NeRF等隐式表示方法在新视角合成方面表现出色，但缺乏对场景形状进行编辑的能力。本文提出了一种基于点云的NeRF编辑方法，通过优化点云的结构和特征，实现了高保真度的形状编辑和场景变形，并支持零样本推理和进一步微调。

- (3):本文提出了一种基于点云的NeRF编辑模型，通过构建K-D树引导的密度自适应体素，实现了高质量的渲染结果和精确的点云优化。NeuralEditor通过映射相关点之间的关联，实现了形状编辑。本文的创新点在于将点云与NeRF结合，实现了更加通用的场景编辑操作，如场景变形。NeuralEditor支持零样本推理和进一步微调，相比现有的NeRF编辑方法，具有更高的精度和更广泛的适用性。

- (4):本文在形状变形和场景变形任务上进行了广泛的评估，结果表明NeuralEditor在形状编辑方面取得了最先进的性能，并且支持更加通用的场景编辑操作。NeuralEditor的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为NeuralEditor的方法，通过操纵点云来编辑神经辐射场。该方法基于K-D树构建多尺度密度自适应体素，并引入确定性样条积分进行渲染。该方法使用Phong反射来模拟颜色，并使用从底层点云中获取的法向量。该方法通过修剪和生长的显式优化以及法向量的隐式优化来优化点云。该方法基于索引点云定义形状编辑任务，并表示广泛的形状编辑任务。NeuralEditor是一种灵活多变的方法，通过操纵点云使神经辐射场可编辑，支持一般形状编辑操作。该模型基于K-D树和确定性积分，可以产生精确的点云并支持一般场景编辑。颜色使用Phong反射进行建模，以分解镜面颜色并更好地表示场景几何形状。该方法还支持对变形场景进行微调，以调整到更好的环境一致性。该方法与其他基于点的NeRF模型（如PointNeRF）和基于网格的NeRF编辑模型进行了比较。

#### 8. 实验设置：
本文使用NeRF Synthetic（NS）数据集中的场景进行实验，该数据集是由Blender场景构建而成的广泛使用的NeRF基准。本文使用Blender构建可重现的基准，包括编辑场景的真实值以进行评估和微调。本文在两种形状编辑任务上评估模型：形状（网格）变形任务和场景变形任务。本文将NeuralEditor与Naive Plotting和PointNeRF等不同类型的基线进行比较。

#### 9. 实验结果与分析：
本文提出了一种点云引导的NeRF模型NeuralEditor，通过操纵底层点云支持一般形状编辑任务。实证评估表明，NeuralEditor以零-shot推理方式产生的渲染结果比基线高得多，并在快速微调后进一步显著提高。NeuralEditor甚至支持多个场景之间的平滑场景变形，这对于先前的工作来说是困难的。定量比较表明，NeuralEditor在零-shot推理和微调设置下始终优于所有基线和变体。使用NeuralEditor生成的精确点云，Naive Plotting基线甚至始终优于PointNeRF。DeformingNeRF在基准测试中表现不佳，指标值显著较低。


# Paper:176     针对部分标签学习的有效视觉表示



#### 1. Title: 
Towards Effective Visual Representations for Partial-Label Learning

#### 2. Authors: 
Shiyu Xia, Jiaqi Lv, Ning Xu, Gang Niu, Xin Geng

#### 3. Affiliation: 
第一作者：东南大学

#### 4. Keywords: 
Partial-Label Learning, Contrastive Learning, Prototypical Classifier, Image Classification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xia_Towards_Effective_Visual_Representations_for_Partial-Label_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是部分标签学习（PLL）问题，即每个训练实例只有一组包含未知真实标签的模糊候选标签集合。这种问题在现实世界中的各种重要应用中自然而然地出现，如网络挖掘和图像注释。本文的研究背景是深度神经网络（DNNs）在大规模完全监督训练数据上表现出色，但高质量数据的要求对实际应用构成了挑战，因此需要使用低质量标签数据进行训练。

- (2):过去的PLL方法可以分为基于识别的策略和基于平均的策略。最近，DNNs将PLL的研究带入了一个新时代，其中PiCO在多个基准测试上实现了最先进的性能。PiCO引入了对比学习模块来解决PLL问题，使用一个线性分类器的预测来选择每个锚点的伪正样本，并维护一个负样本队列。同时，使用动量编码器来提高一致性。此外，PiCO添加了一个原型分类器模块来指导线性分类器的更新，这是基于这样一个想法，即存在一个嵌入空间，其中来自同一类的点围绕其原型聚集。然而，本文指出PiCO中的两个模块在实践中并不像人们想象的那样有效，因为伪正样本的不可靠性和消歧指导的不当方向。 

- (3):本文提出了一个简单的PLL框架PaPi，即带有引导原型分类器的部分标签学习。PaPi直接消除了引入噪声的对比学习模块，并采用与PiCO相反的消歧指导方向。具体来说，PaPi为每个样本产生一个类别的相似度分布，该分布基于在投影的低维空间中到类特定原型的距离上的softmax函数。然后，PaPi将分布与从一个线性分类器预测后处理的消歧概率对齐。同时，线性分类器进行自我教学，其中每个学习阶段都由当前和以前的阶段引导。PaPi在多个图像分类任务上进行了广泛的实验。PaPi在非常困难的情况下在CIFAR-100上取得了4.57％的改进。此外，PaPi有效地学习了表示，而不使用大批量或动量编码器，其中来自同一类的训练实例被分组成更紧密的聚类。 

- (4):本文在多个图像分类数据集上进行了实验，PaPi显著优于最先进的PLL方法。PaPi的性能支持其目标。 


#### 7. 方法详细介绍：
本文提出了一种名为PaPi的框架，用于解决部分标签学习问题。PaPi采用了一种新的指导原型分类器的方法，通过在低维空间中的类特定原型上的距离上进行softmax函数，为每个样本生成了一个类别的原型相似度分布。PaPi还采用了Mixup方法，通过线性插值构造新的训练样本，从而使模型更加鲁棒。此外，PaPi还引入了一种原型对齐损失项，用于指导原型分类器的优化。该方法在Fashion-MNIST和SVHN数据集上的实验结果表明，PaPi在部分标签学习任务中取得了最好的性能，且在高模糊度下性能下降较小。

#### 8. 实验设置：
本文的实验使用了四个数据集，包括CIFAR-10、CIFAR-100、Fashion-MNIST和SVHN，其中包含了部分标签。实验中使用了分类准确率作为性能评价指标。本文提出的PaPi方法在所有数据集上均取得了最好的性能，且在高模糊度下性能下降较小。此外，本文还分析了数据增强的组合方式对性能的影响，发现强和弱增强的组合方式取得了最好的性能。

#### 9. 实验结果和分析：
本文提出的PaPi方法在Fashion-MNIST、SVHN、CIFAR-10、CIFAR-100、CIFAR-100-H和Mini-Imagenet数据集上均取得了最好的性能，且在高模糊度下性能下降较小。与当前最先进的方法相比，PaPi的性能提升显著，尤其是在高模糊度下。与监督学习相比，PaPi的性能下降不到3％，而大多数基线方法表现出显著的性能下降。


# Paper:177     可压缩数据集合成



#### 1. Title: 
Slimmable Dataset Condensation

#### 2. Authors: 
Songhua Liu, Jingwen Ye, Runpeng Yu, Xinchao Wang

#### 3. Affiliation: 
国立新加坡大学

#### 4. Keywords: 
dataset condensation, slimming, neural networks, synthetic dataset, continual learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Slimmable_Dataset_Condensation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度学习中大量数据的存储和传输问题，提出了数据集压缩的概念，即将大型数据集压缩成小型合成数据集，以减轻存储和传输负担。

- (2):过去的方法通常假定一个固定的存储或传输预算，但当预算发生变化时，这些方法必须重新合成数据集，这在许多应用中是不可行的。本文提出了一种新的方法，即可压缩数据集，以从以前的压缩结果中提取更小的合成数据集，解决了现有方法在连续压缩设置下的局限性。本文提出的方法是通过显式考虑神经网络在不同压缩时间上的不一致性和合成数据的欠定解空间来解决这些问题的。本文的方法在多个基准测试中表现优异。

- (3):本文提出了一种新的训练目标，用于显式考虑神经网络在不同压缩时间上的一致性和合成数据的欠定解空间。此外，本文的方法采用了一种重要性感知的合成数据参数化方法，可以在不进行训练的情况下实现上限误差，并且可以作为学习的强初始化，以加速收敛。

- (4):本文的方法在多个基准测试和应用中进行了广泛的实验，包括连续学习和联邦学习，并证明了其有效性。结果表明，本文的方法在连续压缩设置下优于所有现有的基线方法，并且可以实现与使用原始数据集相同大小的合成数据集的性能。
#### 7. 方法详细介绍：
本文提出了一种可压缩数据集精简方法，称为“slimmable dataset condensation”（slimmable DC）。该方法使用了一种新颖的训练目标，以显式调节压缩前后使用合成数据集的训练效果的一致性。该目标由两个部分组成：一阶和无穷阶参数匹配，旨在解决不同压缩场合下神经网络的不一致性和合成数据的欠定解空间问题。此外，还探索了一种意义感知的合成数据集参数化方法，用于实现高效的精简过程。该方法在训练过程中嵌入了具有正交基和偏斜分布奇异值的线性空间。理论推导表明，通过舍弃较小的组件，可以获得上限误差，这可以作为无需学习的可压缩数据集精简解决方案或学习基础设置中的强初始化，以加速收敛。

具体步骤如下：
1. 构建合成数据集，使用意义感知的参数化方法，嵌入具有正交基和偏斜分布奇异值的线性空间。
2. 通过一阶和无穷阶参数匹配目标，调节压缩前后使用合成数据集的训练效果的一致性。
3. 通过优化目标，包括保持原始数据集的性能、维护解空间、鼓励奇异值分布的偏斜和强制基的正交性，精简数据集。
4. 在学习基础设置中，使用上限误差作为无需学习的可压缩数据集精简解决方案或强初始化，以加速收敛。

#### 8. 实验设置：
本文在FashionMNIST、CIFAR10和CIFAR100等基准数据集上评估了所提出方法的性能。对于FashionMNIST和CIFAR10，每个类别的合成数据集大小为50→20→10→5→2→1张图像，对于CIFAR100，序列为20→10→5→2→1。所有定量结果都基于5次重复评估的平均值。在连续学习和联邦学习等应用中，本文还进行了实验。

#### 9. 实验结果和分析：
本文进行了一系列实验，包括对损失项L1pm、L∞pm、Lskew和Lortho的消融研究，以及与FRePo基线的性能比较。实验结果表明，所提出的方法优于现有的典型数据集精简算法。此外，本文还提供了学习基础和无需学习的可压缩数据集精简方法的定性可视化。在连续学习和联邦学习等应用中，本文还比较了所提出方法的性能。


# Paper:178     StyleGene：基于区域级面部基因的亲属人脸合成中的交叉和突变



#### 1. Title: 
StyleGene: Crossover and Mutation of Region-level Facial Genes for Kinship Face Synthesis

#### 2. Authors: 
Hao Li, Xianxu Hou, Zepeng Huang, Linlin Shen

#### 3. Affiliation: 
深圳大学计算机科学与软件工程学院计算机视觉研究所

#### 4. Keywords: 
Kinship face synthesis, StyleGAN, Region-level Facial Gene, Crossover, Mutation

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Li_StyleGene_Crossover_and_Mutation_of_Region-Level_Facial_Genes_for_Kinship_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究的背景是高保真度的亲属人脸合成，其应用包括亲属验证、失踪儿童识别和社交媒体分析等。
- (2):过去的方法可以分为两种范式：一阶段和二阶段。一阶段方法将其视为图像到图像的转换任务，并使用成对数据训练一对一的亲属人脸生成器。然而，这些方法只能产生低分辨率的图像，且结果图像可能模糊且缺乏多样性。二阶段方法首先提取遗传表示，并根据父母的面部组装它们到孩子的表示。现有方法尝试通过训练深度神经网络或通过知识规则来学习面部外观的遗传和变异，但由于缺乏高质量的亲属注释训练数据，学习到的遗传表示容易过拟合，导致生成的孩子缺乏多样性。本文提出了一种基于区域级面部基因的亲属人脸合成方法，通过模拟父母面部部分的交叉和突变过程来组装后代的RFG，进而增加了生成后代的多样性。
- (3):本文提出了一种RFG（Region-level Facial Gene）提取框架，使用IGE（Image-based Gene Encoder）、LGE（Latent-based Gene Encoder）和Gene Decoder来学习给定面部图像的RFG，并将RFG与StyleGAN2的潜在空间之间的关系进行建模。在训练阶段，使用类似循环的损失来衡量Gene Decoder和图像编码器之间的L2距离，以及LGE和IGE之间的距离，只需要面部图像来训练我们的框架，即不需要成对的亲属人脸数据。基于所提出的RFG，进一步设计了交叉和突变模块来继承父母的面部部分。还使用了Gene Pool来引入RFG的突变变化，从而显着增加了后代的面部多样性。在FIW、TSKinFace和FF数据库上进行的定性、定量和主观实验清楚地表明，我们的方法生成的亲属人脸的质量和多样性比现有的最先进方法要好得多。
- (4):本文提出的方法在亲属人脸合成任务上取得了很好的性能，生成的后代面部具有多样性和合理的变化，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为StyleGene的亲缘关系人脸合成方法。该方法使用区域级面部基因（RFGs）来表示父母和他们的后代的面部特征。RFGs是从基因池中学习得到的，基因池是基于FFHQ数据集构建的。该方法使用交叉和变异过程从父母生成后代的RFGs。学习得到的基因解码器被应用于将RFGs映射到StyleGAN2的潜在空间中，进一步进行面部合成。该方法还融合了父母的潜在代码，以继承其他属性，如肤色、发色、图像照明和背景。具体步骤包括：
1. 训练阶段：提出了区域级面部基因（RFG）提取框架，包括基于图像的基因编码器（IGE）、基于潜在空间的基因编码器（LGE）和基因解码器。IGE用于为每个面部部位构建独立的表示，称为RFG，用于控制面部区域的合成。LGE被训练用于直接将StyleGAN2的潜在代码映射到RFGs。基因解码器用于将RFGs映射回StyleGAN2的W+空间。 
2. 推理阶段：首先使用基于潜在空间的基因编码器提取父母的RFGs，然后通过交叉和变异模块组装后代的RFGs。将RFGs使用基因解码器映射回W+空间，最后使用预训练的StyleGAN2生成器生成高保真度的面部。

#### 8. 实验设置：
训练集包括CelebAHQ数据集的所有图像、从MS-Celeb-1M采样的50,000张人脸和StyleGAN2生成的10,000张人脸。在FIW、TSKinFace和FF-Database数据集上进行评估。基因池是基于FFHQ数据集构建的。使用AdamW优化器进行训练，批量大小为32，初始化学习率为0.001，每10个epoch将学习率除以2，训练在第30个epoch停止。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2680 v4 CPU、256GB RAM和四个NVIDIA Tesla V100 GPU。

#### 9. 实验结果和分析：
本文通过定量、定性和主观实验结果对提出的StyleGene方法进行了评估。在TSKinFace、FF-Database和FIW数据集上进行了定量评估，结果表明该方法在亲缘关系验证准确性方面优于现有的最先进方法。通过与真实图像进行比较进行了定性评估，结果表明该方法生成的面孔与给定的父母更为逼真和相似。通过用户研究进行了主观评估，结果表明该方法在质量和与父母的相似性方面均获得了最高排名，表明我们的方法合成的孩子面孔在质量和与父母的相似性方面是最好的。


# Paper:179     基于风格投影聚类的领域泛化语义分割



#### 1. Title: 
Style Projected Clustering for Domain Generalized Semantic Segmentation

#### 2. Authors: 
Wei Huang, Chang Chen, Yong Li, Jiacheng Li, Cheng Li, Fenglong Song, Youliang Yan, Zhiwei Xiong

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Domain Generalization, Semantic Segmentation, Style Projection, Semantic Clustering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Style_Projected_Clustering_for_Domain_Generalized_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://gitee.com/mindspore/models/tree/master/research/cv/SPC-Net

#### 6. Summary : 
- (1):本文研究领域泛化语义分割问题，即在源域训练的模型在目标域上的泛化能力。该问题在自动驾驶等安全关键应用中具有重要意义。

- (2):现有的领域泛化方法主要通过规范化不同图像到一个规范特征空间来提高泛化能力，但这种方法会削弱特征的表达能力。本文提出了一种新的方法，即利用不同图像之间的差异来构建更好的表示空间，提取并存储不同风格特征作为表示的基础，通过将特征投影到已知空间来实现对未知图像风格的泛化。同时，本文还提出了一种基于语义聚类的决策方法，通过测量到语义基础（即原型）的相似距离来预测每个像素的类别。

- (3):本文提出了一种新的领域泛化方法，即基于风格投影的聚类方法，该方法利用不同图像之间的差异来构建更好的表示空间，提取并存储不同风格特征作为表示的基础，通过将特征投影到已知空间来实现对未知图像风格的泛化。同时，本文还提出了一种基于语义聚类的决策方法，通过测量到语义基础（即原型）的相似距离来预测每个像素的类别。本文的创新点在于提出了一种新的领域泛化方法，该方法不同于现有的特征规范化方法，而是利用不同图像之间的差异来构建更好的表示空间。

- (4):本文在多个领域泛化语义分割基准测试上进行了全面的实验，证明了所提出方法的优越性，平均mIoU提高了3.6%。本文提出的方法在未知域上的性能表现支持其目标。
#### 7. 方法详细介绍：
本文提出了一种称为Style Projected Clustering的方法，用于域通用语义分割。该方法包括两个组件：样式投影和语义聚类。在样式投影中，根据输入图像与样式基之间的相似性，将输入图像投影到建立在样式基上的样式表示空间中。在语义聚类中，通过最近的语义基确定每个像素嵌入的类别。该方法利用Wasserstein距离估计样式分布差异，并利用距离的倒数来表征输入图像与样式基之间的相似性。该方法还引入了语义基来保留每个域和每个类别的语义信息。该方法采用标准的交叉熵损失来监督网络的训练，并提出方差和判别项作为两个额外的训练目标。在训练过程中，样式和语义基会在线更新。在推理过程中，该方法利用非参数聚类来获得最终的像素级预测。

#### 8. 实验设置：
本文在两个域通用语义分割基准数据集GTA5-to-Cityscapes和SYNTHIA-to-Cityscapes上进行了实验。实验分为单源和多源两种设置。在实验中，使用DeepLabV3+作为编码器，ResNet-101作为骨干网络。实验在单个NVIDIA Tesla V100 GPU上进行，具有16GB内存。

#### 9. 实验结果和分析：
本文提出的Style Projected Clustering方法在单源和多源设置下均表现出优越的泛化性能。在单源设置下，与朴素基线相比，该方法在目标数据集上平均获得了约14%的mIoU增益。此外，该方法在大型和轻量级骨干网络上均表现出优异的性能。消融实验表明了所提出策略的有效性，包括样式投影和语义聚类以及补充损失函数。t-SNE可视化工具分析表明，样式投影成功地将未见样式投影到建立在样式基上的样式表示空间中，而语义聚类成功地通过保留的语义基在不同域之间实现了类别预测。


# Paper:180     从尺度匹配学习光学扩张



#### 1. Title: 
Learning Optical Expansion from Scale Matching

#### 2. Authors: 
Han Ling, Yinghui Sun, Quansen Sun, Zhenwen Ren

#### 3. Affiliation: 
南京理工大学

#### 4. Keywords: 
Optical expansion, 3D optical flow, scale matching, monocular 3D vision, motion estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ling_Learning_Optical_Expansion_From_Scale_Matching_CVPR_2021_paper.html  Github: https://github.com/HanLingsgjk/TPCV

#### 6. Summary : 
- (1):本文研究了光学扩张（OE）问题，即两帧之间的物体尺度变化，广泛应用于单目3D视觉任务中。OE方案在3D运动估计任务中具有独特的优势，只需要一个单独的相机，能够实现密集和固定基线独立的结果。 

- (2):以往的方法主要是从光流结果中估计光学扩张，但这种两阶段的架构使得结果受到光流精度的限制，且不够稳健。本文提出了将光学扩张集成到2D光流中的3D光流概念，通过一个即插即用的模块TPCV实现，TPCV在正确的位置和尺度上实现匹配特征，从而允许光流和光学扩张任务的同时优化。 

- (3):本文提出了一个插入式模块TPCV，它可以应用于基于匹配的光流框架。基于TPCV，原始的2D光流可以轻松升级为3D光流，提高准确性。我们将TPCV应用于光流框架RAFT，显著提高了性能，特别是对于移动前景目标。 

- (4):本文在各种基准任务中展示了TPCV的有效性，建立了新的SOTA结果，包括场景流、时间到碰撞（TTC）和深度运动估计，同时保持了竞争性的速度。
#### 7. 方法详细介绍：
本文提出了一种新颖的3D光流估计方法，将光流和光学扩张相结合，提供了深度方向上的运动信息，并消除了尺度变化对光流匹配的影响。同时，本文还提出了一个通用的光流升级模块TPCV，可以广泛应用于各种2D光流框架，并升级到3D光流。TPCV由三个步骤组成：构建正向流和反向流的相关性体积，从相关性体积中查询相关性特征，计算逆光流。TPCV的主要功能是索引来自不同尺度和位置的像素对的相关性特征，这些特征将用于后续的光流估计和尺度估计。本文还提出了一种插入式的2D到3D光流升级模块，称为TPCV，它可以与一组监督学习策略相关联，用于提高光流和深度感知任务的性能。

#### 8. 实验设置：
本文在KITTI场景流基准和Flyingthings3D数据集上进行了实验。对于所有实验，作者在Driving数据集上进行了80K次预训练，K-160上进行了细化训练，并在K-40数据集上进行了测试。

#### 9. 实验结果和分析：
本文在各种任务中展示了所提出的TPCV模块的实验结果，包括光流、深度运动、时间到碰撞和场景流。结果表明，在大多数情况下，TPCV在前景光流和深度运动估计方面优于现有的最先进方法。在场景流方面，TPCV在所有单目方法中实现了最佳性能，并在D2-all指标上甚至优于大多数基于深度的方法。添加TPCV还显着提高了RAFT3D在场景流方面的性能。本文还进行了消融实验，比较了TPCV模块与回归和金字塔匹配方案在光流和深度运动估计方面的性能。结果表明，TPCV模块在提高光流和深度运动估计方面发挥了关键作用。


# Paper:181     ScarceNet: 利用稀缺标注进行动物姿态估计



#### 1. Title: 
ScarceNet: Animal Pose Estimation with Scarce Annotations

#### 2. Authors: 
Chen Li, Gim Hee Lee

#### 3. Affiliation: 
第一作者：新加坡国立大学计算机科学系

#### 4. Keywords: 
Animal pose estimation, semi-supervised learning, pseudo labeling, small-loss trick, agreement check

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_ScarceNet_Animal_Pose_Estimation_With_Scarce_Annotations_CVPR_2021_paper.html  Github: https://github.com/chaneyddtt/ScarceNet

#### 6. Summary : 
- (1):本文研究了动物姿态估计这一重要但鲜有研究的任务，主要是由于缺乏标注数据。作者提出了一种基于伪标签的方法，通过利用未标注数据来弥补缺乏标注数据的不足，从而实现动物姿态估计。 

- (2):过去的方法主要是通过从人体姿态数据或合成动物图像中学习来解决动物姿态估计的问题。然而，这些方法存在着领域差异和数据缺乏的问题。本文提出的方法通过伪标签和小损失技巧来解决数据缺乏的问题，同时通过一致性约束和可重用样本重新标记来解决伪标签噪声的问题。 

- (3):本文提出了一种基于伪标签的动物姿态估计方法，通过利用未标注数据来弥补缺乏标注数据的不足。作者提出了一种小损失技巧来选择可靠的伪标签，同时提出了一种基于一致性约束和可重用样本重新标记的方法来解决伪标签噪声的问题。此外，作者还修改了现有的人体姿态估计网络HRNet，将其适应于伪标签学习框架。 

- (4):作者在AP-10K数据集上进行了实验，结果表明本文提出的方法在有限的标注数据下优于现有的半监督方法。作者还在TigDog数据集上进行了测试，结果表明本文提出的方法在只有0.5%的数据标注时优于现有的基于合成数据的方法。本文提出的方法在动物姿态估计任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于伪标签的半监督学习方法ScarceNet，用于稀缺标注的动物姿态估计。该方法采用带有多分支HRNet骨干网络的师生框架。其中，教师网络为无标签数据生成伪标签，学生网络则使用有标签和无标签数据进行训练，使用监督、基于伪标签的、可重用样本重新标记和一致性损失的组合。该方法还使用强弱增强来进行学习和重新标记任务。具体的训练过程在算法1中给出。

#### 8. 实验设置：
本文在两个数据集上评估了所提出的方法：AP-10K和TigDog。AP-10K数据集包含50种动物物种的10,015张标记图像，TigDog数据集包含马和老虎的图像。本文在AP-10K数据集上评估了两种情况下的方法：每个动物物种都有标记的情况和只有一个家族的图像有标记的情况。本文还将该方法与UDA、FixMatch和FlexMatch等半监督方法进行了比较。

#### 9. 实验结果与分析：
本文在TigDog数据集和AP-10K数据集上进行了实验。在TigDog数据集上，所提出的方法在只有极少量标记数据的情况下，与现有的领域自适应方法相比，取得了可比甚至更好的性能。在AP-10K数据集上，进行了消融实验以验证每个组件的相应贡献。当每个组件被移除时，性能显著下降。即使只有很少的标记数据可用，所提出的方法也能取得令人印象深刻的性能。


# Paper:182     基于偏移的扩散模型用于文本到图像生成



#### 1. Title: 
Shifted Diffusion for Text-to-image Generation

#### 2. Authors: 
Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, Jinhui Xu

#### 3. Affiliation: 
第一作者：纽约州立大学水牛城分校

#### 4. Keywords: 
text-to-image generation, diffusion model, CLIP, semi-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2021_paper.html  Github: https://github.com/drboog/Shifted_Diffusion

#### 6. Summary : 
- (1):本文研究文本到图像生成任务，旨在提高生成效率和质量。
- (2):过去的方法主要集中在模型架构和训练数据的规模上，本文提出了一种新的方法，即基于偏移的扩散模型，通过设计新的初始化分布和扩散的新转移步骤，无缝地将预训练的CLIP模型的先验知识编码到扩散过程中，从而提高了从文本生成图像嵌入的效率和效果。与DALL-E 2基线相比，本文方法在生成图像嵌入方面表现更好，从而实现更好的文本到图像生成。此外，本文方法还支持半监督和无语言训练，即训练数据集中只有部分或没有图像标题。在仅有1.7％的图像标题的情况下，本文的半监督模型在MS-COCO上进行了零样本文本到图像生成评估，获得了与DALL-E 2相当的FID结果。本文方法在不同数据集上实现了新的最先进结果，比之前的方法Lafite有了很大的提升。
- (3):本文提出了一种新的基于偏移的扩散模型，将预训练的CLIP模型的先验知识无缝地编码到扩散过程中，从而提高了从文本生成图像嵌入的效率和效果。本文方法支持半监督和无语言训练，可以在不需要大量图像标题的情况下进行文本到图像生成。本文方法在不同数据集上实现了新的最先进结果，比之前的方法Lafite有了很大的提升。
- (4):本文方法在文本到图像生成任务上取得了很好的性能，支持半监督和无语言训练。在仅有1.7％的图像标题的情况下，本文的半监督模型在MS-COCO上进行了零样本文本到图像生成评估，获得了与DALL-E 2相当的FID结果。本文方法在不同数据集上实现了新的最先进结果，比之前的方法Lafite有了很大的提升。
#### 7. 方法详细介绍：
本文提出了一种名为Corgi的灵活文本到图像生成方法。该方法包括一个预训练图像编码器、一个解码器和一个先验模型，用于从文本标题生成图像嵌入。该方法将预训练的CLIP图像编码器用于实现。解码器可以是扩散模型或生成对抗网络（GAN）。先验模型通过顺序采样过程生成目标CLIP图像嵌入。该方法通过设计新的初始化分布和扩散的新转移步骤，将预训练CLIP模型的先验知识纳入其扩散过程中。

#### 8. 实验设置：
本文在不同的文本到图像生成设置下进行了广泛的大规模实验，并从定量和人类评估两个方面进行了评估。实验包括有监督、半监督和无语言设置。训练数据集可以是图像-文本对和未标注标题的纯图像的混合。

#### 9. 实验结果和分析：
本文提出的方法在从文本生成图像嵌入方面优于强基线DALL-E 2，效率和效果都更好，从而实现更好的文本到图像生成。该模型使得半监督和无语言训练成为可能，其中训练数据集中只有部分或没有图像与标题相关联。在仅有1.7％的图像被标注的情况下进行训练，半监督模型在MS-COCO上进行零样本文本到图像生成评估时获得与DALL-E 2相当的FID结果。Corgi还在下游无语言文本到图像生成任务的不同数据集上实现了新的最佳结果，大幅超过了之前的方法Lafite。进行了广泛的大规模实验，定量和定性结果都说明了该方法的有效性。


# Paper:183     ScaleKD：在小目标检测中提取尺度感知知识



#### 1. Title: 
ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector

#### 2. Authors: 
Yichen Zhu, Qiqi Zhou, Ning Liu, Zhiyuan Xu, Zhicai Ou, Xiaofeng Mou, Jian Tang

#### 3. Affiliation: 
Midea Group (美的集团)

#### 4. Keywords: 
Small Object Detection, Knowledge Distillation, Scale-aware, Object Detection, Feature Distillation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究小目标检测的性能和效率问题，提出了一种新的知识蒸馏方法，即ScaleKD，旨在将复杂的教师模型的知识转移到紧凑的学生模型中，以提高小目标检测的性能。

- (2):现有的方法往往难以平衡推理速度和小目标检测性能之间的权衡。本文提出的ScaleKD方法通过两个模块来提高知识蒸馏的质量：1）一个尺度解耦特征蒸馏模块，将教师的特征表示解耦成多尺度嵌入，使得学生模型可以更好地模仿小目标的特征。2）一个跨尺度辅助模块，用于改善学生模型的噪声和无信息的边界框预测，以提高知识蒸馏的效果。本文的方法在COCO和VisDrone数据集上进行了实验，证明了其在一般检测性能和小目标检测性能方面的优越性。

- (3):本文提出了一种新的知识蒸馏方法，即ScaleKD，旨在将复杂的教师模型的知识转移到紧凑的学生模型中，以提高小目标检测的性能。ScaleKD包括两个模块：一个尺度解耦特征蒸馏模块和一个跨尺度辅助模块。尺度解耦特征蒸馏模块将单尺度特征嵌入解耦成多尺度特征嵌入，以便学生模型更好地理解来自对象尺度的特征知识。跨尺度辅助模块通过一个多尺度交叉注意力模块来提高学生模型的性能，以便更好地理解教师模型的知识。

- (4):本文的方法在COCO和VisDrone数据集上进行了实验，证明了其在一般检测性能和小目标检测性能方面的优越性。本文的方法不仅超过了现有的目标检测知识蒸馏方法在一般检测性能方面的表现，而且在小目标检测方面也超过了现有方法。本文的方法还在实例分割和关键点检测等任务上进行了扩展，证明了其在处理视觉任务中的小目标方面的优越性。
#### 7. 方法详细介绍：
本文提出了一种新的小目标检测知识蒸馏方法——Scale-aware Knowledge Distillation (ScaleKD)。该方法采用了一种教师-学生框架，其中教师模型是一个预训练的目标检测器，使用ResNet101骨干网络，而学生模型是一个更小的目标检测器，使用ResNet50或MobileNetV2骨干网络。该方法使用了蒸馏损失、检测损失和Cross-Scale Assistant (CSA)模块来将知识从教师模型传递到学生模型。CSA模块通过建立一个多尺度交叉注意力层来捕获多尺度语义信息，从而改善学生模型的性能。在训练之前，学生模型需要进行30k次迭代的预热。 

#### 8. 实验设置：
本文在两个目标检测数据集上进行了实验：COCO和VisDrone。对于COCO，使用标准的1x训练策略，并在2017年的验证集上评估模型。对于VisDrone，将图像分成四个非重叠的块，并在训练期间独立处理。教师模型使用ResNet101骨干网络，学生模型使用ResNet50或MobileNetV2骨干网络。对于所有两阶段模型，总训练目标的超参数设置为α=0.07，β=0.5，γ=0.2，对于所有一阶段模型，超参数设置为α=0.01，β=0.2，γ=0.05。

#### 9. 实验结果和分析：
本文在COCO和VisDrone数据集上进行了实验，ScaleKD方法在所有方法上都取得了显著的改进，而不会在推理时引入任何额外的计算成本。在COCO上，该方法应用于多个主流基线，包括两阶段检测模型（如Faster RCNN、Cascade RCNN）和一阶段检测模型（如RetinaNet、FCOS、Reppoints）。该方法在APS上取得了显著的改进，特别是在FCOS和RetinaNet上。在VisDrone上，该方法显著提高了学生模型的性能。


# Paper:184     OSAN: 一种一阶段对齐网络，统一多模态对齐和无监督领域适应



#### 1. Title: 
OSAN: A One-Stage Alignment Network to Unify Multimodal Alignment and Unsupervised Domain Adaptation

#### 2. Authors: 
Ye Liu, Lingfeng Qiao, Changchong Lu, Di Yin, Chen Lin, Haoyuan Peng, Bo Ren

#### 3. Affiliation: 
Tencent Youtu Lab (腾讯优图实验室)

#### 4. Keywords: 
Unsupervised Domain Adaptation, Multimodal Alignment, Tensor-based Alignment, Dynamic Domain Generator

#### 5. Paper: 
Paper Link: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_OSAN_A_One-Stage_Alignment_Network_to_Unify_Multimodal_Alignment_and_CVPR_2021_paper.html

Github Link: None

#### 6. Summary:
- (1):本文研究的背景是多模态领域自适应问题，其中包括领域适应和模态对齐两个主要问题。
- (2):过去的方法通常将这两个问题分别解决，但是这种方法忽略了领域和模态之间的关系，不能充分利用它们之间的互补信息。本文提出了一种一阶段对齐网络（OSAN），将多模态对齐和领域适应统一到一个阶段中，通过张量表示的方式探索领域和模态之间的关系，同时提出了动态域生成器（DDG）模块来构建过渡样本，以帮助模型学习领域不变的共同表示空间。
- (3):本文提出的方法是一种一阶段对齐网络（OSAN），通过张量表示的方式探索领域和模态之间的关系，同时提出了动态域生成器（DDG）模块来构建过渡样本，以帮助模型学习领域不变的共同表示空间。本文的创新点在于将多模态对齐和领域适应统一到一个阶段中，同时充分利用领域和模态之间的关系，提高了模型的性能。
- (4):本文在两个真实世界的应用中进行了广泛的实验，证明了我们的方法相对于监督和强无监督方法的优越性能。
#### 7. 方法详细介绍：
本文提出了一种一阶段对齐网络（OSAN），用于统一多模态对齐和无监督域自适应。该模型由三个任务组成：类别分类、域对抗学习和域差异消除。为了解决多模态对齐和域自适应的挑战，作者开发了基于张量的对齐学习（TAL）模块，以捕捉跨模态动态并建立域和模态之间的交互。他们还提出了动态域生成器（DDG）模块，以建立域之间的桥梁并生成软样本以平滑地跨越域差距。该模型通过最小化类别分类损失、对抗损失和差异损失进行训练。

#### 8. 实验设置：
本文在两个不同的任务上进行了实验，包括多模态情感分析和跨模态动作识别。作者使用了两个基准数据集，CMU-MOSI和CMU-MOSEI，用于多模态情感分析，以及两个基准数据集，UCF101和HMDB51，用于跨模态动作识别。他们使用了与之前的工作相同的实验设置，以进行公平比较。

#### 9. 实验结果和分析：
本文报告了OSAN在两个不同任务上的实验结果，包括多模态情感分析和跨模态动作识别。作者将其模型与几种最先进的方法进行比较，并以MAE、Corr、Acc-7、Acc-2和F1等指标报告性能。结果表明，OSAN在两个任务上均优于现有方法，并实现了最先进的性能。


# Paper:185     DropMAE：带有空间注意力Dropout的掩码自编码器用于跟踪任务



#### 1. Title: 
DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks

#### 2. Authors: 
Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Baoyuan Wu, Ying Shan, Antoni B. Chan

#### 3. Affiliation: 
第一作者：香港城市大学计算机科学系

#### 4. Keywords: 
Masked autoencoder, video pre-training, visual object tracking, video object segmentation, spatial-attention dropout

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_DropMAE_Masked_Autoencoders_With_Spatial-Attention_Dropout_for_Tracking_Tasks_CVPR_2021_paper.html  Github: https://github.com/jimmy-dq/DropMAE.git

#### 6. Summary : 
- (1):本文研究了视频匹配任务中的掩码自编码器（MAE）预训练，包括视觉目标跟踪（VOT）和视频目标分割（VOS）。
- (2):过去的方法主要是基于静态图像的预训练，缺乏时间上的对应关系学习。本文提出了DropMAE，通过自适应地在帧重构中执行空间注意力dropout来促进视频中的时间对应关系学习。DropMAE是一种有效且高效的时间匹配学习器，比基于ImageNet的MAE具有更快的预训练速度，并且在VOT和VOS任务上取得了更好的微调结果。本文的贡献在于首次探索了基于掩码自编码器的视频预训练方法，提出了DropMAE方法，实现了在VOT和VOS任务上的最新最优结果。
- (3):本文提出了DropMAE方法，通过自适应地在帧重构中执行空间注意力dropout来促进视频中的时间对应关系学习。DropMAE可以学习可靠的时间对应关系，提高了匹配任务的性能。
- (4):DropMAE在9个VOT和VOS基准测试中均取得了最新最优的结果，其中在GOT-10k上的平均重叠率（AO）为75.9％，在DAVIS-16/17上的J＆F分数分别为92.1％/83.0％，没有使用复杂的在线更新或记忆机制。结果表明DropMAE是一种有效的时间匹配学习器，可以在匹配任务中取得更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为DropMAE的预训练方法，它使用带有空间注意力dropout的掩码自编码器从大规模视频数据中学习时空特征。预训练的DropMAE模型然后在下游的时间匹配任务上进行微调，包括视频目标跟踪（VOT）和视频目标分割（VOS）。对于VOT，使用预训练的DropMAE编码器权重来初始化OSTrack基线跟踪器中使用的ViT骨干网络。对于VOS，构建了一个简单的VOS基线，其中包含一个ViT骨干网络，并使用从ViT的最后一层提取的更新的搜索特征进行掩码预测。使用常用的交叉熵损失来训练整个网络架构。在线推理期间，使用具有掩码注释的第一帧作为内存帧，用于在搜索帧中进行在线目标匹配。

#### 8. 实验设置：
在预训练阶段，探索了各种大规模视频数据源来预训练DropMAE模型，包括Kinetics-400、Kinetics-600、Kinetics-700、Moments in Time和WebVid-2M。对于VOT的微调，使用LaSOT、COCO、TrackingNet和GOT-10k的训练集进行训练。对于GOT-10k评估，模型仅在GOT-10k的训练集上进行微调。对于VOS微调，使用Youtube-VOS和Davis数据集进行微调，遵循标准惯例。

#### 9. 实验结果和分析：
本文在各种视频目标跟踪和分割基准测试中提出了最新的结果。所提出的DropMAE模型在VOT和VOS任务上的表现优于现有的预训练方法。DropTrack模型在包括GOT-10k、LaSOT、LaSOText、TNL2K、ITB、TrackingNet和OTB100在内的七个具有挑战性的跟踪基准测试中取得了最新的最佳表现。DropSeg模型在DAVIS-2016和DAVIS-2017验证集上创造了新的记录。结果表明，所提出的方法在学习时空匹配表示方面是有效的，并且对于已知和未知对象都具有很好的泛化能力。关于dropout比率的消融研究表明，所提出的空间注意力dropout机制在提高模型性能方面是有效的。


# Paper:186     基于干预的多实例学习在全切片病理图像上的应用



#### 1. Title: 
Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images

#### 2. Authors: 
Tiancheng Lin, Zhimiao Yu, Hongyu Hu, Yi Xu, Chang Wen Chen

#### 3. Affiliation: 
上海交通大学数字媒体处理与传输重点实验室

#### 4. Keywords: 
Multi-instance learning, whole-slide pathological images, interventional training, bag-level prediction, causal inference

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Interventional_Bag_Multi-Instance_Learning_on_Whole-Slide_Pathological_Images_CVPR_2021_paper.html  Github: https://github.com/HHHedo/IBMIL

#### 6. Summary : 
- (1):本文研究的是对于WSI分类的MIL问题，WSI是一种具有千兆像素分辨率和昂贵标注的病理图像，MIL是一种有效的范式，将每个WSI视为一个标记的袋子，相应的补丁为未标记的实例。

- (2):现有的MIL方法主要集中在改进特征提取器和聚合器，但这些方法的一个缺陷是袋子上下文先验可能会使模型捕捉到袋子和标签之间的虚假相关性，这是一个混淆因素，限制了现有MIL方法的性能。本文提出了一种新的方案，Interventional Bag Multi-Instance Learning (IBMIL)，以实现去混淆的袋子级预测。与传统的基于似然的策略不同，所提出的方案基于反向门控制来实现干预训练，因此能够抑制袋子上下文先验引起的偏差。IBMIL的原则与现有的袋子MIL方法是正交的。因此，IBMIL能够为现有方案带来一致的性能提升，实现新的最先进的性能。

- (3):本文提出了一种新的MIL方案，Interventional Bag Multi-Instance Learning (IBMIL)，以实现去混淆的袋子级预测。IBMIL包含另一个干预训练阶段，与现有方案不同，IBMIL使用因果干预：P(Y |do(X))进行袋子预测。IBMIL的关键区别在于它包含另一个干预训练阶段，即使用聚合器进行干预，而不是直接使用似然进行推断。通过观察混淆因素，我们通过反向门控制来消除它们的影响，这是一种更有效的机制，可以实现袋子和标签之间的实际因果关系。

- (4):本文在两个公共WSI数据集上进行了实验，即Camelyon16和TCGA-NSCLC。实验结果表明，IBMIL对特征提取器和聚合网络都是不可知的，即它为所有比较的最先进的MIL方法带来了一致的性能提升。进一步的消融研究和分析证明了干预训练的有效性。
#### 7. 方法详细介绍：
本文提出了一种新的方法，称为干预式袋多实例学习（IBMIL）。该方法包括三个阶段：特征提取器的训练、聚合器的训练和通过反向门控调整实现的干预式训练。IBMIL的目标是捕捉全幅病理图像（袋）与袋标签之间的真实因果关系，减轻混淆因素引起的偏差。在干预式训练阶段，通过混淆因素字典和将袋特征和混淆因素信息结合的函数实现反向门控调整。该方法旨在消除颜色等混淆因素对识别的影响。

具体步骤如下：
1. 训练特征提取器：使用ResNet-18、ViT-small和CTransPath等不同的网络架构进行实验，提取袋级别的特征。
2. 训练聚合器：使用ABMIL、DSMIL、TransMIL和DFTD-MIL等四种最先进的方法作为聚合器，将袋级别的特征转换为整体预测结果。
3. 干预式训练：在第二阶段训练的聚合器的基础上，通过混淆因素字典和将袋特征和混淆因素信息结合的函数实现反向门控调整，消除混淆因素的影响。

#### 8. 实验设置：
本文的实验使用了两个公共的全幅图像数据集，分别是Camelyon16和TCGA-NSCLC。对于每个数据集，提供了训练和测试图像的数量。评估协议包括报告类别精度、召回率、准确率和曲线下面积（AUC）得分。

#### 9. 实验结果与分析：
本文的实验结果表明，IBMIL在不同的实验设置下都能够显著提高模型的性能。IBMIL的性能相对稳健，对混淆因素字典的大小不敏感，而且性能并不随维度的增加而单调提高。冻结混淆因素在干预式训练中的表现优于可学习混淆因素。所有反向门控调整的实现都导致了性能的提高，而更多的训练轮次并没有带来性能的提高。在大多数情况下，省略第二阶段不会导致性能下降。IBMIL在所有设置下都带来了显著的性能提升，最佳性能甚至可以与基于注意力的聚合器相媲美。这些改进来自于干预式训练，而不仅仅是后处理。


# Paper:187     基于特征分解和边缘重建的伪装目标检测



#### 1. Title: 
Camouﬂaged Object Detection with Feature Decomposition and Edge Reconstruction

#### 2. Authors: 
Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li

#### 3. Affiliation: 
第一作者：清华大学深圳国际研究生院

#### 4. Keywords: 
Camouﬂaged object detection, feature decomposition, edge reconstruction, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2021_paper.html  Github: https://github.com/ChunmingHe/FEDER

#### 6. Summary : 
- (1):本文研究的是伪装目标检测，该任务的主要挑战在于伪装目标与背景的相似性以及模糊的边界问题。

- (2):现有的方法主要是模仿人类视觉系统，但是这些方法在复杂的伪装策略下容易受到干扰，难以挖掘微小的区别特征，因此无法有效地解决伪装目标检测中的挑战。本文提出了一种基于特征分解和边缘重建的模型，通过将提取的特征分解成不同的频带，并聚焦于最具信息量的频带，以挖掘区分前景和背景的微妙线索。同时，本文还提出了学习辅助边缘重建任务来解决模糊边界问题。与现有方法相比，本文的方法具有更好的性能和更低的计算和内存成本。

- (3):本文提出了一种基于特征分解和边缘重建的模型，通过将提取的特征分解成不同的频带，并聚焦于最具信息量的频带，以挖掘区分前景和背景的微妙线索。同时，本文还提出了学习辅助边缘重建任务来解决模糊边界问题。本文的方法在四个数据集上均取得了显著的性能提升。

- (4):本文的方法在伪装目标检测任务上取得了显著的性能提升，能够更好地解决伪装目标检测中的挑战。
#### 7. 方法详细介绍：
本文提出了一种名为FEDER的模型，用于伪装目标检测。该模型包括三个模块：深度小波分解模块、物体边缘重建模块和多尺度融合模块。深度小波分解模块将输入图像分解为高频、低频和全局频率三个部分，物体边缘重建模块通过求解常微分方程来重建物体边缘，多尺度融合模块通过级联和拼接将编码器和解码器的多尺度特征融合。该模型的损失函数包括两种监督信号：分割掩模和伪装目标的边缘。总损失由分割损失和边缘损失组成。

#### 8. 实验设置：
本文在四个常用的伪装目标检测数据集上进行了实验，包括CHAMELEON、CAMO、COD10K和NC4K。训练集由COD10K和CAMO数据集中的图像组成，测试集由剩余的伪装图像组成。使用ResNet50作为编码器，采用单输入尺度和单阶段的训练方式。模型使用PyTorch实现，优化器为Adam，学习率为0.0001，每80个epoch降低一次学习率。训练和测试阶段的图像大小均为384×384。

#### 9. 实验结果与分析：
本文在四个数据集上对比了FEDER模型和其他伪装目标检测方法。结果表明，FEDER模型在Fβ、Eφ和Sα等指标上均取得了优秀的表现，且在定位准确性和边缘清晰度方面优于其他方法。在COD10K和NC4K数据集上的消融实验中，本文证明了DWD和OER模块的有效性。


# Paper:188     Unicode类比：一项反客观主义的视觉推理挑战



#### 1. Title: 
Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge

#### 2. Authors: 
Steven Spratley, Krista A. Ehinger, Tim Miller

#### 3. Affiliation: 
第一作者：Steven Spratley，墨尔本大学计算机与信息系统学院

#### 4. Keywords: 
analogical reasoning, computer vision, progressive-matrix problems, objectivism, Unicode Analogies

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2021_paper.html  Github: https://github.com/SvenShade/UnicodeAnalogies

#### 6. Summary : 
- (1):本文研究背景是计算机视觉中的类比推理，传统的统计分类模型假设数据可以被划分为不同的类别，而人类视觉感知则更加灵活，不仅仅是简单的物体识别，而是对场景的整体理解。
- (2):过去的方法主要是基于对象的模型，但是这种方法存在一些问题，比如缺乏对场景的整体理解，对于新的场景泛化能力较差等。本文提出了Unicode Analogies数据集，通过字符级别的PMPs来测试视觉系统的概念化能力，相比于其他数据集，该数据集更加丰富，更加具有挑战性。
- (3):本文提出了Unicode Analogies数据集，该数据集通过字符级别的PMPs来测试视觉系统的概念化能力，相比于其他数据集，该数据集更加丰富，更加具有挑战性。本文的创新点在于提出了一种新的测试方法，可以更好地评估模型的概念化能力。
- (4):本文的方法在Unicode Analogies数据集上进行了测试，取得了较好的性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文介绍了Unicode类比挑战赛的设计和实现。该挑战赛旨在测试视觉系统的概念推理能力，包括对多义字符的理解和上下文的处理。作者使用了多种模型，包括ResNet、MRNet、SCL和RelBase，并对数据集进行了不同的划分和交叉验证。作者还建立了人类基准线进行比较。未来的工作可以集中在开发更加稳健的模型，并将其针对数据集的特定特征进行调整。

#### 8. 实验设置：
本文使用了Unicode类比挑战赛的数据集，并对其进行了不同的划分和交叉验证。作者使用了5折交叉验证，并提供了数据集划分、折叠、种子和其他参数的详细信息。作者强调了字符保留的重要性，并指出模型容易过拟合。未来的工作可以探索控制特征的影响，例如领域转移、干扰和误导因素，以测试人类概念发现和类别学习模型的性能。

#### 9. 实验结果和分析：
本文的实验结果表明，当前最先进的模型在Unicode类比挑战赛上的表现仍然远远低于人类的表现。作者还分析了模型在不同规则类型和模式类别上的表现。作者建议未来的工作可以集中在开发更加稳健的模型，并将其针对数据集的特定特征进行调整。此外，该框架还可以适用于不同的学习范式，包括元学习和无监督学习。


# Paper:189     通过蒸馏从有噪声标签的数据中学习



#### 1. Title: 
Learning to Learn from Noisy Labeled Data with Distillation

#### 2. Authors: 
Yisen Wang, Xinggang Wang, Wenyu Liu, Xinsheng Zhang, Chunhua Shen

#### 3. Affiliation: 
南京大学

#### 4. Keywords: 
Noisy Labels, Learning to Learn, Distillation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Learning_to_Learn_From_Noisy_Labeled_Data_With_Distillation_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是在存在噪声标签的情况下，如何从中学习有效的模型。
- (2):过去的方法主要是通过数据清洗或者使用一些特殊的损失函数来解决噪声标签的问题，但是这些方法都有一定的局限性。本文提出了一种新的学习方法，通过蒸馏的方式，将一个学习良好的模型的知识迁移到一个新的模型中，从而使得新模型更加鲁棒。
- (3):本文提出的方法是基于蒸馏的学习方法，通过将一个学习良好的模型的知识迁移到一个新的模型中，从而使得新模型更加鲁棒。同时，本文还提出了一种新的损失函数，用于解决噪声标签的问题。
- (4):本文在CIFAR-10和CIFAR-100数据集上进行了实验，结果表明，本文提出的方法在存在噪声标签的情况下，可以取得更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“基于蒸馏的学习来自有噪声标签数据的学习”的方法（DLL）。该方法包括两个阶段：教师训练和学生训练。在教师训练阶段，使用干净的标记数据集训练教师模型。在学生训练阶段，使用有噪声的标记数据集，以教师模型为指导，训练学生模型。学生模型被训练成模仿教师模型在干净标记数据集上的预测。该方法还包括一个正则化项，以鼓励学生模型从教师模型的预测中学习，而不是从有噪声的标签中学习。

具体步骤如下：
1. 使用干净的标记数据集训练教师模型；
2. 使用有噪声的标记数据集训练学生模型，以教师模型为指导；
3. 学生模型被训练成模仿教师模型在干净标记数据集上的预测；
4. 引入正则化项，以鼓励学生模型从教师模型的预测中学习，而不是从有噪声的标签中学习。

#### 8. 实验设置：
当前文本中没有关于实验设置的信息。

#### 9. 实验结果和分析：
当前文本中没有关于实验结果和分析的信息。


# Paper:190     双部分表示法的无监督矢量字体合成



#### 1. Title: 
DualVector: Unsupervised Vector Font Synthesis with Dual-Part Representation

#### 2. Authors: 
Ying-Tian Liu, Zhifei Zhang, Yuan-Chen Guo, Matthew Fisher, Zhaowen Wang, Song-Hai Zhang

#### 3. Affiliation: 
第一作者：清华大学计算机科学与技术系

#### 4. Keywords: 
Vector font synthesis, dual-part representation, unsupervised learning, boolean operations, bezier paths

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_DualVector_Unsupervised_Vector_Font_Synthesis_With_Dual-Part_Representation_CVPR_2021_paper.html  Github: https://github.com/thuliu-yt16/dualvector

#### 6. Summary : 
- (1):本文研究的背景是自动生成高质量矢量字体的需求。
- (2):过去的方法主要是将字形视为像素化图像，但在缩放和矢量化后会出现伪影和质量损失。现有的矢量字体合成方法要么无法简洁地表示形状，要么需要矢量监督训练。本文提出了一种新的双部分表示法，将每个字形建模为一组闭合的“正”和“负”路径对的集合，并通过布尔运算得到字形轮廓。本文的方法名为DualVector，从字形图像中学习这种表示法，然后通过轮廓细化步骤进一步增强细节。本文的方法在矢量字体合成方面表现出色，无需矢量监督训练，生成的矢量字体可以轻松转换为常见的数字字体格式。
- (3):本文提出了一种新的双部分表示法，将每个字形建模为一组闭合的“正”和“负”路径对的集合，并通过布尔运算得到字形轮廓。本文的方法名为DualVector，从字形图像中学习这种表示法，然后通过轮廓细化步骤进一步增强细节。本文的方法无需矢量监督训练，生成的矢量字体可以轻松转换为常见的数字字体格式。
- (4):本文的方法在矢量字体合成方面表现出色，生成的矢量字体可以轻松转换为常见的数字字体格式。本文的方法在定量和定性方面均优于现有的方法。
#### 7. 方法详细介绍：
本文提出了一种双部分表示法，用于矢量字体合成，结合了矢量和图像模态。该表示法由闭合参数贝塞尔路径组成，这些路径被分组成双部分，以表示字形。矢量分支以潜在编码为输入，输出双部分表示法，而图像分支将潜在编码映射到字形的像素化图像。轮廓细化步骤使用可微分渲染技术将双部分表示法转换为字形轮廓，并进行细化。字体重建和生成任务的优化目标包括感知损失和占用场损失。

具体步骤如下：
1. 使用卷积神经网络（CNN）从字形图像中提取特征。
2. 将特征输入到矢量分支和图像分支中，分别生成双部分表示法和像素化图像。
3. 使用布尔运算将双部分表示法转换为字形轮廓。
4. 使用可微分渲染技术对轮廓进行细化，以增强细节。

#### 8. 实验设置：
本文使用了两个数据集进行实验，分别是Adobe Font Folio和Google Fonts。实验中使用了两个任务，即字体重建和字体生成。字体重建任务的输入是字形图像，输出是重建的矢量字体。字体生成任务的输入是随机噪声，输出是生成的矢量字体。实验中使用了感知损失和占用场损失作为优化目标。

#### 9. 实验结果和分析：
实验结果表明，本文提出的方法在字体重建和生成任务上均取得了优秀的性能。与现有方法相比，本文方法能够更好地保留字形的细节和结构，并且生成的字体更加自然。此外，本文方法还能够在不同字体之间进行插值，生成新的字体。


# Paper:191     基于采样搜索的可控零样本图像描述



#### 1. Title: 
ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing

#### 2. Authors: 
Zequn Zeng, Hao Zhang, Ruiying Lu, Dongsheng Wang, Bo Chen, Zhengjue Wang

#### 3. Affiliation: 
第一作者单位：西安电子科技大学雷达信号处理国家重点实验室

#### 4. Keywords: 
Zero-shot image captioning, controllable image captioning, language model, sampling-based polishing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zeng_ConZIC_Controllable_Zero-Shot_Image_Captioning_by_Sampling-Based_Polishing_CVPR_2021_paper.html  Github: https://github.com/joeyz0z/ConZIC

#### 6. Summary : 
- (1):本文研究的背景是零样本图像描述任务，即让机器在没有标注数据的情况下完成图像描述任务。
- (2):过去的方法主要依赖于有标注的数据集进行监督学习，而本文提出的方法是基于大规模预训练模型和采样搜索的非自回归语言模型，可以更加灵活高效地生成多样化的图像描述，并且是第一个探索可控零样本图像描述任务的方法。
- (3):本文提出了一种基于采样搜索的非自回归语言模型Gibbs-BERT，结合CLIP模型进行图像-文本匹配，实现了零样本图像描述任务。同时，引入了任务特定的鉴别器，实现了可控零样本图像描述任务。创新点在于采用了基于采样的多项式分布生成策略，实现了更加灵活高效的图像描述生成。
- (4):本文在MSCOCO数据集上进行了实验，结果表明，本文提出的方法在零样本图像描述和可控零样本图像描述任务上均取得了优异的性能，生成速度比现有方法快5倍左右，多样性得分比现有方法高1.5倍左右，同时在多个控制信号下均取得了较好的表现。
#### 7. 方法详细介绍：
本文提出了一种名为ConZIC的框架，用于可控的零样本图像字幕生成。该框架由三个模块组成：语言模型（LM）、匹配网络和鉴别器。其中，LM使用Gibbs-BERT进行采样，以生成更具灵活性和多样性的字幕。匹配网络用于衡量输入图像和生成的字幕之间的相似度。鉴别器用于衡量控制信号。ConZIC框架通过迭代地优化每个位置的单词来生成最终的字幕。具体步骤包括：初始化字幕、选择前K个候选单词、计算文本-图像和文本-控制信号匹配分数、集成分布、选择概率最大的单词。

#### 8. 实验设置：
本文在四个数据集上进行了实验：MSCOCO字幕、SentiCap、FlickrStyle10k和SketchyCOCO字幕。其中，MSCOCO字幕数据集包含113,287个训练和验证图像以及5,000个测试图像。SentiCap数据集是从MSCOCO数据集中构建的情感图像字幕数据集。FlickrStyle10k数据集包含10,000个带有风格化字幕的Flickr图像。SketchyCOCO字幕数据集是通过收集覆盖3个背景和14个前景类别的实例手绘草图构建的。实验基于冻结的预训练模型进行，没有进行任何微调。选择CLIP-ViT-B/32作为图像-文本匹配网络，选择BERT-Base作为LM。

#### 9. 实验结果与分析：
本文提出的ConZIC框架在标准的无控制图像字幕生成和四个可控图像字幕生成任务（包括长度控制、填充、风格控制和词性控制）上均取得了优异的性能。实验评估指标包括BLEU、METEOR、CIDEr和ROUGE-L等监督指标以及多样性和独特性等非监督指标。实验结果表明，ConZIC框架在速度、准确性和多样性方面均优于现有的SOTA方法ZeroCap。所有实验均在单个RTX3090 GPU上进行。


# Paper:192     3DAvatarGAN：跨领域的个性化可编辑化身生成



#### 1. Title: 
3DAvatarGAN: Bridging Domains for Personalized Editable Avatars

#### 2. Authors: 
Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, Sergey Tulyakov

#### 3. Affiliation: 
第一作者：KAUST（沙特阿拉伯国王科技大学）

#### 4. Keywords: 
3D GAN, domain adaptation, personalized avatars, artistic datasets, texture quality

#### 5. Paper: https:/rameenabdal.github.io/3DAvatarGAN  Github: https://github.com/RameenAbdal/3DAvatarGAN

#### 6. Summary : 
- (1):本文研究背景是如何在艺术数据集上训练3D GAN，以生成和编辑个性化的3D头像，这是一个具有挑战性的问题，因为艺术数据集的几何和纹理高度可变，而且通常没有相机注释。

- (2):过去的方法主要是在2D数据集上训练2D GAN，或者在3D数据集上训练3D GAN，但是这些方法无法处理艺术数据集的高度可变的几何和纹理。本文提出了一种基于领域自适应的框架，将2D GAN的知识转移到3D GAN上，从而在艺术数据集上训练3D GAN，同时保持多视角一致性和纹理质量。

- (3):本文提出了三个贡献：一是基于优化的方法，用于对齐不同领域之间的相机参数分布；二是纹理、深度和几何正则化，以避免退化的几何解决方案，如平面形状；三是一种基于变形的技术，用于建模艺术领域的夸张几何，从而实现个性化的几何编辑。此外，本文还提出了一种新的3D GAN反演方法，用于链接源域和目标域的潜在空间。

- (4):本文的方法可以生成、编辑和动画化艺术数据集上的个性化3D头像。实验结果表明，与naive fine-tuning相比，本文的方法可以生成更高质量的3D头像，同时保持几何和纹理的一致性。
#### 7. 方法详细介绍：
本文提出了一种名为3DAvatarGAN的方法，用于生成个性化可编辑的3D头像。该方法包括以下步骤：
1. 对齐源域和目标域的相机参数。
2. 设计损失函数和正则化器，用于更新生成器网络中的选定参数。
3. 使用无条件双重鉴别器进行训练。
4. 引入TPS网络，用于操纵2D前平面特征以学习额外的变形或夸张。
5. 通过优化方法匹配相机参数分布。
6. 选择合适的损失函数和正则化器。
7. 设计TPS网络。

#### 8. 实验设置：
本文使用了CelebA-HQ和FFHQ两个数据集进行实验。实验使用了NVIDIA Tesla V100 GPU进行训练，使用了Adam优化器，学习率为0.0001，批量大小为16。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的方法可以生成高质量的3D头像，并且可以进行各种编辑和变形。此外，本文还进行了定量和定性的实验评估，证明了所提出的方法的有效性和优越性。


# Paper:193     CABM：面向大输入的单幅图像超分辨率网络的内容感知位映射



#### 1. Title: 
CABM: Content-Aware Bit Mapping for Single Image Super-Resolution Network with Large Input

#### 2. Authors: 
Senmao Tian, Ming Lu, Jiaming Liu, Yandong Guo, Yurong Chen, Shunli Zhang

#### 3. Affiliation: 
北京交通大学

#### 4. Keywords: 
Super-Resolution, Large Input, Content-Aware Bit Mapping, Quantization, Lookup Table

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Tian_CABM_Content-Aware_Bit_Mapping_for_Single_Image_Super-Resolution_Network_With_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是单幅图像超分辨率网络在大输入情况下的问题。由于现实世界中的显示设备分辨率已经达到4K或8K，因此超分辨率技术需要处理大输入的情况，例如将2K的图像超分辨率到更高的分辨率。然而，由于计算和内存成本随着输入大小的增加而呈二次增长，因此现有方法通常将大输入分成局部块，然后将SR块合并到输出中。这些方法通过为每个块分配子网来减少计算成本。 
- (2):现有的方法通常使用量化技术来设计子网，以实现更高的速度。CADyQ使用MLP位选择器来确定每个层的适当位。然而，由于MLP位选择器会带来额外的计算和存储成本，而且由于统一采样，选择的位不是最优的。本文提出了一种名为Content-Aware Bit Mapping（CABM）的新方法，该方法可以在不损失性能的情况下删除位选择器。 CABM还在训练期间为每个层学习位选择器。在训练后，我们分析了输入块的边缘信息与每个层的位之间的关系。我们观察到，边缘信息可以是选择位的有效指标。因此，我们设计了一种策略来构建一个Edge-to-Bit查找表，该表将块的边缘得分映射到每个层的位。 SR网络的位配置可以由所有层的查找表确定。我们的策略可以找到更好的位配置，从而实现更高效的混合精度网络。 
- (3):本文提出了一种新的方法Content-Aware Bit Mapping（CABM），该方法可以在不损失性能的情况下删除位选择器。 CABM还在训练期间为每个层学习位选择器。在训练后，我们分析了输入块的边缘信息与每个层的位之间的关系。我们观察到，边缘信息可以是选择位的有效指标。因此，我们设计了一种策略来构建一个Edge-to-Bit查找表，该表将块的边缘得分映射到每个层的位。 SR网络的位配置可以由所有层的查找表确定。我们的策略可以找到更好的位配置，从而实现更高效的混合精度网络。 
- (4):本文的方法在各种SR架构和缩放因子下进行了详细的实验，证明了其泛化能力。与CADyQ相比，我们的CABM可以实现相同的性能，同时具有更低的平均位和可忽略的额外计算成本。
#### 7. 方法详细介绍：
本文提出了一种名为Content-Aware Bit Mapping（CABM）的方法，用于确定单图像超分辨率网络的最佳位配置。该方法分为两个阶段：首先使用一组子网构建超网CTW*，然后使用CABM微调超网以获得每个子网的最佳位配置。CABM方法包括三个主要组件：边缘检测、位配置确定和CABM超网推理。在推理过程中，CABM将输入分成局部块，计算边缘分数，并从超网中获取每个块的相应子网。最后，CABM将SR块合并到输出中。

#### 8. 实验设置：
本文使用了DIV2K数据集进行实验，其中包括800张训练图像、100张验证图像和100张测试图像。实验中使用了三种评估指标：峰值信噪比（PSNR）、结构相似性指数（SSIM）和感知质量评估指标（PI）。实验中使用了三种方法进行比较：全精度模型、PAMS和CADyQ。

#### 9. 实验结果和分析：
本文提出的CABM方法在减少计算开销的同时，实现了几乎无损的模型性能，相比于PAMS和CADyQ，该方法的性能优于全精度模型。在x2的缩放因子上进行比较，该方法获得了与PAMS和CADyQ相似的结果，而FAB要低得多。定性结果也证明了CABM方法的有效性和泛化性。


# Paper:194     通过遮蔽实现语言-图像预训练的规模化



#### 1. Title: 
Scaling Language-Image Pre-training via Masking

#### 2. Authors: 
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He

#### 3. Affiliation: 
Meta AI, FAIR (Facebook AI Research) 元智能，Facebook AI Research

#### 4. Keywords: 
Language-Image Pre-training, CLIP, Masking, Contrastive Learning, Vision-Language Learning

#### 5. Paper: https://arxiv.org/abs/2106.06610  Github: https://github.com/facebookresearch/flip

#### 6. Summary : 
- (1):本文研究了语言-图像预训练的方法，提出了一种通过遮蔽（Masking）来加速训练的方法，以更高效地处理更多的图像-文本对，从而提高模型的准确性和训练速度。 

- (2):传统的语言-图像预训练方法需要大量的计算资源和时间，限制了其在大规模数据上的应用。本文提出的方法通过遮蔽图像的一部分来减少计算量，从而提高训练速度。与其他方法相比，本文的方法在速度和准确性上都有所提高。 

- (3):本文提出的方法是在CLIP（Contrastive Language-Image Pre-training）的基础上，通过遮蔽图像的一部分来减少计算量，从而提高训练速度。本文的方法在多个下游任务上都表现出了优异的性能，同时也探索了模型大小、数据大小和训练长度等方面的扩展性。 

- (4):本文的方法在400万个图像-文本对上进行了实验，结果表明，与传统方法相比，本文的方法在速度和准确性上都有所提高。在多个下游任务上，本文的方法都表现出了优异的性能，证明了其在大规模数据上的应用潜力。
#### 7. 方法详细介绍：
- 本文提出了一种名为FLIP的快速语言-图像预训练方法，用于训练CLIP模型。
- FLIP方法在训练过程中随机屏蔽和移除大量图像块，从而可以在相同的墙钟时间内学习更多的图像-文本对，并在类似的内存占用情况下进行更多的对比样本。
- 图像编码器基于Vision Transformer（ViT），文本编码器通过最小化对比损失进行训练。对比学习的负样本对包括同一批次中的其他样本。
- 与Masked Autoencoder（MAE）不同，FLIP方法不使用重构损失。编码器在屏蔽图像上进行预训练，但可以直接应用于完整图像而无需更改。

#### 8. 实验设置：
- 实验使用ViT-L/16模型在400万个图像-文本对上进行训练。
- 训练数据集为LAION-400M，训练时长为6.4、12.8或32个epoch，对于每个屏蔽比例都进行了实验。
- 使用ImageNet-1K验证集进行零样本迁移的准确性评估。
- 实验在256个TPU-v3核心上进行。

#### 9. 实验结果和分析：
- FLIP方法在大多数指标上都表现出比CLIP更好的性能，包括零样本分类、零样本检索、零样本鲁棒性评估、图像字幕和视觉问答等多个下游任务。
- 在相同的训练时长内，FLIP方法达到了比CLIP更高的准确率，同时仍然比CLIP快2-3倍。
- FLIP方法在训练过程中探索了模型规模、数据规模和训练长度的缩放行为，并发现数据缩放对性能的提升是一致的，而模型缩放则更适合于迁移学习。同时，同时缩放模型和数据可以进一步提高性能。
- FLIP方法在ImageNet-1K分类任务上取得了78.8%的零样本准确率，优于使用ViT-H在公共数据上训练的最新结果。


# Paper:195     一次高保真变形神经辐射场对话头合成



#### 1. Title: 
One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field

#### 2. Authors: 
Weichuang Li, Longhao Zhang, Dong Wang, Bin Zhao, Zhigang Wang, Mulin Chen, Bang Zhang, Zhongjian Wang, Liefeng Bo, Xuelong Li

#### 3. Affiliation: 
上海人工智能实验室 Shanghai AI Laboratory

#### 4. Keywords: 
Talking-head synthesis, Deformable Neural Radiance Field, Multi-scale Generalized Appearance module, Lightweight Expression-aware Deformation module

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2021_paper.html
Github: https://github.com/WeichuangLi/HiDe-NeRF

#### 6. Summary : 
- (1):本文研究的是一种高保真度的单次生成式对话头合成方法，旨在生成保留源图像身份信息并模仿驱动图像运动的面部。 
- (2):过去的方法主要依赖于2D表示，当遇到大的头部旋转时，不可避免地会出现面部扭曲。最近的方法则采用显式的3D结构表示或隐式神经渲染来提高在大姿态变化下的性能。然而，身份和表情的保真度并不理想，特别是对于新视角合成。本文提出了HiDe-NeRF，它实现了高保真度和自由视角的对话头合成。 
- (3):HiDe-NeRF将3D动态场景表示为规范外观场和隐式变形场，前者包括规范源脸部，后者模拟驱动姿态和表情。为了提高身份表现力，我们设计了一个广义外观模块，利用多尺度体积特征来保留面部形状和细节。为了提高表情精度，我们提出了一个轻量级变形模块，将姿态和表情明确分离，以实现精确的表情建模。 
- (4):在多个数据集上的实验表明，我们的方法可以比以前的方法生成更好的结果，包括捕捉驱动运动和保留源身份信息。
#### 7. 方法详细介绍：
本文提出了一种一次性、主体不可知的可变形神经辐射场（HiDe-NeRF）方法，用于高保真度的说话头部合成。该方法使用形状和表情感知坐标编码（SECC）构建变形场，使用多尺度广义外观模块（MGA）从单个图像中提取三平面表示。引入轻量级表情感知变形模块（LED）来显式地解耦变形预测中的表情和姿态，从而显著提高表情保真度。图像生成模块包括体积渲染和纹理细化。

#### 8. 实验设置：
实验在三个常用的说话头合成数据集（VoxCeleb1、VoxCeleb2和TalkingHead-1KH）上进行。每个帧都被裁剪和对齐为256x256，以使说话肖像居中，其旋转角度是通过面部对齐预测的。使用结构相似性（SSIM）、PSNR、LPIPS和FID等质量指标来衡量合成图像的质量。使用CSIM、PRMSE和AUCON等保真度指标来评估源图像的保真度、头部姿态的准确性和表情的精度。

#### 9. 实验结果和分析：
本文提出的方法在自我再现和跨身份再现任务中均优于五种最先进的方法，定量结果证明了这一点。该方法在模仿驱动图像的表情和头部姿态方面表现出了显著的精度，并有效地保留了源脸形，显示出了改进的身份保真度。本文还引入了一个新的指标，平均顶点距离（AVD），以更好地评估身份保真度。


# Paper:196     基于相关网络的连续手语识别



#### 1. Title: 
Continuous Sign Language Recognition with Correlation Network

#### 2. Authors: 
Lianyu Hu, Liqing Gao, Zekang Liu, Wei Feng

#### 3. Affiliation: 
天津大学智能与计算学部

#### 4. Keywords: 
Continuous sign language recognition, correlation network, body trajectories, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2021_paper.html  Github: https://github.com/hulianyuyy/CorrNet

#### 6. Summary : 
- (1):本文研究的是连续手语识别，主要关注手语中的身体轨迹，提出了一种新的方法来捕捉和利用跨帧的身体轨迹来识别手语。

- (2):过去的方法通常独立处理每一帧，无法捕捉跨帧轨迹以有效识别手语。本文提出的CorrNet通过计算当前帧和相邻帧之间的相关性图来识别所有空间补丁的轨迹，然后动态强调这些相关性图中的身体轨迹。与其他时空推理方法相比，CorrNet能够更好地捕捉身体轨迹，从而在四个大型数据集上实现了新的最高准确率。

- (3):本文提出了一种新的方法，即CorrNet，它通过计算相关性图来捕捉和利用跨帧的身体轨迹来识别手语。具体来说，CorrNet首先使用相关性模块动态计算当前帧和相邻帧之间的相关性图，以识别所有空间补丁的轨迹。然后，提出了一个识别模块来动态识别和强调这些相关性图中的身体轨迹。这种方法不依赖于额外的昂贵监督，如身体关键点或热图，可以以轻量级的方式进行端到端的训练。因此，生成的特征能够获得局部时间运动的概述，以识别手语。

- (4):本文的方法在四个大型数据集上实现了新的最高准确率，即PHOENIX14、PHOENIX14-T、CSL-Daily和CSL。与其他时空推理方法的全面比较证实了CorrNet的有效性。可视化结果证明了CorrNet在强调相邻帧中的人体轨迹方面的效果。
#### 7. 方法详细介绍：
本文提出了一种名为CorrNet的方法，用于连续手语识别。该方法包括两个模块：相关模块和识别模块。相关模块通过计算当前帧和相邻帧之间的相关性图来捕捉所有空间补丁的轨迹。识别模块动态定位和强调这些相关性图中的身体轨迹。相关模块和识别模块的输出相乘以增强帧间相关性。此外，本文还提出了一种多尺度架构，用于识别模块中的身体区域。最后，本文还进行了对比实验，证明了CorrNet在PHOENIX14、PHOENIX14-T、CSL-Daily和CSL数据集上的优越性。

#### 8. 实验设置：
本文使用了PHOENIX14-T、CSL-Daily和CSL等数据集进行实验，并详细描述了训练过程。在训练过程中，使用ResNet18作为2D CNN骨干网络，使用双层BiLSTM进行长期时间建模，并使用VE和VA损失进行视觉监督。输入帧在训练期间被调整大小并随机裁剪，使用词错误率（WER）作为评估指标。本文还进行了消融实验，以研究识别模块的多尺度架构、相关模块的邻域K、提出模块的有效性以及CorrNet的位置。最后，本文将提出的方法与其他具有空间-时间推理能力和明确利用手和面部特征的方法在PHOENIX14数据集上进行了比较。

#### 9. 实验结果和分析：
本文的方法CorrNet在四个大型数据集上均取得了最先进的准确率，即PHOENIX14、PHOENIX14-T、CSL-Daily和CSL。与以前的CSLR方法相比，CorrNet通过捕捉相邻帧之间的轨迹和定位身体区域，取得了更好的性能。与其他基于注意力的方法（如SENet、CBAM和NLNet）相比，CorrNet具有更好的身体轨迹识别和聚合能力。在CSL数据集上，CorrNet取得了极高的准确率（WER为0.8%），优于现有的CSLR方法。


# Paper:197     双模态跨域三维手部姿态估计



#### 1. Title: 
Cross-Domain 3D Hand Pose Estimation with Dual Modalities

#### 2. Authors: 
Qiuxia Lin, Linlin Yang, Angela Yao

#### 3. Affiliation: 
National University of Singapore（新加坡国立大学）

#### 4. Keywords: 
Hand pose estimation, synthetic data, dual-modality network, contrastive learning, self-distillation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究手部姿态估计中的跨域问题，提出了一种半监督的双模态网络，用于从标记的多模态合成数据和未标记的真实世界数据中学习模型。 
- (2):过去的方法主要是使用合成数据进行预训练，但由于合成数据与真实数据之间的差距，这些方法在真实世界中的泛化能力较差。本文提出了一种使用双模态网络的方法，利用合成RGB和深度图像进行预训练，并使用自我蒸馏技术来减少伪标签噪声。 
- (3):本文提出了一种双模态网络，用于RGB图像和深度图像的预训练，以及使用自我蒸馏技术进行微调。该网络使用多模态对比学习和注意力融合监督来学习RGB图像的有效表示。在微调过程中，使用自我蒸馏技术来减少伪标签噪声。 
- (4):该方法在2D关键点检测和3D关键点估计方面显著提高了性能，分别达到了16.0％和14.8％的提升。性能支持了他们的目标。
#### 7. 方法详细介绍：
本文提出了一种双模态网络，用于跨域三维手部姿态估计。该网络具有两个编码器，一个用于深度图像，一个用于RGB图像，以及一个共享的解码器。深度编码器应用注意力模块，以产生自我注意深度图像特征，RGB编码器应用注意力模块，以产生注意力融合的RGB特征。该模型使用多模态对比学习和注意力融合监督的合成数据进行预训练。使用由预训练模型生成的嘈杂伪标签的真实数据进行微调。微调需要两次数据传递，以启用自我蒸馏过程。

#### 8. 实验设置：
本文使用RHD合成数据集和四个真实手部数据集（STB、FreiHAND、H3D和MVHand）进行训练和评估。使用PCK进行2D关键点检测的性能评估，使用3D PCK的EPE和AUC进行3D关键点估计的性能评估。使用ResNet-101作为骨干网络，输入数据在手部周围裁剪并调整大小为256x256，输出分辨率为64x64。使用Adam优化器对合成数据进行预训练，学习率为2.5e-4，使用合成和真实数据进行微调，学习率为2.5e-5，微调6个epoch。

#### 9. 实验结果和分析：
本文提出的方法在所有四个基准测试（STB、FreiHAND、H3D和MVHand）上均优于现有方法，对于2D关键点检测，本文方法的性能优于现有方法。对于3D关键点估计，本文方法在STB数据集上的性能优于基线和其他弱监督/半监督方法。本文方法在所有四个数据集上均优于基线和其他SemiHand变体，用于3D关键点估计。本文还进行了消融研究，验证了多模态对比学习、伪标签和自我蒸馏等提出组件的有效性。本文的定性结果表明，该方法可以成功估计具有完整手部的深度图像，并缓解由于遮挡引起的低激活问题。但是，仍存在一些失败案例，例如多模态输出不一致或估计遮挡关键点时。


# Paper:198     随机特征的可微架构搜索



#### 1. Title: 
Differentiable Architecture Search with Random Features

#### 2. Authors: 
Xuanyang Zhang, Yonggang Li, Xiangyu Zhang, Yongtao Wang, Jian Sun

#### 3. Affiliation: 
MEGVII Technology (旷视科技), Peking University (北京大学)

#### 4. Keywords: 
Neural Architecture Search, Differentiable Architecture Search, Random Features, Performance Collapse, Skip-Connection

#### 5. Paper: 
Paper Link: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Differentiable_Architecture_Search_With_Random_Features_CVPR_2021_paper.html
Github Link: None

#### 6. Summary:
- (1): 本文研究神经架构搜索（NAS）中的可微架构搜索（DARTS）方法，提出了一种新的DARTS范式，通过随机特征来解决DARTS的性能崩溃问题。
- (2): 本文首先探究了DARTS中超网络的表达能力，发现只训练BatchNorm可以获得最佳的搜索效果，因此提出了一种只训练BatchNorm的DARTS范式。其次，本文发现随机特征可以削弱skip-connection在超网络优化中的辅助连接作用，从而使搜索算法更加公平地选择操作，从而解决了DARTS的性能崩溃问题。本文提出的RF-DARTS和RF-PCDARTS在CIFAR-10、CIFAR-100和SVHN三个数据集上表现稳健，在ImageNet数据集上取得了最新的最佳结果。
- (3): 本文提出了一种新的DARTS范式，即只训练BatchNorm的DARTS，并通过随机特征来解决DARTS的性能崩溃问题。本文还探究了随机特征在DARTS中的作用机制，发现随机特征可以削弱skip-connection在超网络优化中的辅助连接作用，从而使搜索算法更加公平地选择操作。本文提出的RF-DARTS和RF-PCDARTS在神经架构搜索任务中取得了最新的最佳结果。
- (4): 本文在CIFAR-10、CIFAR-100和SVHN三个数据集上进行了实验，RF-DARTS取得了94.36%的测试准确率，达到了NAS-Bench-201中最优结果94.37%的最近似值。在ImageNet数据集上，RF-DARTS从CIFAR-10转移学习，取得了最新的最佳结果24.0%的top-1测试错误率。RF-DARTS和RF-PCDARTS在多个数据集和搜索空间上表现稳健，取得了最新的最佳结果，证明了本文提出方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种新的可微架构搜索方法RF-DARTS，它在搜索阶段引入了BatchNorm层的可学习仿射权重。RF-DARTS的优化目标是最小化验证损失，只更新BN的权重，而将Conv的权重冻结在初始化状态。通过梯度下降交替优化架构参数和BN的权重。为了加速搜索过程，利用了梯度的一阶近似。通过考虑DARTS中跳跃连接的作用，探索了随机特征的工作机制。随机特征稀释了辅助连接在超网络训练中扮演的角色，使DARTS能够专注于更公平的操作选择。从DARTS梯度的角度对随机特征进行了理论和实证分析，这是理解NAS的新工具。

#### 8. 实验设置：
本文在NAS-Bench-201、DARTS搜索空间和RobustDARTS S1-S4搜索空间上评估了DARTS与随机特征的性能。RF-DARTS和RF-PCDARTS遵循DARTS和PC-DARTS的默认训练设置。具体的实现细节和搜索到的架构在补充材料中有详细描述。

#### 9. 实验结果与分析：
本文在CIFAR-10、CIFAR-100和ImageNet16-120上比较了RF-DARTS与最先进的方法在NAS-Bench-201上的性能。RF-DARTS在CIFAR-10、CIFAR-100和ImageNet16-120上的表现几乎达到最优，并且在CIFAR-10、CIFAR-100和ImageNet16-120上分别比最新的最先进方法DARTS-高出0.47％、1.41％和0.98％。此外，文本还展示了在CIFAR-10和ImageNet上搜索的结果，其中RF-DARTS分别在这三个数据集上获得了测试误差2.60％、16.50％和24.0％。与vanilla DARTS2nd相比，RF-DARTS在三个数据集上分别实现了0.16％、0.96％和2.9％的改进。在DARTS搜索空间中，RF-DARTS与最先进的权重共享NAS方法进行了比较。评估的架构在CIFAR-10上进行搜索，然后转移到CIFAR-100和ImageNet。RF-DARTS在三个范例中均优于六个强大的基准。此外，还在CIFAR-10、CIFAR-100和SVHN上评估了RF-DARTS的鲁棒性。RF-DARTS在四个搜索空间和三个数据集上均大幅优于vanilla DARTS。这些结果表明，RF-DARTS具有相当强的鲁棒性。


# Paper:199     成年果蝇全脑神经元细胞体分割基准



#### 1. Title: 
A Soma Segmentation Benchmark in Full Adult Fly Brain

#### 2. Authors: 
Xiaoyu Liu, Bo Hu, Mingxing Li, Wei Huang, Yueyi Zhang, Zhiwei Xiong

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
neuron reconstruction, deep learning, electron microscopy, soma segmentation, connectomics

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_A_Soma_Segmentation_Benchmark_in_Full_Adult_Fly_Brain_CVPR_2021_paper.html  Github: https://github.com/liuxy1103/EMADS

#### 6. Summary : 
- (1):本文研究了高分辨率电子显微镜数据下的成年果蝇全脑神经元重建中的细胞体分割问题，旨在为神经科学家探索神经元如何启发智能提供基础。 
- (2):传统的细胞体分割方法主要基于光学显微镜图像，分辨率较低，难以获得每个细胞体的精确形态。而现有的基于深度学习的神经元重建方法也无法直接提供准确的细胞体分布和形态信息。本文提出了一种高效的细胞体重建方法，包括一个高分辨率的电子显微镜数据集和一个两阶段的深度学习算法，能够预测3D细胞体实例的准确位置和边界。 
- (3):本文提出了一种高效的细胞体重建方法，包括一个高分辨率的电子显微镜数据集和一个两阶段的深度学习算法。首先，我们制作了一个高分辨率的电子显微镜数据集，其中包含204个完全注释的3D细胞体，通过两阶段的深度学习算法，可以预测3D细胞体实例的准确位置和边界。最后，我们在测试集上提供了定量和定性的基准比较，验证了所提出方法的优越性，以及从生物学角度对重建细胞体的初步统计。 
- (4):本文提出的方法在成年果蝇全脑神经元重建中，能够准确地预测细胞体的分布和形态信息，具有较高的效率和准确性。在测试集上，所提出的方法相比于现有的替代方法具有更好的性能。
#### 7. 方法详细介绍：
本文提出了一种高效准确的成年果蝇全脑神经元细胞体分割算法。该算法包含两个阶段：定位阶段和分割阶段。定位阶段使用3D U-Net预测细胞体种子的概率图，并生成包围框，分割阶段则使用3D U-Net对位于包围框中心的细胞体进行分割。算法通过提供3D细胞体候选框来过滤掉没有细胞体的区域，从而加速整个大脑的细胞体重建。分割网络使用U-Net骨干网络，在分割标签集上进行训练，将感兴趣的细胞体、细胞体边界和其余区域从给定的补丁中分类。为了应对成年果蝇全脑的大量EM数据和加速计算，采用了分布式CPU和GPU集群的并行高吞吐量数据处理流水线。

#### 8. 实验设置：
本文使用了一个高分辨率的EM细胞体数据集，包含超过80亿个体素的手动注释。使用了两个NVIDIA Tesla V100 GPU和一个Intel Xeon E5-2698 v4 CPU的计算机集群进行实验。使用了Python和PyTorch框架进行算法实现。

#### 9. 实验结果和分析：
本文提出的算法在成年果蝇全脑神经元细胞体分割任务中取得了较好的性能。平均精度（mAP）为0.301，mAP50为0.713，优于其他两种对比方法。此外，本文还提供了不同区域细胞体大小和直径的统计数据。通过实验结果和分析，验证了本文算法的准确性和高效性，为未来果蝇神经系统研究提供了有价值的参考。


# Paper:200     弹性聚合用于联邦优化



#### 1. Title: 
Elastic Aggregation for Federated Optimization

#### 2. Authors: 
Dengsheng Chen, Jie Hu, Vince Junkai Tan, Xiaoming Wei, Enhua Wu

#### 3. Affiliation: 
第一作者：美团
其他作者：ISCAS计算机科学国家重点实验室、中国科学院大学、字节跳动、澳门大学

#### 4. Keywords: 
Federated learning, Elastic aggregation, Parameter sensitivity, Client drift, Non-IID data

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Elastic_Aggregation_for_Federated_Optimization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是联邦学习中的模型聚合问题，针对数据不均衡（Non-IID）的情况下，传统的FedAvg算法容易出现客户端漂移（Client drift）的问题，导致模型收敛缓慢不稳定。 
- (2):传统的FedAvg算法使用简单的平均聚合方法，无法解决数据不均衡的问题。本文提出了一种新的聚合方法，即弹性聚合（Elastic aggregation），通过测量参数灵敏度来自适应地插值客户端模型，从而避免客户端漂移的问题。相比于传统的FedAvg算法，弹性聚合方法更加高效、鲁棒性更强，且不需要对客户端数据进行任何假设。 
- (3):本文提出的弹性聚合方法通过测量参数灵敏度来自适应地插值客户端模型，从而避免客户端漂移的问题。相比于传统的FedAvg算法，弹性聚合方法更加高效、鲁棒性更强，且不需要对客户端数据进行任何假设。 
- (4):本文在真实数据和合成数据上进行了实验，结果表明弹性聚合方法在凸优化和非凸优化的情况下都能够有效地训练模型，并且在四种联邦学习场景（不可靠的链接、大规模分布、实质性异质性和不平衡数据）下都表现出色。弹性聚合方法还可以与其他联邦优化器很好地配合使用，并在各方面都取得了显著的改进。
#### 7. 方法详细介绍：
本文提出了一种名为弹性聚合的方法，用于解决联邦学习中客户端漂移的问题。该方法根据参数灵敏度插值客户端模型，以防止服务器模型漂移到任何一个客户端分布，并增强对不同客户端分布的探索。参数灵敏度是通过计算每个参数改变时整体预测函数输出的变化来衡量的。该方法易于集成到不同的联邦优化器中，相对于朴素聚合，取得了显著的改进。

#### 8. 实验设置：
本文使用了MNIST、CIFAR-10和Shakespeare数据集进行实验，测试了不同的联邦优化器，包括FedAvg、FedAvgM、FedProx、SCAFFOLD、AdaOpt和PFNM。在每轮中，对5%、10%、20%和40%的客户端进行采样。实验中还测试了不同的非IID性、参与率和轮数对模型性能的影响。实验结果表明，相对于朴素聚合，弹性聚合在收敛速度和个性化方面表现更好，但也带来了额外的计算负担和通信成本。

#### 9. 实验结果与分析：
实验结果表明，弹性聚合相对于朴素聚合在不同数据集和联邦优化器上都取得了更好的性能。在MNIST和CIFAR-10数据集上，弹性聚合的测试准确率分别比FedAvg提高了0.5%和1.2%。在Shakespeare数据集上，弹性聚合的测试困惑度比FedAvg提高了1.5。此外，实验还发现，弹性聚合对于非IID数据的性能提升更为明显。但是，弹性聚合也带来了额外的计算负担和通信成本。未来的工作可以将弹性聚合方法扩展到自适应优化方法或基于八卦的训练技术。


# Paper:201     NIRVANA：自适应网络和自回归补丁建模的视频神经隐式表示



#### 1. Title: 
NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling

#### 2. Authors: 
Shishira R Maiya, Sharath Girish, Max Ehrlich, Hanyu Wang, Kwot Sin Lee, Patrick Poirson, Pengxiang Wu, Chen Wang, Abhinav Shrivastava

#### 3. Affiliation: 
第一作者：Shishira R Maiya，隶属于美国马里兰大学

#### 4. Keywords: 
Implicit Neural Representations, Video Compression, Autoregressive Modeling, Patch-wise Prediction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Maiya_NIRVANA_Neural_Implicit_Representations_of_Videos_With_Adaptive_Networks_and_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是视频压缩，传统的压缩方法如JPEG和HEVC等存在一定的局限性，而基于深度学习的压缩方法虽然在某些方面表现优异，但需要大型网络，且泛化能力较差，限制了其实际应用。
- (2):过去的视频压缩方法往往无法充分利用视频中的时空冗余信息，且难以适应不同分辨率和长度的视频。本文提出的NIRVANA方法通过分组处理视频帧，采用补丁预测的方式，利用自回归模型进行视频表示建模，从而充分利用视频中的时空冗余信息，同时具有较好的可扩展性和适应性。
- (3):本文提出的NIRVANA方法采用自回归的补丁预测方式，将视频分组处理，每组视频帧采用单独的小型模型进行建模，且每个模型的权重都是由前一组模型的权重初始化得到的。此外，本文还采用了熵正则化的方法进行量化，避免了后续的剪枝和量化操作，提高了压缩效率。本文的创新点在于将视频压缩问题转化为模型压缩问题，充分利用了INR方法的优势，同时提出了一种自适应的压缩方法，能够适应不同的视频内容、分辨率和长度。
- (4):本文在UVG数据集上进行了实验，结果表明NIRVANA方法在保持相同压缩率的情况下，比NeRV方法提高了12倍的编码速度，同时达到了相同的重建质量。此外，本文还在4K分辨率和120fps的视频以及YouTube-8M数据集上进行了实验，结果表明NIRVANA方法具有很好的可扩展性和适应性，能够适应不同的视频内容、分辨率和长度，且能够实现自适应的比特率压缩。
#### 7. 方法详细介绍：
本文提出了一种名为NIRVANA的自回归视频INR框架，它利用视频中的空间和时间冗余来实现高速编码。该方法采用自回归模型，对当前组进行拟合，初始化时使用上一组模型的权重。该方法预测补丁，使模型能够适应不同空间分辨率的视频而无需修改架构。为了提高效率，训练过程中对参数进行量化，无需后处理修剪或量化。该方法通过适应具有不同帧间运动的视频实现可变比特率压缩。NIRVANA还实现了6倍的解码速度，可与更多GPU良好扩展，适用于各种部署场景。该方法的具体步骤包括：
1. 将视频分为多个组，每个组包含多个连续的帧。
2. 对于每个组，使用自回归模型进行拟合，预测每个帧的每个补丁。
3. 将预测的补丁组合成预测帧，计算预测帧与原始帧之间的误差，并使用MSE损失函数进行优化。
4. 在训练过程中，使用熵正则化对模型参数进行压缩。
5. 对于每个组，使用前一组的模型权重进行初始化，以提高模型的稳定性和收敛速度。

#### 8. 实验设置：
本文在UVG-HD和UVG-4K数据集上评估了所提出的方法，这些数据集分别包含7个1080p分辨率和120fps的视频以及4K分辨率的4个视频。使用PSNR和BPP来衡量重建质量和压缩率，同时报告编码和解码时间。本文还使用来自Youtube-8M数据集的视频，分别在1080p分辨率和60fps下进行分段，以演示该模型处理长视频的能力。本文使用SIREN和NeRV作为基线，并在补充材料中提供了架构细节。

#### 9. 实验结果和分析：
本文分析了各种参数对PSNR-BPP权衡曲线的影响，包括熵损失系数、补丁大小、组大小和训练迭代次数。在UVG-HD数据集的Jockey视频上进行了研究。结果显示，增加熵系数会导致较低的BPP但也会导致较低的PSNR，而增加补丁大小和组大小会导致较高的PSNR但也会导致较高的BPP。最佳权衡曲线是在组大小为3时获得的。增加训练迭代次数可以改善PSNR-BPP权衡，但收益递减。该研究为所提出的方法的参数调整提供了见解。

#### 整篇论文总结：
本文提出了一种名为NIRVANA的自回归视频INR框架，该框架利用视频中的空间和时间冗余来实现高速编码。该方法采用自回归模型，对当前组进行拟合，预测补丁，使模型能够适应不同空间分辨率的视频而无需修改架构。为了提高效率，训练过程中对参数进行量化，无需后处理修剪或量化。该方法通过适应具有不同帧间运动的视频实现可变比特率压缩。NIRVANA还实现了6倍的解码速度，可与更多GPU良好扩展，适用于各种部署场景。本文在UVG-HD和UVG-4K数据集上评估了所提出的方法，并使用PSNR和BPP来衡量重建质量和压缩率，同时报告编码和解码时间。本文还使用来自Youtube-8M数据集的视频，分别在1080p分辨率和60fps下进行分段，以演示该模型处理长视频的能力。本文分析了各种参数对PSNR-BPP权衡曲线的影响，并为所提出的方法的参数调整提供了见解。


# Paper:202     通过CLIP知识的3D蒸馏实现本地3D编辑



#### 1. Title: 
Local 3D Editing via 3D Distillation of CLIP Knowledge

#### 2. Authors: 
Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo

#### 3. Affiliation: 
第一作者：KAIST AI

#### 4. Keywords: 
3D content editing, Neural Radiance Fields, Local Editing NeRF, CLIP knowledge, 3D GANs

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是3D内容编辑，旨在实现高质量、局部化的编辑。传统的3D表示方法如体素和网格等存在内存占用大、缺乏真实感等问题，而Neural Radiance Fields (NeRF)则通过隐式表示和体积渲染技术实现了高质量的3D环境表示。然而，NeRF的编辑仍然存在一些挑战，如编辑后的视觉质量下降、控制不够精细等。因此，本文提出了一种基于文本输入的局部编辑方法Local Editing NeRF (LENeRF)，旨在实现高质量、局部化的编辑。

- (2):过去的3D编辑方法存在一些问题，如局部编辑能力不足、需要标注语义分割图等。本文提出的方法通过生成3D掩模实现局部编辑，同时利用Contrastive Language Image Pre-training (CLIP)模型的多模态嵌入空间，实现了文本输入的编辑。此外，本文还提出了三个附加模块，分别是Latent Residual Mapper、Attention Field Network和Deformation Network，用于实现3D特征的局部编辑。

- (3):本文提出的方法通过3D distillation of CLIP knowledge实现了3D掩模的生成，具有较好的可扩展性和泛化性。同时，本文提出的方法实现了高质量、局部化的3D编辑，具有较好的实用性和可操作性。

- (4):本文在多个任务上进行了实验，包括顺序编辑、真实图像编辑和跨领域编辑等，取得了较好的效果。实验结果表明，本文提出的方法能够实现高质量、局部化的3D编辑，具有较好的性能和实用性。
#### 7. 方法详细介绍：
本文提出了一种名为LENeRF的局部编辑3D场景的方法。LENeRF包括三个可训练的模块：潜在残差映射器（LRM）、注意力场网络（AFN）和变形网络（DN）。LRM将源潜在编码映射到目标潜在编码，目标编码进一步影响源和目标特征向量。AFN参数化3D注意力场，作为源和目标特征之间插值的软3D掩码。DN处理几何变换，通过变形源特征场。LENeRF使用CLIP损失进行训练，AFN还使用CLIP生成的零样本伪标签进行训练。LENeRF的编辑是实时的。

#### 8. 实验设置：
本文使用预训练的EG3D生成器在FFHQ、AFHQv2 CATS和ShapeNet Cars数据集上进行训练。本文将LENeRF与三种最先进的NeRF编辑方法：CLIP-NeRF、FENeRF和IDE-3D进行比较。本文还创建了两个新的基线，基于两个模型（FENeRF+StyleCLIP和IDE-3D+StyleCLIP），可以使用文本指导编辑辐射场。本文在FFHQ和Cats数据集上报告了LENeRF和各种基线的PSNR、R-precision和FID性能的定量指标。本文还进行了用户研究，以评估LENeRF和各种基线。

#### 9. 实验结果和分析：
本文生成30k个样本，使用随机抽样的文本提示来计算FID，并对每个文本提示生成1K张图像，以计算PSNR和R-precision。本文使用Fréchet Inception Distance（FID）评估源图像和编辑图像的质量，并比较差异以评估后编辑退化。本文通过计算掩码指定的感兴趣区域之外的源图像和编辑图像之间的PSNR来量化意外更改。区域的真实掩码由预训练的面部解析网络检测。本文使用预训练的属性分类器来计算R-precision，以衡量文本提示在编辑结果中的反映程度。本文还进行了消融研究，以展示控制λsparsity的效果，变形网络的重要性以及目标函数LCLIP +和Lmask的影响。


# Paper:203     DiffPose：更可靠的三维姿态估计



#### 1. Title: 
DiffPose: Toward More Reliable 3D Pose Estimation

#### 2. Authors: 
Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein Rahmani, Jun Liu

#### 3. Affiliation: 
第一作者：新加坡科技设计大学

#### 4. Keywords: 
3D pose estimation, diffusion models, uncertainty, indeterminacy

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gong_DiffPose_Toward_More_Reliable_3D_Pose_Estimation_CVPR_2021_paper.html  Github: https://gongjia0208.github.io/Diffpose/

#### 6. Summary : 
- (1):本文研究的是3D人体姿态估计，该任务由于固有的深度模糊和遮挡等问题而具有挑战性，导致高度不确定性和不确定性。 
- (2):现有的方法通常采用两阶段方法进行3D姿态估计，即先使用2D姿态检测器获得2D姿态，然后进行2D到3D的提升。然而，单目3D姿态估计仍然具有挑战性。本文提出了DiffPose，一种新颖的3D姿态估计框架，将3D姿态估计建模为反向扩散过程。 
- (3):DiffPose的创新点在于，它将3D姿态估计过程建模为反向扩散过程，可以自然地处理3D姿态的不确定性和不确定性。在前向过程中，我们逐步将具有低不确定性的“地面真实” 3D姿态分布H0向具有高不确定性的3D姿态分布HK扩散，以产生中间分布来指导模型训练。在反向过程中，我们使用扩散模型g将不确定的3D姿态分布HK逐步转化为具有低不确定性的3D姿态分布H0。DiffPose在两个广泛使用的姿态估计基准测试Human3.6M和MPI-INF-3DHP上显着优于现有方法。 
- (4):DiffPose在3D人体姿态估计任务上取得了最先进的性能，可以支持其目标。
#### 7. 方法详细介绍：
DiffPose是一种基于扩散的生成模型，旨在处理人体姿态估计中3D姿态的不确定性和不确定性。该方法分为两个阶段：（i）基于提取的热图初始化不确定的3D姿态分布HK，该热图捕捉输入2D姿态在3D空间中的基本不确定性；（ii）执行反向扩散过程，其中使用扩散模型g逐步去噪初始分布HK以获得所需的高质量确定分布H0，然后从姿态分布H0合成样本h0 ∈ R3×J以获得最终的3D姿态hs。该方法还利用了基于GCN的体系结构作为扩散模型g，该模型以时空上下文信息为条件，以帮助反向扩散过程并获得准确的3D姿态。

#### 8. 实验设置：
该方法在两个广泛使用的基准数据集Human3.6M和MPI-INF-3DHP上进行评估。实验在单个NVIDIA Tesla V100 GPU上进行，内存为32GB。输入图像被调整为256x256像素，并且姿态估计是在OpenPose检测到的2D关键点上执行的。该方法使用Adam优化器进行训练，学习率为1e-4，批量大小为32。

#### 9. 实验结果和分析：
该论文在Human3.6M和MPI-INF-3DHP两个数据集上展示了DiffPose方法的结果。结果表明，DiffPose在使用检测到的2D姿态和地面实况2D姿态的平均MPJPE方面优于所有现有方法。MPI-INF-3DHP上的基于视频的结果表明，DiffPose实现了最佳性能，显示了DiffPose在提高室外场景中的性能的有效性。定性结果表明，DiffPose可以生成更可靠的3D姿态解决方案，特别是对于模糊的身体部位。消融研究还验证了所提出的扩散管道和GMM设计的有效性。


# Paper:204     利用隐藏的正样本进行无监督语义分割



#### 1. Title: 
Leveraging Hidden Positives for Unsupervised Semantic Segmentation

#### 2. Authors: 
Hyun Seok Seong, WonJun Moon, SuBeen Lee, Jae-Pil Heo

#### 3. Affiliation: 
首尔成均馆大学

#### 4. Keywords: 
Unsupervised Semantic Segmentation, Contrastive Learning, Hidden Positives, Vision Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Seong_Leveraging_Hidden_Positives_for_Unsupervised_Semantic_Segmentation_CVPR_2022_paper.html  Github: https://github.com/hynnsk/HP

#### 6. Summary : 
- (1):本文研究的是无监督语义分割，由于需要标注大量像素级注释，因此需要进行无监督学习。 
- (2):过去的方法主要是基于聚类的方法，但是这些方法高度依赖于数据增强，而且没有先验知识，因此学习语义一致性非常具有挑战性。本文提出了一种新的方法，通过挖掘隐藏的正样本来学习丰富的语义关系，并确保局部区域的语义一致性。 
- (3):本文提出了一种对比学习方法，通过挖掘隐藏的正样本来学习语义关系，并确保局部区域的语义一致性。具体来说，我们首先发现了两种全局隐藏正样本，一种是任务无关的，一种是针对每个锚点的任务特定的，然后逐渐增加后者的贡献，以捕捉任务特定的语义特征。此外，我们引入了一种梯度传播策略，以学习相邻补丁之间的语义一致性，这是因为附近的补丁很可能具有相同的语义。 
- (4):本文在COCO-stuff、Cityscapes和Potsdam-3数据集上取得了新的最先进结果。本文的方法在无监督语义分割任务上取得了很好的性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种利用隐藏正样本进行无监督语义分割的方法。该方法通过发现隐藏的伪正样本来定制无监督分割的对比损失。作者利用自监督预训练的骨干网络作为任务不可知的标准，构建任务不可知的参考池，以评估每个锚点特征的其他特征是否在语义上相似。然后，作者逐步增加以任务特定方式发现的隐藏正样本的贡献来进行训练。作者还利用分割头中的特征构建了额外的任务特定GHP。最后，作者使用梯度传播方案来保留局部性质。

#### 8. 实验设置：
本文在COCO-stuff、Cityscapes和Potsdam-3数据集上进行了评估。模型分别在COCO-stuff、Cityscapes和Potsdam-3数据集上进行了3、20和10个epoch的训练，基于AdamW优化器，学习率为0.0005，权重衰减为0.1。任务特定参考池在整个训练过程中每100个迭代更新一次。负样本使用率设置为2。评估指标，即聚类和线性探针，分别使用Adam优化器进行优化，学习率分别为0.005和0.001。

#### 9. 实验结果和分析：
本文提出的方法在COCO-stuff、Cityscapes和Potsdam-3数据集上均取得了优秀的性能，超过了现有的无监督语义分割方法。作者进行了详细的实验分析，包括对各组件的消融实验、对超参数的敏感性分析等。实验结果表明，任务特定的GHP和LHP对于提高无监督分割任务的性能至关重要，两者的同时使用可以带来16%的性能提升。同时，本文提出的训练方案对于超参数的鲁棒性较强。


# Paper:205     PEFAT：通过伪损失估计和特征对抗训练提高半监督医学图像分类



#### 1. Title: 
PEFAT: Boosting Semi-supervised Medical Image Classification via Pseudo-loss Estimation and Feature Adversarial Training

#### 2. Authors: 
Qingjie Zeng, Yutong Xie, Zilin Lu, Yong Xia

#### 3. Affiliation: 
Qingjie Zeng, Zilin Lu, and Yong Xia are affiliated with the School of Computer Science and Engineering, Northwestern Polytechnical University, China.

#### 4. Keywords: 
Semi-supervised learning, medical image classification, pseudo-labeling, feature adversarial training, loss distribution modeling.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zeng_PEFAT_Boosting_Semi-Supervised_Medical_Image_Classification_via_Pseudo-Loss_Estimation_and_CVPR_2022_paper.html  Github: https://github.com/maxwell0027/PEFAT

#### 6. Summary:
- (1):本文研究半监督医学图像分类问题，旨在通过利用未标记数据提高分类性能。医学图像数据集的标注需要专业知识，因此标记数据往往很少，而未标记数据则更加容易获取。半监督学习方法可以利用未标记数据提高分类性能，但如何挖掘未标记数据的信息是关键问题。
- (2):过去的半监督学习方法主要有伪标签和一致性正则化两种，但伪标签方法往往会引入错误的伪标签，而一致性正则化方法则无法充分利用未标记数据。本文提出了一种新的半监督学习方法PEFAT，通过伪损失估计和特征对抗训练来解决这些问题。PEFAT通过建立损失分布模型来选择高质量的伪标签数据，并通过注入特征级对抗噪声来充分利用未标记数据。这种方法在医学图像分类任务中取得了很好的效果。
- (3):本文提出的PEFAT方法包括两个关键步骤：伪损失估计和特征对抗训练。伪损失估计通过建立损失分布模型来选择高质量的伪标签数据，而特征对抗训练则通过注入特征级对抗噪声来充分利用未标记数据。这种方法在医学图像分类任务中取得了很好的效果。
- (4):本文在三个医学图像数据集和两个自然图像数据集上进行了实验，结果表明PEFAT方法在医学图像分类任务中取得了很好的效果，超过了其他先进的半监督学习方法。这种方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种半监督的医学图像分类方法PEFAT，包括两个主要组成部分：交叉伪损失估计（CPLE）和特征对抗训练（FAT）。PEFAT的算法流程包括以下步骤：（1）使用对比学习在标记数据上预训练模型，（2）在未标记数据上生成伪标签并使用高斯混合模型（GMM）选择高质量的伪标签数据，（3）在剩余的未标记数据上进行特征对抗训练以提高模型性能。其中，CPLE通过建模损失分布和估计标记和未标记数据之间的交叉伪损失来过滤错误预测的伪标签数据。FAT通过寻找能够有效改变特征级别分布的扰动方向，从而有利于模型的鲁棒学习和判别信息挖掘。

#### 8. 实验设置：
本文在两个医学图像数据集上进行了实验，分别是NCT-CRC-HE和ChestX-ray14。实验使用ResNet-18作为基础模型，使用随机水平翻转和随机垂直翻转进行数据增强。在训练过程中，使用Adam优化器，学习率为0.001，批量大小为64。在测试过程中，使用10次随机裁剪和平均池化进行图像分类。

#### 9. 实验结果和分析：
本文提出的PEFAT方法在NCT-CRC-HE和ChestX-ray14数据集上均取得了最优性能，相比于其他伪标签方法，PEFAT的性能提升了0.81%~9.55%。实验结果表明，使用CPLE可以提高10.87%的准确率，使用FAT可以提高1.85%的准确率。t-SNE可视化结果表明，FAT在处理边界分布样本和学习可分离表示方面比VAT更有效。CPLE方法可以在不牺牲阈值和准确率之间进行权衡，收集高质量的伪标签数据。在NCT-CRC-HE验证集上的实验结果表明，CPLE可以选择具有最低错误率的最大伪标签集。


# Paper:206     NeRF-RPN: 一种在NeRF中进行物体检测的通用框架



#### 1. Title: 
NeRF-RPN: A general framework for object detection in NeRFs

#### 2. Authors: 
Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, Chi-Keung Tang

#### 3. Affiliation: 
香港科技大学

#### 4. Keywords: 
NeRF, object detection, voxel representation, 3D bounding boxes, RPN

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2021_paper.html  Github: https://github.com/lyclyc52/NeRF_RPN

#### 6. Summary : 
- (1):本文研究背景是3D物体检测，现有方法需要3D点云输入或至少从3D传感器获取的RGB-D图像。
- (2):现有的3D物体检测方法存在一些问题，如数据收集困难、噪声等，因此本文提出了一种新的方法，即NeRF-RPN，它直接在NeRF上进行操作，可以检测场景中所有物体的边界框。本文的方法是基于一种新的体素表示，它结合了多尺度的3D神经体积特征，可以直接回归NeRF中物体的3D边界框，而无需在任何视点渲染NeRF。本文的方法是一个通用的框架，可以应用于没有类标签的物体检测。
- (3):本文提出的NeRF-RPN是一种新的3D物体检测方法，它直接在NeRF表示中进行操作，可以直接输出3D边界框。本文的方法采用了“3D-to-3D learning”范式，充分利用了NeRF中固有的3D信息，并直接在3D空间中预测3D区域提议。本文的方法是第一个在NeRF中直接进行3D物体检测的尝试。
- (4):本文的方法在多个数据集上进行了实验，包括合成数据和真实数据，结果表明本文的方法可以在4小时内训练，可以在115毫秒内处理给定的NeRF场景，并在3D-FRONT NeRF数据集上实现了99%的召回率。本文的方法在3D物体检测方面取得了很好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为NeRF-RPN的3D物体检测框架，该框架基于从NeRF中提取的体素表示进行操作。该方法使用区域提议网络（RPN）直接从NeRF中回归高质量的边界框，而无需在任何视角下渲染NeRF。该方法采用多种主干网络，包括VGG、ResNet和Swin Transformer，以及多种锚点和无锚点的RPN头和多种损失函数进行评估。

NeRF-RPN方法由两个主要组件组成：特征提取器和RPN头。特征提取器以从NeRF中采样的辐射和密度网格为输入，并生成特征金字塔作为输出。RPN头在特征金字塔上操作并生成物体提议。该方法对NeRF输入特征的形式、特征提取器架构和RPN模块具有灵活性，可以适应多个下游任务。从NeRF中的输入采样涉及在覆盖NeRF模型的完整可追踪体积的3D网格上均匀采样辐射和密度。每个维度中网格的分辨率与该维度的可追踪体积的长度成比例，以保持对象的长宽比。RPN头包括用于3D区域提议网络的基于锚点和无锚点的方法。

#### 8. 实验设置：
本文在由Hypersim、3D-FRONT、ScanNet、SceneNN和INRIA构建的不同NeRF数据集上评估了所提出的方法。作者构建了一个新的基准数据集，其中包括合成和真实世界数据，具有高质量的NeRF重建和仔细的边界框标注和清理。在训练期间，场景进行随机翻转和旋转进行数据增强。模型可以在2个NVIDIA RTX3090 GPU上进行训练，训练时间为4小时。在运行时，它可以在115毫秒内处理给定的NeRF场景（不包括后处理），同时在3D-FRONT NeRF数据集上实现99％的召回率。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的NeRF-RPN框架在新构建的NeRF数据集上实现了最先进的3D物体检测性能。无锚点方法在所有三个数据集上的AP表现优于基于锚点的方法。所提出的方法还在合成和真实世界数据集上实现了高召回率和AP，证明了其对不同类型数据的鲁棒性。


# Paper:207     DKM：用于几何估计的密集核特征匹配



#### 1. Title: 
DKM: Dense Kernelized Feature Matching for Geometry Estimation

#### 2. Authors: 
Johan Edstedt, Ioannis Athanasiadis, M˚arten Wadenb¨ack, Michael Felsberg

#### 3. Affiliation: 
计算机视觉实验室，林雪平大学

#### 4. Keywords: 
Feature matching, Dense matching, Kernel regression, Warp refinement, Certainty estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2021_paper.html  Github: https://github.com/Parskatt/DKM

#### 6. Summary : 
- (1):本文研究的是计算机视觉中的特征匹配问题，旨在找到两个三维场景图像之间的对应关系。 
- (2):传统的特征匹配方法是通过稀疏关键点和描述符提取，然后进行匹配。但是在复杂场景下，准确定位可靠和可重复的关键点是困难的，这会导致匹配和估计错误。为了解决这个问题，本文提出了一种新的密集匹配方法，该方法通过三个方面的创新来提高性能：全局匹配器、Warp Refiners和Certainty and Sampling。 
- (3):本文提出了一种基于核回归的全局匹配器，通过堆叠特征图和深度卷积核进行Warp Refinement，提出了一种从一致深度预测密集置信度的简单方法，并提出了一种平衡采样机制。通过大量实验，证明了本文提出的方法在多个几何估计基准测试中取得了最新的最佳结果。 
- (4):本文提出的方法在多个几何估计基准测试中取得了最新的最佳结果，特别是在MegaDepth-1500上，相对于最佳先前稀疏方法和密集方法，分别提高了+4.9和+8.9 AUC@5◦。本文提出的方法为基于密集匹配的三维重建铺平了道路。
#### 1. 实验结果
(1). 本文提出了一种新的密集特征匹配方法DKM，能够实现最先进的两视图几何估计结果。作者进行了大量实验，展示了他们方法的优越性，MegaDepth-1500基准测试中AUC@5◦增益为+8.9。作者还在ScanNet数据集上评估了他们的方法，并与之前最好的稀疏方法和最好的密集方法相比，获得了+4.0 AUC@5◦和+9.3 AUC@5◦的增益。作者还在使用HLoc的InLoc基准测试中评估了他们的方法，并发现了显著的改进，特别是在DUC2上，他们展示了+8.4%正确定位查询的增益。作者还进行了消融研究，以调查他们方法的设计选择，并展示了他们提出的全局匹配器、warp refiners、match sampling、resolution和bidirectionality的影响。

#### 2. 方法详细介绍
本文提出了一种密集核特征匹配（DKM）方法，用于几何估计。该方法使用具有骨干、解码器和细化器块的深度神经网络。骨干从输入图像中提取特征，解码器生成粗略的warp field，细化器块使用密集核特征匹配来细化warp field。该方法还包括几何估计的确定性估计和采样，以及将回归和确定性损失结合的损失公式。

#### 3. 实验设置
本文分别在室外和室内几何估计上评估了所提出的方法。室外训练使用MegaDepth数据集，室内训练使用ScanNet数据集。本文还使用HPatches和MegaDepth-1500基准测试进行评估。

#### 4. 实验结果与分析
所提出的DKM方法在HPatches、MegaDepth-1500和ScanNet-1500基准测试中均实现了最先进的性能，用于单应性和姿态估计。本文展示了相对于之前的方法，包括PDC-Net+和ASpanFormer，的显着改进。本文还提出了一个基于8个不同MegaDepth场景的新型基准测试，在该测试中，DKM展示了重大的改进。 

#### 5. 方法详细介绍
本文提出了一种新的密集匹配方法，用于3D场景重建中的特征匹配。该方法包括核回归全局匹配器、通过堆叠特征图和深度卷积核进行warp refinement、通过一致深度和平衡采样方法学习密集置信度的方法。该方法与之前的最先进方法进行了比较，并显示出在几何估计基准测试中优于密集和稀疏方法。本文还详细讨论了损失公式。

#### 6. 实验设置
无

#### 7. 方法详细介绍
本文提出的DKM方法包括以下五个阶段： 
（1）阶段I：提取图像A和B的特征金字塔。
（2）阶段II：使用高斯过程公式和深度特征估计粗略的全局warp和确定性。
（3）阶段III：使用CNN refiners对粗略warp进行细化。
（4）阶段IV：使用平衡采样方法选择可靠和准确的匹配，用于场景几何估计。
（5）阶段V：使用鲁棒估计器使用RANSAC估计几何。本文提供了每个阶段和具体技术（如余弦嵌入和局部相关性体积细化）的详细描述。

#### 8. 实验设置
无

#### 9. 实验结果和分析
无


# Paper:208     PersonNeRF：从照片集合中个性化重建



#### 1. Title: 
PersonNeRF: Personalized Reconstruction from Photo Collections

#### 2. Authors: 
Chung-Yi Weng, Pratul P. Srinivasan, Brian Curless, Ira Kemelmacher-Shlizerman

#### 3. Affiliation: 
第一作者：University of Washington（华盛顿大学）

#### 4. Keywords: 
3D reconstruction, neural rendering, free-viewpoint rendering, unstructured photo collections, personalized space

#### 5. Paper: https://grail.cs.washington.edu/projects/personnerf/  Github: None

#### 6. Summary:
- (1):本文研究的背景是如何从多年拍摄的人物照片集合中重建出该人物的3D模型，并能够在该模型上进行自由视角渲染。
- (2):过去的方法主要是基于多视角立体成像或者基于3D扫描技术，但是这些方法需要大量的数据和时间成本，且难以处理不同服装和外貌的变化。本文提出的方法是基于NeRF的神经体积表示，通过建立一个个性化的空间，可以在该空间中进行自由视角渲染。本文的方法能够处理不同服装和外貌的变化，且能够从稀疏的观测数据中恢复出3D模型。
- (3):本文提出的方法是基于HumanNeRF的神经体积表示，通过建立一个个性化的空间，可以在该空间中进行自由视角渲染。本文的方法能够处理不同服装和外貌的变化，且能够从稀疏的观测数据中恢复出3D模型。本文的创新点在于，通过建立一个共享的姿态依赖运动场，可以将不同外貌的观测数据映射到一个共同的姿态空间中，从而实现对不同外貌的建模。此外，本文还提出了一种正则化方法，可以使得恢复的体积几何更加平滑和不透明。
- (4):本文的方法在处理不同服装和外貌的变化方面表现出色，能够从稀疏的观测数据中恢复出3D模型，并能够在该模型上进行自由视角渲染。本文的方法在多个数据集上进行了实验，结果表明，本文的方法在自由视角渲染方面优于现有的方法。
#### 7. 方法详细介绍：
本文提出了一种名为PersonNeRF的方法，用于从照片集合中进行个性化重建。该方法使用骨骼运动而不是非刚性运动来解决对新视角的泛化问题。此外，该方法通过在渲染的深度图上强制执行深度平滑损失来规范身体几何形状，以解决重建的标准姿势人体几何形状不正确的问题。外观建模涉及将所有具有不同外观的照片训练到单个网络中，其中共享的标准外观MLP是外观相关的，但针对所有图像优化的是单个通用运动权重体积。优化涉及最小化多种损失，包括LLPIPS、LMSE、Lgeom和Lopacity。最后，该方法构建了一个个性化空间，该空间由相机视角、身体姿势和外观组成，允许在视角上进行连续变化，但限制身体姿势和外观为观察到的那些。

#### 8. 实验设置：
本文使用Roger Federer的照片集合来评估所提出的方法。数据集包含10个外观集，跨越12年，每场比赛有19到24张照片，每年一张。作者运行SPIN来估计身体姿势和相机姿势，自动分割主体，并手动纠正分割错误和明显错误的3D身体姿势。此外，对于主体被球或球拍遮挡的图像，他们标记遮挡对象的区域，并在优化过程中省略它们。

#### 9. 实验结果和分析：
本文将所提出的方法与HumanNeRF进行比较，并报告每个数据集（每年）的FID分数。FID分数越低越好。结果表明，除了2015年和2019年外，所提出的方法在大多数情况下优于HumanNeRF，具有更低的FID分数。

#### 总结：
本文提出了一种名为PersonNeRF的方法，用于从照片集合中进行个性化重建。该方法使用骨骼运动而不是非刚性运动来解决对新视角的泛化问题。此外，该方法通过在渲染的深度图上强制执行深度平滑损失来规范身体几何形状，以解决重建的标准姿势人体几何形状不正确的问题。外观建模涉及将所有具有不同外观的照片训练到单个网络中，其中共享的标准外观MLP是外观相关的，但针对所有图像优化的是单个通用运动权重体积。优化涉及最小化多种损失，包括LLPIPS、LMSE、Lgeom和Lopacity。最后，该方法构建了一个个性化空间，该空间由相机视角、身体姿势和外观组成，允许在视角上进行连续变化，但限制身体姿势和外观为观察到的那些。本文使用Roger Federer的照片集合来评估所提出的方法，并将其与HumanNeRF进行比较。结果表明，所提出的方法在大多数情况下优于HumanNeRF，具有更低的FID分数。


# Paper:209     可训练的投影梯度方法用于鲁棒的微调



#### 1. Title: 
Trainable Projected Gradient Method for Robust Fine-tuning

#### 2. Authors: 
Junjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, Zsolt Kira

#### 3. Affiliation: 
Georgia Institute of Technology（乔治亚理工学院）

#### 4. Keywords: 
Transfer learning, Fine-tuning, Out-of-distribution robustness, Bi-level optimization, Trainable projection constraints

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tian_Trainable_Projected_Gradient_Method_for_Robust_Fine-Tuning_CVPR_2021_paper.html  Github: https://github.com/PotatoTian/TPGM

#### 6. Summary : 
- (1):本文研究的是深度学习中的迁移学习，特别是在fine-tuning过程中如何提高模型的鲁棒性，以及如何保留预训练模型的泛化能力。
- (2):现有的fine-tuning方法大多采用手动调整超参数或昂贵的超参数搜索，无法扩展到大型数据集和神经网络。本文提出了一种可训练的投影梯度方法（TPGM），通过学习每层的约束来实现细粒度的fine-tuning正则化。TPGM维护一组投影半径，即每层的fine-tuned模型和pretrained模型之间的距离约束，并通过权重投影来强制执行它们。为了学习这些约束，本文提出了一种双层优化算法来自动学习最佳的投影半径集。本文的方法在保持ID性能的前提下，显著提高了OOD泛化性能。
- (3):本文提出了一种可训练的投影梯度方法（TPGM）来支持逐层正则化优化。TPGM采用可训练的权重投影约束，并将它们纳入主模型的前向传递中进行优化。为了以原则性的方式学习权重投影半径，本文提出了一种交替优化算法，该算法通过将模型权重和投影半径进行交替优化来实现。本文的方法在保持ID性能的前提下，显著提高了OOD泛化性能。
- (4):本文在大规模数据集DomainNet和ImageNet上进行了实验，使用不同的架构。在保持ID性能的前提下，TPGM在少量超参数调整的情况下优于现有方法。通过对学习到的投影半径进行进一步分析，本文发现网络中较低层（靠近输入）需要更强的正则化，而较高层（靠近输出）需要更多的灵活性。本文的方法在保持ID性能的前提下，显著提高了OOD泛化性能。
#### 7. 方法详细介绍：
本文提出了一种可训练的投影梯度方法（TPGM）用于fine-tuning，以自动学习神经网络中每个层的距离约束。TPGM采用可训练的权重投影约束，称为投影半径，并将其纳入主模型的前向传递中进行优化。TPGM维护神经网络每个层的一组权重投影半径并更新它们。投影半径控制每个层的“自由度”。为了以原则性的方式学习权重投影半径，本文提出使用模型权重和投影半径之间的交替优化，这是通过将fine-tuning建模为双层约束问题来实现的。TPGM是一个两阶段的优化方法，第一阶段将预训练模型投影到训练数据的跨度上，第二阶段在训练数据上fine-tune投影模型。投影是通过使用奇异值分解（SVD）和投影矩阵解决线性方程组来实现的。投影比率是可训练的，并在单独的验证数据集上进行优化，以在拟合训练数据和推广到新数据之间取得平衡。

#### 8. 实验设置：
本文在两种不同的架构ResNet和Vision Transformers上使用大规模数据集DomainNet和ImageNet-1K进行实验。用于初始化的预训练模型是CLIP预训练ResNet50和ImageNet预训练MOCO-V3 ResNet50。ResNet的训练配方相对简单，使用Adam优化器和默认设置以及批量大小为256进行fine-tune，使用余弦学习率调度fine-tune 50个epochs。所有实验都使用相同的训练配方。

#### 9. 实验结果和分析：
TPGM显著提高了模型的鲁棒性，即使没有零样本分类器也能在OOD上表现出色。结果表明，TPGM在ID和OOD性能方面优于几种现有方法，包括Vanilla fine-tuning、Linear Probing（LP）、Prototype Fitting（PF）、L2-SP和MARS-SP。实验还表明，TPGM自动学习每个层的不同约束，这对于实现更好的性能至关重要。本文还提供了TPGM的理论研究，使用线性模型进行了分析。

#### 论文总结：
本文提出了一种可训练的投影梯度方法（TPGM）用于fine-tuning，以自动学习神经网络中每个层的距离约束。TPGM采用可训练的权重投影约束，称为投影半径，并将其纳入主模型的前向传递中进行优化。本文在ResNet和Vision Transformers上使用大规模数据集进行实验，结果表明TPGM在ID和OOD性能方面优于其他现有方法。TPGM自动学习每个层的不同约束，这对于实现更好的性能至关重要。


# Paper:210     PartManip：从点云观察中学习跨类别通用部件操作策略



#### 1. Title: 
PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations

#### 2. Authors: 
Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao Dong, He Wang

#### 3. Affiliation: 
Haoran Geng, Ziming Li, Yiran Geng, Hao Dong, He Wang: Peking University, China; Jiayi Chen: Beijing Academy of Artificial Intelligence, China.

#### 4. Keywords: 
Object manipulation, cross-category generalization, part-based manipulation, reinforcement learning, domain adversarial learning.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Geng_PartManip_Learning_Cross-Category_Generalizable_Part_Manipulation_Policy_From_Point_Cloud_CVPR_2021_paper.html  Github: https://github.com/PKU-Epic/PartManip

#### 6. Summary : 
- (1):本文研究目标是学习一个通用的物体操作策略，以便在复杂的现实场景中使用。本文提出了一种基于部件的物体操作策略，可以实现跨类别的物体操作，从而提高了操作策略的泛化能力。
 
- (2):以往的方法存在一些问题，如只能处理已知的物体实例，无法泛化到新的物体实例。本文提出的方法可以处理不同物体类别之间的共享部件，从而提高了泛化能力。本文的方法是基于强化学习的，但直接使用强化学习算法进行视觉策略学习的效果不佳。因此，本文提出了一种两阶段训练框架，首先训练一个基于状态的专家，然后将知识转移到基于视觉的学生。为了解决泛化问题，本文引入了领域对抗学习来提取领域不变特征。 

- (3):本文提出了一个大规模的、基于部件的跨类别物体操作基准测试 PartManip，由11个物体类别、494个物体和6个任务类别组成。本文提出了一种基于部件的规范化方法和部件感知奖励，用于训练状态专家，并将知识转移到基于视觉的学生。本文还引入了3D Sparse UNet-based backbone来处理不同的物体，提出了一种领域对抗学习策略来提取领域不变特征。 

- (4):本文的方法在模拟环境中进行了广泛的实验，结果表明，本文的方法在未知物体类别上的表现优于其他方法。本文还在真实世界中进行了实验，证明了本文方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种PartManip方法，该方法从点云观测中学习跨类别的可推广部件操作策略。该方法包括一个部件规范化策略，将所有依赖于坐标的状态转换为目标GAPart的规范空间，使用DAgger和BC的增强状态到视觉策略蒸馏技术，使用3D稀疏Unet-based骨干网络处理复杂和多样的视觉输入，以及使用梯度反转层（GRL）和域分类器的域不变特征学习方法。

#### 8. 实验设置：
本文在PartManip基准测试中进行实验，该基准测试强调现实设置，并使用部分观测作为输入。任务成功率是主要的评估指标，每个实验使用随机种子进行三次以减少评估噪声。

#### 9. 实验结果和分析：
本文报告了PartManip方法在六个任务（CloseDoor、CloseDrawer、OpenDoor、OpenDrawer、PressButton和GraspHandle）上的主要结果。该方法在CloseDoor和CloseDrawer任务上实现了高成功率，即使在看不见的物体类别上也是如此。然而，在OpenDoor和OpenDrawer任务的看不见类别上，性能下降。本文还将所提出的方法与几个基线方法进行了比较，包括PPO、W2A、ILAD和ManiSkill Challenge的获胜者，并表明所提出的方法在所有评估集上表现更好，而且没有强烈的性能下降，表明其很好的泛化能力。


# Paper:211     CHMATCH：对比层次匹配和鲁棒自适应阈值增强的半监督学习



#### 1. Title: 
CHMATCH: Contrastive Hierarchical Matching and Robust Adaptive Threshold Boosted Semi-Supervised Learning

#### 2. Authors: 
Jianlong Wu, Haozhe Yang, Tian Gan, Ning Ding, Feijun Jiang, Liqiang Nie

#### 3. Affiliation: 
Jianlong Wu: 哈尔滨工业大学（深圳）计算机科学与技术学院
其他作者：山东大学计算机科学与技术学院，阿里巴巴集团

#### 4. Keywords: 
Semi-supervised learning, contrastive learning, hierarchical label, adaptive threshold

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_CHMATCH_Contrastive_Hierarchical_Matching_and_Robust_Adaptive_Threshold_Boosted_Semi-Supervised_CVPR_2021_paper.html  Github: https://github.com/sailist/CHMatch

#### 6. Summary : 
- (1):本文研究半监督学习中的一些问题，如不稳定的结果和不可区分的特征表示，特别是在标记样本较少的情况下。 
- (2):过去的方法主要集中在一致性正则化上，但是这些方法存在一些问题。本文提出了一种新的方法，CHMatch，它可以通过对比层次匹配来学习鲁棒的自适应阈值，从而实现实例级别的预测匹配和区分特征。 
- (3):本文提出了一种基于层次标签和对比学习的CHMatch方法，它将实例级别和图级别匹配结合起来，用于分配和特征学习。具体来说，本文首先提出了一种基于内存库的鲁棒自适应阈值学习策略，然后提出了一种基于层次标签的图匹配模块，用于对比学习。 
- (4):在几个常用的基准测试中，CHMatch取得了非常稳定和优越的结果。例如，在每个类别只有4个和25个标记样本的情况下，CHMatch在CIFAR-100上比FlexMatch分别降低了8.44％和9.02％的错误率。
#### 7. 方法详细介绍：
本文提出了一种名为CHMatch的半监督学习方法，该方法将对比学习与半监督学习相结合。CHMatch的框架包括特征编码器、细粒度分类头、粗粒度分类头和用于对比特征表示的投影头。该方法还利用分层标签信息来指导对比特征学习。CHMatch通过基于两个分类头的预测概率进行图匹配来学习准确的图形，从而减少噪声对应的影响。此外，CHMatch提出了一种基于内存库的策略，以学习适用于所有类别的近全局阈值，可以很好地处理选择高置信度样本进行一致性学习的问题。CHMatch的总体训练目标是最小化标记和未标记损失的总和，并对投影头施加对比损失。

具体步骤如下：
1. 计算预测值；
2. 更新内存库；
3. 更新K；
4. 计算W；
5. 更新参数。

#### 8. 实验设置：
本文在四个常用的半监督图像分类数据集上进行了实验，包括CIFAR-10、CIFAR-100、STL-10和ImageNet。对于CIFAR-10，使用了WRN-28-2，而对于CIFAR-100，使用了WRN-28-2和WRN-28-8。STL-10使用ResNet-18，ImageNet使用ResNet-50。内存库的大小N设置为50,000，并采用分布对齐策略。本文采用了与CoMatch相同的数据增强策略，包括弱增强、RandAugment和SimCLR中的增强策略。

#### 9. 实验结果与分析：
本文在WRN-28-2下的CIFAR-10、WRN-28-2和WRN-28-8下的CIFAR-100以及ResNet-18下的STL-10上展示了半监督分类结果。CHMatch方法在所有不同标记样本数量和两种不同的骨干网络下均比所有相关方法表现更好。特别是与强基线FlexMatch相比，CHMatch方法在这六个CIFAR-100设置中平均减少了7.47%的错误率。相比之下，与FixMatch相比，错误率的减少更大，平均为15.29%。这些结果证明了CHMatch的有效性，特别是当类别数量较大且分层结构相对平衡时。


# Paper:212     AutoLabel：基于CLIP的开放式视频域自适应框架



#### 1. Title: 
AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation

#### 2. Authors: 
Giacomo Zara, Subhankar Roy, Paolo Rota, Elisa Ricci

#### 3. Affiliation: 
第一作者：意大利特伦托大学

#### 4. Keywords: 
Open-set Unsupervised Video Domain Adaptation, CLIP, Language and Vision Models, AutoLabel

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zara_AutoLabel_CLIP-Based_Framework_for_Open-Set_Video_Domain_Adaptation_CVPR_2021_paper.html  Github: https://github.com/gzaraunitn/autolabel

#### 6. Summary : 
- (1):本文研究的是Open-set Unsupervised Video Domain Adaptation（OUVDA）领域，即如何将一个动作识别模型从一个标记的源域适应到一个未标记的目标域，目标域包含源域中不存在的“目标专用”类别。 
- (2):以往的方法要么是训练一个专门的开放式分类器，要么是采用加权对抗学习，本文提出了使用预训练的语言和视觉模型（CLIP）的方法。CLIP适用于OUVDA，因为它具有丰富的表示和零样本识别能力。然而，使用CLIP的零样本协议拒绝目标专用实例需要关于目标专用标签名称的oracle知识。为了解决这个问题，本文提出了AutoLabel，它可以自动发现和生成面向对象的组合候选目标专用类名。AutoLabel可以满意地拒绝目标专用实例，从而促进两个域的共享类别之间更好的对齐。 
- (3):本文提出了AutoLabel，一种自动标记框架，用于构建最能代表目标数据集中真实目标专用类名的候选目标专用类名集合。AutoLabel使用外部预训练的图像字幕模型ViLT从视频序列中提取属性名称集合，然后使用无监督聚类方法对目标数据集进行聚类，构建每个聚类分配的最常出现的属性名称的前k个。AutoLabel通过使用集合匹配技术删除与源类名相似的重复属性集，从而减少有效标签集中的冗余。最后，本文采用简单的伪标签机制进行条件对齐，将扩展标签集和目标样本提供给基于CLIP的编码器，然后为每个预测类别选择前k个伪标签样本，并将它们用于优化监督损失。 
- (4):本文在多个基准测试中进行了彻底的实验评估，并超过了现有的OUVDA最新方法。AutoLabel不仅有助于拒绝共享和目标专用实例之间的分离，而且还为开放世界识别打开了大门。
#### 7. 方法详细介绍：
本文提出了一种名为AutoLabel的框架，用于开放式视频领域自适应。该框架使用现成的图像字幕模型ViLT从视频帧中提取属性。然后将这些属性组合起来创建候选动作标签名称。该框架还使用聚类算法将目标视频分组为语义类别，并构建与每个聚类相关的最频繁属性的直方图。过滤最常见和相关的属性以获得候选标签名称。然后使用相似性矩阵将候选标签名称与源域中的共享标签名称进行匹配。使用伪标签计算的条件对齐策略对共享类进行对齐。

具体步骤如下：
1. 使用预训练的ViLT模型从目标视频序列的帧中预测一组属性。
2. 对视频级目标特征进行无监督聚类，将视频序列分配到相应的语义聚类中。
3. 过滤所有预测的帧级属性，以获得前k个最显著的属性。将这些属性连接起来形成代理动作标签名称。
4. 通过扩展标签集创建有效标签集，其中包括共享类名称和候选目标专用类名称。
5. 为未标记的目标样本计算伪标签，将目标视频序列和扩展标签集分别提供给ActionCLIP的文本和视觉编码器作为输入。然后使用每个预测类的置信度最高的前k％作为硬伪标签，以优化ActionCLIP多模态训练目标。

#### 8. 实验设置：
本文在两个基准测试集HMDB↔UCF和Epic-Kitchens上进行了实验，用于开放式视频领域自适应。使用ResNet-50和I3D作为骨干模型进行实验评估。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的AutoLabel框架优于所有基线方法，包括ActionCLIP-Oracle基线。AutoLabel框架有效地缩小了Oracle和基线方法之间的性能差距，接近上限性能。实验还强调了精心建模候选目标专用标签名称对于动作识别任务的重要性。结果表明，由于动作的广泛多样性，通过更高的粒度对目标域进行建模可以增加框架的区分度。而对于Epic-Kitchens基准测试，由于多个动作的视觉相似性，标签集中发现的粒度远远不如地面实况动作类别。消融研究表明，由2-3个属性令牌组成的候选目标标签是有效描述候选目标动作标签名称的合理选择。


# Paper:213     文档图像中鲁棒的篡改文本检测：新数据集和新解决方案



#### 1. Title: 
Towards Robust Tampered Text Detection in Document Image: New dataset and New Solution

#### 2. Authors: 
Chenfan Qu, Chongyu Liu, Yuliang Liu, Xinhong Chen, Dezhi Peng, Fengjun Guo, Lianwen Jin

#### 3. Affiliation: 
第一作者：华南理工大学

#### 4. Keywords: 
Tampered text detection, document image, frequency perception, multi-view iterative decoder, curriculum learning, large-scale dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2021_paper.html  Github: https://github.com/qcf-568/DocTamper

#### 6. Summary : 
- (1):本文研究的是文档图像中篡改文本的检测问题，由于文档图像中篡改文本的视觉特征不明显，因此检测难度较大。

- (2):过去的方法主要是基于文本的外观特征进行检测，但是这些方法在复杂的拍摄场景下效果不佳。本文提出了一种新的框架，利用频域感知和多视角迭代解码器来捕捉复杂场景下的更细粒度的线索。同时，本文还提出了一种新的训练范式，称为课程学习，以提高模型的鲁棒性和泛化能力。

- (3):本文提出了一种名为DTD的多模态Transformer方法，用于文档图像篡改检测。该模型利用视觉域和频域的特征，通过融合模块和注意力模块有效地将这两个模块的特征结合起来，并将其输入到基于Swin-Transformer的编码器中。最后，本文引入了一种新的多视角迭代解码器，以逐步感知篡改文本区域。

- (4):本文构建了一个大规模的文档图像数据集DocTamper，包含各种类型的170,000个文档图像。实验结果表明，本文提出的DTD在DocTamper测试集和跨域测试集上的F-measure分别比之前的最新方法提高了9.2％，26.3％和12.3％。
#### 7. 方法详细介绍：
本文提出了一种基于多模态Transformer的文档图像篡改检测方法，称为文档篡改检测器（DTD）。该方法包括四个模块：视觉感知头、频率感知头、多模态编码器和多视角迭代解码器。其中，视觉感知头从原始图像中提取视觉特征，频率感知头将图像的离散余弦变换（DCT）系数转换为频域特征嵌入。多模态编码器通过多模态Transformer融合频域和视觉域的特征，多视角迭代解码器利用不同大小的特征逐步感知篡改文本区域。训练时采用交叉熵损失和Lovasz损失的组合作为损失函数。此外，本文还提出了一种名为Curriculum Learning for Tampering Detection（CLTD）的训练策略，通过动态控制图像压缩增强的质量，从易到难地训练机器学习模型。

#### 8. 实验设置：
本文构建了一个大规模文档图像数据集，称为DocTamper，包含170,000个不同类型的文档图像。数据集用于评估所提出方法的性能。数据集中的篡改方式包括复制移动、拼接和生成，且大致均匀分布。数据集分为四个子集：包含120k个样本的训练集、30k个样本的通用测试集以及两个跨域测试集，分别包含2k和18k个样本。所有篡改图像都没有经过压缩，因此可以使用自定义压缩配置进行训练或测试。对于所有图像，提供了标注表示篡改文本区域。两个跨域测试子集用于全面评估不同方法的泛化能力。

#### 9. 实验结果与分析：
实验结果表明，所提出的DTD方法在DocTamper测试集和跨域测试集上的F-measure指标均优于先前的最先进方法，分别提高了9.2％、26.3％和12.3％。定性和定量结果表明，DTD可以显著优于先前的最先进方法。


# Paper:214     使用泛化调整的联邦领域泛化



#### 1. Title: 
Federated Domain Generalization with Generalization Adjustment

#### 2. Authors: 
Ruipeng Zhang, Qinwei Xu, Jiangchao Yao, Ya Zhang, Qi Tian, Yanfeng Wang

#### 3. Affiliation: 
上海交通大学合作式中间件创新中心

#### 4. Keywords: 
Federated Learning, Domain Generalization, Privacy-preserving, Fairness, Generalization Adjustment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2021_paper.html  Github: https://github.com/MediaBrain-SJTU/FedDG-GA

#### 6. Summary : 
- (1):本文研究的是联邦领域泛化问题，即如何在保护隐私的前提下，让模型在新的客户端上具有良好的泛化性能。
 
- (2):现有的联邦领域泛化方法主要集中在设计每个单独领域内的无偏训练策略，但是由于缺乏多领域数据的支持，几乎所有方法都无法保证在领域转移下的泛化性能。本文提出了一种新的全局目标，包括一个新的方差约束项，以鼓励公平性。同时，提出了一种名为Generalization Adjustment（GA）的新方法，通过动态校准聚合权重来优化上述目标。GA的理论分析表明，通过显式的重新加权聚合，可以实现更紧密的泛化界限，代替仅适用于传统DG设置的隐式多领域数据共享。此外，所提出的算法是通用的，可以与任何基于本地客户端训练的方法结合使用。在几个基准数据集上的广泛实验表明，所提出的方法的有效性，与几个FedDG算法结合使用时，始终能够实现一致的改进。

- (3):本文提出了一种新的优化目标，包括一个新的方差约束项，以鼓励公平性。同时，提出了一种名为Generalization Adjustment（GA）的新方法，通过动态校准聚合权重来优化上述目标。GA的理论分析表明，通过显式的重新加权聚合，可以实现更紧密的泛化界限，代替仅适用于传统DG设置的隐式多领域数据共享。GA利用了领域平坦性约束，这是难以处理的领域发散约束的替代品，通过跟踪领域泛化差距动态计算每个隔离领域的权重，然后将其纳入FedDG的聚合中以增强泛化能力。由于差距信息不包含每个客户端的领域信息，因此GA不会引起额外的隐私泄露风险。

- (4):在几个基准数据集上的广泛实验表明，所提出的方法的有效性，与几个FedDG算法结合使用时，始终能够实现一致的改进。
#### 7. 方法详细介绍：
本文提出了一种新的联邦域泛化方法，称为Generalization Adjustment (GA)，以提高模型在未见过的领域上的泛化能力。GA方法通过动态校准权重来优化每个客户端/领域的贡献，以更好地保护每个客户端的隐私并防止模型偏向采样量更大的客户端。GA方法分为两个阶段：本地训练阶段和聚合阶段。本地训练阶段使用梯度下降法优化模型参数，聚合阶段使用GA算法优化权重。GA算法根据泛化差距和先前的权重更新权重，并使用泛化权重聚合全局模型。本文还提供了理论分析，证明了所提出方法的有效性。GA方法与现有方法兼容，因为它不对本地训练阶段施加任何限制。

#### 8. 实验设置：
本文在四个广泛使用的联邦域泛化基准测试数据集（PACS、OfficeHome、TerraInc、DomainNet）上评估了所提出的方法。对于所有基准测试数据集，采用留一领域法进行评估，其中一个领域被选为未见过的客户端，所有剩余领域被用作源客户端进行训练。在每个源领域内，训练和验证集的拆分与以前的工作相同。本地训练使用ImageNet预训练的ResNet18（PACS和OfficeHome）、ResNet50（TerraInc）和AlexNet（DomainNet）。本地训练期间，批量大小和学习率分别设置为16和0.001，本地训练轮数E为5，总通信轮数R为40。

#### 9. 实验结果和分析：
本文在四个联邦域泛化基准测试数据集上评估了所提出的方法，并将其与其他最先进的方法进行了比较。实验结果表明，GA方法在所有三个基准测试数据集上均优于基线FedAvg和其他最先进的方法，在PACS和TerraInc上表现最佳。本文还提供了一张表格，展示了详细的实验结果。在PACS数据集上，本文还提供了泛化差距的均值和标准差，证明了GA方法对泛化差距的减少具有显著影响。在TerraInc数据集上，本文还提供了测试领域上的损失曲面，证明了GA方法在目标领域上诱导出更具泛化性的解决方案。


# Paper:215     简单线索引领强大的多目标跟踪器



#### 1. Title: 
Simple Cues Lead to a Strong Multi-Object Tracker

#### 2. Authors: 
Jenny Seidenschwarz, Guillem Brasó, Victor Castro Serrano, Ismail Elezi, Laura Leal-Taixé

#### 3. Affiliation: 
Technical University of Munich (慕尼黑工业大学)

#### 4. Keywords: 
Multi-Object Tracking, Tracking-by-Detection, Appearance-Based Tracking, Motion-Based Tracking, Re-Identification Networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2021_paper.html  Github: https://github.com/dvl-tum/GHOST

#### 6. Summary : 
- (1):本文研究的是多目标跟踪领域，其中传统的跟踪方法是基于检测的跟踪，即先检测出物体再将其关联起来形成轨迹。 
- (2):过去的方法主要是基于运动和外观特征的关联，例如重新识别网络。然而，这些方法存在一些问题，例如在不同时间段内外观变化大，重新识别网络的性能不稳定等。本文提出了两个关键因素，使标准的重新识别网络在外观跟踪方面表现出色。同时，本文还分析了其失败案例，并表明将外观特征与简单的运动模型相结合可以获得强大的跟踪结果。 
- (3):本文提出了一个简单但强大的跟踪器GHOST，它将外观和运动结合起来，可以在四个公共数据集上实现最先进的性能。本文的创新点在于，它不需要在任何跟踪数据集上进行训练，而是将外观和运动结合在一起的简单跟踪器。 
- (4):本文的方法在多个数据集上实现了最先进的性能，表明了其有效性。本文的贡献在于提出了一种简单但强大的跟踪器，将外观和运动结合起来，可以在不需要大量训练数据的情况下实现最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种简单的基于检测的多目标跟踪方法，称为GHOST。该方法使用匈牙利算法进行二分图匹配，将一组检测结果分配给轨迹。匹配过程由成本矩阵驱动，该矩阵比较新检测结果与先前帧中已获得的轨迹。外观模型基于ResNet50，使用常见的行人重识别数据集进行训练。外观模型通过处理活动和非活动轨迹以及添加即时域自适应进行增强。检测和轨迹之间的外观距离使用余弦距离计算，运动距离使用两个边界框之间的交集来计算。

#### 8. 实验设置：
本文在四个数据集上评估了所提出的GHOST跟踪器：MOT17、MOT20、BDD和DanceTrack。对于私有检测设置，使用YOLOX-X生成检测结果。评估指标包括HOTA、IDF1和MOTA。

#### 9. 实验结果和分析：
所提出的GHOST跟踪器在所有数据集和评估指标上均优于现有方法。在MOT17数据集上，GHOST在HOTA上比最佳先前方法提高了1.8pp。在MOT20数据集上，GHOST在IDF1和HOTA上分别比现有最佳方法提高了2.6pp和1.3pp。在DanceTrack数据集上，GHOST在测试集上的表现优于所有其他方法，HOTA提高了2.5pp，IDF1提高了3.8pp，MOTA提高了0.7pp。在BDD100k数据集上，GHOST在验证集上的mHOTA和mIDF1上优于现有最佳方法0.3pp和2pp，在测试集上的mIDF1上优于现有最佳方法1.2pp。


# Paper:216     可靠且可解释的个性化联邦学习



#### 1. Title: 
Reliable and Interpretable Personalized Federated Learning

#### 2. Authors: 
Zixuan Qin, Liu Yang, Qilong Wang, Yahong Han, Qinghua Hu

#### 3. Affiliation: 
天津大学智能与计算学部，天津大学机器学习重点实验室

#### 4. Keywords: 
Federated learning, personalized federated learning, collective intelligence, Dempster-Shafer evidence theory, Bayesian decision rules

#### 5. Paper: 
Paper Link: https://openaccess.thecvf.com/content/CVPR2021/html/Qin_Reliable_and_Interpretable_Personalized_Federated_Learning_CVPR_2021_paper.html
Github Link: None

#### 6. Summary:
- (1):本文研究的背景是联邦学习中数据分布差异较大的情况下，如何设计可靠的客户端选择策略和可解释的客户端通信框架，更好地利用群体知识。
- (2):过去的方法包括FedProx、SCAFFOLD、MOON、Clustered federated learning等，但它们的客户端选择和训练通常不可靠和不可解释，导致训练过程中存在不确定性和忽略客户之间的协同作用。本文提出了一种可靠的个性化联邦学习方法，称为RIPFL，从社会学习的角度进行了全面解释。RIPFL可靠地选择和划分参与培训的客户端，使每个客户端可以使用不同数量的社会信息并更有效地与其他客户端进行通信。同时，该方法从贝叶斯决策规则和证据理论的角度有效地将个人信息与全局模型生成的社会信息相结合，使个人能够在集体智慧的帮助下更好地成长。本文提出的可解释的联邦学习思想具有良好的可扩展性，实验结果表明，该方法具有比其他最先进的联邦学习算法更强的鲁棒性和准确性。
- (3):本文提出了一种可靠的、可解释的个性化联邦学习框架RIPFL，从社会学习的角度进行了全面解释，包括可解释的本地训练、可靠的客户端选择和划分以及有效的联邦聚合。为了可靠地选择所需的客户端，本文引入了证据理论到客户端的本地训练中，从而量化每个客户端的不确定性并提供可解释的训练方法。同时，本文提出了一种基于贝叶斯规则的证据融合方法，当客户端之间的数据分布存在差异时，将全局模型视为客户端的先验信息。因此，客户端在本地训练中不会忘记全局模型的知识。
- (4):本文在MNIST和CIFAR-10数据集上进行了实验，结果表明，RIPFL方法在准确性和鲁棒性方面均优于其他最先进的联邦学习算法。
#### 7. 方法详细介绍：
本文提出了一种可靠且可解释的个性化联邦学习方法（RIPFL）。该方法包括三个可解释的组成部分：本地训练可解释性、可靠的个体选择和分割、有效的联邦聚合。RIPFL 可靠地选择和分割参与训练的客户端，使每个客户端可以使用不同数量的社交信息，并更有效地与其他客户端进行通信。该方法有效地将个人信息与全局模型生成的社交信息从贝叶斯决策规则和证据理论的角度进行了整合，使个体能够在集体智慧的帮助下更好地成长。该框架包括三个部分和六个步骤，包括可解释的本地训练、可靠的客户端选择和分割以及有效的联邦聚合。方法引入了Dempster-Shafer证据理论来量化每个客户端的不确定性和性能，并提供可靠的客户端选择策略。当客户端之间的数据分布存在差异时，引入了基于贝叶斯规则的证据融合方法，将全局模型视为客户端的先验信息。
具体步骤如下：
1. 本地训练可解释性：引入证据分类损失，使模型训练更具可解释性。
2. 可靠的个体选择和分割：选择所有智能客户端和一部分不智能的个体参与聚合。
3. 有效的联邦聚合：引入灵活且可解释的聚合方法，不受单一全局模型的限制。

#### 8. 实验设置：
本文在两个不同的真实数据集 CIFAR10 和 CIFAR100 上进行了实验，使用了 [4] 的数据分区方法。通过控制每个客户端的最大采样类别数σ来控制数据分布的变化程度。对于 CIFAR10，使用了包含两个卷积层和三个全连接层的简单 CNN 网络，对于 CIFAR100，使用了 ResNet18 网络。本地训练轮数为 5（CIFAR10）和 10（CIFAR100），全局轮数均为 200。使用 Adam 优化器，统一学习率为 0.0003。

#### 9. 实验结果与分析：
本文提出的 RIPFL 方法在 CIFAR10 数据集上进行了实验，共有 30 个客户端，其中选择了 20 个参与聚合。分析了客户端包含的类别数、准确率和客户端不确定性之间的关系。发现客户端的不确定性随着训练/测试集准确率的变化而变化，两者之间呈负相关。实验还表明，包含较少类别的客户端具有更好的分类能力和低的不确定性，因此不需要太多的社交信息。将 RIPFL 的鲁棒性与 FedAvg 在客户端数据攻击下进行比较，发现 RIPFL 更加鲁棒。通过将客户端分为表现良好和表现差的两组来验证所提出方法的可解释性。结果表明，当选择更多表现良好的客户端时，准确率更高，表明表现良好的客户端更能帮助其他客户端提高性能。


# Paper:217     超级令牌采样的视觉Transformer



#### 1. Title: 
Vision Transformer with Super Token Sampling

#### 2. Authors: 
Huaibo Huang, Xiaoqiang Zhou, Jie Cao, Ran He, Tieniu Tan

#### 3. Affiliation: 
中国科学院自动化研究所

#### 4. Keywords: 
Vision Transformer, Super Token Sampling, Self-Attention, Global Context Modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Huang_Vision_Transformer_With_Super_Token_Sampling_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是视觉任务中的全局依赖性建模问题，传统的Transformer网络在处理高分辨率图像时计算复杂度过高，而局部自注意力机制或早期卷积机制则牺牲了全局依赖性建模的能力。
- (2):过去的方法包括局部自注意力机制和早期卷积机制，但这些方法牺牲了全局依赖性建模的能力。本文提出了一种新的方法，即超级令牌采样，通过学习稀疏关联来将视觉令牌转换为超级令牌，从而在早期阶段实现有效的全局依赖性建模。
- (3):本文提出了一种简单而强大的超级令牌注意力机制，包括三个步骤：从视觉令牌中通过稀疏关联学习采样超级令牌，对超级令牌进行自注意力计算，最后将超级令牌映射回原始令牌空间。本文提出的超级令牌注意力机制可以将传统的全局注意力分解为稀疏关联映射和低维度注意力的乘积，从而提高了全局依赖性建模的效率。基于超级令牌注意力机制，本文提出了一种层次化的视觉Transformer网络，称为STViT。实验结果表明，STViT在各种视觉任务上均取得了强大的性能，例如在ImageNet-1K图像分类任务上，STViT-L模型在没有额外训练数据或标签的情况下实现了86.4%的top-1准确率，在COCO检测任务上实现了53.9的box AP和46.8的mask AP，在ADE20K语义分割任务上实现了51.9的mIOU。
- (4):本文提出的方法在各种视觉任务上均取得了优异的性能，证明了超级令牌注意力机制的有效性和STViT网络的优越性。
#### 7. 方法详细介绍：
本文提出了一种超级令牌视觉变换器（STViT）架构，使用超级令牌注意力（STA）通过高效地探索和充分利用长距离依赖性来提取全局上下文表示。STViT架构由一个干线、四个阶段的堆叠的超级令牌变换器（STT）块和一个最终的投影层组成。每个STT块包含三个关键模块：卷积位置嵌入（CPE）、超级令牌注意力（STA）和卷积前馈网络（ConvFFN）。STA模块包括三个过程：超级令牌采样（STS）、多头自注意力（MHSA）和令牌上采样（TU）。STS过程自适应地将令牌聚合成超级令牌，然后在MHSA过程中使用它们来建模全局依赖性。最后，TU过程将超级令牌映射回视觉令牌并将其添加到原始令牌中。

#### 8. 实验设置：
本文在各种视觉任务上进行了实验，包括ImageNet-1K上的图像分类、COCO 2017上的目标检测和实例分割以及ADE20K上的语义分割。模型从头开始训练300个epoch，输入大小为224×224。采用AdamW优化器和余弦衰减学习率调度器以及5个epoch的线性预热。初始学习率、权重衰减和批量大小分别为0.001、0.05和1024。在384×384分辨率上微调模型时，学习率、权重衰减、批量大小和总epoch分别设置为5e-6、1e-8、512和30。

#### 9. 实验结果和分析：
本文提出的STViT模型在各种任务中均表现出色，包括图像分类、目标检测、实例分割和语义分割。结果表明，在不同设置下，STViT在准确性和模型大小方面均优于现有的最先进模型。例如，STViT-S仅使用4.4G FLOPs即可达到83.6％的Top-1准确率，超过Swin-T、CSwin-T和MPViT-S分别2.3％、0.9％和0.6％。STViT-B达到84.8％的准确率，超过中等模型大小的对应模型，甚至超过大型模型。STViT-L达到86.4％的准确率，超过Swin-B和CSwin-B分别2.2％和0.9％，并且比CaiT-s48快两倍，FLOPs少22％，准确率高1.3％。STViT在目标检测和实例分割任务中的表现也优于所有其他视觉骨干。对于语义分割，STViT-B的mIOU比Swin变换器高3.1，与相似的模型大小的Swin变换器相比，甚至可以实现比Swin变换器大2.6的mIOU。


# Paper:218     SimpSON：使用单击干扰对象分割网络简化照片清理



#### 1. Title: 
SimpSON: Simplifying Photo Cleanup with Single-Click Distracting Object Segmentation Network

#### 2. Authors: 
Chuong Huynh, Yuqian Zhou, Zhe Lin, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, Abhinav Shrivastava

#### 3. Affiliation: 
第一作者：University of Maryland, College Park（马里兰大学帕克分校）
其他作者：Adobe Research

#### 4. Keywords: 
Photo editing, Distracting object segmentation, Interactive segmentation, Transformer-based module

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huynh_SimpSON_Simplifying_Photo_Cleanup_With_Single-Click_Distracting_Object_Segmentation_Network_CVPR_2021_paper.html  Github: https://github.com/hmchuong/SimpSON

#### 6. Summary : 
- (1):本文研究的背景是照片编辑中的视觉干扰物去除问题，传统的手动选择和去除这些小而密集的干扰区域是一项费时费力的任务。
- (2):过去的方法包括训练实例分割模型来监督性地检测和分割干扰物，但是这种方法存在主观性和数据集收集的问题。本文提出了一种交互式干扰选择方法，通过单击来实现任务，优于传统方法的精度和召回率。
- (3):本文提出了一种基于Transformer的模块，可以用于识别与用户单击位置相似的更多干扰区域。实验结果表明，该模型可以有效地和准确地交互式地分割未知的干扰物体，并且可以在组中进行分割。本文提出的模型显著简化了照片清理和修饰过程，为探索单击实现稀有对象分割和组选择提供了启示。
- (4):本文的方法在干扰物体分割任务上取得了良好的性能，可以有效地和准确地分割未知的干扰物体，并且可以在组中进行分割。实验结果表明，本文提出的方法可以显著简化照片清理和修饰过程。
#### 7. 方法详细介绍：
本文提出了一种交互式干扰物选择方法，旨在通过单击完成任务。该方法使用基于Transformer的模块来识别与用户点击位置相似的更多干扰区域。该方法超越了传统方法（运行全景分割，然后选择包含点击的分割）所达到的精度和召回率。具体步骤包括：
(1) 使用全景分割模型对输入图像进行分割；
(2) 根据用户点击位置，使用Transformer模块识别更多的干扰区域；
(3) 根据干扰区域的特征，选择最可能是干扰物的区域作为输出。

#### 8. 实验设置：
本文使用了两个数据集进行实验，分别是COCO和PASCAL VOC。实验中使用了两个评估指标，分别是精度和召回率。实验中使用了一台配备了NVIDIA Tesla V100 GPU的服务器进行实验。

#### 9. 实验结果和分析：
本文提出的方法在COCO和PASCAL VOC数据集上均取得了优于传统方法的精度和召回率。在COCO数据集上，本文方法的精度和召回率分别为0.87和0.85，在PASCAL VOC数据集上，精度和召回率分别为0.89和0.87。实验结果表明，本文提出的方法可以更准确地选择干扰物，提高任务完成效率。


# Paper:219     分数雅可比链：将预训练的2D扩散模型转化为3D生成模型



#### 1. Title: 
Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation

#### 2. Authors: 
Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich

#### 3. Affiliation: 
Haochen Wang, Xiaodan Du, Jiahao Li, Greg Shakhnarovich: TTI-Chicago

#### 4. Keywords: 
Diffusion models, 3D generation, Score Jacobian Chaining, differentiable renderer, voxel radiance field

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Score_Jacobian_Chaining_Lifting_Pretrained_2D_Diffusion_Models_for_3D_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了将预训练的2D扩散生成模型转化为3D生成模型的方法。
- (2):过去的方法需要训练3D数据集，而本文提出的方法可以使用预训练的2D模型，避免了3D数据集的获取成本。本文提出的方法是通过将扩散模型的梯度应用于可微分渲染器的雅可比矩阵来实现的。本文还解决了在这种应用中出现的分布不匹配问题。本文的方法可以通过多个视角的2D分数聚合成3D分数，并将预训练的2D模型重新用于3D数据生成。
- (3):本文提出了一种新的方法，即Score Jacobian Chaining (SJC)，通过将扩散模型的梯度应用于可微分渲染器的雅可比矩阵，将预训练的2D扩散生成模型转化为3D生成模型。本文还提出了一种新的估计机制来解决分布不匹配问题。本文的方法可以通过多个视角的2D分数聚合成3D分数，并将预训练的2D模型重新用于3D数据生成。
- (4):本文的方法在3D文本驱动生成任务上取得了良好的性能。本文的方法可以使用预训练的2D模型，避免了3D数据集的获取成本。本文提出的方法可以通过多个视角的2D分数聚合成3D分数，并将预训练的2D模型重新用于3D数据生成。本文的方法可以解决分布不匹配问题。
#### 7. 方法详细介绍：
本文提出了一种名为Score Jacobian Chaining（SJC）的方法，用于将预训练的2D扩散模型提升为3D生成模型。该方法使用了一个体素辐射场表示3D模型，其中包括密度体素网格和外观特征的体素网格。通过每个像素沿相机射线独立进行反向体积渲染，并通过反向传播将分数通过渲染图像计算出向量雅可比积。该方法还包括几种正则化策略，例如空洞损失、空洞损失计划和中心深度损失。具体步骤如下：
1. 使用预训练的2D扩散模型和可微分渲染器计算3D分数。
2. 使用Perturb-and-Average Scoring（PAAS）方法计算非噪声图像上的2D分数。
3. 使用不同的正则化策略，例如空洞损失、空洞损失计划和中心深度损失。

#### 8. 实验设置：
本文在FFHQ、LSUN Bedroom和LAION5B数据集上对未条件和条件扩散模型进行了实验。模型在256x256的图像分辨率上进行训练，指导比例参数控制语言条件的强度。本文还比较了SJC和DreamFusion两种方法在性能上的差异。

#### 9. 实验结果和分析：
本文提供了详细的实验结果和定性比较。结果表明，SJC在图像质量和多样性方面优于DreamFusion。本文还展示了所提出的正则化策略（例如空洞损失、空洞损失计划和中心深度损失）的有效性。本文还展示了使用SJC从预训练的Stable Diffusion（2D）图像模型纯粹生成3D模型的几个定性结果。


# Paper:220     使用预训练的深度骨架特征的提示引导零样本异常行为识别



#### 1. Title: 
Prompt-Guided Zero-Shot Anomaly Action Recognition using Pretrained Deep Skeleton Features

#### 2. Authors: 
Fumiaki Sato, Ryo Hachiuma, Taiki Sekii

#### 3. Affiliation: 
Konica Minolta, Inc.（柯尼卡美能达公司）

#### 4. Keywords: 
Anomaly action recognition, skeleton-based approaches, deep neural networks, zero-shot learning, prompt-guided

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sato_Prompt-Guided_Zero-Shot_Anomaly_Action_Recognition_Using_Pretrained_Deep_Skeleton_Features_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究无监督异常行为识别，即在没有异常样本的情况下以无监督方式识别视频级别的异常人类行为事件，并同时解决传统基于骨架的方法中的三个限制：目标域依赖的深度神经网络（DNN）训练、对骨架错误的鲁棒性以及缺乏正常样本。 
- (2):过去的方法可以分为两种，一种是利用视频的外观信息，另一种是仅利用人体骨架。本文提出的方法通过预训练的骨架特征提取器，使用目标域无关的骨架特征提取器，模拟正常行为的骨架特征分布，从而消除了DNN训练的需要。此外，为了增加对骨架错误的鲁棒性，本文提出了一种受点云深度学习范例启发的DNN架构，该架构在关节之间稀疏地传播特征。为了防止未观察到的正常行为被错误地识别为异常行为，本文将用户提示嵌入和骨架特征对齐到公共空间中的相似度分数纳入异常分数中，间接补充了正常行为。 
- (3):本文提出了一种统一的、用户提示引导的零样本学习框架，使用预训练的骨架特征提取器，该提取器在大规模动作识别数据集上进行了预训练。在训练阶段，本文利用正常样本模拟了正常行为的骨架特征分布，同时冻结了DNN的权重，并在推理阶段使用该分布估计异常分数。为了增加对骨架错误的鲁棒性，本文引入了一种受点云深度学习范例启发的DNN架构，该架构在关节之间稀疏地传播特征。此外，为了防止未观察到的正常行为被错误地识别为异常行为，本文将用户提示嵌入和骨架特征对齐到公共空间中的相似度分数纳入异常分数中，间接补充了正常行为。 
- (4):本文在两个公开数据集上进行了实验，测试了所提出方法在上述限制方面的有效性。实验结果表明，本文提出的方法可以消除DNN训练的需要，同时提高了对骨架错误的鲁棒性，并且可以正确地识别未观察到的正常行为。
#### 7. 方法详细介绍：
本文提出了一种基于预训练的、目标域不变的骨架特征提取器的异常行为识别方法。该方法将骨架特征与用户提示嵌入在共同空间中对齐的相似度得分整合到异常得分中。DNN架构是置换不变的，并且对骨架错误具有抵抗力。该方法仅使用少数类别的标签文本作为文本提示，并在条件异常行为时更新提示引导的动作得分的定义。

具体步骤如下：
1. 预训练阶段：使用对比学习方法在大规模动作识别数据集上训练DNN，其中对骨架特征和从动作类别名称中提取的文本嵌入进行对比学习。
2. 训练阶段：计算正常样本的分布，但不训练DNN。异常得分定义为表示x不属于正常样本的概率和表示x包含用户指定的异常动作的概率的联合概率。 
3. 推理阶段：使用分布和未见过的动作的文本提示计算异常得分。流程包括多人姿态估计、特征提取和异常得分计算。

#### 8. 实验设置：
本文使用了RWF-2000、Kinetics-250、Kinetics-400和NTU RGB+D 120等数据集进行评估，并使用PPN和HRNet等姿态检测器。其中，RWF-2000数据集中将暴力和非暴力动作定义为异常和正常，使用五个不同的文本提示表达暴力动作进行评估。Kinetics-250数据集中使用Few vs. Many设置，将三到五个动作类别定义为正常，其余为异常。使用随机和有意义的两种数据分割方式进行评估。

#### 9. 实验结果和分析：
本文提出的方法在RWF-2000数据集上的准确率优于几种先前的有监督方法。使用文本提示可以提高所提出方法的准确性。即使用户仅定义正常动作，该方法也可以检测到异常行为。该方法对骨架检测错误和域漂移具有鲁棒性。文本提示引导的零样本学习的准确性取决于文本提示的质量。


# Paper:221     MaskCLIP: 基于遮蔽自蒸馏的对比学习提升视觉-语言预训练



#### 1. Title: 
MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining

#### 2. Authors: 
Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Vision-language pretraining, Masked self-distillation, Contrastive learning, Local patch representation, Semantic feature learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2022_paper.html  Github: https://github.com/LightDXY/MaskCLIP

#### 6. Summary : 
- (1):本文研究背景是视觉-语言(VL)对比学习在预训练中的成功应用，但是VL对比学习只关注全局表示，忽略了局部信息，因此需要一种方法来充分利用图像本身来促进VL对比学习以进一步提高可转移的视觉模型。

- (2):过去的方法主要是基于对比学习或遮蔽图像建模，但是这些方法存在一些问题，如对于全局表示的关注不足，低级别预测目标效率低等。本文提出了一种新的方法，即MaskCLIP，它将遮蔽自蒸馏方法引入到对比学习中，以提高VL对比学习的可转移性。

- (3):本文提出的方法是将遮蔽自蒸馏方法与VL对比学习相结合，以提高可转移的视觉编码器。遮蔽自蒸馏的核心思想是从完整图像中提取表示，到从遮蔽图像中预测的表示中提取表示。这种方法有两个重要的好处：一是遮蔽自蒸馏针对局部补丁表示学习，这是对于以文本为中心的表示学习的补充；二是遮蔽自蒸馏也与视觉-语言对比学习一致，从训练目标的角度来看，两者都利用视觉编码器进行特征对齐，因此能够学习到间接从语言中获得的局部语义。本文还在文本分支中引入了局部语义监督，进一步提高了预训练性能。

- (4):本文在多个视觉基准测试中进行了广泛的实验，包括分类、语义分割、检测和分割等任务。实验结果表明，MaskCLIP在零样本、线性探测和微调等方面均取得了优异的性能。
#### 7. 方法详细介绍：
本文提出了一种名为MaskCLIP的视觉-语言预训练框架。该框架包括视觉编码器、文本编码器、视觉解码器和文本解码器。视觉编码器采用Vision Transformer（ViT）实现，从输入图像中提取视觉特征标记。文本编码器也是基于Transformer的模型，从输入文本中提取语言特征。在预训练期间，使用图像-文本对比损失对图像和文本的全局特征表示进行对齐。该框架还将掩码自蒸馏用于增强视觉表示。使用平均教师自蒸馏来改进学生模型，并将掩码图像建模引入到自蒸馏中。通过这些损失的组合，从头开始训练该框架。

#### 8. 实验设置：
本文在多个数据集上评估了MaskCLIP的性能，包括ImageNet-1K、ADE20K、MS-COCO和Flicker30K。对于ImageNet-1K，作者报告了零样本、线性探针和微调性能。对于线性探针，他们固定了骨干网络，并为其训练了一个新的线性分类器，共90个时期。对于微调，他们遵循了BEiT的设置，并使用层衰减的学习率微调模型100个时期。

#### 9. 实验结果和分析：
本文在多个基准测试中评估了MaskCLIP模型的性能，包括ICinW分类数据集的零样本评估和Flickr30K、MS-COCO数据集的零样本图像-文本检索。结果表明，MaskCLIP在大多数基准测试中优于其他最先进的方法，在ICinW挑战赛和MS-COCO数据集上取得了最佳性能。消融研究也证明了所提出的模型设计的有效性。在ImageNet-1K分类任务中，MaskCLIP在零样本、线性探针和微调方面的性能分别比CLIP高6.9％、7.2％和1.3％。在Flicker30K数据集上，MaskCLIP在图像-文本检索和文本-图像检索方面分别比CLIP高17.2％和12.8％。在ADE20K上的语义分割任务中，MaskCLIP的mIoU为50.5，比基线方法CLIP高2.7。在MS-COCO上的目标检测和实例分割任务中，MaskCLIP的box AP和mask AP分别为45.4和40.9，比CLIP高1.8 / 1.4，比SLIP高1.4 / 0.6。


# Paper:222     3D中的常见宠物：真实可变形类别的动态新视角合成



#### 1. Title: 
Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories

#### 2. Authors: 
Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ignacio Rocco, Natalia Neverova, Andrea Vedaldi, David Novotny

#### 3. Affiliation: 
第一作者：University of Toronto（多伦多大学）

#### 4. Keywords: 
new-view synthesis, dynamic objects, 3D reconstruction, deformable objects, large-scale dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sinha_Common_Pets_in_3D_Dynamic_New-View_Synthesis_of_Real-Life_Deformable_Categories_CVPR_2021_paper.html  Github: https://github.com/coppelia-robotics/cop3d

#### 6. Summary : 
- (1):本文研究的背景是如何从少量视角的图像中重建出物体的3D模型，特别是对于动态物体的重建，这一问题更加困难。

- (2):过去的方法主要是基于大规模数据集的学习，但这些方法只适用于刚性物体的重建。本文提出了一种新的方法，可以从少量视角的视频中重建出动态物体的3D模型。

- (3):本文提出了一个新的数据集CoP3D，其中包含了4200个猫和狗的视频，这些视频是由普通人使用智能手机拍摄的，因此更贴近真实场景。同时，本文提出了一种新的方法Tracker-NeRF，该方法可以从少量视角的视频中重建出动态物体的3D模型。Tracker-NeRF通过学习一个类别级别的重建先验，可以在测试时预测出未见过的物体的3D点轨迹，并生成新的视角。实验结果表明，Tracker-NeRF在CoP3D数据集上的表现优于现有的基线方法。

- (4):本文的方法在CoP3D数据集上取得了较好的表现，可以重建出动态物体的3D模型，支持了本文的研究目标。
#### 7. 方法详细介绍：
本文提出了一种名为Tracker-NeRF的方法，用于学习视频中物体变形的先验知识。该方法包括三个主要组件：3D物体跟踪器、规范表面嵌入（CSE）网络和神经辐射场（NeRF）网络。首先，3D物体跟踪器用于估计每帧中物体的3D姿态和形状。然后，CSE网络用于学习不同视角下物体表面之间的密集对应关系。最后，NeRF网络用于预测3D空间中每个点的颜色和不透明度。该方法在MSSSR和FSCR任务上进行了评估，并在定量和定性上优于所有基线方法。

#### 8. 实验设置：
本文使用了一个名为Common Pets in 3D（CoP3D）的新数据集，其中包含使用智能手机设备在野外拍摄的4200个猫和狗的视频。数据集用于探索从随意记录的视频中进行4D重建的问题。模型使用PyTorch进行训练，并使用Adam进行优化，学习率为10^-4。实验在单个NVIDIA V100 GPU上进行。本文还评估了在微调MSSSR任务之前在FSCR任务上进行预训练的模型的效果，这可以提高单序列重建的质量和训练时间。

#### 9. 实验结果和分析：
本文在MSSSR和FSCR任务上进行了实验，并使用多个指标（包括PSNR、ℓ1距离、LPIPS和预测和真实对象掩码之间的IoU）评估了重建图像的质量。实验结果表明，TrackeRF方法在两个任务上均优于基线方法，并且可以产生更锐利和更准确的新视图合成。该方法通过补偿源视图中基础3D点的运动来实现这一点。实验还表明，引入流一致性和规范表面嵌入可以提高性能。优化使用Adam优化器进行，初始学习率为5×10^-4，当总损失停滞时，将其衰减10倍。


# Paper:223     面向RAW目标检测：一个新的基准和一个新模型



#### 1. Title: 
Toward RAW Object Detection: A New Benchmark and A New Model

#### 2. Authors: 
Ruikang Xu, Chang Chen, Jingyang Peng, Cheng Li, Yibin Huang, Fenglong Song, Youliang Yan, Zhiwei Xiong

#### 3. Affiliation: 
Ruikang Xu: 中国科学技术大学 (University of Science and Technology of China)

#### 4. Keywords: 
HDR, RAW sensor data, object detection, dynamic range adjustment, dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Toward_RAW_Object_Detection_A_New_Benchmark_and_A_New_Model_CVPR_2021_paper.html  Github: https://gitee.com/mindspore/models/tree/master/research/cv/RAOD

#### 6. Summary:
- (1):本文旨在实现对RAW传感器数据的目标检测，以便在处理各种光照条件（如强烈的耀斑）时，算法能够处理高动态范围（HDR）数据。 
- (2):现有的目标检测方法都是针对标准动态范围（SDR）数据设计的，无法适应HDR RAW传感器数据，这导致了性能的显著下降。本文提出了一种简单而有效的调整方法，可以有效地检测HDR RAW传感器数据，并且与下游检测器一起在端到端方案中进行联合优化。 
- (3):本文提出了一种适用于HDR RAW传感器数据的目标检测方法，该方法是基于一个新的RAW传感器数据集（ROD）构建的。该方法采用了一种图像自适应处理网络，通过可学习的变换函数来调整RAW传感器数据的动态范围。具体而言，该方法设计了两个模块，通过图像级和像素级信息来调整RAW传感器数据的动态范围。 
- (4):在ROD数据集上的实验结果表明，与SDR数据相比，HDR RAW传感器数据上的目标检测性能显著提高。本文提出的方法还在最新的神经ISP方法上取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种新的原始数据目标检测方法，包括三个模块：图像级调整、像素级调整和下游检测网络。图像级调整模块增强全局特征，以自适应方式增强HDR信息以实现有效检测。像素级调整模块利用像素级转换函数探索局部信息以调整原始传感器数据。下游检测网络用于检测调整后的原始传感器数据中的对象。损失函数由分类和回归损失组成，与对象检测器一起在端到端方案中进行优化，没有对处理后的图像表示的约束。

#### 8. 实验设置：
本文在提出的ROD数据集上进行了评估，该数据集根据场景分为白天和夜晚两个子集。训练数据集包括约9k个白天场景的注释图像和约13k个夜晚场景的注释图像。其余数据用作相应的测试数据集。采用YOLOX模型作为下游检测网络，以满足自动驾驶场景的实时要求。在训练和测试过程中，将原始传感器数据调整为1280×1280的大小。平均精度和平均召回率在所有IOU阈值（AP和AR）、IOU阈值0.5（AP50）和0.75（AP75）上作为评估指标。

#### 9. 实验结果和分析：
在提出的ROD数据集上的广泛实验结果表明，原始传感器数据上的目标检测性能显着优于SDR数据在不同场景下的检测。所提出的方法也优于最近的最先进的神经ISP方法。全面的消融实验表明，所提出的方法有效地提高了DNNs-based目标检测算法在HDR原始传感器数据上的性能。此外，作者分析了输入数据的纹理信息和像素分布对下游检测网络性能的影响。


# Paper:224     基于内容感知共形映射的超广角图像矫正



#### 1. Title: 
Wide-angle Rectification via Content-aware Conformal Mapping

#### 2. Authors: 
Qi Zhang, Hongdong Li, Qing Wang

#### 3. Affiliation: 
1. 腾讯 AI Lab

#### 4. Keywords: 
Wide-angle image rectification, Conformal Mapping, Deep Learning, Polar Domain, Local Adaptive Rectification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Wide-Angle_Rectification_via_Content-Aware_Conformal_Mapping_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了超广角镜头下的图像畸变问题，提出了一种新的图像矫正方法，旨在最小化最具视觉显著性的畸变，同时保留原始的超广视野，以提供用户所需的沉浸式体验。

- (2):现有的超广角图像矫正方法可以大致分为两类：全局矫正和局部自适应矫正。全局矫正方法采用单一的全局几何映射来消除镜头畸变，但它们的性能并不完全令人满意，留下许多不必要的残留畸变或牺牲了预期的广视野。局部自适应矫正方法通过局部优化来矫正曲线结构，但是边界上的视觉内容会丢失。本文提出了一种自适应极坐标共形映射来矫正超广角图像，通过深度神经网络分析图像内容来自动找到映射的参数。本文的方法在保留广视野的同时，有效地减少了超广角畸变，同时保留了原始的超广视野，不会牺牲显著的图像内容。

- (3):本文提出了一种自适应极坐标共形映射来矫正超广角图像，通过深度神经网络分析图像内容来自动找到映射的参数。本文的方法在保留广视野的同时，有效地减少了超广角畸变，同时保留了原始的超广视野，不会牺牲显著的图像内容。本文的方法在多个数据集上进行了实验，证明了其在超广角图像矫正方面的优越性。

- (4):本文提出的方法在多个数据集上进行了实验，证明了其在超广角图像矫正方面的优越性。本文的方法在保留广视野的同时，有效地减少了超广角畸变，同时保留了原始的超广视野，不会牺牲显著的图像内容。
#### 7. 方法详细介绍：
本文提出了一种内容感知的共形映射方法，用于广角矫正。该方法通过导出极坐标形式的共形映射，并在极域中强制执行柯西-黎曼条件来实现。通过将任务定义为极坐标网格上的优化问题，得到最佳网格布局。通过最小二乘极坐标共形映射，将柯西-黎曼方程引入极坐标网格布局。该方法还包括使用深度图像内容分析进行自动内容感知矫正，并使用能量函数搜索最佳形状保持的LSCM变换。

#### 8. 实验设置：
本文使用了来自Flickr的150张广角图像，范围从100°到180°。将方法与全局方法和局部方法进行比较，并将网格分辨率设置为192×122和201×105，而所提出的方法在所有测试中使用100×180的网格分辨率。评估指标包括StraightAcc、ShapeAcc和ConformalAcc，这些指标是从标记线的端点和线段的中点计算出来的。

#### 9. 实验结果与分析：
实验结果表明，所提出的方法在失真最小化和视场保留之间取得了良好的平衡，并在ShapeAcc和ConformalAcc方面优于基线方法。60名参与者的用户研究证实，所提出的方法始终以较大的优势优于所有竞争方法。


# Paper:225     基于动作子集的对比学习方法用于无监督骨架动作识别



#### 1. Title: 
Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition

#### 2. Authors: 
Lilang Lin, Jiahang Zhang, Jiaying Liu

#### 3. Affiliation: 
北京大学王选计算机技术研究所

#### 4. Keywords: 
Skeleton-based action recognition, self-supervised learning, contrastive learning, actionlet, motion-adaptive transformation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Actionlet-Dependent_Contrastive_Learning_for_Unsupervised_Skeleton-Based_Action_Recognition_CVPR_2021_paper.html  Github: https://github.com/langlands/ActCLR

#### 6. Summary : 
- (1):本文研究骨架动作识别中的自监督预训练方法，提出了一种基于动作子集的对比学习方法，旨在实现对不同部位的自适应建模，提高动作识别的准确性。

- (2):传统的骨架动作识别方法通常依赖于大量标记数据，而自监督学习方法则通过预训练和微调的两个阶段来实现无监督学习。现有的对比学习方法将骨架序列的不同区域等同对待，缺乏对不同部位的自适应设计，这对动作识别的准确性有负面影响。本文提出了一种基于动作子集的对比学习方法，通过将动作序列与无运动的静态锚点进行对比，提取骨架数据的运动区域作为动作子集，实现了对不同部位的自适应建模。

- (3):本文提出了一种动作子集依赖的对比学习方法（ActCLR），通过对比静态锚点和动作子集，提取骨架数据的运动区域作为动作子集，然后构建了一种基于动作子集的运动自适应数据转换方法。同时，提出了一种语义感知的特征池化方法，以区分运动和静态区域的特征表示。在NTU RGB+D和PKUMMD数据集上进行了广泛的实验，证明了所提出方法的有效性。

- (4):本文提出的方法在NTU RGB+D和PKUMMD数据集上均取得了优异的动作识别性能，相比于现有的自监督学习方法，具有更好的自适应性和更强的泛化能力。
#### 7. 方法详细介绍：
本文提出了一种名为Actionlet-Dependent Contrastive Learning (ActCLR)的无监督骨架动作识别方法。该方法包括以下步骤：
1. 无监督动作元素选择：通过比较动作序列和静态序列之间的差异来获取动作区域，称为动作元素。
2. 动态自适应数据转换：为了进行对比学习，设计了一种动态自适应转换策略。在动作元素区域，采用保留语义的数据转换来学习语义一致性。在非动作元素区域，应用更强的数据转换以获得更强的泛化性能。
3. 语义感知特征池化：采用语义感知特征池化方法来提取动作元素区域的运动特征，使特征更加关注运动关节而不被静止关节所干扰。
4. 对比学习：采用对比学习来学习表示，通过将正样本与负样本进行对比，使正样本之间的表示更加相似，而负样本之间的表示更加不同。

#### 8. 实验设置：
本文使用了NTU RGB+D数据集和PKU多模态数据集进行评估。网络中使用的编码器和投影头也进行了描述，以及优化方法和训练细节。还提到了训练的批量大小、时代数和GPU。

#### 9. 实验结果和分析：
本文进行了消融实验来详细分析所提出的方法。测试了动态自适应数据转换和不同数据转换组合对对比学习效果的影响。对不同流进行了语义感知特征池化，并分析了仅提取动作元素区域信息和非动作元素区域信息进行动作识别的性能。还展示了平均运动和动作元素的可视化。实验结果表明，所提出的方法取得了显著的性能，并验证了设计的有效性。

#### 整篇论文总结：
本文提出了一种无监督骨架动作识别方法，称为Actionlet-Dependent Contrastive Learning (ActCLR)。该方法通过无监督动作元素选择、动态自适应数据转换、语义感知特征池化和对比学习等步骤来学习表示。实验结果表明，所提出的方法在NTU RGB+D数据集和PKU多模态数据集上取得了显著的性能，验证了方法的有效性。


# Paper:226     NAR-Former: 面向全面属性预测的神经架构表示学习



#### 1. Title: 
NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction

#### 2. Authors: 
Yun Yi, Haokui Zhang, Wenze Hu, Nannan Wang, Xiaoyu Wang

#### 3. Affiliation: 
第一作者：西安电子科技大学
First author's affiliation: Xidian University

#### 4. Keywords: 
Neural architecture representation learning, transformer, accuracy prediction, latency prediction, data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yi_NAR-Former_Neural_Architecture_Representation_Learning_Towards_Holistic_Attributes_Prediction_CVPR_2021_paper.html  Github: https://github.com/yuny220/NAR-Former

#### 6. Summary : 
- (1):本文的研究背景是深度学习模型的广泛应用，需要对神经网络进行建模和学习表示，以便预测模型属性，如准确性和延迟，而无需运行实际训练或推理任务。

- (2):过去的方法包括使用LSTM和MLP等模型进行准确性预测，以及使用GCN等模型进行图形格式数据的编码。这些方法在编码拓扑结构方面存在局限性，而且在处理大型网络时存在编码维度的问题。本文提出了一种基于transformer的神经网络表示学习框架，使用一种简单而有效的编码方法，将神经网络节点的操作类型、位置和输入编码为向量，并使用transformer结构进行编码和融合，以生成紧凑的表示。此外，还提出了一种信息流一致性增强方法和相应的架构一致性损失，以提高模型的训练效率和性能。

- (3):本文提出了一种基于transformer的神经网络表示学习框架，使用一种简单而有效的编码方法，将神经网络节点的操作类型、位置和输入编码为向量，并使用transformer结构进行编码和融合，以生成紧凑的表示。此外，还提出了一种信息流一致性增强方法和相应的架构一致性损失，以提高模型的训练效率和性能。实验结果表明，该框架可以用于预测细胞结构和整个深度神经网络的准确性和延迟属性，并取得了良好的性能。

- (4):本文提出的方法在NAS-Bench-101、NAS-Bench-201、DARTS搜索空间和NNLQP等数据集上进行了实验，可以用于预测细胞结构和整个深度神经网络的准确性和延迟属性，并取得了良好的性能。与其他预测器相比，本文提出的准确性预测器在NAS-Bench-101和NAS-Bench-201数据集上的细胞结构上取得了高竞争性的准确性表现。在DARTS数据集上，只查询了100个神经结构，就实现了97.52%的准确率。本文还在深度超过200层的神经网络上进行了延迟预测实验，证明了该模型的普适性。
#### 7. 方法详细介绍：
NAR-Former是一种神经架构表示学习方法，包括三个主要组成部分：架构编码、多阶段融合Transformer和信息流一致性增强。架构编码使用tokenizer将神经网络的操作和拓扑信息编码成一个token序列。多阶段融合Transformer将token序列转换为一个特征表示。信息流一致性增强方法通过维护增强结构和原始结构之间的信息流一致性来进一步提高属性预测的准确性。训练过程中使用MSE损失、序列排序损失和架构一致性损失函数。

#### 8. 实验设置：
在NAS-Bench-101、NAS-Bench-201、DARTS搜索空间和NNLQP数据集上进行实验，预测细胞结构和整个深度神经网络的准确性和延迟属性。

#### 9. 实验结果和分析：
在NAS-Bench-101和NAS-Bench-201上，NAR-Former的准确性预测表现优于TNASP，平均提高了0.047和0.159-0.175。在DARTS上，NAR-Former仅查询了100个神经结构就实现了97.52%的测试准确率，优于TNASP和CTNAS。在预测深度超过200层的神经网络的延迟方面，该方法也取得了良好的性能。信息流一致性增强和架构一致性损失相对于现有的随机增强策略，使用更少的增强样本带来更多的好处。


# Paper:227     基于解耦元标签净化器的噪声标签学习



#### 1. Title: 
Learning from Noisy Labels with Decoupled Meta Label Purifier

#### 2. Authors: 
Yuanpeng Tu, Boshen Zhang, Yuxi Li, Liang Liu, Jian Li, Yabiao Wang, Chengjie Wang, Cai Rong Zhao

#### 3. Affiliation: 
第一作者：同济大学电子与信息工程系

#### 4. Keywords: 
Deep learning, Noisy labels, Label correction, Meta-learning, Representation learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tu_Learning_From_Noisy_Labels_With_Decoupled_Meta_Label_Purifier_CVPR_2021_paper.html  Github: https://github.com/yuanpengtu/DMLP

#### 6. Summary : 
- (1):本文研究的是深度学习中的噪声标签问题，即如何在存在噪声标签的情况下训练深度神经网络以提高泛化能力。

- (2):过去的方法主要集中在噪声样本的检测和减少其对参数更新的影响，或者通过修正噪声标签来增加干净的训练样本。然而，这些方法在极度嘈杂和复杂的情况下表现不佳。最近，基于元学习的标签纠正策略被广泛采用来解决这个问题，通过识别和纠正潜在的噪声标签来提高性能。然而，解决元学习问题不可避免地涉及到模型权重和超参数（即标签分布）之间的双层优化嵌套循环。为了解决这个问题，以前的方法采用交替更新的耦合学习过程。本文提出了一种新的多阶段标签净化器DMLP，将标签纠正过程分解为无标签表示学习和简单的元标签净化器，从而可以专注于提取有区别的特征和标签纠正两个不同的阶段。

- (3):本文提出了一种新的多阶段标签净化器DMLP，将标签纠正过程分解为无标签表示学习和简单的元标签净化器，从而可以专注于提取有区别的特征和标签纠正两个不同的阶段。DMLP是一个即插即用的标签净化器，净化后的标签可以直接在朴素的端到端网络重新训练或其他鲁棒学习方法中重复使用，其中在几个合成和真实世界的嘈杂数据集上获得了最先进的结果，特别是在高噪声水平下。DMLP的核心是一个基于元学习的标签净化器，但为了避免使用耦合的解决方案来解决双层优化问题，DMLP将这个过程分解为自监督表示学习和一个线性元学习器，以适应潜在的正确标签分布，从而简化了标签净化阶段作为单层优化问题。简单的元学习器是通过两个相互增强的校正过程设计的，分别称为内在主要校正（IPC）和外在辅助校正（EAC）。IPC在全局范围内以稳定的速度纠正标签，而EAC通过使用IPC更新的标签来加速纠正过程。这两个过程可以增强彼此的能力并形成标签校正的正
#### 7. 方法详细介绍：
本文提出了一种名为“解耦元标签净化器”（Decoupled Meta Label Purifier，DMLP）的标签纠正方法，它利用元学习策略。DMLP将模型权重和标签的学习过程解耦为单独的阶段。该方法包括一个带有噪声的训练数据集和一个小的干净验证数据集。首先，设计一个预训练的特征提取器来提取d维图像表示f。然后，利用对比自监督学习框架来更新参数θG，作为预训练而不使用噪声标签。建立的特征提取器f = G（x;θ∗G）可以学习到一种与噪声无关的图像数据描述符，该描述符在高维特征空间中也具有高度可分性。该方法涉及两个相互增强的解决方案，以寻求纯化的训练标签，即内在主要纠正（IPC）和外在辅助纠正（EAC）过程。IPC旨在以缓慢而稳定的速度执行全局标签纯化，而EAC是一种外部纠正过程，旨在加速标签纠正过程。

#### 8. 实验设置：
在自监督预训练阶段，采用流行的SimCLR算法，ResNet作为骨干网络。元学习器中的分类器使用Adam优化器进行100个epoch的训练。为了公平比较，采用ResNet18进行最终的DivideMix算法。ηI和ηE分别设置为0.01和1.0。缩放因子α设置为1.0。为了确保公平评估，将1000张图像随机分为干净的验证集，其余作为训练样本。实验在不同的噪声率下进行：π∈{20％，50％，80％，90％}用于对称噪声，π∈{20％，40％}用于非对称噪声。

#### 9. 实验结果和分析：
本文在多个合成和真实噪声数据集上评估了所提出的DMLP方法的性能，包括CIFAR-10、CIFAR-100和Clothing1M。结果表明，DMLP在所有噪声设置下的纠正标签准确性优于其他竞争方法。DMLP在与其他流行的LNL方法（如ELR+、Co-teaching和CDR）一起使用时也表现出一致的改进。此外，DMLP即使在小的验证集大小下也能实现高精度，并在极端噪声情况下使用验证集作为标记样本时表现最佳。DMLP-Naive方法在CIFAR-10/100数据集上取得了竞争性的结果，而DMLP-DivideMix方法在所有设置下均取得了最先进的性能。DMLP-DivideMix在不同噪声比下比REED高0.2％和2.7％。DMLP方法表现出对变异噪声水平的鲁棒性，并且比其他竞争方法更少受到噪声比例的影响。在大规模真实噪声数据集Clothing1M上，简单的DMLP-Naive方法优于所有其他方法，并且DMLP-DivideMix通过约0.46％进一步提高了准确性。


# Paper:228     MetaViewer：面向统一多视图表示的研究



#### 1. Title: 
MetaViewer: Towards A Unified Multi-View Representation

#### 2. Authors: 
Ren Wang, Haoliang Sun, Yuling Ma, Xiaoming Xi, Yilong Yin

#### 3. Affiliation: 
Ren Wang, Haoliang Sun, and Yilong Yin are affiliated with Shandong University.

#### 4. Keywords: 
Multi-view representation learning, meta-learning, bi-level optimization, fusion function, unified representation.

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_MetaViewer_Towards_A_Unified_Multi-View_Representation_CVPR_2021_paper.html
Github: None

#### 6. Summary: 
- (1):本文研究多视图表示学习，旨在从多个可观察视图中学习实体的统一表示，以便于下游任务的使用。

- (2):现有的多视图表示学习方法通常遵循特定到统一的流程，从每个视图中提取潜在特征，然后融合或对齐它们以获得统一的对象表示。然而，手动预先指定的融合函数和对齐标准可能会降低所得到的表示的质量。为了克服这些问题，本文提出了一种新颖的从元学习角度的统一到特定的多视图学习框架，其中统一表示不再涉及手动操作，而是从名为MetaViewer的元学习器自动导出。具体而言，我们将视图特定潜在特征的提取和融合形式化为一个嵌套优化问题，并通过使用双层优化方案来解决它。这样，MetaViewer自动将视图特定特征融合成一个统一的特征，并通过观察从统一到特定的重构过程来学习最佳融合方案。本文的创新点在于提出了一种新的元学习视角，提出了一种新的统一到特定的多视图学习框架，并提出了MetaViewer，一种元学习器，它可以自动学习数据驱动的最佳融合方案。

- (3):本文提出了一种新的元学习视角，提出了一种新的统一到特定的多视图学习框架，以学习统一的多视图表示。我们将视图特定潜在特征的提取和融合形式化为一个嵌套优化问题，并通过使用双层优化方案来解决它。MetaViewer自动将视图特定特征融合成一个统一的特征，并通过观察从统一到特定的重构过程来学习最佳融合方案。本文的创新点在于提出了一种新的元学习视角，提出了一种新的统一到特定的多视图学习框架，并提出了MetaViewer，一种元学习器，它可以自动学习数据驱动的最佳融合方案。

- (4):在下游分类和聚类任务中，MetaViewer的表现与现有方法相当。本文提出的方法可以自动融合视图特定特征，学习最佳融合方案，从而提高了多视图表示学习的效率和效果。
#### 7. 方法详细介绍：
本文提出了一种名为MetaViewer的多视图学习框架，旨在学习异构数据的统一多视图表示。该框架包括三个模块：嵌入模块、表示学习模块和自监督学习模块。嵌入模块将异构视图转换为潜在特征空间，表示学习模块将得到的嵌入映射到视图特定或统一表示。自监督学习模块通过构建一系列预文本任务为模型训练提供有效的监督信息。该框架通过双层优化进行训练，将统一实体表示与视图特定特征分离，并实现数据驱动融合。具体步骤包括：
1. 对于每个视图，使用卷积神经网络提取特征。
2. 将每个视图的特征嵌入到潜在空间中。
3. 使用元学习器将嵌入特征融合为统一表示。
4. 使用自监督学习模块进行模型训练。
5. 通过双层优化更新模型参数。

#### 8. 实验设置：
本文在多个数据集上进行了实验，包括Handwritten、RGB-D、Animal、Fashion-MV和Caltech101-20。每个数据集的视图数、类别数和样本数也提供了。比较的方法包括两种经典方法（DCCA和DCCAE）和五种最先进的方法（WTNNM、TLRR、MIB、MFLVC和DCP）。实现细节也被提及，包括所有方法都是从头开始训练的，所有基于深度网络的方法使用相同的网络架构。

#### 9. 实验结果和分析：
本文在分类和聚类任务上评估了MetaViewer方法的性能，并使用标准评估指标进行了测量。实验结果表明，MetaViewer在所有数据集上都取得了优于或与现有方法相当的性能。此外，本文还进行了消融实验，证明了MetaViewer的有效性。


# Paper:229     AdamsFormer用于未来空间动作定位



#### 1. Title: 
AdamsFormer for Spatial Action Localization in the Future

#### 2. Authors: 
Hyung-gun Chi, Kwonjoon Lee, Nakul Agarwal, Yi Xu, Karthik Ramani, Chiho Choi

#### 3. Affiliation: 
第一作者：Purdue University

#### 4. Keywords: 
Spatial Action Localization, NeuralODE, Adams method, long-range temporal modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chi_AdamsFormer_for_Spatial_Action_Localization_in_the_Future_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是预测未来动作位置对于人机协作等应用至关重要，但准确地定位这些动作在未来帧中仍有改进的空间。

- (2):过去的方法包括动作识别、时间动作定位和时空动作定位等，但是这些方法无法准确地定位未来动作位置。本文提出了一种新的任务，即未来空间动作定位（SALF），旨在准确预测观察和未来帧中的动作位置。为了解决SALF，本文使用了神经ODE的概念，通过使用神经网络解决ODE来模拟连续时间序列的潜在动态。本文提出了一种新的架构AdamsFormer，通过模拟连续时间动态来将观察帧特征扩展到未来时间范围内，从而预测未来动作位置。

- (3):本文提出的AdamsFormer模型使用Adams方法，这是一种多步方法，可以有效地利用先前步骤的信息而不丢弃它。AdamsFormer模型通过将观察帧的潜在特征外推到我们想要预测的未来时间范围来预测未来动作位置。AdamsFormer模型在UCF101-24和JHMDB-21数据集上进行了广泛的实验，证明了其在SALF任务中优于现有的长期时间建模方法。

- (4):本文的方法在SALF任务中取得了良好的性能，可以准确地预测未来动作位置和类别。AdamsFormer模型在UCF101-24和JHMDB-21数据集上的实验结果表明，其在帧mAP方面显著优于现有的长期时间建模方法。
#### 7. 方法详细介绍：
本文提出了一种名为AdamsFormer的新型框架，用于未来的时空动作定位（SALF）。AdamsFormer采用三个步骤来解决SALF问题。首先，视频编码器从视频剪辑中提取潜在特征。接下来，未来特征预测器通过求解在公式（3）中定义的初值问题（IVP）来外推未来特征。最后，解码器使用外推的未来特征定位和分类未来剪辑的动作。AdamsFormer使用线性多步方法来解决IVP，该方法利用以前的步骤计算下一个值，而单步方法仅采用一个先前的步骤。ODE函数f（·）使用具有因果掩码的Transformer解码器对潜在特征的动态进行建模。解码器采用从公式（5）中导出的Zt，回归动作边界框并对动作进行分类。

#### 8. 实验设置：
本文使用UCF101-24和JHMDB-21数据集进行广泛的实验，以展示所提出的架构的优势，并对现有的长期时间依赖性建模算法在SALF上进行基准测试。

#### 9. 实验结果和分析：
实验结果表明，AdamsFormer在SALF任务中显著优于其他最先进的长期特征建模模型，表现为帧级平均精度（Frame-mAP）。对模型在SALF上的性能进行了深入分析，为研究人员提供了提高模型性能的直觉。


# Paper:230     用于领域自适应目标检测的对比均值教师



#### 1. Title: 
Contrastive Mean Teacher for Domain Adaptive Object Detectors

#### 2. Authors: 
Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang

#### 3. Affiliation: 
第一作者：伊利诺伊大学香槟分校

#### 4. Keywords: 
Object detection, domain adaptation, mean-teacher self-training, contrastive learning

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cao_Contrastive_Mean_Teacher_for_Domain_Adaptive_Object_Detectors_CVPR_2021_paper.html
Github: https://github.com/Shengcao-Cao/CMT

#### 6. Summary: 
- (1):本文研究领域自适应目标检测中的众所周知的领域差异问题，提出了一种新的方法来解决这个问题。

- (2):过去的方法包括对抗性训练、领域随机化、图像翻译等，但是这些方法都存在一些问题。本文提出的方法是将均值教师自训练和对比学习相结合，通过对比学习来优化伪标签，从而解决了均值教师自训练中伪标签质量差的问题。同时，本文提出了一种对象级对比学习策略，以更精细的方式适应目标域。

- (3):本文提出的方法是将均值教师自训练和对比学习相结合，通过对比学习来优化伪标签，从而解决了均值教师自训练中伪标签质量差的问题。同时，本文提出了一种对象级对比学习策略，以更精细的方式适应目标域。本文的创新点在于将均值教师自训练和对比学习相结合，提出了一种新的方法来解决领域自适应目标检测中的问题。

- (4):本文的方法在Foggy Cityscapes数据集上取得了51.9%的mAP，优于之前最好的方法2.1%的mAP。本文的方法可以稳定性能，并在伪标签噪声增加时提供更显著的收益。本文的方法支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于对比学习和均值教师自训练的对比均值教师（CMT）框架，用于域自适应目标检测。该方法使用 Faster R-CNN 目标检测器，提取 VGG-16 或 ResNet-101 的多尺度特征，并基于 Detectron2 和 Adaptive Teacher（AT）和 Probabilistic Teacher（PT）的公开代码实现。CMT 是一个通用的框架，可以与现有的均值教师自训练方法结合使用。该方法使用伪标签提取对象级别特征，并通过对比学习进行优化，无需在目标域中使用标签。CMT 的对象级别对比损失作为特征学习的一种增强，不会改变原始的训练流程。 

#### 8. 实验设置：
本文在 Cityscapes、Foggy Cityscapes、KITTI、Pascal VOC 和 Clipart1k 数据集上进行了实验。Cityscapes 数据集包含 2,975 张训练图像和 500 张验证图像，Foggy Cityscapes 数据集是在 Cityscapes 的基础上添加了雾气合成的。KITTI 数据集包含 7,481 张训练图像，Pascal VOC 数据集包含 11,540 张图像，Clipart1k 数据集包含 500 张训练和验证图像。本文在三个域自适应任务上进行了实验：从正常天气到恶劣天气（Cityscapes → Foggy Cityscapes）、从合成到真实（Clipart1k → Pascal VOC）和从白天到黑夜（KITTI → Cityscapes）。

#### 9. 实验结果与分析：
本文在两个域自适应任务上进行了实验：KITTI 到 Cityscapes 和 Pascal VOC 到 Clipart。在 KITTI 到 Cityscapes 任务中，均值教师自训练方法 PT 的性能优于所有先前的方法，提高了约 15% 的 AP。此外，当与提出的对比均值教师（CMT）框架结合使用时，PT 获得了额外的 4.1% AP 性能提升。在 Pascal VOC 到 Clipart 任务中，AT + CMT 的组合比 AT 提高了 1.3% mAP，并超过了先前最好的 TIA 0.7% mAP。本文还提供了对所提出的 CMT 框架的剖析研究，证明了类别对比和多尺度特征在对象级别对比学习中的有效性。此外，本文还展示了定性结果，表明 CMT 学习到的更好的对象级别表示有助于检测器区分前景对象类别并更好地定位它们。


# Paper:231     零样本物体计数



#### 1. Title: 
Zero-Shot Object Counting

#### 2. Authors: 
Jingyi Xu, Hieu Le, Vu Nguyen, Viresh Ranjan, and Dimitris Samaras

#### 3. Affiliation: 
Jingyi Xu, Vu Nguyen, and Dimitris Samaras are affiliated with Stony Brook University.

#### 4. Keywords: 
Object counting, zero-shot learning, exemplar-free counting, class-agnostic counting, patch selection.

#### 5. Paper: 
The paper is available at: https://openaccess.thecvf.com/content_CVPR_2021/papers/Xu_Zero-Shot_Object_Counting_CVPR_2021_paper.pdf

Github code link: https://github.com/cvlab-stonybrook/zero-shot-counting

#### 6. Summary:
- (1): This paper addresses the problem of object counting, which aims to count the number of objects in an image. The authors propose a new setting called zero-shot object counting, where only the class name is available during test time, and no human-annotated exemplars are required.
 
- (2): Previous methods for object counting require human-annotated exemplars, which are often unavailable for novel categories, especially for autonomous systems. The proposed approach is motivated by the need for a counting system that can operate automatically without human annotators in the loop. The authors compare their approach with previous exemplar-free counting methods and show that their method outperforms them by a large margin.

- (3): The proposed method consists of two steps: patch selection and exemplar-based counting. To localize the patches containing the objects of interest, the authors construct a class prototype and select the patches whose embeddings are the k-nearest neighbors of the class prototype as the class-relevant patches. Then, they select the optimal patches to be used as counting exemplars by training a model to measure the goodness of an input patch based on its corresponding feature maps. The selected exemplars, together with a pre-trained exemplar-based counting model, are used to achieve exemplar-free object counting.

- (4): The proposed method is evaluated on the FSC-147 dataset, and the results show that it outperforms previous exemplar-free counting methods. The performance of the method supports the authors' goals of achieving exemplar-free object counting without human-annotated exemplars.
#### 7. 方法详细介绍：
本文提出了一种零样本物体计数的方法。首先，使用条件变分自编码器（VAE）构建类原型，以生成与语义嵌入有关的任意类的特征。类原型是通过对生成的特征取平均值计算得出的。然后，根据类原型选择类相关补丁，选择那些嵌入是类原型的k个最近邻的补丁。其次，训练一个误差预测器，基于其相应的特征映射来衡量输入补丁的好坏。选择预测误差最小的补丁作为最终的样本进行计数。这些选择的样本与预训练的基于样本的计数模型一起用于实现零样本物体计数。

#### 8. 实验设置：
本文在最近的类不可知计数数据集FSC-147上进行了评估。该数据集包含147张图像，共有20个类别的2000个对象。使用平均绝对误差（MAE）和均方误差（MSE）作为评估指标。

#### 9. 实验结果和分析：
实验结果表明，所提出的方法在FSC-147数据集上的表现优于以前的零样本计数方法。所提出的方法在测试集上实现了2.68的MAE和10.12的MSE，而以前的方法实现了4.12的MAE和22.11的MSE。通过消融研究和可视化结果验证了所提出方法的有效性。所选择的补丁也可以用于其他基于样本的计数方法，以实现零样本计数。


# Paper:232     PiMAE: 基于点云和图像交互式掩蔽自编码器的三维物体检测



#### 1. Title: 
PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection

#### 2. Authors: 
Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang, Yuheng Lu, Yandong Guo, Shanghang Zhang

#### 3. Affiliation: 
第一作者：北京大学计算机科学学院，多媒体信息处理国家重点实验室

#### 4. Keywords: 
Masked Autoencoders, Point Cloud, RGB Image, Multi-Modality, 3D Object Detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2021_paper.html  Github: https://github.com/BLVLab/PiMAE

#### 6. Summary : 
- (1):本文研究点云和RGB图像的交互式自编码器，用于3D物体检测。点云和RGB图像是现实世界中经常一起出现的两种模态，但很少有研究探讨它们在多模态设置下的能力。
 
- (2):现有的多模态方法往往依赖于辅助模型来生成伪模态，这是次优的，难以研究跨模态交互。另一方面，自监督的3D和2D表示学习的对比方法在生成负样本和数据增强时存在采样偏差，使它们在实际场景中不可行。本文提出了PiMAE，一个自监督的预训练框架，通过三个方面促进3D和2D的交互。具体来说，我们首先注意到两个源之间的掩蔽策略的重要性，并利用投影模块来补充对齐两种模态的掩蔽和可见标记。然后，我们利用一个精心设计的两个分支MAE管道和一个新颖的共享解码器来促进掩蔽标记中的跨模态交互。最后，我们设计了一个独特的跨模态重构模块，以增强两种模态的表示学习。 

- (3):本文提出了PiMAE，一个简单而有效的管道，通过增加它们的交互来学习强大的3D和2D特征。具体来说，我们预训练点和图像对作为输入，采用两个分支的MAE学习框架，分别为两种模态学习嵌入。为了进一步促进特征对齐，我们设计了三个主要特征。首先，我们对图像和点输入进行标记化，并将点标记投影到图像补丁中，明确对齐它们之间的掩蔽关系。接下来，我们利用一个新颖的对称自编码器方案来促进强大的特征融合。编码器从[31]中汲取灵感，包括模态特定编码器的分离分支和共享编码器。然而，我们注意到，由于MAE的掩蔽标记只通过解码器[20]，所以在我们的方案中，共享解码器的设计对于掩蔽标记在执行分别的模态解码器之前学习相互信息至关重要。最后，为了学习更强的特征，受[15,67]的启发，PiMAE的多模态重构模块通过增强从图像特征中学习来明确地编码图像级理解。 

- (4):在大规模RGB-D场景理解基准测试（SUN RGB-D和ScannetV2）上进行的广泛实验表明
#### 7. 方法详细介绍：
本文提出了一种名为PiMAE的自监督预训练框架，通过三个方面促进3D和2D的交互。首先，使用投影模块来补充对齐两种模态的掩码和可见标记。其次，使用经过精心设计的两个分支MAE管道和一个新颖的共享解码器来促进掩码标记的跨模态交互。最后，设计了独特的跨模态重构模块，以增强两种模态的表示学习。多模态重构模块通过增强对图像特征的学习，使点云特征明确地编码图像级别的理解。

#### 8. 实验设置：
本文在大规模RGB-D场景理解基准测试（SUN RGB-D和ScannetV2）以及多个2D检测和分类数据集上评估了所提出的方法。实验使用不同的微调架构和任务进行，包括3D和2D物体检测和少样本图像分类。

#### 9. 实验结果和分析：
通过大量实验，PiMAE显示出极大地提高了多个3D检测器、2D检测器和少样本分类器的性能，分别提高了2.9％、6.7％和2.4％。预训练模型大幅提高了2D和3D检测器的性能，证明了PiMAE的有效性。


# Paper:233     灌木横截面显微镜图像中树轮实例分割的迭代下一边界检测



#### 1. Title: 
Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections

#### 2. Authors: 
Alexander Gillert, Giulia Resente, Alba Anadon-Rosell, Martin Wilmking, Uwe Freiherr von Lukas

#### 3. Affiliation: 
第一作者：Fraunhofer Institute for Computer Graphics Research (IGD), Rostock

#### 4. Keywords: 
Instance segmentation, tree rings, microscopy images, shrub cross sections, iterative method

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gillert_Iterative_Next_Boundary_Detection_for_Instance_Segmentation_of_Tree_Rings_CVPR_2021_paper.html  Github: http://github.com/alexander-g/INBD 

#### 6. Summary : 
- (1):本文研究了在灌木横截面的显微镜图像中检测树轮的问题，这可以被视为实例分割任务的一个特殊情况，但由于对象的同心圆环形状和高精度要求，现有方法的性能不足。

- (2):过去的方法包括自顶向下和自下而上两种方法，但由于同心圆形状的对象，这些方法都存在一些问题。自顶向下的方法由于非极大值抑制而无法处理重叠或同心的对象，自下而上的方法则无法处理难以识别的对象边界和不连续的树轮。本文提出了一种新的迭代方法，称为迭代下一边界检测（INBD），它从灌木横截面的中心开始，模拟自然生长方向，在每个迭代步骤中检测下一个树轮边界。INBD在实验中表现出优异的性能，并且是唯一具有内置时间顺序概念的方法。

- (3):本文提出了一种新的迭代方法INBD，它由两个神经网络组成。第一个网络是一个简单的语义分割网络，用于检测背景、环边界和中心环。第二个网络是INBD网络，它从检测到的中心环开始，迭代地检测下一个年轮的边界。这个过程还包括一个循环楔形环检测模块，以解决不完整环的问题。本文的贡献包括发布了一个新的具有挑战性的数据集，提出了INBD方法，对以前的通用实例分割方法进行了评估，并与它们进行了比较。

- (4):本文的方法在灌木横截面的显微镜图像中检测树轮的实例分割任务上取得了优异的性能，是唯一具有内置时间顺序概念的方法。
#### 7. 方法详细介绍：
本文提出了一种名为迭代下一边界检测（INBD）的方法，用于树木年轮在灌木横截面显微镜图像中的实例分割。该方法由两个神经网络组成。第一个网络是语义分割网络，用于检测三个类别：背景、年轮边界和中心年轮。第二个网络是主要的INBD网络，用于在每个迭代步骤中检测下一个年轮边界，遵循植物的自然生长过程。该过程还包括一个循环楔形环检测模块，以解决不完整年轮的问题。INBD流程还包括极坐标网格采样步骤，以处理对象的同心圆环形状。该方法在实验中与自顶向下和自底向上的通用实例分割方法进行了比较，结果表明其效果更好。此外，它是第一个自动为检测到的对象分配时间顺序的方法。

#### 8. 实验设置：
本研究使用了来自比利牛斯山、挪威南部和瑞典北部的亚高山、高山和亚北极地区的213个灌木横截面高分辨率图像数据集。数据集根据植物物种分为三个子集。注释过程耗时长，单张图像可能需要6小时。本研究使用平均平均召回率（mAR）和适应性Rand误差（ARAND）作为评估指标。

#### 9. 实验结果与分析：
本文提出的INBD方法在实验中表现出比通用实例分割方法更好的效果。在三个子集中，INBD方法的mAR分别为0.87、0.85和0.83，而Mask-R-CNN、Deep Snake、Multicut和GASP的mAR分别为0.81、0.79、0.77和0.75。此外，INBD方法还能够自动为检测到的对象分配时间顺序，这是其他方法所不具备的。作者还对INBD方法的鲁棒性和泛化能力进行了分析，结果表明该方法对于不同物种和不同成像条件的图像都具有较好的性能。


# Paper:234     超几何对比学习：超越对象的视觉表示学习



#### 1. Title: 
Hyperbolic Contrastive Learning for Visual Representations beyond Objects

#### 2. Authors: 
Songwei Ge, Shlok Mishra, Simon Kornblith, Chun-Liang Li, David Jacobs

#### 3. Affiliation: 
第一作者：马里兰大学帕克分校

#### 4. Keywords: 
Visual representation learning, hyperbolic space, contrastive learning, object-centric scene hierarchy, downstream tasks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ge_Hyperbolic_Contrastive_Learning_for_Visual_Representations_Beyond_Objects_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了视觉表示学习中的对象和场景之间的结构，并提出了一种超几何对比学习框架，以学习场景表示，使其与其组成对象的表示在超几何空间中接近。

- (2):现有的自监督方法通常使用相同的方法处理对象和场景。本文提出了一种对象中心的场景层次结构，以便于学习场景表示。为了解决场景表示的建模难题，本文提出了一种超几何对比学习框架，以学习场景表示，使其与其组成对象的表示在超几何空间中接近。本文的方法在多个数据集和任务上均取得了优异的性能。

- (3):本文提出了一种超几何对比学习框架，以学习场景表示，使其与其组成对象的表示在超几何空间中接近。本文的方法在多个数据集和任务上均取得了优异的性能。

- (4):本文的方法在多个数据集和任务上均取得了优异的性能，包括图像分类、目标检测和语义分割等任务。本文的方法可以在零样本情况下解决涉及场景和对象交互的各种视觉任务。
#### 7. 方法详细介绍：
本文提出了一种超几何对比学习框架，用于学习超越对象的视觉表示。该框架采用两种损失函数来学习对象和场景表示。对于对象表示，使用标准的归一化温度缩放交叉熵损失，该损失在欧几里得空间的超球面上操作。对于场景表示，提出了一种超几何对比目标，该目标使用指数映射将欧几里得空间中的特征投影到Poincaré球中，并将交叉熵损失中的内积替换为负超距离。模型的整体损失函数是欧几里得损失和超几何损失的组合，由一个缩放参数控制。将所提出的框架与三种流行的对比学习方法进行了比较：MoCo-v2、Dense-CL和ORL。

#### 8. 实验设置：
本文在多个基准测试中评估了预训练模型，包括图像分类、目标检测和语义分割任务。结果表明，所提出的超几何对比学习（HCL）方法在COCO目标检测和COCO语义分割上始终优于基线。HCL还改善了场景级（VOC）和对象级（ImageNet）数据集上的图像分类。本文还展示了使用HCL训练的模型的属性，包括标签不确定性量化和上下文外检测。结果展示了所提出方法的有效性和潜在应用。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的超几何对比学习方法在多个基准测试中均优于其他对比学习方法。在COCO目标检测和COCO语义分割任务中，HCL方法的性能均优于其他方法。在图像分类任务中，HCL方法在场景级和对象级数据集上均有所提高。此外，使用HCL训练的模型还具有标签不确定性量化和上下文外检测等有用的属性。这些结果表明了所提出方法的有效性和潜在应用。


# Paper:235     自适应图卷积子空间聚类



#### 1. Title: 
Adaptive Graph Convolutional Subspace Clustering

#### 2. Authors: 
Lai Wei, Zhengwei Chen, Jun Yin, Changming Zhu, Rigui Zhou, Jin Liu

#### 3. Affiliation: 
上海海事大学

#### 4. Keywords: 
Subspace clustering, graph convolutional networks, feature extraction, coefficient matrix, adaptive

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Adaptive_Graph_Convolutional_Subspace_Clustering_CVPR_2020_paper.pdf
Github: https://github.com/weilyshmtu/AGCSC

#### 6. Summary : 
- (1):本文研究的是子空间聚类问题，即将高维数据样本分配到多个线性子空间中。子空间聚类在图像处理、运动分割、人脸聚类等领域有广泛应用。 
- (2):现有的谱聚类算法主要集中在设计约束条件或特征提取方法上，但是这些方法存在一些问题。本文提出了一种新的方法，即使用图卷积技术同时设计特征提取方法和约束条件，并且采用自适应的方式更新图卷积算子。相比于现有方法，本文的方法更加适合子空间聚类问题，并且能够更准确地揭示数据集的内在结构。 
- (3):本文提出的方法是使用图卷积技术设计特征提取方法和约束条件，并且采用自适应的方式更新图卷积算子。通过使用自适应的图卷积算子，本文的方法能够更好地聚合原始数据样本的特征表示，并且能够更准确地揭示数据集的内在结构。 
- (4):本文的方法在多个子空间聚类任务上进行了实验，结果表明本文的方法在性能上优于现有的相关方法和深度模型。
#### 7. 方法详细介绍：
本文提出的自适应图卷积子空间聚类（AGCSC）方法，使用图卷积技术同时设计特征提取函数和约束函数。该方法将子空间聚类问题转化为一个带有约束的优化问题，通过交替方向乘子法（ADMM）算法求解。算法每次迭代更新三个变量，即系数矩阵、新表示和辅助变量。该方法的时间复杂度为O(n^3)。通过构建系数矩阵C，该方法使用图卷积算子进行特征提取，并通过迭代自适应更新图卷积算子。该方法的系数矩阵具有块对角性和双重随机性，适合于子空间聚类问题。

#### 8. 实验设置：
本文使用了多个基准数据库，包括三个人脸图像数据集（ORL、扩展的Yale B和Umist）、两个物体图像数据集（COIL-20和COIL-40）和一个手写数字数据集（MNIST）。每个图像的像素值在[0, 255]之间，但为了进行高效计算，每个像素值都被255除以，使得每个图像的像素值都在[0, 1]之间。本文将AGCSC与多个代表性和相关的子空间聚类模型进行比较，包括SSC、LRR、LSR、LRSC、BDR、TRR、FTRR、GCSC和FLSR。本文使用了两个常用的度量指标，即聚类准确度（ACC）和归一化互信息（NMI），来定量评估模型的性能。本文在集合{1e-5, 1e-4, 1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5}中调整AGCSC的两个参数alpha和beta，并按照相应论文中的建议调整其他评估方法中的所有参数。

#### 9. 实验结果和分析：
本文的实验结果表明，AGCSC方法在多个基准数据库上的性能均优于其他子空间聚类方法。在ORL、扩展的Yale B和MNIST数据集上，AGCSC方法的ACC和NMI指标均优于其他方法。在COIL-20和COIL-40数据集上，AGCSC方法的ACC指标优于其他方法，但NMI指标略逊于一些方法。本文还对AGCSC方法的参数敏感性进行了分析，结果表明该方法对参数的选择不敏感。


# Paper:236     基于列行交错像素合成的高效尺度不变生成器



#### 1. Title: 
Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis

#### 2. Authors: 
Thuan Hoang Nguyen, Thanh Van Le, Anh Tran

#### 3. Affiliation: 
VinAI Research, Hanoi, Vietnam (越南VinAI研究所)

#### 4. Keywords: 
GAN, image synthesis, scale-invariant, column-row entangled pixel synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Efficient_Scale-Invariant_Generator_With_Column-Row_Entangled_Pixel_Synthesis_CVPR_2021_paper.html  Github: https://github.com/VinAIResearch/CREPS

#### 6. Summary : 
- (1):本文研究任意尺度图像合成，提出了一种高效且可扩展的生成模型，能够在任意分辨率下合成逼真的图像。 
- (2):传统的基于卷积的GAN模型在缩放输出分辨率时会出现不一致性和“纹理粘连”问题。而基于INR的生成器在设计上是尺度等变的，但它们的巨大内存占用和缓慢的推理阻碍了这些网络在大规模或实时系统中的应用。本文提出了一种新的生成模型Column-Row Entangled Pixel Synthesis (CREPS)，它既高效又尺度等变，而不使用任何空间卷积或粗到细的设计。为了节省内存占用并使系统可扩展，我们采用了一种新颖的双线性表示，将层次特征图分解为单独的“厚”列和行编码。 
- (3):本文提出了一种简单而优雅的网络，仅配备调制线性层，中间没有上采样层。它支持任意尺度图像合成的尺度一致输出。为了进一步提高效率，我们引入了一种厚双线性表示，将2D网络特征分解为两个轻量级的行和列嵌入。与完整的2D特征相比，它显著节省了训练和推理复杂度。我们在四个数据集上进行了一系列实验，包括FFHQ、LSUN-Church、MetFaces和Flickr-Scenery，证实了我们提出的CREPS结构的有效性。 
- (4):本文提出的CREPS模型能够在任意分辨率下生成尺度一致和无混叠的图像，与先前的任意尺度GAN（如AnyresGAN和ScaleParty）不同。使用512×512图像训练的CREPS模型仍然可以在更高的分辨率下生成接近逼真的图像。最后，我们展示了CREPS模型在保持属性一致性的同时，支持复杂的几何变换和扭曲的能力。
#### 1. 实验结果
本文提出了一种名为CREPS的新型生成器架构，它是一种成本效益高且具有尺度等变性的生成器，可以合成任何目标分辨率的图像。CREPS模型可以产生高度逼真的图像，并在大多数情况下超越基于INR的模型CIPS。本文进行了多项实验，探讨了这种全连接生成器的一些有吸引力的特性，并讨论了CREPS在各种场景中的应用。本文还提到了CREPS的局限性，即缺乏空间偏差，因为每个像素都是独立生成的，导致生成的图像偶尔会出现一些与空间相关的伪影。

#### 2. 方法
本文提出的方法称为CREPS，即具有列-行交错像素合成的高效尺度不变生成器。它旨在支持任意尺度的图像合成，同时在给定单个模型的情况下强制执行不同尺度的一致性。该方法涉及使用厚双线性表示将特征图分解为行和列嵌入，从而显着减少了内存使用和计算成本。采用逐层特征组合方案来丰富表示能力。该方法与两种现有的GAN结构AnyresGAN和CIPS进行了比较，并在速度和内存之间取得了最佳平衡。

#### 3. 实验设置
本文在四个基准数据集上进行了实验，包括FFHQ、MetFaces、LSUN-Church和Flickr-Scenery。模型在512×512的图像上进行训练，并在多种分辨率高达2048×2048的图像上进行测试。实验在单个NVIDIA V100 GPU上进行，内存为32GB。

#### 4. 实验结果和分析
在各种数据集上的实验证实了CREPS在适当的训练和推理速度下合成尺度一致且无混叠的图像的能力。该模型产生尺度等变的图像，并在缩放输出分辨率时保持对象细节不变，不像先前的任意尺度GAN（如AnyresGAN和ScaleParty）。在FFHQ、LSUN-Church、MetFaces和Flickr-Scenery数据集上，提出的CREPS模型展示了无条件图像合成的竞争结果，同时具有以一致的细节生成每个图像的能力。该模型还支持复杂的几何变换和扭曲，同时保持属性一致性。该模型的代码可在https://github.com/VinAIResearch/CREPS上获得。


# Paper:237     查询中的魔鬼：推进面具变换器用于现实世界医学图像分割和分布外定位



#### 1. Title: 
Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization

#### 2. Authors: 
Mingze Yuan, Yingda Xia, Hexin Dong, Zifan Chen, Jiawen Yao, Mingyan Qiu, Ke Yan, Xiaoli Yin, Yu Shi, Xin Chen, Zaiyi Liu, Bin Dong, Jingren Zhou, Le Lu, Ling Zhang, Li Zhang

#### 3. Affiliation: 
第一作者：Mingze Yuan，就职于北京大学。

#### 4. Keywords: 
Medical image segmentation, Out-of-distribution localization, Mask Transformers, Object queries, Query-distribution loss.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yuan_Devil_Is_in_the_Queries_Advancing_Mask_Transformers_for_Real-World_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是医学图像分割中的实际问题，即如何在长尾分布的数据中检测和定位罕见疾病的异常情况。这些异常情况对于临床医学具有重要意义，因此医学AI算法需要在这些情况下表现出良好的效果，以避免对患者造成危险。
 
- (2):过去的方法主要是基于监督学习，但是由于医学图像的长尾分布，这些方法很难训练出可靠的模型。本文提出了一种新的方法，即使用Mask Transformers中的对象查询来定位异常情况。然而，由于前景和背景之间的差异较小，对象查询可能会过度关注背景，因此本文还提出了查询分布损失来改善分割效果和异常情况的检测。
 
- (3):本文提出了一种新的方法，即MaxQuery，它使用对象查询来定位异常情况。对象查询在训练期间适应内部者的特征级聚类中心。因此，在实际场景中对医学图像进行推断时，像素与查询之间的相似性可以检测和定位异常情况。本文还提出了查询分布损失来改善分割效果和异常情况的检测。本文的方法在两个真实的医学图像分割任务中进行了测试，即胰腺和肝脏肿瘤的分割，相比之前的最先进算法，平均AUROC提高了7.39％，平均AUPR提高了14.69％，平均FPR95提高了13.79％。同时，本文的方法还提高了与领先基线nnUNet相比的内部分割性能平均5.27％的DSC。
  
- (4):本文提出的方法在两个真实的医学图像分割任务中进行了测试，即胰腺和肝脏肿瘤的分割。本文的方法相比之前的最先进算法，平均AUROC提高了7.39％，平均AUPR提高了14.69％，平均FPR95提高了13.79％。同时，本文的方法还提高了与领先基线nnUNet相比的内部分割性能平均5.27％的DSC。这些结果表明，本文提出的方法在医学图像分割中具有很高的性能和实用性。
#### 7. 方法详细介绍：
本文提出了一种基于Mask Transformer的语义分割方法，称为MaxQuery。该方法使用对象查询来将语义分割建模为软聚类分配。在训练期间，查询适应于内部样本的特征级聚类中心。在推理期间，像素与查询之间的相似性检测和定位OOD区域。MaxQuery框架生成像素级异常分数图，评估查询响应以找到代表像素和其分配的聚类中心之间相似性的最大值。该方法还提出了查询分布（QD）损失，以在查询级别上强制执行分割目标和其他区域之间的清晰边界，从而提高内部分割和OOD指示的性能。

#### 8. 实验设置：
本文使用了两个真实的医学图像数据集，分别是胰腺和肝脏肿瘤分割数据集。这些数据集包括连续患者的增强3D CT扫描图像。每个数据集中的所有肿瘤都经过病理学确认，除了肝脏中的囊肿。每个数据集中的器官首先通过在公共数据集（例如Medical Decathlon）上训练的自学习方法进行自动注释，然后由工程师进行编辑。网络架构使用医学图像分割中当前基准模型nnUNet作为CNN骨干，包括具有跳跃连接的像素编码器和像素解码器。块中的自注意层具有8个头。对象查询（即聚类中心）的数量N为32，查询分布（N1，N2，N3）设置为（16，4，12）。QD损失的损失权重λ为0.1。网络使用RAdam进行训练，初始学习率为1×10−4，并使用多项式学习率衰减。在训练期间，使用即时数据增强来提高泛化性能，包括随机旋转和缩放、弹性变形、添加亮度和伽马缩放。

#### 9. 实验结果和分析：
本文提出的MaxQuery和QD损失显著提高了两个真实医学图像数据集（肝脏和胰腺肿瘤）的分割和OOD定位/检测性能。该方法在所有评估指标中均优于以前的基线，包括FPR95、AUPR和DSCinlier。MaxQuery框架在OOD定位方面的性能优于以前的最佳方法SML，AUROC提高了12.66％，AUPR提高了25.16％，FPR95提高了20.46％，在病例级OOD检测方面AUC提高了4.55％。在内部分割方面，该方法优于nnUNet 6.30％的DSC。该方法对查询分布的不同设置表现出鲁棒性。使用预softmax分数的MaxQuery在OOD定位的AUPR方面优于使用后softmax的MaxQuery。查询级别的异常分数在OOD定位方面优于类别级别的异常分数。


# Paper:238     系统异构下联邦学习的自适应通道稀疏化



#### 1. Title: 
Adaptive Channel Sparsity for Federated Learning under System Heterogeneity

#### 2. Authors: 
Dongping Liao, Xitong Gao, Yiren Zhao, Chengzhong Xu

#### 3. Affiliation: 
Dongping Liao and Chengzhong Xu are affiliated with State Key Lab of IoTSC, CIS Dept, University of Macau, Macau SAR, China.

#### 4. Keywords: 
Federated learning, system heterogeneity, channel sparsity, trajectory alignment, communication efficiency.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liao_Adaptive_Channel_Sparsity_for_Federated_Learning_Under_System_Heterogeneity_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是随着个人数据的重要性和隐私保护法规的出台，中心化的数据上传、存储和使用变得不再可行，因此需要一种能够在不收集本地训练数据的情况下，让多个边缘设备协作学习全局共享模型的方法，即联邦学习。
- (2):现有的联邦学习算法中，由于客户端数据的非独立同分布性质，通道神经元可能会为不同客户端专门化到不同的特征。然而，现有的通道稀疏联邦学习算法为客户端模型规定了固定的稀疏策略，可能会阻止客户端协同训练通道神经元。为了最小化稀疏对联邦学习收敛的影响，本文提出了Flado，通过为每个客户端中的单个神经元量身定制稀疏性，来提高客户端模型更新轨迹的对齐度。本文的方法在实验中表现出色，不仅在一系列数据集上实现了最高的任务准确性，而且在相同通信预算下，将训练所需的浮点运算量显著降低了10倍以上，并将通信/计算权衡的Pareto前沿推向了比竞争联邦学习算法更远的地方。
- (3):本文提出了Flado，一种优化通道激活概率以稀疏化客户端模型并使其更新轨迹与全局轨迹对齐的方法。Flado的优点是双重的。首先，粗粒度的通道dropout可以通过现有的模型和硬件设备轻松实现和利用，以加速客户端训练。其次，轻量级的轨迹对齐算法通过极低的客户端开销来优化每个客户端中每个通道的稀疏性，并可以获得巨大的计算优势。 
- (4):本文的方法在数据和系统异构性下，实现了比竞争联邦学习方法更好的收敛速度和更好的通信/计算权衡。在更高程度的异质性下，本文的方法表现更好。此外，本文的方法在更大的模型和分数客户端参与度下也具有良好的可扩展性。
#### 7. 方法详细介绍：
本文提出了一种名为Flado的方法，通过优化通道激活概率，实现对客户端模型的稀疏化，并实现全局轨迹的轨迹对齐。Flado根据训练轨迹为每个客户端的每个神经元适应不同的稀疏度。该方法涉及优化每个客户端中每个通道的通道激活概率，以加速每个通道的稀疏训练。轻量级的轨迹对齐算法可以优化每个客户端中每个通道的稀疏度，对客户端的开销非常低。现有模型和硬件设备可以轻松实现粗粒度通道丢失，以加速客户端训练。

#### 8. 实验设置：
本文在多个数据集上评估了所提出的Flado方法，包括Fashion-MNIST、CIFAR-10和CIFAR-100。实验涉及使用不同计算能力的多个客户端进行训练。通信预算是固定的，并且实验在数据和系统异构性下进行。

#### 9. 实验结果和分析：
本文表明，Flado可以在无限预算下在多个数据集上获得最高的任务准确性。在相同的通信预算下，Flado可以将训练所需的浮点运算（FLOPs）量减少10倍以上。所提出的方法还将通信/计算权衡的Pareto前沿推向了更远，比竞争FL算法更具优势。当数据分布和系统能力存在高度异构性时，该方法的收敛速度更快。该方法可以很好地扩展到更大的模型和分数客户端参与。


# Paper:239     利用移动传感器进行长期视觉定位



#### 1. Title: 
Long-term Visual Localization with Mobile Sensors

#### 2. Authors: 
Shen Yan, Yu Liu, Long Wang, Zehong Shen, Zhen Peng, Haomin Liu, Maojun Zhang, Guofeng Zhang, Xiaowei Zhou

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Visual localization, mobile sensors, GPS, compass, gravity sensor, image matching, pose estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yan_Long-Term_Visual_Localization_With_Mobile_Sensors_CVPR_2021_paper.html  Github: https://zju3dv.github.io/sensloc/

#### 6. Summary : 
- (1):本文研究了在移动设备上利用GPS、罗盘和重力传感器等多种传感器信息，解决在室外环境下图像定位的问题。由于室外环境的外观变化较大，传统的基于图像匹配和姿态估计的方法难以处理。因此，本文提出了一种新的框架，利用多种传感器信息来提高定位的鲁棒性和准确性。

- (2):传统的视觉定位方法通常涉及匹配预先构建的场景的3D点和查询图像中的2D像素。现代方法通常使用图像检索来确定查询图像中可能可见的场景部分，以处理大规模场景。然而，在真实的室外场景中，由于照明（例如，白天和黑夜）、季节性（例如，夏季和冬季）和结构变化，这些对应关系的获取和进一步恢复6-DoF姿态是困难的。本文提出的方法利用了移动设备上的多种传感器信息，如GPS、罗盘和重力方向，来解决这个问题。同时，本文还提出了一种直接的2D-3D匹配网络，以有效地建立2D-3D对应关系，而不是现有系统中繁琐的2D-2D匹配。

- (3):本文提出了一种名为SensLoc的新框架，它通过紧密耦合视觉感知和补充移动传感器信息来实现鲁棒的定位。在第一阶段，该方法利用GPS和罗盘来约束图像检索的搜索空间，从而减少全局特征的误差风险，并加速检索过程。在第二阶段，本文设计了一种基于Transformer的网络，以粗到细的方式直接将检索到的子地图的3D点与查询照片的密集2D像素进行匹配。在最后一阶段，本文实现了一个简单而有效的重力验证算法，并将其集成到RANSAC循环中，以过滤错误的姿态假设，从而提高了RANSAC的效率和准确性。

- (4):本文提出的方法在新的SensLoc数据集上进行了评估，并与几种现有的图像检索和定位方法进行了比较。实验结果表明，利用移动设备上可用的传感器先验信息，如GPS、罗盘和重力方向，可以显著提高现有方法的性能。本文的方法在挑战性的夜间环境中表现出比现有方法HLoc更好的性能，同时在GPU上仅需66ms即可找到2D-3D对应关系，PnP RANSAC仅需8ms。
#### 7. 方法详细介绍：
本文提出了一种名为SensLoc的框架，利用手机上的GPS、指南针和重力传感器解决了在时间变化的室外环境中基于图像的定位问题。该框架包括三个阶段：
1）使用GPS和指南针缩小图像检索的搜索空间，减少全局特征的误差风险，并加速检索过程。
2）设计了一个基于Transformer的网络，以粗到细的方式直接将检索到的子地图的3D点与查询照片的密集2D像素进行匹配，加速了2D-3D对应关系的建立，并在具有挑战性的外观变化下表现出更好的性能。
3）实现了一个简单而有效的重力验证算法，并将其集成到RANSAC循环中，以过滤错误的姿态假设，利用移动重力传感器提供的精确横滚和俯仰角度。重力验证提高了RANSAC的效率和准确性，因为错误的假设可以提前被排除。

#### 8. 实验设置：
作者创建了一个新的基准数据集SensLoc，用于多传感器视觉定位，包括季节性和照明变化。该数据集使用消费级全景相机（Insta360）和手持式实时动态差分（RTK）记录仪捕获，以重建大规模参考地图。通过绑定RTK的手机记录所有可用的内置传感器数据，收集具有大场景外观变化的查询序列。作者开发了一个伪地面真值（GT）生成算法，通过结合特征匹配、视觉惯性测距、RTK位置和重力方向，精确地将每个查询序列注册到辅助地图上。GT生成算法不需要任何手动干预或环境额外设置，实现了可扩展的姿态标记。

#### 9. 实验结果和分析：
在SensLoc数据集上，将所提出的方法与几种最先进的图像检索和定位方法进行了基准测试。实验表明，通过考虑移动设备中可用的传感器先验知识，如GPS、指南针和重力方向，可以大大提高现有方法的性能。在具有挑战性的夜间环境中，所提出的方法在性能上大幅优于最先进的方法HLoc，同时在GPU上仅需66ms即可找到2D-3D对应关系，在PnP RANSAC上仅需8ms。


# Paper:240     CFA: 面向类别的校准公平对抗训练



#### 1. Title: 
CFA: Class-wise Calibrated Fair Adversarial Training

#### 2. Authors: 
Zeming Wei, Yifei Wang, Yiwen Guo, Yisen Wang

#### 3. Affiliation: 
Zeming Wei, Yifei Wang, and Yisen Wang are affiliated with Peking University, China.

#### 4. Keywords: 
Adversarial training, fairness, robustness, deep neural networks.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wei_CFA_Class-Wise_Calibrated_Fair_Adversarial_Training_CVPR_2021_paper.html  Github: https://github.com/PKU-ML/CFA

#### 6. Summary: 
- (1): This paper addresses the issue of robustness fairness in deep neural networks (DNNs) against adversarial examples. Although adversarial training has been shown to improve overall model robustness, there still exists a disparity in robustness among classes, which can lead to safety concerns in real-world applications. 
- (2): Previous methods have focused on enhancing overall model robustness, but few have attempted to make adversarial training fair at the class level without sacrificing overall robustness. The proposed approach, Class-wise calibrated Fair Adversarial training (CFA), is motivated by the preference of different classes for adversarial configurations and dynamically customizes specific training configurations for each class automatically. 
- (3): The proposed CFA framework is the first to theoretically and empirically investigate the preference of different classes for adversarial configurations and customize specific training configurations for each class automatically. The approach also modifies the weight averaging technique to address the fluctuation issue during the training process. 
- (4): Experiments on benchmark datasets demonstrate that the proposed CFA can improve both overall robustness and fairness notably over other state-of-the-art methods. The performance achieved by the methods in this paper supports their goals of improving robustness fairness in DNNs against adversarial examples.
#### 7. 方法详细介绍：
本文提出了一种基于类别校准的公平对抗训练（CFA）框架，旨在提高整体和类别鲁棒性。该框架由三个组件组成：基于类别校准的对抗训练（CCAT）、基于类别校准的正则化（CCR）和公平性感知的权重平均（FAWA）。CCAT根据每个类别的训练鲁棒准确性调整扰动边界，CCR为不同类别定制不同的鲁棒性正则化，FAWA消除权重平均过程中的不公平检查点，以提高最差类别的鲁棒性。该CFA框架可以与现有的对抗训练算法相结合。

具体步骤如下：
1. 对抗训练（AT）：在每个训练批次中，使用PGD攻击生成对抗样本，并将其添加到训练数据中。
2. 基于类别校准的对抗训练（CCAT）：根据每个类别的训练鲁棒准确性调整扰动边界，以提高每个类别的鲁棒性。
3. 基于类别校准的正则化（CCR）：为不同类别定制不同的鲁棒性正则化，以提高每个类别的鲁棒性。
4. 公平性感知的权重平均（FAWA）：消除权重平均过程中的不公平检查点，以提高最差类别的鲁棒性。

#### 8. 实验设置：
本文在CIFAR-10数据集上使用PreActResNet-18模型进行实验。基线包括原始对抗训练（AT）、TRADES和指数移动平均（EMA）的权重平均方法。此外，本文将提出的CFA框架与公平鲁棒学习（FRL）和实例自适应对抗训练方法进行比较。

#### 9. 实验结果和分析：
提出的CFA框架在整体和最差类别鲁棒性方面均优于基线。具体来说，CFA框架通过消除权重平均过程中的不公平检查点，提高了最差类别的鲁棒性。该框架还优于FRL，后者专注于提高类别鲁棒性的公平性，但会降低整体鲁棒性。实验结果表明，提出的CFA框架在提高整体和类别鲁棒性方面具有有效性。


# Paper:241     模糊插值变压器用于从模糊中恢复真实世界运动



#### 1. Title: 
Blur Interpolation Transformer for Real-World Motion from Blur

#### 2. Authors: 
Zhihang Zhong, Mingdeng Cao, Xiang Ji, Yinqiang Zheng, Imari Sato

#### 3. Affiliation: 
第一作者：东京大学，日本；第二作者：东京大学，日本；第三作者：东京大学，日本；第四作者：东京大学，日本；第五作者：国立信息学研究所，日本。

#### 4. Keywords: 
Blur interpolation, motion from blur, transformer, real-world dataset, temporal correlation.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhong_Blur_Interpolation_Transformer_for_Real-World_Motion_From_Blur_CVPR_2021_paper.html  Github: https://github.com/zzh-tech/BiT

#### 6. Summary : 
- (1):本文研究了从模糊图像中恢复运动的挑战性问题，即联合去模糊和插值或模糊时间超分辨率。这个联合任务可以用于各种应用，如视频视觉感知增强、慢动作生成和快速移动物体分析。最近的工作表明，联合方法在性能上优于级联分离去模糊和视频帧插值方法。

- (2):当前的方法在合成数据集上的视觉质量仍有很大的提升空间，而且对真实世界数据的泛化能力较差。本文提出了一种模糊插值变压器（BiT），以有效地展开模糊中编码的基础时间相关性。基于多尺度残差Swin变换块，引入双端时间监督和时间对称集成策略，生成有效的特征，用于时变运动渲染。此外，设计了一个混合相机系统，收集了第一个真实世界数据集，包含一对多的模糊-清晰视频对。实验结果表明，BiT在公共数据集Adobe240上比现有方法有显著的提升。此外，所提出的真实世界数据集有效地帮助模型很好地泛化到真实的模糊场景。

- (3):本文提出了一种基于变压器的模型BiT，用于任意时间运动从模糊中重建。BiT采用多尺度残差Swin变换块，引入双端时间监督和时间对称集成策略，以增强共享的时间特征，用于任意时间运动重建。由于我们的设计，BiT即使没有基于光流的扭曲操作，也可以在公共基准性能上取得最先进的性能。同时，为了向社区提供真实世界基准，我们进一步设计了一个准确的混合相机系统，用于捕获包含时间对齐的低帧率模糊和高帧率清晰视频对的数据集（RBI）。由于RBI，模糊插值的真实数据泛化问题可以得到很大缓解，并且可以提供更合理的评估平台。通过这些改进，我们的模型呈现出令人印象深刻的任意模糊插值性能。

- (4):本文的方法在任意时间运动从模糊中重建方面取得了最先进的性能，BiT在公共数据集Adobe240上比现有方法有显著的提升。同时，所提出的真实世界数据集有效地帮助模型很好地泛化到真实的模糊场景。
#### 7. 方法详细介绍：
本文提出了一种模型——模糊插值变换器（BiT），用于在曝光时间内的任意时刻插值出清晰的运动。该模型采用三个连续的模糊图像作为输入，生成共享的时间特征。然后，将共享特征和归一化的时间t结合起来，为曝光时间内的指定时刻呈现出清晰的运动。该模型分为两个阶段，包括共享时间特征提取阶段和任意运动渲染阶段。共享时间特征提取阶段包括一个共享下采样卷积块用于浅层特征提取，然后是N个多尺度残差Swin变换块（MS-RSTB）。任意运动渲染阶段包括t编码模块和M个MS-RSTB。该模型使用双端时间监督（DTS）和时间对称集成（TSE）策略进行训练。

#### 8. 实验设置：
作者设计了一个混合相机系统来收集一个真实世界的数据集（RBI），其中包含时间对齐的低帧率和高帧率的模糊-清晰视频对。该系统由两个BITRAN CS-700C相机组成，通过激光校准与分光镜物理对准。低帧率相机采用长曝光方案来捕获模糊视频，而高帧率相机捕获相应的清晰帧。模糊视频和相应的清晰视频的帧率分别为25 fps和500 fps。模糊图像的曝光时间为18 ms，而清晰图像的曝光时间为近2 ms。图像大小为640×480。

#### 9. 实验结果和分析：
作者在合成数据集Adobe240和真实世界数据集RBI上将所提出的方法与现有方法进行了比较。结果表明，所提出的方法在定量指标和视觉质量方面均优于其他方法。作者还在合成数据集Adobe240和真实世界数据集RBI之间进行了交叉验证，结果表明，在合成数据上训练的模型在真实数据上测试会产生伪影。


# Paper:242     TranSG：基于Transformer的骨架图原型对比学习，结构-轨迹提示重构用于人物再识别



#### 1. Title: 
TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification

#### 2. Authors: 
Haocong Rao, Chunyan Miao

#### 3. Affiliation: 
LILY Research Center, Nanyang Technological University, Singapore (新加坡南洋理工大学LILY研究中心)

#### 4. Keywords: 
Person re-identification, skeleton data, skeleton graph, contrastive learning, Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Rao_TranSG_Transformer-Based_Skeleton_Graph_Prototype_Contrastive_Learning_With_Structure-Trajectory_CVPR_2021_paper.html  Github: https://github.com/Kali-Hac/TranSG

#### 6. Summary : 
- (1):本文研究的是基于骨架数据的人物再识别，通过骨架图的构建和关系学习，提出了一种新的Transformer-based Skeleton Graph prototype contrastive learning方法，以更好地捕捉骨架关系和空间-时间语义信息。

- (2):现有的骨架数据再识别方法通常使用原始身体关节或骨架序列表示，但往往不能同时建模不同身体部件之间的关系，并且很少从细粒度的身体关节表示中探索有用的语义信息。本文提出的方法通过骨架图Transformer（SGT）同时学习骨骼图中的身体和运动关系，以将关键相关节点特征聚合到图形表示中。然后，提出了图原型对比学习（GPC）来挖掘每个身份的最典型图形特征（图原型），并从骨架和序列级别对图形表示和不同原型之间的内在相似性进行对比，以学习具有区分性的图形表示。最后，提出了图形结构-轨迹提示重构（STPR）机制，以利用图形节点的空间和时间上下文来提示骨架图的重构，从而有助于捕捉更多有价值的模式和图形语义，以用于人物再识别。 

- (3):本文提出了一种通用的TranSG范式，用于从骨架图中学习有效的表示以用于人物再识别。TranSG是第一个将骨架关系学习和骨架图对比学习统一起来，专门用于基于骨架的人物再识别的Transformer范式。本文提出了骨架图Transformer（SGT）来完全捕捉骨架图中的关系，并将关键相关节点特征集成到图形表示中。提出了图原型对比学习（GPC）来对比和学习每个身份的最具代表性的骨架图特征和身份相关的语义，从而用于人物再识别。提出了图形结构-轨迹提示重构（STPR）来利用骨架图的空间-时间上下文来提示骨架图的重构，以用于人物再识别。

- (4):本文在五个公共基准测试中进行了实证评估，结果表明TranSG明显优于现有的最先进方法，并且高度可扩展，可应用于不同的图形建模、RGB估计或无监督场景。
#### 7. 方法详细介绍：
本文提出了一种名为TranSG的基于Transformer的骨架图原型对比学习方法，结合结构-轨迹提示重构机制用于人物重识别。该方法包括三个主要组件：骨架图Transformer（SGT），图原型对比学习（GPC）和图结构-轨迹提示重构（STPR）。具体步骤如下：
1. 将每个骨架序列表示为骨架图，并将图形位置信息整合到其节点表示中。
2. 将骨架图输入SGT中，使用多个全关系（FR）头完全捕获身体组件关系。
3. 利用不同身份的图特征的质心生成图原型，并通过优化LGPC增强骨架级别和序列级别图表示与其对应的原型的相似性，同时最大化它们与其他原型的不相似性。
4. 随机屏蔽骨架图结构和节点轨迹，并利用相应屏蔽的图表示作为上下文来提示重构，通过最小化LSTPR来实现。
5. 利用从该方法中学习到的骨架图表示进行人物重识别。

#### 8. 实验设置：
本文在四个基于骨架的人物重识别基准数据集（IAS、KS20、BIWI、KGBD）和一个大规模多视角步态数据集（CASIA-B）上评估了所提出的方法。每个数据集的身体关节数量、序列长度和嵌入大小的节点表示都有所不同。该方法结合了所提出的GPC和STPR来执行骨架表示学习。用于训练的两个损失的权重系数也有所不同。使用Adam优化器进行模型优化，学习率为3.5×10−4，所有数据集的批量大小均设置为256。评估指标包括累积匹配特征（CMC）曲线、排名1准确度（R1）、排名5准确度（R5）、排名10准确度（R10）和平均精度（mAP）。

#### 9. 实验结果和分析：
TranSG方法在大多数情况下优于许多现有的基于骨架的模型，并在大多数条件下取得了显着的改进，同时在使用RGB特征或/和视觉度量学习的代表性经典外观方法方面也取得了优越的性能。该方法还可以在不使用地面真实标签的情况下以无监督的方式进行。t-SNE可视化显示，所学习的骨架表示具有比其他方法更具有区分性的类间分离，表明TranSG可能捕获了更丰富的类相关语义。


# Paper:243     60秒内从单目视频中学习化身



#### 1. Title: 
InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds

#### 2. Authors: 
Tianjian Jiang, Xu Chen, Jie Song, Otmar Hilliges

#### 3. Affiliation: 
ETH Zurich (苏黎世联邦理工学院)

#### 4. Keywords: 
Avatar reconstruction, neural radiance fields, monocular video, articulation, empty space skipping

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2022_paper.html  Github: https://github.com/tianjianjiang/InstantAvatar

#### 6. Summary : 
- (1):本文旨在通过提出InstantAvatar系统，从单目视频中重建人类化身，该系统可以在60秒内完成重建，提供姿势和掩码，并且可以在交互速率下对模型进行动画化和渲染。 
- (2):现有的方法需要使用校准的多相机系统，需要大量的计算成本，而本文提出的方法可以从单目视频中学习3D虚拟人物，具有更广泛的应用前景。与现有方法相比，InstantAvatar的收敛速度快130倍，可以在几分钟内完成训练，而不是几个小时。 
- (3):本文提出了一种精心设计和工程的系统，将加速的神经辐射场与快速对应搜索模块相结合，以实现关节的动画化。在训练和推理中，采用了一种有效的空间跳过策略，使得学习速度更快。 
- (4):在合成和真实的单目视频上进行了评估，与现有方法相比，InstantAvatar在重建质量和新颖姿势合成结果方面具有可比性甚至更好。当给定相同的时间预算时，该方法显著优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种名为InstantAvatar的方法，可以在60秒内从单目视频中学习人物角色模型。该方法使用了一种加速的神经辐射场（NeRF）来建模人物的形状和外观，该模型在一个独立于姿态的规范空间中进行。同时，该方法还使用了一个高效的关节模块Fast-SNARF，将规范辐射场变形为姿态空间。为了渲染实际的体积，该方法提出了一种空间跳过方案，该方案专门针对具有已知关节模式的动态场景。该方法的训练时间只需要几分钟，而不是超过10小时，可以实现与现有最先进方法相当甚至更好的重建质量和新姿态合成质量。

具体步骤如下：
1. 使用不可微分的渲染器将规范辐射场渲染为2D图像。
2. 使用可微分的渲染器将规范辐射场渲染为2D图像，并计算渲染的2D alpha值。
3. 使用Huber损失函数对颜色预测进行训练，并使用渲染的2D alpha值进行训练，以减少空间中的浮动伪影。
4. 添加硬表面正则化，以鼓励NeRF模型预测实体表面。

#### 8. 实验设置：
本文在PeopleSnapshot和SURREAL数据集上进行了实验，评估了InstantAvatar方法的性能。评估指标包括重建质量、速度和新姿态合成质量。同时，本文还与两种最先进的方法Anim-NeRF和Neural Body进行了比较。

#### 9. 实验结果和分析：
本文的实验结果表明，InstantAvatar方法可以在60秒内从单目视频中学习人物角色模型，并且可以在15 FPS的速度下进行动画和渲染。该方法的重建质量和新姿态合成结果与现有最先进方法相当甚至更好，同时只需要几分钟的训练时间。此外，本文还进行了空间跳过方案和基于占用的正则化损失的消融实验，证明了这些技术的有效性。然而，该方法仍然存在一些局限性，例如对于复杂的动态场景，其性能可能会受到影响。


# Paper:244     基于成分的多降解学习图像恢复



#### 1. Title: 
Ingredient-oriented Multi-Degradation Learning for Image Restoration

#### 2. Authors: 
Jinghao Zhang, Jie Huang, Mingde Yao, Zizheng Yang, Hu Yu, Man Zhou, Feng Zhao

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Image restoration, multi-degradation, ingredients-oriented, degradation reformulation, meta-prior learning

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Ingredient-Oriented_Multi-Degradation_Learning_for_Image_Restoration_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究了多种图像恢复任务之间的关系，提出了一种基于成分的方法，通过探索不同降解的内在成分来实现可扩展的学习。
- (2):现有的多降解方法大多是基于任务的，无法挖掘不同降解之间的内在关系，随着任务数量的增加，可扩展性较差。本文提出了一种基于成分的降解重构框架(IDR)，通过两个阶段的学习，即任务导向的知识收集和成分导向的知识整合，协作处理降解表示和降解操作，实现了对多种降解的处理。 
- (3):本文提出的IDR框架通过探索不同降解的内在成分来实现可扩展的学习，具有以下创新点：(1)提出了一种基于成分的降解重构框架(IDR)，通过两个阶段的学习，即任务导向的知识收集和成分导向的知识整合，协作处理降解表示和降解操作，实现了对多种降解的处理。(2)在第一阶段，我们根据不同降解的物理原理进行特定操作，并为每种降解建立相应的先验中心。在第二阶段，我们通过可学习的主成分分析(PCA)逐步将先前的任务导向中心转化为单个成分导向中心，以在成分级别上争取多种降解之间的共性，同时尽可能保留各自的差异信息。(3)在MPL中采用动态软路由机制，根据第一阶段嵌入的操作先验进行概率未知降解去除。 
- (4):本文在各种图像恢复任务上进行了广泛的实验，证明了我们方法的有效性和可扩展性。IDR展现了对未知下游任务的良好泛化能力。
#### 7. 方法详细介绍：
本文提出了一种基于成分的退化重构框架(IDR)，包括两个阶段：任务导向的知识收集和成分导向的知识整合。第一阶段对不同的退化进行特定的操作，建立独立的任务导向的先验中心，以挖掘特定的退化成分。第二阶段通过可学习的主成分分析(PCA)逐步将前面的任务导向的先验中心重构为单个成分导向的中心，以在成分级别上寻求多个退化之间的共性，同时尽可能保留各自的方差信息。在Meta-Prior Learning模块中采用动态软路由机制进行概率性未知退化去除，根据第一阶段嵌入的操作先验。IDR可以通过嵌入Meta-Prior Learning模块并与相应的先验中心P协作，集成到任何Transformer骨干网络中。

#### 8. 实验设置：
本文在多个图像退化数据集上进行了实验，包括Rain100L、SOTS、BSD68、GoPro和LOL等，涉及去雨、去雾、去噪、去模糊和低光增强等不同的退化任务。网络在单个NVIDIA Geforce RTX 3090 GPU上进行训练，使用Adam优化器进行1200个epoch的训练。初始学习率设置为1×10−4，逐渐降低到1e−6，采用余弦退火。批量大小为8，第一阶段为单一退化类型，第二阶段为混合类型。在Lcls中采用标签平滑策略，ϵ=0.1，λcls设置为0.01。本文采用修剪的Restormer骨干网络，并在多个阶段的末尾嵌入MPL。

#### 9. 实验结果与分析：
本文在五个具有挑战性的图像恢复数据集上，包括BSD68、Set5、Set14、Urban100和DIV2K，将提出的IDR方法与11种最先进的方法进行了比较。定量结果表明，IDR在所有数据集上都取得了最佳或次佳的性能。本文还提供了提出的IDR方法与其他方法的视觉比较结果，证明了该方法在恢复具有各种退化的图像方面的有效性。消融研究表明，所提出方法的每个阶段都对整体性能的提高做出了贡献。


# Paper:245     学习紧凑表示以实现LiDAR点云的补全和生成



#### 1. Title: 
Learning Compact Representations for LiDAR Completion and Generation

#### 2. Authors: 
Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, Raquel Urtasun

#### 3. Affiliation: 
Waabi公司, 多伦多大学, 麻省理工学院

#### 4. Keywords: 
LiDAR completion, LiDAR generation, representation learning, point cloud, self-driving

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xiong_Learning_Compact_Representations_for_LiDAR_Completion_and_Generation_CVPR_2021_paper.html  Github: https://github.com/Waabi-ai/ultralidar

#### 6. Summary : 
- (1):本文研究的背景是自动驾驶领域中，LiDAR点云数据的稀疏性和高成本问题。

- (2):过去的方法主要是直接预测点云的语义标签，但缺乏细节信息，且无法支持下游任务。本文提出了一种紧凑的离散表示方法，通过对稀疏点云的表示与密集点云的表示进行对齐，实现了点云的稠密化。同时，通过学习离散码本的先验分布，可以生成多样化、逼真的LiDAR点云数据。这种方法可以大大降低数据采集成本，同时提高数据的多样性和数量。

- (3):本文提出了一种基于离散表示学习的LiDAR点云数据处理框架，包括稀疏点云到密集点云的转换和点云生成两个任务。该方法通过学习点云的离散表示，实现了对点云数据的紧凑表示和高效处理。同时，该方法还提出了一种基于Transformer的点云生成模型，可以生成逼真的LiDAR点云数据，并支持多种操作。

- (4):本文在稀疏点云到密集点云的转换和点云生成两个任务上进行了实验，结果表明，该方法可以显著提高下游任务的性能，并且生成的点云数据质量更高。该方法在自动驾驶领域具有广泛的应用前景。
#### 7. 方法详细介绍：
本文提出了一种名为UltraLiDAR的方法，用于场景级别的LiDAR补全、生成和操作。该方法的核心思想是学习一种紧凑的、离散的3D表示（码本），它编码了场景的几何结构和我们世界的物理规则。该方法包括两个主要组件：稀疏到密集的LiDAR补全和LiDAR生成。对于稀疏到密集的LiDAR补全，稀疏编码器将稀疏点云映射到离散码，密集解码器从中重构密集数据。对于LiDAR生成，变换器模型从空白画布或从部分观察到的点云映射的码开始，迭代地预测和更新缺失的部分。解码器根据预测的码生成LiDAR输出作为生成结果。该方法在Pandaset和KITTI-360数据集上进行了评估，结果表明，与现有技术相比，所提出的方法可以显著提高下游感知系统的性能，并生成更逼真的点云。

#### 8. 实验设置：
本文使用了Pandaset和KITTI-360数据集进行实验评估。对于稀疏到密集的LiDAR补全任务，本文使用了Pandaset数据集中的稀疏点云，将其转换为密集点云。对于LiDAR生成任务，本文使用了KITTI-360数据集中的点云数据。本文使用了Tesla V100 GPU进行训练和评估，并使用了Adam优化器进行训练。本文还使用了多个评估指标，包括平均重建误差、3D检测和场景补全指标等。

#### 9. 实验结果和分析：
本文在Pandaset和KITTI-360数据集上进行了实验评估，并与现有技术进行了比较。实验结果表明，所提出的UltraLiDAR方法在稀疏到密集的LiDAR补全和LiDAR生成任务中均取得了优异的性能。此外，本文还进行了人类研究，结果表明，所提出的方法可以生成更逼真的点云。本文还展示了条件和无条件生成和操作任务的结果。


# Paper:246     鲁森林回归的鲁棒性和可扩展性及其应用



#### 1. Title: 
Robust and Scalable Gaussian Process Regression and Its Applications

#### 2. Authors: 
Yifan Lu, Jiayi Ma, Leyuan Fang, Xin Tian, and Junjun Jiang

#### 3. Affiliation: 
Yifan Lu, Jiayi Ma, and Xin Tian are affiliated with Wuhan University, China. Leyuan Fang is affiliated with Hunan University, China. Junjun Jiang is affiliated with Harbin Institute of Technology, China.

#### 4. Keywords: 
Gaussian process regression, robustness, scalability, variational learning, mixture likelihood model, inducing variable approximation, stochastic variational inference.

#### 5. Paper: 
Paper link: https://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Robust_and_Scalable_Gaussian_Process_Regression_and_Its_Applications_CVPR_2020_paper.pdf
Github link: https://github.com/YifanLu2000/Robust-Scalable-GPR

#### 6. Summary: 
- (1): This paper aims to address the non-robustness of Gaussian process regression (GPR) in the presence of outliers, which is a common problem in real-world applications. 
- (2): Previous methods for robust GPR have used non-Gaussian likelihoods with heavy tails, but they suffer from high computational complexity. In this paper, the authors propose a mixture likelihood model where outliers are assumed to be sampled from a uniform distribution, and a variational formulation that jointly infers the mode of data (inlier or outlier) as well as hyperparameters by maximizing a lower bound of the true log marginal likelihood. The proposed formulation approximates the exact posterior distribution and is less likely to overfit. The authors also introduce the inducing variable approximation and stochastic variational inference to extend the model to large-scale data. 
- (3): The proposed robust and scalable GPR model is applied to two challenging real-world applications, feature matching and dense gene expression imputation. The experiments demonstrate the superiority of the model in terms of robustness and speed. 
- (4): The proposed model achieves state-of-the-art performance on the two real-world applications, and the performance supports the authors' goals of addressing the non-robustness of GPR in the presence of outliers and extending the model to large-scale data.
#### 7. 方法详细介绍：
本文提出了一种鲁棒且可扩展的高斯过程回归（GPR）模型，通过变分学习实现。该模型采用混合似然模型，其中假设异常值是从均匀分布中采样的。推导出一种变分公式，通过最大化真实对数边际似然的下限来联合推断数据的模式，即内点或异常点，以及超参数。引入诱导变量近似和随机变分推断到变分框架中，将模型扩展到大规模数据。

具体步骤如下：
1. 使用混合模型将每个数据点与一个潜变量相关联，表示它是内点还是异常点。
2. 在潜函数上放置高斯过程先验，其他变量使用非信息先验。
3. 使用变分推断技术来近似后验分布，使用稀疏诱导变量来降低GPR的时间复杂度。
4. 使用坐标上升技术进行优化，并使用反Jensen不等式获得最优超参数。

#### 8. 实验设置：
本文在两个数据集Neal和Friedman上评估了所提出的模型，其中包含不同的异常值比率。输入特征进行归一化，使用平方指数核作为协方差函数。诱导变量从训练数据中随机选择，小批量大小设置为max{n/8, 200}。本文还将该模型应用于特征匹配和基因表达量插值任务，并针对每个任务进行了特定的考虑。

#### 9. 实验结果与分析：
本文将所提出的模型与几种最先进的GPR模型在Neal和Friedman数据集上进行比较，使用平均绝对误差、均方根误差和预测概率的负对数作为指标。结果表明，所提出的模型在鲁棒性和准确性方面优于其他替代方案。本文还提供了Neal数据集上模型性能的可视化示例，证明了该模型仅使用内点即可近似精确后验GP，即使存在大量异常值。在特征匹配、基础矩阵估计和基因表达量插值任务中，本文的方法使用随机变分推断和诱导变量来提高可扩展性和计算效率，并与其他方法进行了比较，显示出竞争性或优越性能。


# Paper:247     基于Softmax的Out-of-Distribution检测的极限挑战



#### 1. Title: 
Pushing the Limits of Softmax-Based Out-of-Distribution Detection

#### 2. Authors: 
Xixi Liu, Yaroslava Lochman, Christopher Zach

#### 3. Affiliation: 
Xixi Liu: 瑞典查尔默斯理工大学 (Chalmers University of Technology)

#### 4. Keywords: 
Out-of-distribution detection, deep neural networks, entropy-based score function, softmax-based classifier

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2021_paper.html  Github: https://github.com/XixiLiu95/GEN

#### 6. Summary : 
- (1):本文研究深度神经网络中的out-of-distribution（OOD）检测问题，即如何区分测试时输入的样本是in-distribution（ID）还是OOD。这个问题在安全关键的应用中尤为重要，如自动驾驶和医学图像分析。然而，神经网络通常会对OOD样本做出过于自信的预测，因此需要设计一个合适的分数函数来区分ID和OOD数据。
- (2):过去的OOD检测方法需要访问训练数据进行分数设计或在训练过程中暴露模型于异常值。一些后续方法可以避免上述限制，但竞争力较差。本文提出了一种基于熵的分数函数——Generalized ENtropy score（GEN），它可以应用于任何预训练的softmax-based分类器。与其他后续方法相比，GEN的分数分布导致更好的ID/OOD分离。本文的方法不需要重新训练和/或异常值暴露，也不使用任何训练数据统计信息，但表现非常好，可以潜在地在更受限制的模型部署场景中使用。
- (3):本文提出的GEN方法是一种简单但有效的基于熵的OOD检测方法。GEN仅使用预测分布，不需要重新训练和/或异常值暴露，也不使用任何训练数据统计信息。与其他后续方法相比，GEN的分数分布导致更好的ID/OOD分离。本文的方法在ImageNet-1k OOD检测基准测试中表现出色，可以进一步提高OOD检测性能。
- (4):本文的方法在ImageNet-1k OOD检测基准测试中表现出色，可以进一步提高OOD检测性能。GEN方法在六个常用的CNN-based和visual transformer分类器上的平均AUROC上持续提高，平均AUROC提高至少3.5%。此外，我们还将GEN与基于特征增强的方法以及使用训练统计信息的方法相结合，进一步提高OOD检测性能。
#### 7. 方法详细介绍：
本文提出了一种新的基于广义熵的异常检测方法，称为广义熵（GEN）。该方法使用广义熵函数来放大预测分布与理想的one-hot编码之间的微小偏差。该方法通过截断前M个类别的总和来使广义熵函数在合成设置中更加鲁棒。该方法的得分旨在捕捉前M个类别中的小熵变化。该方法与其他后处理方法（包括MSP、MaxLogit、Energy、GradNorm、ODIN和ReAct）以及需要ID方法（包括KL匹配、马氏距离、ReAct、pNML、残差和ViM）进行了比较。本文还将GEN与ReAct和Residual相结合，以实现更好的性能。

#### 8. 实验设置：
本文在以ImageNet-1K为ID数据集的大规模基准测试中进行了OOD检测，并使用四个常用的OOD数据集（包括OpenImage-O、Texture、iNaturalist和ImageNet-O）评估了所提出的方法。本文使用了几种常用的卷积和基于transformer的大规模图像分类架构，包括Big Transfer、Vision Transformer、RepVGG、ResNet-50-D、DeiT和Swin。本文遵循最近的评估策略发展，并使用AUROC和FPR95值作为评估指标。本文研究的所有方法都是确定性的。

#### 9. 实验结果与分析：
本文提出的基于熵的方法GEN在平均AUROC方面优于MSP，差距显著，近5%。此外，GEN在四个数据集和四个架构上获得了最低的平均FPR95，比所有其他后处理方法在FPR95方面显着更好，差距近4%。将GEN与ID数据集的信息相结合的结果可以在表4的下半部分找到，表明GEN在使用Swin架构时与ViM的性能相当。当与ReAct和Residual方法相结合时，GEN在四个数据集上的平均AUROC和FPR95方面实现了最先进的性能。GEN的性能也与ViM相当。GEN和Energy分数的分数分布表明，GEN使ID/OOD分离更好。


# Paper:248     通过弱监督异常值分割实现鲁棒的基于模型的人脸重建



#### 1. Title: 
Robust Model-based Face Reconstruction through Weakly-Supervised Outlier Segmentation

#### 2. Authors: 
Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger, Adam Kortylewski

#### 3. Affiliation: 
Chunlu Li: 1. School of Automation, Southeast University
Bernhard Egger: 3. Friedrich-Alexander-Universität Erlangen-Nürnberg

#### 4. Keywords: 
3D face reconstruction, model-based face autoencoders, outlier segmentation, weakly-supervised learning, EM algorithm

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Robust_Model-Based_Face_Reconstruction_Through_Weakly-Supervised_Outlier_Segmentation_CVPR_2021_paper.html  Github: https://github.com/unibas-gravis/Occlusion-Robust-MoFA

#### 6. Summary : 
- (1):本文研究的背景是3D人脸重建中的模型异常值问题，这些异常值包括遮挡物和化妆等，会导致模型拟合失真，影响重建效果。
- (2):过去的方法通常采用自底向上的方法，如多视角一致性损失、面部对称性检测和面部分割等，但这些方法需要大量的标注数据，且对于一些特定的异常值难以处理。本文提出了一种联合人脸自编码器和异常值分割网络的方法，通过EM算法进行训练，实现了无需标注数据的异常值分割和3D人脸重建。
- (3):本文提出的方法是一种联合学习的方法，通过人脸自编码器和异常值分割网络相互协作，实现了无需标注数据的异常值分割和3D人脸重建。具体来说，本文利用了异常值不能很好地被人脸模型拟合的事实，通过多个损失函数来指导异常值分割网络的决策过程。本文的核心创新是通过EM算法进行训练，解决了异常值分割和模型拟合相互依赖的问题，从而实现了协同效应，提高了重建质量。
- (4):本文在NoW数据集上实现了SOTA的3D人脸重建性能，同时在CelebA-HQ和AR数据库上验证了本文方法能够准确预测遮挡物掩模，且无需任何监督。
#### 7. 方法详细介绍：
本文提出了一种基于神经网络的管道，FOCUS，它可以同时进行三维人脸重建和异常值分割。该管道将基于模型的人脸自编码器R与分割网络S集成在一起，并在它们之间创建协同作用。人脸自编码器期望从目标图像中的可见人脸区域重建完整的人脸外观，而分割网络预测一个二进制掩模，描述一个像素是否描绘了人脸或异常值。两个网络在训练期间耦合在一起，以诱导协同效应，使分割更准确，重建在异常值下更加鲁棒。训练过程从模型参数的粗略初始化开始，该初始化以无监督的方式获得。两个网络使用期望最大化（EM）类似的策略交替优化。分割网络使用四个损失进行训练，强制在图像之间具有内在的相似性，包括像素级邻居损失，感知级距离损失，面积损失和感知级保持损失。

#### 8. 实验设置：
本文在NoW测试集、CelebA-HQ数据集和AR数据库上进行了实验，并与几种最先进的方法进行了比较。

#### 9. 实验结果和分析：
本文提出的弱监督异常值分割方法可以提高无约束环境下基于模型的人脸自编码器的鲁棒性。该方法在NoW测试集上实现了最先进的重建精度，且提供了有前途的分割掩模。所提出的管道的形状重建精度几乎不受遮挡的影响，并达到了与完全监督管道相似的水平。所提出的方法预测的异常值分割网络显示出更高的准确性、召回率和F1分数，以及与[5]中使用的皮肤检测器和[8]中提出的分割方法相竞争的精度。消融研究的结果验证了分割网络和所提出的邻居损失和耦合感知损失的有用性。误配先验有助于减少总体重建误差。


# Paper:249     AutoRecon：自动化三维物体发现与重建



#### 1. Title: 
AutoRecon: Automated 3D Object Discovery and Reconstruction

#### 2. Authors: 
Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, Xiaowei Zhou

#### 3. Affiliation: 
浙江大学CAD&CG国家重点实验室

#### 4. Keywords: 
3D object reconstruction, self-supervised learning, neural scene representation, point cloud segmentation, unsupervised object discovery

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_AutoRecon_Automated_3D_Object_Discovery_and_Reconstruction_CVPR_2021_paper.html  Github: https://zju3dv.github.io/autorecon/

#### 6. Summary : 
- (1):本文研究的是自动化3D物体重建的问题，旨在实现从多视图图像中自动发现和重建物体，同时去除背景以获得干净的物体模型。 
- (2):传统的多视图立体视觉方法和最近的神经场景重建方法都不能识别物体，重建的物体模型通常与周围背景相耦合。现有的一些方法尝试从3D场景中自动分解物体，但需要手动注释，限制了更可扩展的3D内容创建的可行性。本文提出了一种全自动的3D重建框架，实现了从多视图图像中重建无背景物体模型，无需任何注释。 
- (3):本文提出了一种新的两阶段框架，用于自动重建显著物体的3D模型，首先通过自监督2D视觉Transformer特征从SfM点云中稳健地定位和分割前景物体，然后通过点云分割Transformer提供的密集监督来重建分解的神经场景表示，从而实现准确的物体重建和分割。本文的创新点在于提出了一种无需注释的全自动物体重建流程，实现了全局推理，从而提高了3D物体重建的可用性和可扩展性。 
- (4):本文在DTU、BlendedMVS和CO3D-V2数据集上进行了实验，证明了AutoRecon的有效性和鲁棒性。实验结果表明，即使在杂乱的背景下，我们的方法也可以自动且稳健地从RGB视频中恢复准确的3D物体模型和高质量的分割掩模。
#### 7. 方法详细介绍：
本文提出了一种自动化三维物体发现和重建的新型粗到细的流程。该流程包括一个粗分解阶段，用于从场景级别的SfM点云中分割出前景物体并估计其紧凑的三维边界框。然后，在粗分解的显式监督下，恢复前景物体的分解神经场景表示。该流程还包括一个点云分割Transformer，用于将神经点云分割为前景和背景点。然后，使用上述定义的图上的NCut对前景点云进行分割，并基于平面对齐的主成分分析推断定向的三维边界框。最后，使用粗分解结果对场景进行分区，并使用神经场景表示分别对每个分区进行建模。在优化中融入了多个约束条件以促进收敛和从周围环境中分解前景物体。

#### 8. 实验设置：
本文在CO3D-V2、BlendedMVS和DTU三个数据集上进行了评估。用于SfM重建的输入图像被调整为最大面积为720,000。作者从DINO-ViT的ViT-S/8版本中提取了逐帧2D特征。作者使用他们的数据生成管道处理了CO3D-V2数据集中的880个以物体为中心的视频，其中包括各种类别。所有椅子对象都保留为验证集。这导致训练800个对象和验证80个对象。作者对他们的3D分割Transformer进行了20个时期的训练，对他们的场景表示进行了60k次迭代，这需要在单个NVIDIA V100 GPU上花费2小时。

#### 9. 实验结果和分析：
本文提出的方法在CO3D、BlendedMVS和DTU数据集上均取得了优于基线的表现，并在BlendedMVS和DTU数据集上实现了最先进的性能。具体而言，在CO3D上，作者报告了AP@0.5为0.908，AP@0.7为0.581，在BlendedMVS上，AP@0.5为1.00，AP@0.7为1.00，在DTU上，AP@0.5为0.833，AP@0.7为0.833。本文的方法可以自动且鲁棒地从RGB视频中恢复准确的三维物体模型和高质量的分割掩码，即使在杂乱的背景下也能实现。


# Paper:250     指代多目标跟踪



#### 1. Title: 
Referring Multi-Object Tracking

#### 2. Authors: 
Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, Jianbing Shen

#### 3. Affiliation: 
第一作者：北京理工大学

#### 4. Keywords: 
referring understanding, multi-object tracking, natural language processing, computer vision, benchmark

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Referring_Multi-Object_Tracking_CVPR_2022_paper.html  Github: https://github.com/referringmot/Refer-KITTI

#### 6. Summary : 
- (1):本文研究的背景是自然语言处理与计算机视觉的结合，旨在通过语言表达来指导多目标跟踪的预测，提出了一种新的、通用的指代理解任务，即指代多目标跟踪（RMOT）。

- (2):过去的方法往往只能检测单个文本指代的对象，而本文提出的RMOT任务可以实现任意数量的指代目标预测。此外，现有数据集往往只能描述视频中的部分帧，对于多个目标的情况，一个单一的表达式无法涵盖所有短期状态。本文提出的Refer-KITTI数据集解决了这些问题，提供了高度灵活的指代对象和高时空动态的标注。

- (3):本文提出了一种基于Transformer的框架TransRMOT，用于在线处理RMOT任务。该模型采用编码器-解码器结构，设计了早期融合模块和可变形注意力层，以实现跨模态特征的进一步细化。在解码器中，查询嵌入与跨模态特征交互，以预测指代框。为了跟踪多个目标，我们将对象查询分解为跟踪查询和检测查询。

- (4):本文在Refer-KITTI数据集上进行了实验，结果表明，TransRMOT模型在RMOT任务上取得了出色的检测性能，优于其他手工设计的RMOT方法。本文提出的RMOT任务和Refer-KITTI数据集为指代多目标跟踪领域的研究提供了新的思路和工具。
#### 7. 方法详细介绍：
本文提出了一种新的通用的指代多目标跟踪任务，称为指代多目标跟踪（RMOT），它利用语言表达作为语义线索来指导多目标跟踪的预测。所提出的端到端框架TransRMOT基于Transformer，具有编码器-解码器架构。它在编码器中具有早期融合模块，用于密集地集成视觉和语言特征，随后是一堆可变形注意力层，用于进一步细化跨模态表示。在解码器中，基于查询的嵌入与跨模态特征交互，以预测指代框。为了跟踪多个对象，类似于MOTR，将对象查询分解为跟踪查询，用于跟踪前一帧的对象，以及检测查询，用于预测新生对象的边界框。

#### 8. 实验设置：
本文基于公共KITTI数据集构建了一个新的基准Refer-KITTI，用于评估所提出的RMOT任务。Refer-KITTI提供了18个视频，包含818个表达式，每个视频中的每个表达式平均注释了10.7个对象。该基准具有指代对象的高灵活性，高时态动态性和低标签成本。

#### 9. 实验结果和分析：
所提出的TransRMOT框架在Refer-KITTI基准上实现了令人印象深刻的检测性能，并且在其他对比方法上表现出色。本文对所提出的方法进行了彻底的分析，包括消融研究和与其他最先进方法的比较。结果表明，TransRMOT在Refer-KITTI基准上实现了最先进的性能。


# Paper:251     利用转移学习进行属性推断攻击



#### 1. Title: 
Manipulating Transfer Learning for Property Inference

#### 2. Authors: 
Yulong Tian, Fnu Suya, Anshuman Suri, Fengyuan Xu, David Evans

#### 3. Affiliation: 
第一作者：南京大学新软件技术国家重点实验室，中国

#### 4. Keywords: 
Transfer learning, property inference, deep learning, privacy leakage, downstream model

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Tian_Manipulating_Transfer_Learning_for_Property_Inference_CVPR_2021_paper.html  Github: https://github.com/yulongt23/Transfer-Inference

#### 6. Summary : 
- (1):本文研究了在转移学习中，攻击者如何利用上游模型对受害者的下游模型进行属性推断攻击，以揭示下游训练数据的敏感属性。例如，攻击者可能想要推断下游训练集中是否包含某个特定个体的图像。这种推断可能会导致严重的隐私泄露。 
- (2):以往的方法主要集中在成员推断攻击上，而本文则关注属性推断攻击。本文提出了一种新的漏洞，即攻击者可以通过精心制作的预训练模型，对下游模型进行推断攻击，从而揭示下游训练数据的敏感属性。本文的方法可以在不影响主要任务性能的情况下，实现高效和特定的属性推断攻击。 
- (3):本文提出了一种方法，通过操纵上游模型的训练，生成一个模型，用于训练下游模型，从而诱导下游模型揭示其训练数据的敏感属性。本文的创新点在于，攻击者可以通过操纵上游模型，使其生成具有不同分布的激活（中间特征），从而使攻击者能够轻松区分训练示例中是否具有目标属性。 
- (4):本文的方法在多个任务上进行了实验，包括性别识别、微笑检测和年龄预测等。实验结果表明，攻击者可以通过本文提出的方法，实现高效和特定的属性推断攻击，AUC得分高达0.9以上，即使只有0.1%的样本具有目标属性。本文的方法可以有效地揭示下游训练数据的敏感属性，从而导致严重的隐私泄露。
#### 7. 方法详细介绍：
本文提出了一种攻击方法，包括两个阶段：（1）训练特殊设计的上游模型，以放大属性推断攻击；（2）使用推断攻击推断受害者下游模型训练数据的属性。攻击通过诱导分泌参数来操纵上游模型，这些参数可以揭示下游训练数据是否包含具有目标属性的示例。攻击者最小化上游模型训练的损失函数，该函数包括原始上游训练任务损失和与上游模型操纵相关的损失。损失函数旨在鼓励分泌激活在没有目标属性的样本中被禁用，并在具有目标属性的样本中具有非零值。本文还提出了两种黑盒攻击方法和两种白盒攻击方法，用于从下游训练集中推断敏感属性。

#### 8. 实验设置：
本文考虑了三个迁移学习任务：性别识别、微笑检测和年龄预测。下游模型修改上游模型的后几层，同时保持前几层（特征提取器）不变。下游训练使用与上游训练样本不重叠的训练样本。实验考虑了不同大小的下游集，具有不同数量的具有目标属性的样本。使用曲线下面积（AUC）评估攻击效果，以区分具有和不具有目标属性的发布下游模型。

#### 9. 实验结果和分析：
本文展示了攻击者可以操纵上游模型进行高度有效和特定的属性推断攻击（AUC得分> 0.9），而不会对主要任务产生显著的性能损失。操纵模型的性能下降可以忽略不计（<0.9％）。本文还比较了正常上游模型和操纵上游模型在不同百分比的具有目标属性的下游训练集上的推断AUC得分。本文提出的攻击方法在不同设置下均取得了良好的攻击效果。


# Paper:252     基于神经速率估计器和无监督学习的Split-DNN模型下高效分布式图像分析



#### 1. Title: 
Neural Rate Estimator and Unsupervised Learning for Efficient Distributed Image Analytics in Split-DNN models

#### 2. Authors: 
Nilesh Ahuja, Parual Datta, Bhavya Kanzariya, V. Srinivasa Somayazulu, Omesh Tickoo

#### 3. Affiliation: 
第一作者：Nilesh Ahuja，Intel Labs

#### 4. Keywords: 
Split-DNN computing, neural rate-estimator, unsupervised learning, image analytics, variational autoencoder

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ahuja_Neural_Rate_Estimator_and_Unsupervised_Learning_for_Efficient_Distributed_Image_CVPR_2022_paper.html  Github: https://github.com/intellabs/spic

#### 6. Summary : 
- (1):本文研究背景是云端视觉分析的需求增长，传统编解码器压缩图像数据会引入伪影，严重影响下游分析任务的性能。Split-DNN计算是一种解决方案，将DNN分为客户端和服务器端，通过引入低复杂度的神经网络“瓶颈单元”将中间层特征转换为更适合压缩和传输的低维表示。优化管道的压缩和任务性能需要中间特征的信息论速率的高质量估计。本文提出了一种高质量的“神经速率估计器”来解决这个问题。

- (2):过去的方法包括使用启发式方法估计速率，这导致了次优的性能。本文提出了一种基于变分自编码器的方法，将低维瓶颈输出解释为中间特征的潜在表示，并将速率失真优化问题转化为训练等效的变分自编码器。本文进一步提出了一种基于蒸馏的无监督训练瓶颈单元的方法，这使得我们的方法非常适合实际部署，因为访问标记的训练数据是困难或昂贵的。本文的方法在图像分类和语义分割任务上实现了更好的任务准确性和更低的比特率。

- (3):本文提出了一种基于变分自编码器的神经速率估计器，将中间特征的低维瓶颈输出解释为中间特征的潜在表示，并将速率失真优化问题转化为训练等效的变分自编码器。本文进一步提出了一种基于蒸馏的无监督训练瓶颈单元的方法，这使得我们的方法非常适合实际部署，因为访问标记的训练数据是困难或昂贵的。

- (4):本文的方法在图像分类和语义分割任务上实现了更好的任务准确性和更低的比特率。本文的方法在实际部署中具有很大的潜力，因为它可以在没有标记的训练数据的情况下进行无监督训练，同时可以在动态的可变比特率使用中进行部署。
#### 7. 方法详细介绍：
本文提出了一种神经速率估计器，用于在Split-DNN模型中进行高效的分布式图像分析。该方法涉及使用瓶颈层将高维中间特征转换为较低维度的空间，从而实现更大的压缩。将速率失真优化问题转化为使用适当的损失函数训练等效变分自编码器。本文还提出了一种基于蒸馏的方法，以实现瓶颈层的无监督训练。

#### 8. 实验设置：
本文在Imagenet分类和COCO2017分割任务上进行了实验，使用了所提出的神经压缩方法。实验中使用了多种模型和数据集，包括ResNet-50和MobileNetV2等模型。实验中使用的评估指标包括准确率、mIOU和比特率等。

#### 9. 实验结果和分析：
实验结果表明，所提出的方法在压缩率和准确率方面均优于标准图像压缩算法，如JPEG和HEIC，以及最近的基于机器学习的图像压缩方法，如Entropic Student和变分图像压缩。所提出的神经速率估计器在所有分割点上均优于ℓ1范数速率估计器。在所有分割点上，所提出的方法在相似的准确率水平下实现了显著更低的比特率。在分类任务中，该方法实现了更高的峰值准确率，在分割任务中实现了更高的峰值mIOU。实验结果还表明，所提出的无监督训练方法仍然优于其他最先进的方法，尽管峰值准确率有所下降。最低计算复杂度的瓶颈编码器拓扑结构，即单个深度可分离卷积层，提供了最佳性能。


# Paper:253     通过强健平滑分类器的单图像后门反演



#### 1. Title: 
Single Image Backdoor Inversion via Robust Smoothed Classifiers

#### 2. Authors: 
Mingjie Sun, Zico Kolter

#### 3. Affiliation: 
第一作者：卡内基梅隆大学（Carnegie Mellon University）

#### 4. Keywords: 
Backdoor attacks, Backdoor inversion, SmoothInv, Robust smoothed classifiers, Image synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Single_Image_Backdoor_Inversion_via_Robust_Smoothed_Classifiers_CVPR_2021_paper.html  Github: https://github.com/locuslab/smoothinv

#### 6. Summary : 
- (1):本文研究的是反向工程机器学习模型中的后门触发器，即找到插入到机器学习模型中的后门触发器的过程。这是许多后门检测和防御方法的基础。以往的后门反演方法通常通过优化过程来恢复后门，将一组干净图像翻转为目标类。然而，很少有人研究和理解这个支持集应该有多大才能恢复成功的后门。
- (2):以往的后门反演方法通常需要多个干净实例才能给出合理的结果。本文提出了一种名为SmoothInv的方法，它可以可靠地从单个图像中恢复后门触发器。SmoothInv首先构建一个强健的平滑版本的后门分类器，然后通过向目标类进行引导图像合成来揭示后门模式。SmoothInv不需要通过掩码变量来显式建模后门，也不需要任何复杂的正则化方案，这已经成为后门反演方法的标准实践。与现有方法相比，SmoothInv能够从单个图像中恢复成功的后门，同时保持对原始后门的高保真度。
- (3):本文提出了一种新的后门反演方法SmoothInv，它使用单个图像来合成后门模式。SmoothInv首先构建一个强健的平滑版本的后门分类器，然后通过向目标类进行引导图像合成来揭示后门模式。SmoothInv不需要通过掩码变量来显式建模后门，也不需要任何复杂的正则化方案，这已经成为后门反演方法的标准实践。SmoothInv的创新之处在于，它使用单个图像来合成后门模式，而不是使用多个干净实例。SmoothInv的方法简单易行，同时能够从单个图像中恢复成功的后门，保持对原始后门的高保真度。
- (4):本文在多个先前发表的后门攻击研究中评估了SmoothInv方法。我们证明了与现有方法相比，SmoothInv能够从单个图像中恢复成功的后门，同时保持对原始后门的高保真度。我们还展示了如何从后门分类器中识别目标后门类。最后，我们提出并分析了两种对抗我们方法的对策，并展示了SmoothInv在适应性攻击面前的鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为SmoothInv的方法，用于通过鲁棒平滑分类器进行单图像后门反演。该方法包括两个步骤。首先，使用随机平滑技术构建后门分类器的鲁棒版本，以诱导有意义和显著的梯度信号。然后，通过鲁棒平滑分类器引导图像合成，以恢复鲁棒平滑分类器认为是目标类的后门图像。合成过程涉及最小化与目标类相关的标准交叉熵损失，不依赖于任何额外的正则化项。该方法不需要通过掩码变量或任何复杂的正则化方案来明确建模后门。

#### 8. 实验设置：
本文通过将提出的方法与现有的后门反演方法进行比较，包括NC、TopoTrigger和PixelInv，来评估所提出的方法。评估是在四个先前的后门攻击方法得到的后门分类器上进行的，包括TrojAI、HTBA、Blind-P和Blind-S。对这些后门分类器的统计数据列在表1中，包括相关的模型信息和详细的后门条件。评估在相同的单图像后门反演设置下进行，并为每种方法报告10个随机起始图像的平均ASR。本文还使用了SmoothInv w/扩散中的扩散模型，这是来自[2]的预训练类无条件256×256扩散模型。噪声样本数N选择为40，本文使用投影梯度下降来优化方程6中的目标，总共进行400步，步长选择为0.5×ϵ/10。本文使用两个扰动大小ϵ∈{5,10}，像素范围在[0,1]内。对于每个后门分类器，本文构建了四个噪声水平{0.12,0.25,0.50,1.00}的平滑分类器，共8个优化配置。

#### 9. 实验结果与分析：
本文提出了一种使用底层数据分布中的单个干净图像进行后门反演的方法。所提出的方法利用对抗鲁棒性的最新进展，创建了分类器的平滑版本，然后修改图像以通过这个鲁棒平滑分类器提取后门。结果表明，SmoothInv能够恢复高度成功且极其视觉上类似于真实后门的后门扰动。本文还讨论了训练时干预对后门攻击的影响，并表明所提出的方法在这些具有挑战性的设置中仍然具有鲁棒性。


# Paper:254     从相机图像和毫米波雷达点云中推断深度



#### 1. Title: 
Depth Estimation from Camera Image and mmWave Radar Point Cloud

#### 2. Authors: 
Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard Zhang, Achuta Kadambi, Stefano Soatto, Mani Srivastava, Alex Wong

#### 3. Affiliation: 
第一作者：加州大学洛杉矶分校（University of California, Los Angeles）

#### 4. Keywords: 
Depth estimation, camera, mmWave radar, point cloud, fusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Singh_Depth_Estimation_From_Camera_Image_and_mmWave_Radar_Point_Cloud_CVPR_2021_paper.html  Github: https://github.com/nesl/radar-camera-fusion-depth

#### 6. Summary : 
- (1):本文研究了如何从相机图像和稀疏噪声雷达点云中推断出密集深度。相机提供了密集的像素强度值，但是由于遮挡或光圈问题，很多像素无法建立唯一对应关系，因此深度估计是稀疏的。而雷达则提供了一部分场景中的点云，但是由于其大的波束宽度，只有很少的点能够反射回雷达接收器，因此雷达点云是非常稀疏的。本文的研究背景是如何利用相机和雷达的互补性质，实现相机-雷达深度估计。

- (2):过去的方法主要是基于相机和激光雷达的深度估计，而本文则是基于相机和毫米波雷达的深度估计。相比于激光雷达，毫米波雷达更加轻便、便宜，但是由于其大的波束宽度，只有很少的点能够反射回雷达接收器，因此雷达点云是非常稀疏的。过去的方法主要是将雷达点云投影到图像上，然后通过多帧图像和雷达点云的信息来进行深度估计。本文则是通过学习雷达点云和图像之间的对应关系，实现单帧图像和雷达点云的深度估计。本文的方法考虑了雷达点云中存在的高度模糊和噪声问题，提出了一种基于ROI对齐和门控融合的深度估计方法。

- (3):本文提出了一种基于ROI对齐和门控融合的深度估计方法。首先，通过学习雷达点云和图像之间的对应关系，将每个雷达点对应到可能的图像表面上，得到一个半密集的雷达深度图。然后，通过门控融合机制，将雷达深度图和图像信息相融合，得到一个密集的深度图。本文的方法考虑了雷达点云中存在的高度模糊和噪声问题，提出了一种基于ROI对齐和门控融合的深度估计方法。

- (4):本文在NuScenes数据集上进行了实验，结果表明，本文的方法在平均绝对误差和均方根误差方面分别比最佳方法提高了10.3%和9.1%。本文的方法可以实现单帧图像和雷达点云的深度估计，具有较好的实用性。
#### 7. 方法详细介绍：
本文提出的方法包括两个深度神经网络：RadarNet和FusionNet。RadarNet将RGB图像和雷达点云作为输入，并输出点云在图像中对应的表面的置信度图。置信度图用于构建半密集雷达深度图，通过选择对应最大响应大于阈值的雷达点的z分量来实现。FusionNet进一步将半密集雷达深度图和相机图像融合在一起，输出密集深度图。雷达到图像的对应问题被制定为给定雷达点的每个像素的二进制分类，其中置信度图中的高响应表示给定点的可能表面。模型使用二进制交叉熵损失进行训练。FusionNet使用两个编码器，其中一个编码器编码图像，另一个编码器编码深度图和置信度图的连接。两个分支分别处理，然后通过自适应加权层融合在一起，学习深度编码的贡献。重新加权的深度编码添加到图像编码中，并作为跳过连接传递给解码器，以产生密集深度图。

#### 8. 实验设置：
本文使用NuScenes室外驾驶数据集进行评估，该数据集包含1000个20秒的场景。数据集提供了时间戳非常接近的所有传感器数据的帧，称为关键帧，这些帧带有对象边界框注释。数据集包含约40,000个关键帧。使用nuScenes训练-测试拆分-700个场景进行训练，150个进行验证，150个进行测试。使用累积的激光雷达点进行训练，对于评估，使用数据集提供的激光雷达深度图。

#### 9. 实验结果和分析：
本文提出的方法在NuScenes数据集上的实验结果表明，与其他方法相比，该方法在获取通过雷达-相机融合的密集深度方面取得了更好的结果，平均绝对误差（MAE）提高了10.3％，均方根误差（RMSE）提高了9.1％。该方法仅使用单个图像和雷达帧就优于使用多个图像和雷达帧的最佳方法。


# Paper:255     利用GPU友好的稀疏性和量化提升视觉Transformer



#### 1. Title: 
Boost Vision Transformer with GPU-Friendly Sparsity and Quantization

#### 2. Authors: 
Chong Yu, Tao Chen, Zhongxue Gan, Jiayuan Fan

#### 3. Affiliation: 
第一作者：复旦大学工程与技术学院；

#### 4. Keywords: 
Vision Transformer, Sparsity, Quantization, Compression, GPU

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Boost_Vision_Transformer_With_GPU-Friendly_Sparsity_and_Quantization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是视觉Transformer模型在GPU硬件上的加速部署具有挑战性且鲜有研究。

- (2):过去的方法主要是基于CNN压缩方法，但是这些方法存在一些缺点，如模型大小和FLOPs的减少并不直接与硬件上的效率提升成正比，压缩模式不匹配硬件特性等。本文提出了一种灵活的压缩方案，最大限度地利用了GPU友好的2:4细粒度结构稀疏性和量化。该方法包括两个主要工作流程：2:4稀疏剪枝和稀疏-蒸馏感知量化训练。本文的方法是有动机的，因为它利用了GPU友好的稀疏模式，可以在GPU平台上实现最先进的部署效果。

- (3):本文提出的压缩方案是灵活的，支持监督和无监督学习风格。实验结果表明，GPUSQ-ViT方案在ImageNet分类、COCO检测和ADE20K分割基准任务上，通过将视觉Transformer模型的大小减少6.4-12.7倍和FLOPs减少30.3-62倍，实现了最先进的压缩。此外，GPUSQ-ViT可以在A100 GPU上提高实际部署性能1.39-1.79倍和3.22-3.43倍的延迟和吞吐量，并在AGX Orin上提高1.57-1.69倍和2.11-2.51倍的延迟和吞吐量。

- (4):本文的方法在视觉Transformer模型上取得了最先进的压缩效果，可以在GPU平台上实现最先进的部署效果。
#### 1. 方法详细介绍：
本文提出了一种名为GPUSQ-ViT的压缩方案，该方案利用2:4细粒度结构化稀疏性和量化来压缩视觉变换器模型。该方案包括两个主要工作流程：2:4稀疏剪枝和稀疏蒸馏感知量化训练。该方案灵活支持监督和无监督学习风格。具体而言，该方法包括以下步骤：权重剪枝、量化感知训练、权重因子调整、特征图可视化和知识蒸馏。其中，权重剪枝步骤从模型中删除不重要的权重，量化感知训练步骤训练模型更加鲁棒，权重因子调整步骤调整权重因子以平衡准确性和压缩率。特征图可视化和知识蒸馏用于提高模型的压缩效果和准确性。

#### 2. 实验设置：
本文在多个基准测试任务上评估了所提出的GPUSQ-ViT压缩方案，包括ImageNet分类、COCO检测和ADE20K分割。实验在多个GPU上进行，包括A100和AGX Orin。

#### 3. 实验结果与分析：
实验结果表明，GPUSQ-ViT在模型大小和FLOPs上实现了6.4-12.7×的压缩，同时几乎没有准确性降低。此外，GPUSQ-ViT可以在A100 GPU上提高1.39-1.79×的实际部署性能和3.22-3.43×的吞吐量，以及在AGX Orin上提高1.57-1.69×的延迟和2.11-2.51×的吞吐量。该压缩方案可以普遍适用于视觉变换器模型和基准测试任务，在多个GPU上证明了其在模型大小、FLOPs和实际部署性能方面的卓越性能。

#### 4. 实验结果：
本文在多个视觉模型上评估了GPUSQ-ViT压缩方法的效果，包括DeiT和Swin Transformer模型。实验结果表明，GPUSQ-ViT相比于稀疏剪枝和量化先前的方法，可以提供更多的模型大小和FLOPs压缩。对于INT8压缩模型，GPUSQ-ViT可以带来6.4×的模型大小压缩和31×的FLOPs压缩，同时几乎没有准确性下降。对于INT4压缩模型，GPUSQ-ViT可以获得12.7×和62×的模型大小和FLOPs压缩，准确性下降较小。此外，GPUSQ-ViT可以在支持2:4稀疏性的TensorRT工具包的GPU上大大提高压缩模型的部署效率。

#### 5. 方法：
本文提出的GPUSQ-ViT压缩方法主要包括2:4结构化稀疏剪枝和稀疏蒸馏感知量化训练两个工作流程。2:4细粒度结构化稀疏模式应用于压缩视觉变换器模型的每个部分，包括Q、K和V投影层、多头注意力模块中的线性投影层以及前馈模块中的线性投影层。补丁嵌入层采用卷积层实现，输入通道为C，输出通道为Dembed，内核大小和步幅均为P。补丁嵌入层的总浮点操作（FLOPs）为2×N×C×H×W×Dembed。2:4稀疏GEMM支持INT8和INT4等低精度格式。因此，自然地将稀疏性和量化联合应用于所提出的策略中，并进一步提高GPU上的实际部署性能。在2:4结构化稀疏剪枝工作流程中，三种知识蒸馏策略联合使用，以最佳补偿压缩模型的准确性。

#### 6. 实验设置：
实验使用PyTorch 1.12.0框架在A100 GPU集群上进行。加速性能结果使用A100 GPU和AGX Orin芯片获得，分别代表服务器和边缘设备场景。A100和Orin都支持2:4结构化稀疏性和FP16、INT8和INT4之间的混合精度计算。

#### 7. 方法详细介绍：
本文提出了一种名为GPUSQ-ViT的压缩方案，该方案利用2:4细粒度结构化稀疏性和量化来压缩视觉变换器模型。该方案包括两个主要工作流程：2:4稀疏剪枝和稀疏蒸馏感知量化训练。该方案灵活支持监督和无监督学习风格。具体而言，该方法包括以下步骤：权重剪枝、量化感知训练、权重因子调整、特征图可视化和知识蒸馏。其中，权重剪枝步骤从模型中删除不重要的权重，量化感知训练步骤训练模型更加鲁棒，权重因子调整步骤调整权重因子以平衡准确性和压缩率。特征图可视化和知识蒸馏用于提高模型的压缩效果和准确性。

#### 8. 实验设置：
实验在COCO数据集上进行目标检测和ADE20K数据集上进行语义分割。目标检测的目标模型为Mask R-CNN、DETR和Deformable-DETR，而UPerNet被选为语义分割的目标模型。压缩结果使用GPUSQ-ViT在这些模型上进行评估。

#### 9. 实验结果与分析：
GPUSQ-ViT压缩方法在目标检测和语义分割任务上提供了良好的压缩效果，与密集基线模型相比，准确性差距较小。表3和表4展示了检测和分割任务的压缩效果。当缺少地面真实标签注释时，压缩模型仍然可以从密集模型的预测中学习目标的表示，GPUSQ-ViT在无监督训练中仍然可以很好地工作，如表5所示。


# Paper:256     LVQAC：基于格矢量量化和空间自适应压缩映射的高效学习图像压缩



#### 1. Title: 
LVQAC: Lattice Vector Quantization Coupled with Spatially Adaptive Companding for Efficient Learned Image Compression

#### 2. Authors: 
Xi Zhang, Xiaolin Wu

#### 3. Affiliation: 
Xi Zhang: 上海交通大学 (Shanghai Jiao Tong University)

#### 4. Keywords: 
Image compression, deep learning, lattice vector quantization, spatially adaptive companding

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhang_LVQAC_Lattice_Vector_Quantization_Coupled_With_Spatially_Adaptive_Companding_for_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是基于深度学习的图像压缩方法，旨在提高压缩效率和重构质量。

- (2):过去的方法大多采用均匀量化器，而非信息论上最优的矢量量化器。本文提出了一种新的基于格矢量量化的压缩方案，可以更好地利用特征之间的相关性，同时与标量均匀量化器的计算复杂度相当。此外，为了提高LVQ对源统计数据的适应性，本文将空间自适应压缩映射与LVQ相结合。本文的方法可以轻松嵌入任何端到端优化的图像压缩系统中，且在不显著增加模型复杂度的情况下，取得了更好的速率失真性能。

- (3):本文提出了一种新的基于格矢量量化的压缩方案，可以更好地利用特征之间的相关性，同时与标量均匀量化器的计算复杂度相当。此外，为了提高LVQ对源统计数据的适应性，本文将空间自适应压缩映射与LVQ相结合。本文的方法可以轻松嵌入任何端到端优化的图像压缩系统中。

- (4):本文的方法在多个数据集上进行了实验，结果表明，与均匀量化器相比，LVQAC可以在不显著增加模型复杂度的情况下，取得更好的速率失真性能。
#### 7. 方法详细介绍：
本文提出了一种新的图像压缩方法，称为LVQAC。该方法将格状向量量化（LVQ）与空间自适应压缩（AC）映射相结合。LVQAC可以取代特征值的均匀量化，用格状向量量化模块和空间自适应压缩映射模块来量化CNN特征向量。空间自适应压缩映射模块通过学习不同空间位置的压缩函数来进一步提高编码效率。本文还介绍了如何将LVQAC嵌入到任何端到端优化的图像压缩系统中。 

#### 8. 实验设置：
本文在两个代表性的端到端CNN压缩架构上进行了实验，并评估了使用均匀标量量化和LVQAC的模型的性能。使用Kodak数据集和CLIC Professional验证集进行测试，并提供了模型之间的速率失真性能比较。作者还提供了视觉比较和编码和解码延迟分析。训练数据集包括来自ImageNet的8000张图像，使用Adam优化器进行训练，初始学习率为10^-4，训练2000个epoch。所有实验都使用四个RTX 2080ti GPU进行。 

#### 9. 实验结果和分析：
实验结果表明，对于任何端到端CNN图像压缩模型，用LVQAC替换均匀量化可以实现更好的速率失真性能，而不会显著增加模型复杂度。实验还表明，上下文敏感熵模型越简单，性能提升越大。LVQAC设计与最初提出的[3]框架一样简单，易于嵌入到CNN压缩模型的端到端训练中。 

#### 全文总结：
本文提出了一种新的图像压缩方法LVQAC，该方法将格状向量量化（LVQ）与空间自适应压缩（AC）映射相结合。实验结果表明，LVQAC可以取代特征值的均匀量化，用格状向量量化模块和空间自适应压缩映射模块来量化CNN特征向量，从而实现更好的速率失真性能。此外，LVQAC设计与最初提出的[3]框架一样简单，易于嵌入到CNN压缩模型的端到端训练中。


# Paper:257     小波扩散模型是快速可扩展的图像生成器



#### 1. Title: 
Wavelet Diffusion Models are fast and scalable Image Generators

#### 2. Authors: 
Hao Phung, Quan Dao, Anh Tran

#### 3. Affiliation: 
VinAI Research (越南VinAI研究所)

#### 4. Keywords: 
Diffusion models, wavelet decomposition, image generation, generative adversarial networks, real-time applications

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Phung_Wavelet_Diffusion_Models_Are_Fast_and_Scalable_Image_Generators_CVPR_2021_paper.html  Github: https://github.com/VinAIResearch/WaveDiff.git

#### 6. Summary : 
- (1):本文研究背景是高保真图像生成，Diffusion模型在许多情况下优于GANs，但其训练和推理速度缓慢，阻碍了其在实时应用中的使用。

- (2):过去的方法主要是通过减少采样步骤来减少推理时间，但速度仍然远远落后于GANs。本文提出了一种新的基于小波分解的扩散方案，通过从图像和特征级别提取低频和高频分量，自适应地处理这些分量以实现更快的处理速度，同时保持良好的生成质量。此外，本文提出了一种重构项，有效地提高了模型的训练收敛性。

- (3):本文提出的Wavelet Diffusion框架利用小波子带的降维优势加速Diffusion模型，通过高频分量保持生成结果的良好视觉质量。我们在图像和特征空间中都使用小波分解来提高生成模型的鲁棒性和执行速度。我们的Wavelet Diffusion提供了最先进的训练和推理速度，是实现实时和高保真扩散模型的一个基石。

- (4):本文在CelebA-HQ、CIFAR-10、LSUN-Church和STL-10数据集上进行了实验，证明了我们的解决方案是实现实时和高保真扩散模型的一个基石。我们的模型显著缩小了扩散模型和GANs之间的速度差距，目标是大规模和实时系统。
#### 7. 方法详细介绍：
本文提出了一种基于小波扩散的生成模型——Wavelet Diffusion。该方法通过小波变换在图像和特征空间中进行频率分解，利用高频信息加速扩散模型，同时保持生成结果的视觉质量。Wavelet Diffusion框架包括频率感知的下采样和上采样块、频率瓶颈块和频率残差连接，以增强高频组件的感知和保持小波子带的一致性。生成器通过对抗损失和重构损失进行优化。实验结果表明，Wavelet Diffusion在多个数据集上均取得了优异的性能，具有高效和可扩展的特点。

#### 8. 实验设置：
本文在CIFAR-10、STL-10、CelebA-HQ和LSUN-Church数据集上进行了实验。作者提供了实验设置的详细信息，包括训练轮数、批次大小、学习率和优化器等。同时，作者还介绍了实验所使用的硬件和软件配置。

#### 9. 实验结果与分析：
Wavelet Diffusion模型在图像保真度和采样速度方面均优于基线DDGAN。完整的模型包括残差连接、上采样和下采样块以及瓶颈块，其在CelebA-HQ 256x256数据集上的FID得分为5.94。瓶颈块被发现是生成器设计中最不重要的组件。Wavelet Diffusion模型可以在0.1秒内生成高达1024x1024的图像，这是扩散模型首次实现如此接近实时的性能。小波变换的频率分解被认为是提高模型收敛速度和稳定性的原因。该方法通过在特征空间的多个尺度上对粗略和详细信息进行分离，实现了高效训练。在CIFAR-10数据集上，Wavelet Diffusion方法比DDGAN快2.5倍，接近StyleGAN方法的实时速度，同时仍然实现了可比较的FID得分。


# Paper:258     SmartBrush: 基于扩散模型的文本和形状引导的物体修复



#### 1. Title: 
SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model

#### 2. Authors: 
Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, Kun Zhang

#### 3. Affiliation: 
第一作者：卡内基梅隆大学

#### 4. Keywords: 
Image inpainting, diffusion model, text guidance, shape guidance, background preservation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是图像修复领域，传统的图像修复方法只能通过周围像素信息来填补缺失区域，缺乏对修复内容的控制。而多模态图像修复则通过额外的信息（如类别标签、文本描述、分割图等）提供更多的控制。本文提出了一种新的基于扩散模型的SmartBrush方法，用于通过文本和形状引导来完成缺失区域的物体修复。

- (2):现有的文本引导图像修复方法，如DALLE-2和Stable Diffusion，虽然可以进行文本引导修复，但不支持形状引导，并且往往会修改生成物体周围的背景纹理。本文提出的模型结合了文本和形状引导，并具有精度控制。为了更好地保留背景，本文提出了一种新的训练和采样策略，通过增加扩散U-net的对象掩模预测来实现。最后，本文引入了一种多任务训练策略，通过联合训练修复和文本到图像生成来利用更多的训练数据。

- (3):本文提出的SmartBrush方法是一种基于扩散模型的文本和形状引导的物体修复方法，可以根据不同精度的对象掩模进行控制。为了保留背景，本文的模型在修复过程中不仅鼓励模型修复掩蔽区域，还使用正则化损失来鼓励模型预测正在生成的对象的实例掩模。在测试时，本文用预测掩模替换粗略掩模以尽可能地保留背景，从而获得更一致的结果。本文在多个具有挑战性的物体修复任务上进行了评估，并表明其在视觉质量、掩模可控性和背景保留方面优于所有基线方法。

- (4):本文提出的SmartBrush方法在多模态图像修复任务中取得了良好的性能，可以通过文本和形状引导来完成缺失区域的物体修复。本文的方法在视觉质量、掩模可控性和背景保留方面优于所有基线方法，表明其在多模态图像修复任务中具有很好的应用前景。
#### 7. 方法详细介绍：
本文提出了一种基于扩散模型的智能修复方法SmartBrush，该方法利用现有的实例或全景分割数据集中的文本和形状信息来指导对象修复。该模型通过训练来预测噪声和准确的掩模，并且可以在不改变损失函数的情况下以文本和形状信息为条件。该模型还允许通过生成不同精度级别的掩模来进行形状精度控制，并通过从粗糙输入版本预测准确的实例掩模来保留背景信息。该模型采用多任务训练策略，联合训练修复任务和基础文本到图像生成任务。

#### 8. 实验设置：
本文使用批量大小为1024和总损失函数λ=0.01进行训练。训练策略包括使用80％的概率训练修复任务和20％的概率训练文本到图像生成任务。该模型在8个A100 GPU上进行了约20K步的训练。该模型在两个流行的分割数据集OpenImages和MSCOCO上进行了评估，并且输入提示直接来自分割类标签。

#### 9. 实验结果和分析：
本文提出的SmartBrush方法在准确的对象掩模和边界框掩模上均取得了最佳性能，证明了采用文本和形状指导的训练策略的有效性。用户研究结果表明，超过50％的用户在每个问题上都投票支持SmartBrush结果。提出的背景保留方法通过保留更多像素来提高定量结果。没有BLIP和多任务训练的性能下降也证明了它们对于对象修复任务的有用性。在多个数据集和示例上，该模型在对象修复方面取得了最先进的结果，并且在掩模形状和文本描述的一致性方面表现出色。


# Paper:259     视觉识别中的子模型协同训练



#### 1. Title: 
Co-training 2L Submodels for Visual Recognition


#### 2. Authors: 
Hugo Touvron, Matthieu Cord, Maxime Oquab, Piotr Bojanowski, Jakob Verbeek, Hervé Jégou


#### 3. Affiliation: 
Meta AI / FAIR Paris (Hugo Touvron), Sorbonne University (Matthieu Cord)


#### 4. Keywords: 
Co-training, submodel co-training, visual recognition, deep neural networks, regularization


#### 5. Paper: https://arxiv.org/abs/2106.07525  Github: None


#### 6. Summary : 
- (1):本文研究的背景是深度神经网络的优化问题，尤其是在视觉识别任务中的应用。
 
- (2):过去的方法主要包括残差连接和预训练模型等，但这些方法存在一些问题，如训练时间长、需要大量的数据和计算资源等。本文提出了一种新的正则化方法——子模型协同训练，可以有效地解决这些问题。
 
- (3):本文提出的子模型协同训练方法是一种基于深度神经网络的正则化方法，通过随机深度的方式，对每个样本隐式地实例化两个“子模型”，并相互作为软教师提供互补的损失函数，从而提高模型的泛化能力。该方法不需要预训练模型或时间平均，且适用于多种架构。
 
- (4):本文在图像分类和语义分割等任务上进行了实验，结果表明子模型协同训练方法可以有效地提高模型的性能，例如，在ImageNet-21k数据集上，使用ViT-B模型进行预训练，再使用子模型协同训练方法，在ImageNet-val数据集上可以获得87.4%的top-1准确率。因此，该方法具有很好的应用前景。
#### 7. 方法详细介绍：
本文提出了一种名为“cosub”的正则化方法，它是一种基于子模型共同训练的方法。对于每个样本，通过随机深度实例化两个不同的网络子模型，每个子模型都作为另一个子模型的软教师，提供一个补充标签的损失。这种方法不需要预训练的外部模型或时间平均，使用单一的权重集合，可以与多种架构兼容，包括RegNet、ViT、PiT、XCiT、Swin和ConvNext。本文详细介绍了cosub方法的实现细节，包括模型增强操作、子模型实例化、损失函数的设计等。

#### 8. 实验设置：
本文在多个数据集上验证了cosub方法的有效性，包括ImageNet-1k、ImageNet-v2和ADE20k等。作者使用了多种架构，包括RegNet、ViT、PiT、XCiT、Swin和ConvNext等，对比了不同的训练策略和超参数设置。本文提供了详细的实验设置，包括数据集、模型架构、超参数、优化器等。

#### 9. 实验结果与分析：
本文的实验结果表明，cosub方法可以显著提高现有深度残差网络的性能。在ImageNet-1k上，使用cosub方法训练的模型与现有最先进的模型相比具有竞争力。在其他数据集上，使用cosub方法训练的模型也取得了很好的效果。作者还对cosub方法进行了多种分析和实验，证明了其有效性和鲁棒性。


# Paper:260     基于优化启发的交叉注意力变换器的压缩感知



#### 1. Title: 
Optimization-Inspired Cross-Attention Transformer for Compressive Sensing

#### 2. Authors: 
Jiechong Song, Chong Mou, Shiqi Wang, Siwei Ma, Jian Zhang

#### 3. Affiliation: 
第一作者：北京大学深圳研究生院

#### 4. Keywords: 
Compressive sensing, deep unfolding network, cross-attention, optimization, image reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Song_Optimization-Inspired_Cross-Attention_Transformer_for_Compressive_Sensing_CVPR_2022_paper.html  Github: https://github.com/songjiechong/OCTUF

#### 6. Summary : 
- (1):本文研究的是压缩感知（CS）领域的图像重建问题，CS是一种联合采样和重建方法，可以在保持稀疏或可压缩信号的合理重建的同时减少测量和存储空间。本文的研究背景是CS领域中的深度展开网络（DUN），DUN将某些优化求解器与深度神经网络集成，通过端到端学习的方式训练一个截断的展开推理，具有良好的可解释性和高性能。

- (2):过去的DUN方法通常通过一些有效的卷积神经网络（CNN）去噪器集成到一些优化方法中，例如，近端梯度下降（PGD）算法，AMP和非凸优化的惯性近端算法（iPiano）。然而，现有的DUN通常需要大量的迭代和参数，且在迭代过程中信息传输受限，导致表示能力有限。本文提出了一种高效的优化启发式交叉注意力变换器（OCT）模块作为迭代过程，并建立了一个轻量级的基于OCT的展开框架（OCTUF）来解决这些问题。

- (3):本文提出了一种优化启发式交叉注意力变换器（OCT）模块作为迭代过程，用于图像CS。OCT模块由一个双交叉注意力（Dual-CA）子模块和一个前馈网络（FFN）子模块组成。其中，Dual-CA子模块包含一个惯性提供的交叉注意力（ISCA）块和一个投影引导的交叉注意力（PGCA）块。ISCA块在相邻迭代信息之间计算交叉注意力，并通过交叉注意力机制引入多通道惯性力和增加记忆效应。PGCA块通过交叉注意力块将惯性力引入梯度下降步骤，以指导通道特征的精细融合。本文的创新点在于提出了一种高效的解决方案，使OCTUF在比现有方法更少的参数下实现了更好的性能。

- (4):本文在Set11数据集上进行了广泛的CS实验，结果表明，与现有的最先进方法相比，OCTUF在训练更低的复杂度下实现了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于优化启发的交叉注意力变换器（OCT）模块的迭代过程，形成了一种轻量级的基于OCT的展开框架（OCTUF），用于图像压缩感知（CS）。OCT模块由双重交叉注意力（Dual-CA）子模块和前馈网络（FFN）子模块组成，形成每个迭代过程。Dual-CA子模块包含惯性提供的交叉注意力（ISCA）块和投影引导的交叉注意力（PGCA）块。ISCA块通过相邻迭代之间的交叉注意力机制引入多通道惯性力，并增加了记忆效应。PGCA块通过交叉注意力块实现了增强的信息交互，通过交叉注意力块将惯性力引入梯度下降步骤。OCTUF在比其他状态下的CS方法更少的参数下表现出色。

#### 8. 实验设置：
本文在BSD500数据集的训练和测试数据集中使用400张图像进行训练。训练图像被裁剪为96×96像素大小的89600个补丁，并进行数据增强。对于给定的CS比率M/N，相应的可学习测量矩阵Φ通过卷积层构造，卷积核大小为M×1×√N×√N，用于对原始图像x进行采样。块大小为32，批量大小默认为16，默认特征图数C为32，可学习参数ρ（k）初始化为0.5。使用Adam优化器训练网络，初始学习率为，通过余弦退火策略在100个时期内降至5×10−5，热身时期为3。

#### 9. 实验结果与分析：
本文提出的OCTUF和OCTUF+在Set11数据集上的PSNR和SSIM方面优于所有其他竞争方法。在CS比率为25％的情况下，OCTUF+在PSNR方面比ISTA-Net+，DPA-Net，AMP-Net，MAC-Net，COAST，MADUN，CAS-Net，TransCS和FSOINet分别提高了3.60 dB，3.91 dB，1.25 dB，3.36 dB，2.21 dB，0.51 dB，0.41 dB，1.16 dB和0.29 dB。此外，OCTUF+的平均SSIM可以分别提高0.0389，0.0280，0.0113，0.0381，0.0202，0.0013，0.0015，0.0073和0.0010。在Urban100数据集上，OCTUF和OCTUF+在所有采样比率下均实现了更好的重建质量。本文还在Set11数据集上进行了消融研究，结果表明我们提出的OCTUF实现了最佳性能。复杂度分析表明，我们提出的方法具有与其他最先进方法相当的参数数量和FLOPs。


# Paper:261     DR2：基于扩散的鲁棒性盲目人脸修复



#### 1. Title: 
DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration

#### 2. Authors: 
Zhixin Wang, Ziying Zhang, Xiaoyun Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, Yanfeng Wang

#### 3. Affiliation: 
第一作者：上海交通大学计算机科学与工程系

#### 4. Keywords: 
Blind face restoration, denoising diffusion probabilistic models, robustness, degradation removal, enhancement module

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_DR2_Diffusion-Based_Robust_Degradation_Remover_for_Blind_Face_Restoration_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究盲目人脸修复，旨在从低质量的人脸图像中恢复高质量的人脸图像。由于真实世界中的人脸图像可能存在各种各样的复杂降级，因此现有的方法往往无法处理这些情况，导致输出结果存在明显的伪影。因此，本文旨在提出一种鲁棒性强的降级去除方法，以应对各种复杂的降级情况。

- (2):现有的方法通常使用预定义的降级模型进行训练，但这种方法无法处理真实世界中的各种复杂降级情况，因此输出结果存在明显的伪影。为了解决这个问题，本文提出了一种基于扩散模型的鲁棒性强的降级去除方法，该方法可以将输入图像转换为粗略但降级不变的预测结果，然后使用增强模块将粗略预测结果恢复为高质量图像。本文使用了一种表现良好的去噪扩散概率模型来实现降级去除，该模型可以通过迭代去噪步骤捕获语义信息，从而使DR2对常见的降级（如模糊、噪声和压缩）具有鲁棒性，并且与不同的增强模块设计兼容。

- (3):本文提出了一种基于扩散模型的鲁棒性强的降级去除方法DR2，该方法可以将输入图像转换为粗略但降级不变的预测结果，然后使用增强模块将粗略预测结果恢复为高质量图像。DR2使用预训练的扩散概率模型进行降级去除，该模型可以通过迭代去噪步骤捕获语义信息，从而使DR2对常见的降级具有鲁棒性。本文的创新点在于使用扩散模型进行降级去除，从而使DR2具有鲁棒性，并且与不同的增强模块设计兼容。

- (4):本文在各种设置下进行了实验，结果表明，本文提出的DR2E框架在重度降级的合成和真实世界数据集上均优于现有方法。本文的方法可以处理各种复杂的降级情况，具有鲁棒性，并且可以产生高质量的人脸修复结果，支持其目标。
#### 7. 方法详细介绍：
本文提出了一个盲人脸修复的两阶段框架，包括去除降噪模块和增强模块。去除降噪模块基于Diffusion-based Robust Degradation Remover (DR2)，利用预训练的Denoising Diffusion Probabilistic Model (DDPM)从输入图像中去除降噪。增强模块是一个神经网络，将去除降噪模块的输出映射到高质量图像。该框架旨在最大化给定低质量输入图像的高质量图像的可能性。

具体步骤如下：
1. 去除降噪模块：将输入图像通过DR2模块进行去噪处理，得到一个降噪后的中间结果。
2. 增强模块：将降噪后的中间结果通过增强模块进行高质量图像的生成。
3. 训练：DR2模块使用合成数据集进行训练，增强模块使用高质量数据集进行训练。

#### 8. 实验设置：
本文在FFHQ数据集上进行实验，该数据集包含70,000张高质量人脸图像。DR2和增强模块分别在该数据集上进行独立训练。增强模块使用SPARNetHD和VQFR两种不同的backbones。

#### 9. 实验结果和分析：
本文在CelebA-Test数据集上进行定量和定性实验，评估了所提出方法的性能。结果表明，所提出的方法在恢复质量和减少伪影方面优于现有的方法。作者还进行了消融实验，分析了超参数（如下采样因子N和输出步长τ）对输出的保真度和“清洁度”的影响。本文还建议使用DR2输出构建增强模块的训练数据，以帮助模块更好地适应和快速训练。


# Paper:262     LayoutDiffusion：可控扩散模型用于布局到图像生成



#### 1. Title: 
LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation

#### 2. Authors: 
Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, Xi Li

#### 3. Affiliation: 
第一作者：浙江大学计算机科学与技术学院

#### 4. Keywords: 
Layout-to-image generation, diffusion model, layout fusion, object-aware cross attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2021_paper.html  Github: https://github.com/ZGCTroy/LayoutDiffusion

#### 6. Summary : 
- (1):本文研究的是布局到图像生成的问题，旨在通过布局作为输入来控制生成图像中的多个对象的位置、大小和类别等信息，以提高生成图像的质量和可控性。

- (2):过去的方法主要是基于生成对抗网络（GAN）的布局到图像生成方法，但容易出现不稳定收敛和模式崩溃等问题。本文提出了一种基于扩散模型的布局到图像生成方法，通过构建结构化图像块和对象感知的交叉注意力机制，将图像和布局融合为统一的形式，从而提高了生成图像的质量和可控性。

- (3):本文提出了一种名为LayoutDiffusion的布局条件扩散模型，通过Layout Fusion Module（LFM）和Object-aware Cross Attention（OaCA）等模块，实现了对多个对象的位置、大小和类别等信息的精确控制。具体地，本文将图像分成多个结构化图像块，并将其转换为特殊类型的对象，然后将其与输入的布局在同一空间中进行融合。同时，本文还提出了一种基于插值的无分类器训练和采样策略，加速了模型的训练和采样过程。

- (4):本文在COCO-stuff和Visual Genome（VG）数据集上进行了实验，结果表明，LayoutDiffusion相对于现有的布局到图像生成方法，在FID、DS和CAS等多个指标上均取得了显著的性能提升。这表明本文提出的方法可以有效地提高生成图像的质量和可控性。
#### 1. 实验结果
(1). 本文提出的LayoutDiffusion模型在FID性能方面优于LDM模型，以更高的代价获得31.6的得分。在模型中应用LFM和OaCA可以显著提高FID、IS、CAS和YOLOScore的性能，分别在FID/IS/CAS/YOLOScore上获得13.37/6.58/39.77/27.0的性能提升。由于生成的图像具有更精确的控制，因此多样性得分（DS）与基线相比略有下降。

#### 2. 方法详细介绍
本文提出了一种名为LayoutDiffusion的扩散模型，用于布局到图像的生成。该方法由三个主要模块组成：布局融合模块（LFM）、图像-布局融合模块和布局条件扩散模型。LFM使用多层自注意力来融合布局嵌入，并促进布局中多个对象之间的交互。图像-布局融合模块构建结构化图像块，并定义图像和布局的位置嵌入。它还使用点加法进行全局条件和对象感知的交叉注意力进行局部条件。布局条件扩散模型使用无分类器的指导来支持布局条件，并通过从具有和不具有条件输入的扩散模型中进行采样来生成布局条件图像。

#### 3. 实验设置
本文的实验在COCO-stuff和Visual Genome（VG）数据集上进行。使用了各种指标来评估LayoutDiffusion的性能，包括质量、多样性和可控性。

#### 4. 实验结果与分析
本文提出的LayoutDiffusion方法在FID、IS、DS、CAS和YOLOScore方面优于SOTA模型。具体而言，LayoutDiffusion在FID和IS方面表现最佳，分别最多超过SOTA模型46.35%和29.29%。在可控性方面，LayoutDiffusion在YOLOScore和CAS方面最多分别超过SOTA模型122.22%和41.82%。定性结果表明，LayoutDiffusion生成更准确、高质量的图像，与其布局相对应的对象可识别且准确。LayoutDiffusion的多样性也得到了证明，相同布局的图像具有高质量和多样性。


# Paper:263     进化部分遮盖的自监督学习



#### 1. Title: 
Evolved Part Masking for Self-Supervised Learning

#### 2. Authors: 
Zhanzhou Feng, Shiliang Zhang

#### 3. Affiliation: 
National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University (北京大学)

#### 4. Keywords: 
Self-supervised learning, Masked Image Modeling, Part-based masking, Graph cut, Visual cues modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Feng_Evolved_Part_Masking_for_Self-Supervised_Learning_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究自监督学习中的Masked Image Modeling方法，该方法通过遮盖图像的一部分区域来训练视觉模型。现有的方法采用固定的遮盖模式来指导自监督训练，但是这些模式只能遮盖局部区域，无法提供更广泛的视觉线索。因此，本文提出了一种基于部分的进化遮盖方法，通过自适应部分划分模块来构建部分图，并使用图割来划分部分，从而生成不同训练阶段的进化遮盖模式。该方法不需要额外的预训练模型或注释，并通过进化训练难度有效地保证了训练效率。 

- (2):现有的Masked Image Modeling方法采用固定的遮盖模式来指导自监督训练，但是这些模式只能遮盖局部区域，无法提供更广泛的视觉线索。本文提出了一种基于部分的进化遮盖方法，通过自适应部分划分模块来构建部分图，并使用图割来划分部分，从而生成不同训练阶段的进化遮盖模式。该方法不需要额外的预训练模型或注释，并通过进化训练难度有效地保证了训练效率。 

- (3):本文提出的方法是基于部分的进化遮盖方法，通过自适应部分划分模块来构建部分图，并使用图割来划分部分，从而生成不同训练阶段的进化遮盖模式。该方法不需要额外的预训练模型或注释，并通过进化训练难度有效地保证了训练效率。本文的创新点在于提出了一种新的遮盖方法，可以更好地模拟视觉线索，从而提高自监督学习的性能。 

- (4):本文的方法在多个任务上进行了测试，包括图像分类、目标检测和语义分割等。实验结果表明，本文的方法在各个任务上都取得了显著的性能提升，特别是在语义分割任务上，mIoU提高了2%。与最近的自监督学习方法相比，本文的方法在更少的预训练时期内实现了可比的性能，并在相似的训练时期内实现了更好的性能。因此，本文的方法具有很好的应用前景。
#### 1. 方法详细介绍：
本文提出了一种进化的部分遮罩方法，用于自监督学习。该方法基于自适应部分划分模块，利用正在训练的视觉模型构建部分图，并使用图割对部分进行划分。划分后的部分准确性与预训练模型的能力相当，导致不同训练阶段的进化掩模模式。自适应部分划分模块根据视觉模型推断的图像块之间的关系生成部分。视觉变换器学习的块之间的相关性在注意力图中编码。生成的掩模嵌入图像块之间的额外上下文提示，以监督视觉模型的训练。更新的模型反过来提高了部分划分的准确性。迭代地进行掩模生成和模型训练会导致在未标记的数据集上训练视觉模型的循环。因此，掩模模式可以进化以呈现不同的视觉提示学习任务。

#### 2. 实验设置：
本文在ImageNet-1K和ADE20K数据集上测试了三种流行的MIM架构，即MAE、BEiT和SimMIM。预训练在ImageNet-1K数据集上进行，微调在ADE20K数据集上进行。实验在一台8个NVIDIA V100 GPU的单机上进行。

#### 3. 实验结果与分析：
本文提出的方法为三种测试架构带来了显着的性能提升，特别是在语义分割任务上，例如将mIoU提高了2%。与最近的自监督学习方法相比，本文提出的方法在更少的预训练时期内实现了可比的性能，并在类似的训练时期内实现了更好的性能。例如，它在ImageNet-1K分类和ADE20K分割上的表现优于最近的MAE方法，训练时期相同，分别提高了0.69%和1.61%。

#### 4. 方法：
本文提出了一种进化的部分遮罩方法，用于自监督学习。该方法旨在通过自监督学习在未标记的数据集上训练视觉变换器。该方法涉及为输入图像生成二进制掩模，并将其应用于自监督学习。训练目标是最小化原始图像和遮罩输入之间的重构误差。掩模在不同的训练阶段进化，以增强模型的泛化能力。进化过程通过相应的掩模概率的加权求和实现。该方法涉及使用正在训练的模型的注意力图进行自适应部分生成以进行部分划分。然后将生成的部分注释馈入掩模生成过程以生成部分掩模。还使用网格掩模，为网格中相同相对位置的补丁分配相同的分数。该方法在预训练期间在空间和时间上的消耗效率高。

#### 5. 实验设置：
本文在ImageNet-1K训练集的1.28M张图像上进行自监督预训练。本文评估了所提出的方法在分类、分割和检测任务上的性能。

#### 6. 实验结果与分析：
本文将所提出的方法与静态遮罩方法进行比较，并显示所提出的方法在性能和效率方面均优于这些静态方法。本文还验证了所提出的方法对三种流行的MIM模型，即MAE、BEiT和SimMIM的影响，并显示所提出的方法为所有三种方法带来了性能改进，特别是在分割任务上。本文进一步将所提出的方法与最近的SSL方法进行比较，并显示所提出的方法在常见的ImageNet-1K分类设置上获得了可比的性能，并且在更少的预训练时期内实现了可比的性能。


# Paper:264     一种缓解时间动作定位任务中任务差异问题的软着陆策略



#### 1. Title: 
Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks

#### 2. Authors: 
Hyolim Kang, Hanjung Kim, Joungbin An, Minsu Cho, Seon Joo Kim

#### 3. Affiliation: 
Yonsei University (庆熙大学)

#### 4. Keywords: 
Temporal Action Localization, Soft-Landing Strategy, Similarity Matching, Pretrained Encoder, Task Discrepancy Problem

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kang_Soft-Landing_Strategy_for_Alleviating_the_Task_Discrepancy_Problem_in_Temporal_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是长视频的处理，其中的Temporal Action Localization (TAL)任务是从视频中找到动作实例并对其进行分类，是视频理解任务的基础。
- (2):现有的TAL方法通常在预训练的Trimmed Action Classification (TAC)任务的基础上操作，这导致了任务差异问题。现有的TAL方法通过重新训练编码器或端到端微调来缓解这个问题，但通常需要大量的内存和计算。本文提出了一种高效而有效的框架，即Soft-Landing (SoLa)策略，通过在冻结编码器之上加入一个轻量级神经网络，即SoLa模块，来缩小预训练编码器和下游任务之间的可转移性差距。我们还提出了一种无监督的训练方案，用于SoLa模块的学习，它使用帧间相似性匹配作为其监督信号，消除了对时间注释的需求。与现有方法相比，我们的方法在各种下游TAL任务的各种基准测试中有效地缓解了任务差异问题，并具有显着的计算效率。
- (3):本文提出了一种新的方法，即Soft-Landing (SoLa)策略，它在预训练编码器和下游任务之间加入了一个轻量级神经网络，即SoLa模块，以缓解任务差异问题。我们还提出了一种无监督的训练方案，用于SoLa模块的学习，它使用帧间相似性匹配作为其监督信号，消除了对时间注释的需求。与现有方法相比，我们的方法不需要重新训练编码器，具有更广泛的适用性，并且具有显着的计算效率。
- (4):本文在各种下游TAL任务的各种基准测试中，通过使用SoLa策略和相似性匹配，有效地缓解了任务差异问题，并取得了显着的性能提升。这表明我们的方法可以支持其目标。
#### 7. 方法详细介绍：
本文提出了一种Soft-Landing (SoLa)策略，用于缓解时间动作定位任务中的任务差异问题。该策略将SoLa模块放置在预训练编码器和下游头之间，工作在片段特征级别。SoLa模块可以是任何神经网络，其输入为形状为(L, m)的张量，输出为相同形状的张量。SoLa模块的训练采用自监督方式进行，使用相似性匹配损失，该损失以帧间隔为唯一的学习信号。λ函数旨在增强经验相似性得分分布的时间相似性结构。相似性匹配损失计算为标准的二元交叉熵（BCE）损失，具有软目标。具体步骤包括：
1. 将SoLa模块放置在预训练编码器和下游头之间。
2. SoLa模块可以是任何神经网络，其输入为形状为(L, m)的张量，输出为相同形状的张量。
3. SoLa模块的训练采用自监督方式进行，使用相似性匹配损失，该损失以帧间隔为唯一的学习信号。
4. λ函数旨在增强经验相似性得分分布的时间相似性结构。
5. 相似性匹配损失计算为标准的二元交叉熵（BCE）损失，具有软目标。

#### 8. 实验设置：
本文在THUMOS14、ActivityNet1.3和Charades-STA等数据集上进行了实验。使用Kinetics400预训练的两流TSN网络作为ActivityNet1.3和THUMOS14实验的冻结片段编码器，而直接使用I3D特征进行HACS1.1实验。评估指标包括平均精度（mAP）、平均召回率（AR）、召回曲线下面积（AUC）和预测与真实值之间的平均tIoU。

#### 9. 实验结果和分析：
SoLa模块相对于大多数最近的片段编码器训练方法表现出了卓越的性能，并且在各种数据集和不同的片段编码器上具有良好的泛化能力。该框架具有广泛的适用性，如在VG任务上的性能提升。SoLa模块具有计算效率高、计算成本低等优点。线性评估结果表明，使用相似性匹配的SoLa模块提高了时间敏感性，使线性分类器更容易区分前景/背景片段。消融研究结果表明，局部特征聚合在SoLa模块中起着关键作用，使用相似性匹配进行训练对于成功训练SoLa模块至关重要。

#### 整篇论文总结：
本文提出了一种Soft-Landing (SoLa)策略，用于缓解时间动作定位任务中的任务差异问题。该策略将SoLa模块放置在预训练编码器和下游头之间，工作在片段特征级别。SoLa模块可以是任何神经网络，其输入为形状为(L, m)的张量，输出为相同形状的张量。SoLa模块的训练采用自监督方式进行，使用相似性匹配损失，该损失以帧间隔为唯一的学习信号。实验结果表明，SoLa模块相对于大多数最近的片段编码器训练方法表现出了卓越的性能，并且在各种数据集和不同的片段编码器上具有良好的泛化能力。该框架具有广泛的适用性，如在VG任务上的性能提升。SoLa模块具有计算效率高、计算成本低等优点。


# Paper:265     基于两阶段补偿和对齐框架的JPEG图像恢复



#### 1. Title: 
Bitstream-Corrupted JPEG Images are Restorable: Two-stage Compensation and Alignment Framework for Image Restoration

#### 2. Authors: 
Wenyang Liu, Yi Wang, Kim-Hui Yap, Lap-Pui Chau

#### 3. Affiliation: 
第一作者：南洋理工大学电气与电子工程学院

#### 4. Keywords: 
JPEG image restoration, bit errors, two-stage compensation and alignment, robust JPEG decoder, guided-compensation and alignment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Bitstream-Corrupted_JPEG_Images_are_Restorable_Two-Stage_Compensation_and_Alignment_Framework_CVPR_2021_paper.html  Github: https://github.com/wenyang001/Two-ACIR

#### 6. Summary : 
- (1):本文研究了一种实际的JPEG图像恢复问题，即加密比特流上的位错误。位错误会在解码后的图像内容上带来不可预测的色偏和块移，这些问题无法通过现有的主要依赖于像素域中预定义的退化模型的图像恢复方法来解决。 
- (2):过去的方法主要依赖于像素域中预定义的退化模型，但是这些方法无法解决由位错误带来的不可预测的色偏和块移问题。本文提出了一个鲁棒的JPEG解码器，以及一个两阶段的补偿和对齐框架来恢复位流损坏的JPEG图像。 
- (3):本文提出了一个两阶段的补偿和对齐框架，包括自补偿和对齐（SCA）阶段和引导补偿和对齐（GCA）阶段。SCA根据图像内容相似性自适应地执行基于估计的颜色和块偏移的块状图像颜色补偿和对齐。GCA利用JPEG头中提取的低分辨率缩略图来引导全分辨率像素级图像恢复。GCA由粗略引导的pix2pix网络和细化引导的双向拉普拉斯金字塔融合网络实现。 
- (4):本文在三个基准测试中进行了实验，结果表明了我们提出的方法的优越性。即使对于2k分辨率的图像，我们提出的方法也可以恢复具有忠实细节的高保真度图像，与基线EPDN方法相比，PSNR可达38.92 dB，有5.52 dB的显着改进。
#### 7. 方法详细介绍：
本文提出了一种用于恢复位流损坏的JPEG图像的两阶段补偿和对齐框架。第一阶段是自我补偿和对齐，包括分段检测和归一化以及块对齐。第二阶段是引导补偿和对齐，涉及缩略图集成和双向拉普拉斯金字塔融合网络。使用pix2pix网络来粗略地指导具有更一致颜色和对齐纹理的结果图像，并使用双向拉普拉斯金字塔融合网络逐渐完善缩略图在不同比例下的粗略图像指导下的像素级全分辨率图像恢复。

#### 8. 实验设置：
在三个基准测试集上进行了实验，这些测试集具有不同的比特错误率。将所提出的方法与几种最先进的方法进行了比较，包括EPDN、DnCNN和JPEG Ghost。实验在一台配备Intel Core i7-8700K CPU和NVIDIA GeForce GTX 1080 Ti GPU的计算机上进行。

#### 9. 实验结果和分析：
所提出的方法在各项指标上均优于现有的最先进方法。即使对于2k分辨率的图像，所提出的方法也可以恢复具有忠实细节的高保真图像，其PSNR高达38.92 dB，比基线EPDN方法提高了5.52 dB。消融实验表明了所提出方法的每个组件的有效性。所提出的方法还展示了其处理不同未见损坏缩略图和不同比特错误率的JPEG文件的泛化能力，而无需重新训练。


# Paper:266     3D线段映射再探



#### 1. Title: 
3D Line Mapping Revisited

#### 2. Authors: 
Shaohui Liu, Yifan Yu, R´emi Pautrat, Marc Pollefeys, Viktor Larsson

#### 3. Affiliation: 
第一作者：ETH Zurich计算机科学系

#### 4. Keywords: 
3D line mapping, Structure-from-Motion, line detection, line matching, visual localization

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Liu_3D_Line_Mapping_Revisited_CVPR_2021_paper.html  Github: https://github.com/cvg/limap

#### 6. Summary : 
- (1):本文研究的背景是3D计算机视觉中的稀疏地图构建，目前主要使用基于点的方法，但在室内等场景中，点难以检测和匹配，而线段则更加普遍和具有结构性。
- (2):过去的线段重建方法存在一些问题，如端点不一致、线段分割、几何验证等，本文提出的方法通过重新审视线段三角化的退化问题、精心设计的评分和轨迹构建、利用线段的结构先验等方面，解决了这些问题。本文的方法可以与现有的基于点的SfM方法无缝集成，并可以利用它们的3D点来进一步改进线段重建。此外，本文的方法还能够恢复线段与点/消失点之间的3D关联图。 
- (3):本文提出了一种新的3D线段映射系统LIMAP，可以从多视图图像中稳健高效地创建3D线段映射。该系统通过自动识别和利用结构先验，如重合、平行和正交等，实现了线段的三角化、评分、轨迹构建和联合优化。本文的方法可以与现有的基于点的SfM方法无缝集成，并可以利用它们的3D点来进一步改进线段重建。此外，本文的方法还能够恢复线段与点/消失点之间的3D关联图。 
- (4):本文的方法在多个数据集上进行了实验，结果表明LIMAP显著优于现有的3D线段映射方法。本文的方法还在视觉定位和SfM中取得了很好的表现。
#### 7. 方法详细介绍：
本文提出了一种三维线条映射系统，包括三个主要步骤：1）线条检测和匹配，2）线条跟踪和三维重建，3）线条和结构的联合优化。其中，线条检测和匹配步骤使用传统和基于学习的方法相结合，跨多个视角检测和匹配二维线条。线条跟踪和三维重建步骤使用基于图的方法跟踪二维线条并在三维空间中重建它们。联合优化步骤使用非线性优化方法对三维线条地图进行细化，同时结合几何和结构信息。

#### 8. 实验设置：
本文在多个数据集上评估了所提出的系统，包括Hypersim、Tanks and Temples、Aachen和Rome city。评估指标包括长度召回率、内点百分比和平均支持。本文使用LSD和SOLD2两种线条检测器，将所提出的系统与两种最先进的方法L3D++和ELSR进行比较。

#### 9. 实验结果与分析：
所提出的三维线条映射方法在多个数据集上均取得了显著的性能提升，比现有方法更加完整和准确。本文还展示了所提出方法在定位、SfM和MVS等多个应用中的优越性能。同时，本文还进行了详细的实验分析，包括不同组件的消融实验和不同指标的对比分析。


# Paper:267     基于核的感知重采样



#### 1. Title: 
Kernel Aware Resampler

#### 2. Authors: 
Michael Bernasconi, Abdelaziz Djelouah, Farnood Salehi, Markus Gross, Christopher Schroers

#### 3. Affiliation: 
Michael Bernasconi: ETH Zurich
Abdelaziz Djelouah, Farnood Salehi, Christopher Schroers: DisneyResearch|Studios
Markus Gross: ETH Zurich, DisneyResearch|Studios

#### 4. Keywords: 
Image resampling, deep learning, super-resolution, kernel estimation, anti-aliasing

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Bernasconi_Kernel_Aware_Resampler_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1): This paper addresses the problem of generic image resampling, which includes not only upscaling but also other transformations such as rotation and lens distortion. Existing deep learning-based methods for super-resolution are limited to fixed integer scaling factors and do not provide a sound framework to handle more complex resampling tasks.
 
- (2): Previous works in image super-resolution have explored different models and training strategies, but they are limited to integer scaling factors. Some recent works have proposed to model the implicit blur kernel in the image for deep learning-based super-resolution, but they are not optimally suited for real-world content that might suffer from different kinds of implicit degradations from different blur kernels. Existing methods for image resampling are based on traditional interpolation-based resampling approaches, which can result in a noticeable and unnecessary loss in quality. The proposed approach in this paper is motivated by the need for a lean and better applicable framework for generic neural image resampling that can handle implicit degradations and predict kernels instead of directly outputting color values. 

- (3): The proposed framework decomposes the resampling process into different stages, namely reconstruction, geometric distortion, and anti-aliasing, and creates proper training examples to better handle and interactively control spatially variant degradation maps that are expected in image resampling. The approach models the implicit image degradation that takes into account the reconstruction kernel, the local geometric distortion, and the anti-aliasing kernel, and uses this spatially variant degradation map as conditioning for the resampling model. The proposed method can address both global transformations, such as upscaling or rotation, and locally varying transformations such as lens distortion or undistortion. Another important contribution is the automatic estimation of the degradation map in this more complex resampling setting (i.e. blind image resampling). The proposed method achieves state-of-the-art results by predicting kernels to apply on the input image instead of direct color prediction, which makes the model more robust and enables consistent resampling of other channels, such as normals.

- (4): The proposed method achieves state-of-the-art results on various image resampling tasks, including image rectification, retargeting, lens (un)distortion, and image warping. The performance supports the goals of the paper, which is to provide a lean and better applicable framework for generic neural image resampling that can handle implicit degradations and predict kernels instead of directly outputting color values.
#### 7. 方法详细介绍：
本文提出了一种基于深度学习的图像重采样方法，该方法能够处理模糊核、非整数缩放因子和通用变换等问题。该方法将重采样过程分解为不同的阶段，包括重建、几何失真和抗锯齿。该方法使用隐式图像退化的局部表示，考虑了重建核、局部几何失真和抗锯齿核。空间变异的退化图被用作重采样模型的条件，该模型可以处理全局变换，如放大或旋转，以及局部变换，如镜头畸变或去畸变。该方法还包括盲图像重采样中的自动估计退化图。该模型预测要应用于输入图像的核，而不是直接预测颜色值，使其适用于训练期间未见过的不同类型的数据，如法线。
具体步骤如下：
1. 生成不同退化的训练对。
2. 使用MLP对核图进行编码，并将其与输入图像连接。
3. 使用ProSR网络处理连接数据，然后使用重采样层使用warp网格对提取的特征进行重采样。
4. 重采样层产生两个输出-最接近的3x3特征和有关warp的几何信息。
5. 预测层直接产生输出图像颜色或要应用于输入图像的核。

#### 8. 实验设置：
本文在DIV2Ks测试集的100个大小为512×512的裁剪上进行了评估，并在不同的核心范围内对结果进行了平均。实验使用了NVIDIA Tesla V100 GPU进行训练和测试。本文的方法在不同的上采样和投影变换任务中优于现有的方法，如LIIF和SRWarp。该方法还能够很好地推广到不同类型的数据，如带有额外通道的渲染图像。与SRWarp和LIIF相比，该方法的推理时间显着缩短。

#### 9. 实验结果和分析：
本文提出的基于深度学习的图像重采样方法在不同的上采样和投影变换任务中优于现有的方法，如LIIF和SRWarp。该方法能够很好地推广到不同类型的数据，如带有额外通道的渲染图像。与SRWarp和LIIF相比，该方法的推理时间显着缩短。本文还提出了一种预测核而不是颜色的变体，该变体适用于训练期间未见过的不同类型的数据，并在标准重采样任务中产生最佳结果。未来的工作可以改进退化估计过程，减少估计退化和正确退化之间的性能差距。


# Paper:268     利用文本信息提高弱监督下的时间动作定位



#### 1. Title: 
Boosting Weakly-Supervised Temporal Action Localization with Text Information

#### 2. Authors: 
Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao

#### 3. Affiliation: 
第一作者：西安电子科技大学

#### 4. Keywords: 
Weakly-Supervised Temporal Action Localization, Text Information, Video Analysis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Boosting_Weakly-Supervised_Temporal_Action_Localization_With_Text_Information_CVPR_2021_paper.html  Github: https://github.com/lgzlIlIlI/Boosting-WTAL

#### 6. Summary : 
- (1):本文研究的是弱监督下的时间动作定位问题，由于缺乏时间注释，当前的弱监督方法通常会陷入过度或不完整的本地化。本文旨在从两个方面利用文本信息来提高弱监督下的时间动作定位，即(a)通过区分性目标来扩大类间差异，从而减少过度；(b)通过生成目标来增强类内完整性，从而找到更完整的时间边界。

- (2):现有的弱监督方法通常利用视频信息训练分类器，用于生成一系列类别激活图（T-CAM）。虽然取得了显著的改进，但当前方法仍然存在两个问题，即不完整和过度本地化。本文提出了一种文本段挖掘（TSM）机制，该机制基于动作类别标签构建文本描述，并将文本视为查询以挖掘所有与类别相关的段。此外，本文还引入了一种名为视频文本语言完成（VLC）的生成目标，该目标专注于从视频中提取所有语义相关的段以完成文本句子。通过自监督约束将TSM和VLC相结合，本方法在两个公共数据集上实现了最新的最佳性能。

- (3):本文提出了一种新的框架，包括两个目标：文本段挖掘和视频文本语言完成，以利用动作标签文本信息来提高弱监督下的时间动作定位。TSM机制将动作标签文本用作查询，以在视频中挖掘所有相关段。VLC模型专注于从视频中提取与动作文本相关的所有段以完成掩码关键字。本文还设计了TSM和VLC模型之间的自监督约束，以实现更完整的本地化结果。

- (4):本文在THUMOS14和ActivityNet1.3上实现了最新的最佳性能。本文的方法可以无缝地应用于现有方法，并以明显的优势提高其性能。
#### 7. 方法详细介绍：
本文提出的方法主要包含三个模块：文本片段挖掘（TSM）模块、视频文本语言补充（VLC）模块和自监督一致性约束。其中，TSM模块用于替代现有的只使用视频信息的WTAL模型，通过将文本作为查询来挖掘所有与类别相关的视频片段。VLC模块则用于约束WTAL模型，通过关注所有与文本相关的视频片段来完成文本句子。最终的目标函数为L = Lmil + αLrec + βLc + λLcon，其中α、β、λ是用于平衡四个损失项的超参数。此外，本文还引入了自监督一致性约束来缓解TSM模块对最具语义相关的片段的过度关注。

#### 8. 实验设置：
本文在THUMOS14和ActivityNet数据集上进行了实验。对于THUMOS14，使用200个验证视频进行训练，213个测试视频进行测试；对于ActivityNet，使用全部10,024个训练视频进行训练，5,044个测试视频进行测试。评估指标为平均精度（mAP）。

#### 9. 实验结果与分析：
本文提出的方法在THUMOS14和ActivityNet数据集上均取得了最先进的性能。作者还发现，他们的方法可以应用于现有方法并显著提高它们的性能。全面的消融实验表明了所提出目标的有效性。


# Paper:269     深度图形重编程



#### 1. Title: 
Deep Graph Reprogramming

#### 2. Authors: 
Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, Dacheng Tao

#### 3. Affiliation: 
第一作者：Yongcheng Jing，澳大利亚悉尼大学

#### 4. Keywords: 
Graph neural networks, model reusing, deep graph reprogramming, data reprogramming, model reprogramming

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jing_Deep_Graph_Reprogramming_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了图神经网络（GNNs）的模型重用任务，提出了一种针对GNNs的新型模型重用任务，称为“深度图形重编程”。
- (2):现有的GNNs重用方法主要是基于知识蒸馏的，但是这种方法存在着存储和计算负担大的问题，且只能处理相同任务，无法处理多个任务。本文提出的深度图形重编程任务可以重用单个预训练的GNNs处理多个任务，且无需重新训练或微调。同时，本文提出了数据重编程（DARE）和模型重编程（MERE）两种新的重编程范式，以解决异构和同构维度的图形特征问题，以及模型容量不足的问题。
- (3):本文提出了一种通用的Meta-FeatPadding（MetaFP）方法，用于处理异构维度的图形，以及一种针对同构维度的图形的转导性Edge-Slimming（EdgSlim）和归纳性Meta-GraPadding（MetaGP）方法。此外，本文还提出了一种新颖的任务自适应的可重编程聚合器（ReAgg）方法，以增强模型的表达能力。这些方法可以使预训练的GNNs处理各种下游任务。
- (4):本文在14个数据集上进行了实验，包括节点/图分类/回归、3D物体识别和分布式动作识别等任务。实验结果表明，所提出的方法与从头开始重新训练的方法相当，可以处理各种下游任务。
#### 7. 方法详细介绍：
本文提出了一种新的模型重用任务，称为“深度图重编程”，用于图神经网络（GNN）。该方法旨在重新编程预训练的GNN以处理各种跨级别下游任务，而不更改原始节点特征或模型参数。该方法包括两种范式：数据重编程（DARE）和模型重编程（MERE）。DARE解决了输入端各种任务的多样化图特征维度的挑战，而MERE则缓解了模型端固定的每个任务-每个模型行为的困境。该方法包括Meta-FeatPadding用于异构-DARE，Edge-Slimming和Meta-GraPadding用于同构-DARE，以及Reprogrammable-Aggregator用于MERE。

具体步骤如下：
1. 数据重编程（DARE）：为了解决输入端多样化特征的问题，本文提出了通用的Meta-FeatPadding（MetaFP）方法，通过对抗节点特征扰动，在异构-DARE中生成原始特征周围的填充特征，除了节点级扰动外，其他针对图数据的扰动类型也应该对DARE有效，例如边级扰动和结构级扰动，这促使本文开发了同构-DARE的转导性Edge-Slimming（EdgSlim）和归纳性Meta-GraPadding（MetaGP）方法。
2. 模型重编程（MERE）：本文根据对抗性重编程攻击的理论，开发了MERE范式来增强模型能力，作为DARE在巨大领域差异GNN重用场景下的补充。本文通过在Cora数据集上进行先前研究，尝试观察重用一个预先训练的GNN，该GNN预先训练用于分类{Case-Based，Genetic-Algorithm，Neural-Network，Probabilistic-Method}节点类别，直接处理{Reinforcement-Learning，Rule-Learning，Theory}不同下游类别的多样性表现，只替换预先训练的聚合器与其他聚合方法。因此，本文推导出了可重编程聚合（ReAgg）方法作为MERE范式的特定实现，旨在在各种下游场景下动态更改聚合行为。

#### 8. 实验设置：
本文在14个公开数据集上进行了实验，包括节点/图分类/回归，3D物体识别和分布式动作识别，以评估所提出方法的性能。所有实验都是在单个NVIDIA GeForce RTX 2080 Ti GPU上进行的。实验的目标不是达到最先进的性能，而是重用预训练的GNN，在有限的计算资源下为尽可能多的下游任务产生有利的结果。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的方法产生了令人满意的结果，与从头开始重新训练的结果相当。具体来说，使用GARE的预训练GNN能够胜任各种下游任务。在节点分类任务中，MetaFP使得跨领域GNN重用成为可能，而ReAgg进一步提高了下游性能。EdgSlim在同构节点属性预测任务中以低计算成本获得了竞争性的性能。MetaGP在同构交叉领域图级任务中重用图分类模型以进行图回归时表现出色。在ShapeNet中，使用预先训练的DGCNN处理不同的下游类别的结果也得到了展示。


# Paper:270     通过平滑视角轨迹加速NeRF渲染的SteerNeRF



#### 1. Title: 
SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory

#### 2. Authors: 
Sicheng Li, Hao Li, Yue Wang, Yiyi Liao, Lu Yu

#### 3. Affiliation: 
浙江大学

#### 4. Keywords: 
Neural Radiance Fields, Novel View Synthesis, Rendering Acceleration, Smooth Viewpoint Trajectory, Volume Rendering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_SteerNeRF_Accelerating_NeRF_Rendering_via_Smooth_Viewpoint_Trajectory_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是神经辐射场（NeRF）的渲染加速问题，NeRF在新视角合成方面表现出色，但渲染速度较慢。为了加速体积渲染过程，已经提出了许多加速方法，但这些方法通常需要大量的内存消耗。本文提出了一种新的视角，利用连续的视角变化来加速NeRF渲染，从而在效率和内存消耗之间取得平衡。

- (2):现有的NeRF加速方法通常是针对每个视角单独减少渲染时间，但本文提出的方法是通过利用多个连续视角之间的信息重叠来加速渲染。与现有的NeRF加速方法不同，本文的方法结合了低分辨率体积渲染和高分辨率2D神经渲染，利用前一帧和当前帧的特征图来恢复目标分辨率的图像。本文的方法可以在保持高保真度和时间一致性的同时，减少渲染时间，内存开销较小。

- (3):本文提出的方法是通过利用连续的视角变化来加速NeRF渲染。具体而言，我们通过渲染缓冲区、神经特征场和轻量级2D神经渲染器来实现渲染加速。首先，我们通过体积渲染在给定视角下渲染出低分辨率的特征图和深度图。然后，我们利用前一帧和当前帧的特征图来生成目标分辨率的输出图像。本文的方法可以在保持高保真度的同时，减少渲染时间和内存开销。

- (4):本文的方法在合成和真实世界数据集上进行了实验，可以在800×800像素的图像分辨率下实现近100 FPS的渲染速度，在1920×1080像素的图像分辨率下实现30 FPS的渲染速度。本文的方法比其他低内存NeRF加速方法更快，并缩小了低内存和基于缓存的方法之间的速度差距。
#### 7. 方法详细介绍：
本文提出的SteerNeRF方法通过平滑的视点轨迹加速NeRF渲染。该方法学习神经特征场，渲染适合后续神经渲染器的低分辨率特征图。该框架的加速来自于以较低分辨率渲染特征图并通过渲染缓冲区减少采样范围。缓冲区引导的采样范围缩减是通过使用先前渲染的相邻帧的深度信息作为指导来确定采样位置，从而加速渲染。神经渲染器是一个轻量级的2D卷积网络，它以重新投影的高分辨率特征图和上采样的特征图作为输入来生成输出图像。训练策略包括在目标分辨率上预训练神经特征场，然后以端到端的方式联合训练整个模型。

#### 8. 实验设置：
测试系统由一台NVIDIA GTX 3090消费级GPU、一个主频为3.80GHz的Intel Core i7-10700K CPU和32GB内存组成。网络架构基于Instant-NGP，使用多分辨率哈希表，其中每个分辨率的表长度固定为219。2D神经渲染器是一个浅层的U-Net，详细架构描述在补充材料中。推理优化是通过利用NVIDIA TensorRT将2D神经渲染器优化为FP16和INT8精度分别的两个版本来实现的。

#### 9. 实验结果与分析：
本文通过与先前工作的定量比较、两个代表性数据集的运行时分析和广泛的消融研究来评估所提出的SteerNeRF方法的性能。评估基于效率、质量和内存使用度指标。结果表明，所提出的方法在保持内存占用不太大的情况下实现了高帧率渲染。该方法在FPS方面优于最佳基线方法PlenOctree，同时保持类似的内存使用。所提出的方法显著加速了实现基线在直接可比较的情况下。消融研究表明，增加前面帧的数量可以提高重建质量，但由于额外的变形操作而导致渲染时间更长。特征图通道数对视觉质量有逐渐的影响。联合训练的必要性得到了验证。


# Paper:271     针对半监督语义分割的实例特定和模型自适应监督



#### 1. Title: 
Instance-specific and Model-adaptive Supervision for Semi-supervised Semantic Segmentation

#### 2. Authors: 
Zhen Zhao, Sifan Long, Jimin Pi, Jingdong Wang, Luping Zhou

#### 3. Affiliation: 
第一作者：University of Sydney（悉尼大学）

#### 4. Keywords: 
Semi-supervised learning, semantic segmentation, instance-specific supervision, model-adaptive supervision, hardness evaluation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Instance-Specific_and_Model-Adaptive_Supervision_for_Semi-Supervised_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/zhenzhao/iMAS

#### 6. Summary : 
- (1):本文研究半监督语义分割，提出了一种实例特定和模型自适应的监督方法，名为iMAS。与现有方法不同的是，iMAS考虑了未标记数据之间的差异性，通过实例硬度评估来动态地调整数据扰动和无监督损失权重，以更好地适应模型的泛化能力。
 
- (2):现有的半监督语义分割方法大多数将所有未标记数据视为相同，忽略了它们之间的差异性。本文提出的iMAS方法通过实例硬度评估来区分不同的未标记样本，并在训练过程中动态地调整数据扰动和无监督损失权重，以更好地适应模型的泛化能力。与现有方法相比，iMAS不需要引入额外的网络组件或训练过程，可以获得更好的性能。

- (3):本文提出的iMAS方法通过实例硬度评估来区分不同的未标记样本，并在训练过程中动态地调整数据扰动和无监督损失权重，以更好地适应模型的泛化能力。iMAS方法在师生模型框架下进行训练，通过加权一致性损失来学习未标记实例，并动态调整数据扰动程度。iMAS方法不需要引入额外的网络组件或训练过程，可以获得更好的性能。

- (4):本文在Pascal VOC 2012和Cityscapes数据集上进行了实验，结果表明，iMAS方法可以在不同的半监督分区协议下获得最先进的性能。例如，在VOC 2012数据集上，iMAS方法只使用了183个标记数据，就可以获得75.3%的mIOU，比有监督基线高出17.8%，比之前的最先进方法高出4.3%。iMAS方法通过实例硬度评估来区分不同的未标记样本，并在训练过程中动态地调整数据扰动和无监督损失权重，以更好地适应模型的泛化能力，从而获得更好的性能。
#### 7. 方法详细介绍：
本文提出了一种半监督语义分割方法，称为iMAS。该方法采用一种基于一致性正则化的半监督框架，通过量化每个未标记实例的难度进行监督，以模型自适应的方式跨训练过程对未标记数据进行训练。iMAS采用实例特定和模型自适应的监督方法，包括模型自适应的CutMix数据增强和模型自适应的无监督损失。iMAS方法采用教师-学生框架，其中教师模型是历史学生模型的集合，并为未标记数据生成稳定的伪标签。iMAS方法在语义分割任务中实现了实例特定和模型自适应的监督，从而取得了优异的性能。

#### 8. 实验设置：
本文在两个标准的半监督语义分割基准数据集Pascal VOC2012和Cityscapes上评估了所提出的方法。混合训练集也被涉及，包括来自分割边界（SBD）数据集的额外训练图像。采用基于Resnet的DeepLabv3+作为分割骨干网络。对于Pascal VOC2012和Cityscapes，训练图像分别随机裁剪为513×513和769×769。总训练时期为80和240。

#### 9. 实验结果和分析：
本文在PASCAL VOC 2012和Cityscapes数据集上比较了所提出的iMAS方法与几种最先进的方法在不同的半监督分区协议下的性能。性能以平均交并比（mIOU）为评估指标。结果表明，所提出的iMAS方法在两个数据集上均优于其他方法，并取得了最先进的性能。本文还进行了广泛的消融研究，进一步验证了所提出方法的优越性和稳定性。


# Paper:272     飞机上的晃动：从非稳定摄影中无监督地估计深度



#### 1. Title: 
Shakes on a Plane: Unsupervised Depth Estimation from Unstabilized Photography

#### 2. Authors: 
Ilya Chugunov, Yuxuan Zhang, Felix Heide

#### 3. Affiliation: 
普林斯顿大学

#### 4. Keywords: 
Depth estimation, unsupervised learning, neural RGB-D representation, long-burst photography, camera motion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chugunov_Shakes_on_a_Plane_Unsupervised_Depth_Estimation_From_Unstabilized_Photography_CVPR_2021_paper.html  Github: https://github.com/light-field-imaging/soap

#### 6. Summary : 
- (1):本文研究的背景是现代移动摄影管道捕捉和合并短序列帧以恢复增强图像，但往往忽略了它们捕捉的场景的三维性质，将图像之间的像素运动视为二维聚合问题。

- (2):过去的方法包括被动单目深度估计和多视角深度估计，但是这些方法都有一些问题，如对训练数据的依赖性强、对于分布不同的场景的泛化能力差等。本文提出的方法是一种无监督的端到端方法，可以从不稳定的长曝光摄影中联合估计高质量的物体深度和相机运动。

- (3):本文提出了一种测试时间优化方法，通过将神经RGB-D表示拟合到长曝光数据中，并同时估计场景深度和相机运动来实现。该模型是端到端训练的，通过控制网络在训练期间访问哪些多分辨率体积特征以及何时访问这些特征来进行粗到精的细化。本文的方法不需要额外的硬件或单独的数据预处理和姿态估计步骤，可以实现几何精确的深度重建。

- (4):本文的方法在长曝光摄影数据上进行了实验验证，并展示了几何精确的深度重建，没有额外的硬件或单独的数据预处理和姿态估计步骤。本文的方法在物体重建方面表现出色，可以用于物体检测、分割和跟踪等任务。
#### 7. 方法详细介绍：
本文提出了一种无监督的端到端方法，可以从不稳定的长曝光摄影中联合估计高质量的物体深度和相机运动。该方法将问题形式化为图像合成任务，类似于神经辐射方法，通过连续的深度和姿态模型分解为显式的几何投影。该方法将神经RGB-D表示拟合到长曝光数据中，并同时估计场景深度和相机运动。平面加深度模型是端到端训练的，并通过控制网络在训练期间何时访问哪些多分辨率体积特征来执行粗到细的细化。

#### 8. 实验设置：
本文设计了自己的数据收集工具，用于长曝光记录，因为没有商品移动应用程序允许连续流Bayer RAW帧和元数据。该工具具有实时取景器，预览RGB、设备深度和自动调整的ISO和曝光值。按下按钮后，锁定ISO、曝光和对焦，并将两秒钟、42帧的长曝光记录到设备中。该方法使用记录的时间戳、相机内参、陀螺仪驱动的设备旋转估计和1200万像素RAW帧。

#### 9. 实验结果和分析：
本文通过在各种场景和复杂的3D对象上评估所提出的方法，使用商业高精度转台结构光扫描仪生成地面真实物体网格，并将其注册和渲染到深度与匹配的相机参数。使用相对绝对误差和尺度不变误差进行定量深度评估，并展示了优于现有的学习、混合和多视图方法的高质量物体深度重建。文本还包括关于固定图像表示、替换RAW数据或从模型中删除设备初始旋转估计的影响的消融研究。


# Paper:273     使用对比度权重剪枝训练去偏差子网络



#### 1. Title: 
Training Debiased Subnetworks with Contrastive Weight Pruning

#### 2. Authors: 
Geon Yeong Park, Sangmin Lee, Sang Wan Lee, Jong Chul Ye

#### 3. Affiliation: 
韩国科学技术院（KAIST）生物与脑工程系

#### 4. Keywords: 
Neural networks, spurious correlations, debiasing, weight pruning, bias-conflicting samples

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Park_Training_Debiased_Subnetworks_With_Contrastive_Weight_Pruning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究神经网络中的偏差问题，提出了一种新的去偏差方法，即对权重进行对比度剪枝，以提取无偏子网络。 
- (2):过去的方法主要是基于标注的偏差属性或假设特定类型的偏差，而本文提出的方法不需要昂贵的组注释，而是利用偏差冲突样本来发现偏差权重。本文的方法在多个数据集上都取得了优异的性能，相比于现有的去偏差方法，具有更少的参数和更好的性能。
- (3):本文提出了一种去偏差的对比度权重剪枝算法（DCWP），该算法利用偏差冲突样本来探索无偏子网络，而不需要昂贵的组注释。DCWP由两个阶段组成：（1）识别偏差冲突样本，（2）训练剪枝参数以获得具有稀疏约束和去偏损失函数的权重剪枝掩码。本文的方法在多个数据集上都取得了优异的性能，相比于现有的去偏差方法，具有更少的参数和更好的性能。
- (4):本文的方法在多个数据集上都取得了优异的性能，相比于现有的去偏差方法，具有更少的参数和更好的性能。本文的方法可以提取无偏子网络，从而提高模型的泛化性能。
#### 7. 方法详细介绍：
本文提出了一种名为去偏置对比度权重剪枝（Debiased Contrastive Weight Pruning，DCWP）的算法，用于训练去偏置的子网络。该算法包括两个阶段：（1）识别出在具有偏差的数据集上提供重要线索的少数样本，（2）使用对比度权重剪枝算法对网络进行剪枝，得到去偏置的子网络，并进行微调以提高性能。DCWP算法采用二值权重剪枝掩码来建模子网络，采用ℓ1正则化项作为稀疏性约束，采用Gumbel-softmax技巧进行掩码采样，采用梯度下降优化去偏置损失函数，最后使用加权交叉熵损失和去偏置对比度损失进行微调。

#### 8. 实验设置：
本文在多个具有偏差的数据集上评估了所提出的DCWP算法，包括彩色MNIST（Color-MNIST）、损坏的CIFAR-10（Corrupted CIFAR-10）、偏置的FFHQ（Biased FFHQ）和CelebA。作者证明了DCWP算法在这些数据集上始终优于其他最先进的去偏置方法，即使没有直接监督偏差类型。对于某些实验，报告了无偏准确性和偏差冲突准确性。文本还将所提出的方法与其他去偏置方法进行了比较，包括EnD、Rebias、MRM、LfF、JTT和DisEnt。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的DCWP算法在多个数据集上均取得了优异的性能，证明了其有效性和鲁棒性。与其他最先进的去偏置方法相比，DCWP算法在无偏准确性和偏差冲突准确性方面均表现出色。此外，本文还对DCWP算法的可解释性和泛化能力进行了分析，证明了其在去偏置和泛化方面的优势。


# Paper:274     通过对比学习实现合成到真实新视角合成的通用神经辐射场



#### 1. Title: 
ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real Novel View Synthesis via Contrastive Learning

#### 2. Authors: 
Hao Yang, Lanqing Hong, Aoxue Li, Tianyang Hu, Zhenguo Li, Gim Hee Lee, Liwei Wang

#### 3. Affiliation: 
第一作者：北京大学数据科学研究中心

#### 4. Keywords: 
Novel view synthesis, Neural Radiance Fields, Contrastive Learning, Synthetic-to-real generalization

#### 5. Paper: https://arxiv.org/abs/2106.13281  Github: https://haoy945.github.io/contranerf/

#### 6. Summary : 
- (1):本文研究了神经辐射场（NeRF）在合成数据到真实数据的新视角合成中的泛化问题，提出了一种几何感知的对比学习方法，以学习具有几何约束的多视角一致特征表示，从而提高模型的泛化能力。

- (2):过去的方法主要集中在如何将NeRF推广到未见过的场景上，而很少考虑合成数据到真实数据的泛化问题。本文通过实验发现，使用合成数据训练的模型往往会产生更多的伪影，但同时会获得更好的细节。为了保持使用合成数据的优势，同时避免其负面影响，本文提出了几何感知的对比学习方法，以学习具有几何约束的多视角一致特征表示。本文的方法在合成到真实的设置下，可以渲染出更高质量、更好细节的图像，优于现有的泛化新视角合成方法。

- (3):本文提出了一种名为ContraNeRF的方法，通过几何感知的对比学习来实现从合成数据到真实数据的泛化。具体来说，本文提出了几何感知的对比学习来学习多视角一致的特征表示，通过比较每对源视图的局部特征的相似性来计算InfoNCE损失。同时，本文采用跨视图注意力来进一步增强特征的几何感知能力。最后，本文通过学习一个通用的视图插值函数来渲染射线。实验结果表明，本文的方法在合成数据的情况下优于现有的泛化NeRF方法，并且可以渲染出高质量的新视角图像，同时保留细节。在真实数据的情况下，本文的方法也优于现有的神经辐射场泛化方法。

- (4):本文的方法在合成到真实的设置下，可以渲染出更高质量、更好细节的图像，优于现有的泛化新视角合成方法。在真实数据的情况下，本文的方法也优于现有的神经辐射场泛化方法。
#### 7. 方法详细介绍：
本文提出了一种名为ContraNeRF的方法，它通过对比学习和几何一致性来实现从合成数据到真实数据的泛化。该方法包括几何感知特征提取和几何感知对比学习两个主要部分。在几何感知特征提取部分，使用共享CNN从每个图像中提取特征，然后通过交换信息来实现源视图之间的几何增强特征。在几何感知对比学习部分，对每对源视图进行对比学习，以增强多视图一致性并考虑视图之间的几何约束。定义对比损失以确定正样本和负样本。最后，通过学习通用的视图插值函数来渲染光线。

#### 8. 实验设置：
本文使用3D-FRONT和ScanNet数据集作为合成训练集和真实测试集。对于3D-FRONT，随机选择88个场景，对于每个场景，使用BlenderProc在640×480分辨率下渲染200个相机视图。对于ScanNet，随机选择88个场景作为真实训练数据集，8个场景作为测试数据集。在每个测试场景中，留出1/8的图像作为测试视图，其余图像用作源视图。

#### 9. 实验结果与分析：
本文将提出的ContraNeRF方法与最先进的可泛化NeRF方法进行比较，包括PixelNeRF、IBRNet、MVSNeRF、GeoNeRF和Neuray。使用PSNR、SSIM和LPIPS指标评估方法。结果表明，所提出的方法在所有指标上均优于以前的方法。该方法能够生成具有几何和外观细节的图像，并且在合成到真实和真实到真实的情况下，与以前的可泛化NeRF方法相比，生成的伪影更少。


# Paper:275     自适应高斯混合的稀疏标注语义分割



#### 1. Title: 
Sparsely Annotated Semantic Segmentation with Adaptive Gaussian Mixtures

#### 2. Authors: 
Linshan Wu, Zhun Zhong, Leyuan Fang, Xingxin He, Qiang Liu, Jiayi Ma, and Hao Chen

#### 3. Affiliation: 
第一作者：湖南大学电气与信息工程学院

#### 4. Keywords: 
Semantic Segmentation, Sparsely Annotated, Gaussian Mixtures, Self-Supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Sparsely_Annotated_Semantic_Segmentation_With_Adaptive_Gaussian_Mixtures_CVPR_2021_paper.html  Github: https://github.com/Luffy03/AGMM-SASS

#### 6. Summary : 
- (1):本文研究的是稀疏标注语义分割（SASS）的问题，即通过稀疏标注（如点或涂鸦）的图像来学习分割模型。 
- (2):现有的SASS方法主要集中在引入低级别的亲和力或生成伪标签以加强监督，但往往忽略了标记和未标记像素之间的内在关系。本文提出了一种新的SASS框架，该框架配备了自适应高斯混合模型（AGMM），可以根据标记和未标记像素的分布有效地为未标记像素提供可靠的监督。 
- (3):本文提出的AGMM框架通过将标记像素作为高斯混合的中心，建立了每个类别的特征分布模型。然后，利用标记像素和相对相似的未标记像素建立高斯混合模型，生成软GMM预测，从概率角度监督未标记区域。GMM的参数是动态适应输入特征的，实现了端到端的在线自我监督。通过捕捉类别特定的高斯混合，AGMM鼓励模型以端到端的对比学习方式学习有区别的类别决策边界。 
- (4):在PASCAL VOC 2012和Cityscapes数据集上进行的实验结果表明，我们的AGMM可以建立新的SASS性能最佳状态。本文提出的方法不需要额外的信息进行监督，也不需要多阶段训练和耗时的后处理。
#### 7. 方法详细介绍：
本文提出了一种自适应高斯混合模型（AGMM）框架，用于稀疏标注的语义分割（SASS）。AGMM将GMM分支整合到传统的分割分支中，其中标记像素作为高斯混合的中心，以建模高维特征空间中每个类别的数据分布。GMM分支生成软GMM预测，从概率角度监督未标记区域，并且GMM公式化过程以自适应方式工作，其中GMM的参数动态适应输入特征，实现端到端在线自我监督。AGMM以端到端对比学习的方式鼓励模型学习有区别的类别决策边界。

具体步骤如下：
1. 使用ResNet在ImageNet上进行预训练，作为骨干网络。
2. 使用DeeplabV3+作为分割头构建网络结构。
3. 将GMM分支整合到传统的分割分支中，以建模高维特征空间中每个类别的数据分布。
4. GMM分支生成软GMM预测，从概率角度监督未标记区域。
5. GMM公式化过程以自适应方式工作，其中GMM的参数动态适应输入特征，实现端到端在线自我监督。
6. AGMM以端到端对比学习的方式鼓励模型学习有区别的类别决策边界。

#### 8. 实验设置：
本文在PASCAL VOC 2012和Cityscapes两个广泛使用的语义分割数据集上进行了实验。使用Pytorch框架，4个NVIDIA 3090 GPU进行训练。使用多种数据增强方法，采用随机梯度下降优化器进行训练。

#### 9. 实验结果与分析：
本文在PASCAL VOC 2012和Cityscapes数据集上进行了点和涂鸦监督的实验，结果表明AGMM可以建立新的SASS性能最佳值。AGMM优于现有的SASS方法，实现了最佳性能。在Cityscapes数据集上，AGMM方法在20个点击、50个点击和100个点击时分别达到了62.1％，68.3％和71.6％的性能。当与多阶段训练相结合时，AGMM的性能可以进一步提高。


# Paper:276     基于变分期望最大化的置信度感知个性化联邦学习



#### 1. Title: 
Confidence-aware Personalized Federated Learning via Variational Expectation Maximization

#### 2. Authors: 
Junyi Zhu, Xingchen Ma, Matthew B. Blaschko

#### 3. Affiliation: 
Junyi Zhu: KU Leuven（鲁汶大学）；Xingchen Ma: Amazon Web Services；Matthew B. Blaschko: KU Leuven（鲁汶大学）

#### 4. Keywords: 
Federated Learning, Personalized Federated Learning, Variational Expectation Maximization, Bayesian modeling, Confidence-aware

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhu_Confidence-Aware_Personalized_Federated_Learning_via_Variational_Expectation_Maximization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是联邦学习中的个性化问题，即不同客户端的数据集可能分布不同，大小不同，导致传统的联邦学习方法难以处理。 
- (2):过去的方法包括联邦平均（FedAvg）和一些个性化联邦学习方法，但是这些方法在处理高度异构的数据集时表现不佳。本文提出了一种基于层次贝叶斯建模和变分推断的个性化联邦学习框架，通过引入全局模型作为潜在变量来增强客户端参数的联合分布，从而捕捉不同客户端的共同趋势。同时，本文提出了一种置信度值的概念，用于衡量客户端参数的不确定性和局部模型与全局模型的偏差，以调整模型聚合的权重和全局模型的正则化效果。 
- (3):本文提出的方法是一种基于变分推断的贝叶斯框架，通过引入全局模型作为潜在变量来增强客户端参数的联合分布，从而捕捉不同客户端的共同趋势。同时，本文提出了一种置信度值的概念，用于衡量客户端参数的不确定性和局部模型与全局模型的偏差，以调整模型聚合的权重和全局模型的正则化效果。 
- (4):本文在多个数据集上进行了广泛的实证研究，实验结果表明，本文提出的方法在轻微异构的情况下获得了竞争性的结果，在高度异构的情况下显著优于现有的个性化联邦学习框架。
#### 7. 方法详细介绍：
本文提出了一种基于层次贝叶斯建模和变分推断的个性化联邦学习方法。该方法引入了全局模型作为潜在变量，以增强客户端参数的联合分布并捕捉不同客户端的共同趋势。优化过程基于最大化边缘似然的原则，并使用变分期望最大化进行。算法实现包括数值稳定性和重参数化、蒙特卡罗近似和头部基础架构。具体步骤如下：
1. 定义全局模型和客户端模型的先验分布；
2. 通过变分推断估计后验分布；
3. 通过变分期望最大化优化全局模型和客户端模型；
4. 在聚合阶段，根据置信度值加权上传参数；
5. 根据置信度值调整KL散度项的正则化效果。

#### 8. 实验设置：
本文在Fashion-MNIST、CIFAR10、CIFAR100和SUN397数据集上评估了所提出的方法的性能。实验中客户端数量从50到200不等，考虑了慢速客户端的存在。实验比较了所提出的方法与多个联邦学习框架，包括FedAvg、FedProx、Scaffold、FedPer、FedRep、IFCA、PerFedAvg、pFedME、pFedBayes以及本地训练方案。

#### 9. 实验结果和分析：
实验结果表明，所提出的方法在准确性和收敛速度方面优于其他联邦学习框架。实验还表明，所提出的方法能够有效处理统计异质数据和数据数量差异，并且对慢速客户端具有鲁棒性。实验还提供了关于客户端数量和统计异质性程度对联邦学习框架性能影响的见解。


# Paper:277     JRDB-Pose：用于多人姿态估计和跟踪的大规模数据集



#### 1. Title: 
JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking

#### 2. Authors: 
Edward Vendrow, Duy Tho Le, Jianfei Cai, Hamid Rezatoﬁghi

#### 3. Affiliation: 
Edward Vendrow: Stanford University
Duy Tho Le, Jianfei Cai, Hamid Rezatoﬁghi: Monash University

#### 4. Keywords: 
Multi-person pose estimation, multi-person pose tracking, robotic perception, human-robot interaction, dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2021_paper.html  Github: https://github.com/edwardvendrow/JRDB-Pose

#### 6. Summary : 
- (1):本文研究的背景是自主机器人在人类环境中的操作，需要对周围的人进行准确的姿态估计和跟踪，以便进行安全决策。
- (2):过去的方法往往无法应对人类环境中的多样性和高度不平衡的人体运动，而本文提出的JRDB-Pose数据集则是一个大规模的数据集，可以提供多人姿态估计和跟踪的标注，同时也提供了一些新的评价指标。
- (3):本文提出了JRDB-Pose数据集，该数据集是一个大规模的数据集，包含了多人姿态估计和跟踪的标注，同时也提供了一些新的评价指标。该数据集可以帮助研究人员更好地理解人类环境中的姿态估计和跟踪问题，并为机器人感知和导航任务提供更好的数据支持。
- (4):本文在JRDB-Pose数据集上进行了实验，展示了现有方法的新挑战。实验结果表明，本文提出的数据集可以帮助研究人员更好地理解人类环境中的姿态估计和跟踪问题，并为机器人感知和导航任务提供更好的数据支持。
#### 7. 方法详细介绍：
本文评估了多种最新的多人姿态估计模型，包括基于HRNet骨干网络的自上而下方法和三种最新的自下而上方法，DEKR、CID和YoloPose。所有方法都是在单个相机图像上进行训练，然后组合成拼接视图预测。使用NMS算法消除拼接注释集中的重复姿态。使用Opose定位误差和基数误差以及AP和AR指标评估每种方法的性能。其中，YoloPose是表现最好的基线方法。

#### 8. 实验设置：
本文使用JRDB-Pose数据集进行实验，该数据集是一个用于多人姿态估计和跟踪的大规模数据集和基准。该数据集包含由12个相机拍摄的全景图像，以及每个人的姿态和可见性的注释。使用Opose和AP指标评估每种方法在单个图像和注释上的性能，使用MOTA、IDF1和IDSW指标评估每种方法在姿态跟踪上的性能。

#### 9. 实验结果与分析：
本文评估了三种最新的多目标跟踪方法，ByteTrack、Unitrack和OC-SORT，在其跟踪基准测试中的表现。其中，OC-SORT的表现优于其他方法。本文还研究了预训练策略如何影响最终模型性能。在姿态估计和跟踪任务中，从COCO进行微调通常比从头开始训练表现更好，O2 pose的性能提高了1.7%。本文提出了新的指标OSPApose和OSPA2 pose，用于多人姿态估计和跟踪。本文得出结论，JRDB-Pose将有助于解决人类行为理解在人机交互和人类环境导航中的局限性，并通过提供大规模和高质量的注释来推进研究。


# Paper:278     ARCTIC：用于灵巧双手手-物体操作的数据集



#### 1. Title: 
ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation

#### 2. Authors: 
Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, Otmar Hilliges

#### 3. Affiliation: 
ETH Zurich, Switzerland (Zicong Fan)

#### 4. Keywords: 
Hand-object interaction, articulated objects, dataset, 3D annotations, motion reconstruction, interaction field estimation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2021_paper.html  Github: https://github.com/otmar-hilliges/arctic-capabilities

#### 6. Summary : 
- (1): The study of the physically consistent dynamics of hands and objects during manipulation has been under-researched in the hand pose estimation literature. Existing hand-object datasets are mostly limited to grasping of rigid objects and contain few examples of rich and dexterous manipulation of articulated objects. 
- (2): Past methods mostly focus on hand-only reconstructions or single-hand grasping interaction, with relatively little pose variation over time. The unconstrained interaction causes more variations in hand pose and contact than in existing datasets. The approach proposed in this paper is well motivated to enable the study of dexterous articulated hand-object manipulation.
- (3): The authors introduce ARCTIC, a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. They propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction, and (2) Interaction field estimation. They also provide baselines for both tasks on ARCTIC. 
- (4): The performance achieved by the methods in this paper supports their goals. The dataset enables the study of dexterous bimanual manipulation of articulated objects and provides detailed contact information between the hands and objects during manipulation. The proposed tasks and baselines can be used for future research in the field of hand-object interaction.
#### 7. 方法详细介绍：
本文提出了一个名为ARCTIC的数据集，其中包含2.1M帧的视频，配有准确的3D手部和物体网格以及详细的动态接触信息。该数据集使用高质量的运动捕捉系统进行捕捉，并包含来自八个第三人称allocentric视图和一个第一人称egocentric视图的视频。本文提出了两个新的手-物体交互任务：（1）一致的运动重建，旨在重建两只手和关节物体的3D运动，使它们的运动在时空上保持一致，（2）交互场估计，旨在从图像中估计密集的相对手-物体距离。本文分别介绍了两个基线模型ArcticNet和InterField，并在ARCTIC上进行了定性和定量评估，以便进行未来比较。本文还介绍了用于一致的运动重建的神经网络架构ArcticNet-SF，其中包括CNN编码器、手部解码器和物体解码器。

#### 8. 实验设置：
本文使用高质量的运动捕捉系统捕捉了准确的3D网格，通过将彩色相机与54个高分辨率Vicon MoCap相机同步，捕捉了旋转共享轴的两个刚性部分的物体，并将预扫描的人体和物体网格拟合到观察到的标记上。数据集包含10个参与者与11个关节物体的交互数据，共计2.1M个RGB图像。图像从多个同步和校准的视图中捕捉，包括8个静态allocentric视图和1个移动的egocentric视图。

#### 9. 实验结果和分析：
本文在ARCTIC数据集上评估了两个任务：一致的运动重建和交互场估计。对于一致的运动重建，本文从视频中重建了两只手和关节物体的3D运动，并要求重建的手-物体网格在物体关节运动和操作期间保持时空上一致的手-物体接触和运动。本文使用MANO表示手部姿态和形状，并使用扫描的物体网格、估计的旋转轴和标记-顶点对应关系构建了每个物体的3D模型。本文提出了一个名为ArcticNet-SF的神经网络架构，用于一致的运动重建，其中包括CNN编码器、手部解码器和物体解码器。本文定义了一致的运动重建的度量，包括接触偏差（CDev）、运动偏差（MDev）和加速度误差（ACC）。对于交互场估计，本文估计了每个手部顶点与物体之间的最近距离，并测量了平均距离误差和加速度误差。本文在ARCTIC上评估了两个基线模型：一致的运动重建和交互场估计。结果表明，ArcticNet-LSTM模型在手-物体接触和运动方面具有更加一致的时空性，并且具有更加平滑的运动。结果还表明，随着时间建模手-物体交互场会产生更准确的结果和更平滑的预测。本文还包括模型的定性结果，这些结果被可视化为相应手部或物体的网格上的热图。


# Paper:279     超越注意力令牌：融合令牌重要性和多样性的高效视觉Transformer



#### 1. Title: 
Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers

#### 2. Authors: 
Sifan Long, Zhen Zhao, Jimin Pi, Shengsheng Wang, Jingdong Wang

#### 3. Affiliation: 
第一作者：吉林大学计算机科学与技术学院，吉林大学教育部符号计算与知识工程重点实验室，中国吉林省长春市

#### 4. Keywords: 
Vision transformers, token pruning, token importance, token diversity, self-attention mechanism

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Long_Beyond_Attentive_Tokens_Incorporating_Token_Importance_and_Diversity_for_Efficient_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了视觉transformer中的token pruning问题，提出了一种同时考虑token importance和token diversity的token decoupling and merging方法，以提高模型的计算效率。

- (2):过去的方法主要关注保留local attentive tokens，但完全忽略了global token diversity的重要性。本文提出了一种新的方法，既保留最具有区分性的local tokens，又最大化token diversity。与现有方法相比，本文的方法在ImageNet数据集上取得了更好的性能。

- (3):本文提出的方法通过efficient token decoupling and merging来同时考虑token importance和token diversity。首先，根据class token attention将token分为attentive和inattentive两部分。然后，对inattentive tokens进行简化的density peak clustering算法，将相似的inattentive tokens聚类并将同一组中的tokens合并为一个新的token。同时，我们设计了一个简单的matching算法来融合homogeneous attentive tokens，以进一步提高计算效率。

- (4):本文的方法在DeiT-S和LV-ViT上取得了新的SOTA性能，同时在ImageNet上取得了更好的性能和更高的计算效率。
#### 7. 方法详细介绍：
本文提出了一种名为“Token Decoupling and Merging”的方法，以同时考虑Token的重要性和多样性。该方法包括四个子模块：Attentive Token Preservation、Inattentive Token Pack、Inattentive Token Clustering和Attentive Token Matching。其中，Clustering模块通过密度峰值聚类算法将相似的Inattentive Token聚类在一起，以保留Token的多样性。Attentive Token Matching模块通过匹配相同的Attentive Token来进一步减少模型的FLOPs，同时保持准确性。

#### 8. 实验设置：
本文使用ImageNet-1K数据集进行实验，包括1.28万张训练图像和5万张验证图像。使用Top-1分类准确率和FLOPs来评估模型的效率。在DeiT-T、DeiT-S、DeiT-B和LV-ViT-S等不同的ViT模型上应用Token Pruning方法，分别在DeiT-T/S/B模型的第4、7、10层和LV-ViT-S模型的第4、8、12层进行操作。使用与DeiT和LV-ViT原始论文相同的训练设置，并将余弦调度器纳入学习策略。所有模型在8个NVIDIA V100上从头开始训练300个epoch，使用单个NVIDIA V100 GPU进行吞吐量测量，批量大小固定为256。

#### 9. 实验结果与分析：
本文进行了消融实验以评估每个模块的有效性。聚类模块在保持率为0.7和0.5时将准确率分别提高了0.2%和0.8%。本文还比较了几种常见的Token Merging策略，聚类策略通常比其他Token Merging策略提高0.4%的准确率。本文提出了一种简化的高效DPC算法，在准确性和效率方面均优于其他策略。所提出的方法在不引入额外参数的情况下实现了准确性和FLOPs之间的SOTA性能平衡。


# Paper:280     修复噪声：解开源特征以实现可控领域翻译



#### 1. Title: 
Fix the Noise: Disentangling Source Feature for Controllable Domain Translation

#### 2. Authors: 
Dongyeun Lee, Jae Young Lee, Doyeon Kim, Jaehyun Choi, Jaejun Yoo, Junmo Kim

#### 3. Affiliation: 
第一作者：KAIST（韩国科学技术院）

#### 4. Keywords: 
Domain translation, Controllability, StyleGAN2, Feature matching loss, Disentangled feature space

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Fix_the_Noise_Disentangling_Source_Feature_for_Controllable_Domain_Translation_CVPR_2021_paper.html  Github: https://github.com/LeeDongYeun/FixNoise

#### 6. Summary : 
- (1):本文研究的是领域翻译中的可控性问题，旨在通过使用单个模型控制不同领域特征之间的关系。目前的方法需要额外的模型，计算成本高，且控制步骤有限，无法实现平滑过渡。本文提出了一种新的方法，通过在目标特征空间的解缠子空间中保留源特征，实现了更好的可控性和高质量的领域翻译。

- (2):过去的方法通常需要额外的模型，计算成本高，且控制步骤有限，无法实现平滑过渡。本文提出的方法通过在目标特征空间的解缠子空间中保留源特征，实现了更好的可控性和高质量的领域翻译。本文的方法是有动机的，因为源特征的保留对于领域翻译的成功非常重要。

- (3):本文提出了一种新的训练策略FixNoise，用于跨领域可控领域翻译。为了在单个模型中控制不同领域的特征，本文认为应该保留源特征，但将其与目标特征在模型继承空间中解缠。为了实现这一点，本文关注了StyleGAN2的噪声输入，该噪声输入在每个卷积之后添加，扩展了由潜在代码表达式组成的功能空间。为了仅将源特征保留到目标模型的特定子空间中，本文在应用简单的特征匹配损失时固定了噪声输入。解缠的特征空间使得我们的方法可以在单个模型中细粒度地控制保留的源特征，而不受控制步骤的限制，通过固定和随机噪声之间的线性插值实现平滑的跨领域控制。

- (4):本文的方法在跨领域特征控制方面取得了更好的结果，并在定量和定性上表现出更好的领域翻译性能。本文的方法可以在单个模型中实现对不同变换级别的精确控制，并且生成的图像比以前的方法更一致和逼真。
#### 7. 方法详细介绍：
本文提出了一种名为FixNoise的训练策略，用于跨域可控域翻译。该方法通过关注StyleGAN2中噪声输入扩展潜在表达的功能空间这一事实，将源特征和目标特征在目标模型的特征空间中进行解耦。该方法假设源特征可以映射到目标模型特征空间中的特定子空间中。为了实现这一点，该方法引入了一个锚点，即一个固定的噪声值，以及一个相应的子空间，称为锚定子空间。源特征仅映射到锚定子空间，而目标特征则在目标模型的整个特征空间中进行学习。该方法使用对抗损失和特征匹配损失与FixNoise一起训练生成器网络。噪声输入用于在目标模型的特征空间中解耦不同的域特征，并被赋予控制某些全局方面的角色。该方法还通过在锚点和其他随机采样的噪声之间进行插值，实现了源特征和目标特征之间的平滑过渡。

#### 8. 实验设置：
本文在几个源和目标设置下进行了实验，考虑源域和目标域之间的空间相似性。对于相似域设置，本文将FFHQ转移到MetFaces和AAHQ。对于远程域设置，本文将LSUN Church转移到WikiArt Cityscape。所有实验都在256×256分辨率的图像上进行。本文使用StyleGAN2-ADA的官方Pytorch实现以及在FFHQ和LSUN Church上训练的源模型和鉴别器的预训练权重。本文使用非饱和对抗损失进行对抗训练，并将损失权重设置为0.05。本文还使用样式混合正则化和路径长度正则化与总损失。批量大小设置为64，模型根据设置的不同图像数量进行训练。

#### 9. 实验结果和分析：
本文评估了所提出的方法在不同训练设置下进行跨域特征控制的有效性。本文报告了最佳的Fréchet Inception Distance（FID）并使用相同的网络快照测量了学习的感知图像补丁相似性（LPIPS）。实验结果表明，所提出的方法在所有设置中都优于基线方法，具有更好的图像质量和一致的过渡效果。本文还展示了所提出的方法通过在锚点和其他随机采样的噪声之间进行插值，实现了源特征和目标特征之间的平滑过渡。本文提供了可视化结果，展示了应用FixNoise时噪声输入的效果以及它如何改变生成图像的噪声输入的效果。

本文还将所提出的方法与无条件GAN方法和传统的域翻译方法进行了比较。评估指标包括FID、KID和PS。实验结果表明，所提出的方法在PS方面优于竞争方法，并在FID和KID方面达到了与StyleGAN2-ADA类似的性能。与域翻译方法相比，所提出的方法生成的图像最为逼真和适应，并保留了源特征。这些保留的源特征可以在单个模型中进行控制。


# Paper:281     TimeBalance：用于半监督动作识别的时间不变和时间不同的视频表示



#### 1. Title: 
TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition

#### 2. Authors: 
Ishan Rajendrakumar Dave, Mamshad Nayeem Rizve, Chen Chen, Mubarak Shah

#### 3. Affiliation: 
第一作者：Ishan Rajendrakumar Dave，中央佛罗里达大学计算机视觉研究中心

#### 4. Keywords: 
Semi-supervised learning, action recognition, self-supervised learning, temporally-invariant, temporally-distinctive

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_CVPR_2021_paper.html  Github: https://github.com/DAVEISHAN/TimeBalance

#### 6. Summary : 
- (1):本文研究半监督动作识别任务，提出了一种基于自监督视频表示学习的学生-教师半监督学习框架，旨在利用大规模未标记视频数据提供额外的监督信息。
 
- (2):现有的半监督学习方法主要依赖于硬输入归纳偏差，如使用两种模态（RGB和Optical-flow）或不同播放速率的两个流。本文提出的方法不依赖于这些硬设计选择，而是利用两种互补的自监督教师模型，即时间不变和时间不同的表示。本文方法在三个动作识别基准数据集上均取得了最先进的性能。

- (3):本文提出了一种基于学生-教师半监督学习框架的方法，其中教师监督包括使用时间不变和时间不同的自监督视频表示进行预训练。在半监督训练期间，我们根据未标记视频实例的性质对每个教师模型进行加权。我们通过提出的基于时间相似性的重新加权方案来实现这一点。本文方法在UCF101、HMDB51和Kinetics400等动作识别基准数据集上均取得了最先进的性能。

- (4):本文提出的方法在三个动作识别基准数据集上均取得了最先进的性能，证明了其有效性。本文方法的创新点在于利用两种互补的自监督教师模型，即时间不变和时间不同的表示，以及提出的基于时间相似性的重新加权方案。
#### 7. 方法详细介绍：
本文提出了一种半监督动作识别框架TimeBalance，采用了师生模型的方法。该框架利用两个教师模型：时间不变教师fI和时间有区别教师fD，这两个教师模型是通过自监督训练得到的。教师模型的自监督预训练包括最大化同一视频实例中两个不同剪辑的投影的一致性，同时最大化不同视频实例的剪辑的投影的不一致性。学生模型从两个来源获得监督：地面真实标签和教师监督。地面真实标签的监督使用标准的交叉熵损失。本文还提出了一种基于时间相似性的教师重新加权方案（TSTR），用于组合fI和fD的监督信息。

#### 8. 实验设置：
本文在三个数据集上进行了实验：UCF101、HMDB51和Kinetics400。对于UCF101和HMDB51，使用split-1进行训练和测试。对于Kinetics400，使用240k训练视频和20k验证视频的标准分割。输入剪辑分辨率设置为224×224，使用最常用的增强方法进行预训练和半监督训练。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的TimeBalance方法在UCF101、HMDB51和Kinetics400数据集上优于现有的半监督动作识别方法。该方法在UCF101上实现了83.3%的准确率，在HMDB51上实现了54.5%的准确率，在Kinetics400上实现了61.2%的准确率，这些结果是所有比较方法中最好的。

#### 论文总结：
本文提出了一种半监督动作识别框架TimeBalance，采用了师生模型的方法。该框架利用两个教师模型：时间不变教师fI和时间有区别教师fD，这两个教师模型是通过自监督训练得到的。本文还提出了一种基于时间相似性的教师重新加权方案（TSTR），用于组合fI和fD的监督信息。实验结果表明，所提出的TimeBalance方法在UCF101、HMDB51和Kinetics400数据集上优于现有的半监督动作识别方法。


# Paper:282     通用辐射场表示的局部隐式射线函数



#### 1. Title: 
Local Implicit Ray Function for Generalizable Radiance Field Representation

#### 2. Authors: 
Xin Huang, Qi Zhang, Ying Feng, Xiaoyu Li, Xuan Wang, Qing Wang

#### 3. Affiliation: 
第一作者：西北工业大学计算机学院
First Author Affiliation: School of Computer Science, Northwestern Polytechnical University

#### 4. Keywords: 
Neural rendering, radiance fields, novel view synthesis, generalizable, local implicit ray function

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Local_Implicit_Ray_Function_for_Generalizable_Radiance_Field_Representation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究神经渲染中的新视角合成问题，提出了一种通用的神经辐射场（NeRF）方法——LIRF（Local Implicit Ray Function）。当前的通用NeRF方法在渲染像素时只对应一个射线，当输入视角和渲染视角的分辨率不同时，会导致过度模糊或混叠。本文提出的LIRF通过聚合锥形视锥体内的信息来构建射线，从而解决了这个问题。

- (2):过去的方法包括基于图像的渲染和显式体积表示。基于图像的方法在密集采样的情况下表现良好，但在稀疏采样的情况下表现不佳。显式体积表示方法需要大量的存储空间，限制了渲染分辨率。通用NeRF方法在新场景中表现出色，但在不同分辨率或不同焦距的情况下，会产生模糊或混叠的问题。本文提出的LIRF方法通过聚合锥形视锥体内的信息来构建射线，从而解决了这个问题。

- (3):本文提出了一种新的通用方法，通过投射锥体来渲染像素，并在新视角合成方面优于现有方法。LIRF简化了对锥形视锥体的表示，并通过连续采样来支持任意上采样率的渲染。此外，本文还提出了一种基于Transformer的可见性权重估计模块，以减轻遮挡问题。实验结果表明，LIRF在多个场景中的新视角合成方面优于现有的通用NeRF方法。

- (4):本文的方法在多个场景中进行了实验，结果表明，LIRF在新视角合成方面优于现有的通用NeRF方法。本文的方法可以在任意上采样率下渲染高质量的新视角，并且通过Transformer-based可见性权重估计模块来减轻遮挡问题。
#### 7. 方法详细介绍：
本文提出了一种名为LIRF（Local Implicit Ray Function）的方法，用于预测体积辐射场并输出连续尺度的新视角。该方法包括以下步骤：
1. 从源图像中提取2D特征图。
2. 通过局部隐式射线函数获取目标射线上样本的图像特征。
3. 通过匹配特征块预测每个源视图的可见性权重。
4. 聚合来自不同源视图的局部图像特征，并将其映射到颜色和密度。
5. 通过体积渲染渲染目标像素。

LIRF方法通过聚合圆锥体来构建射线，并将3D坐标和圆锥体的特征作为输入，预测局部体积辐射场。连续采样的位置使LIRF能够任意上采样渲染射线，并在多个细节级别（抗模糊和抗锯齿）合成同一未见过的场景的新视角。此外，引入了基于自注意力变换器的可见性权重估计模块，以缓解遮挡问题。

#### 8. 实验设置：
本文使用三个真实数据集进行LIRF模型的训练：真实DTU多视图数据集、来自LLFF的真实前向数据集和IBRNet。所有190个场景用于训练，从LLFF数据集中选择了八个未见过的场景进行测试。在多尺度训练期间，所有输入视图的分辨率保持一致，而每个目标视图的分辨率从1到4倍的输入分辨率随机选择。本文通过渲染多个尺度的新视角（×0.5、×1、×2和×4）来评估模型。

#### 9. 实验结果与分析：
本文在×1尺度上比较了单尺度数据集上训练的LIRF与基线方法的定量结果。结果表明，LIRF方法在PSNR、SSIM和LPIPS方面优于其他方法。还进行了消融研究，以调查我们模型的关键模块的个体贡献。结果表明，我们的局部隐式射线函数对于渲染多尺度新视角至关重要，而考虑源射线的方向和估计可见性权重以缓解遮挡问题也非常重要。


# Paper:283     学习图像-文本匹配中实例间的语义关系



#### 1. Title: 
Learning Semantic Relationship among Instances for Image-Text Matching

#### 2. Authors: 
Zheren Fu, Zhendong Mao, Yan Song, Yongdong Zhang

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Image-text matching, cross-modal embedding, relation modeling, instance-level interaction

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2021_paper.html  Github: https://github.com/CrossmodalGroup/HREM

#### 6. Summary : 
- (1):本文研究了图像和文本之间的语义关系，提出了一种新的层次关系建模框架，旨在捕捉样本之间的实例级交互，以学习更具有区分度和鲁棒性的跨模态嵌入。

- (2):现有的图像-文本匹配方法通常只关注于捕捉特定模态中样本内部的片段级关系，而忽略了不同样本和模态之间的实例级交互。本文提出的框架通过显式地捕捉片段级和实例级关系，学习更好的跨模态嵌入。此外，本文提出了一种新的跨嵌入关联图，以及两种关系交互机制，以提高嵌入的表现力。

- (3):本文提出了一种新的层次关系建模框架（HREM），旨在捕捉样本之间的实例级交互，以学习更具有区分度和鲁棒性的跨模态嵌入。HREM通过显式地捕捉片段级和实例级关系，学习更好的跨模态嵌入。此外，本文提出了一种新的跨嵌入关联图，以及两种关系交互机制，以提高嵌入的表现力。

- (4):在Flickr30K和MS-COCO数据集上的实验结果表明，本文提出的方法在rSum指标上优于现有的最先进方法4％-10％。本文提出的方法可以更好地学习跨模态嵌入，从而提高图像-文本匹配的性能。
#### 7. 方法详细介绍：
本文提出了一种层次关系建模框架（HREM）用于图像-文本匹配，该框架捕捉单模态和样本内的片段级别关系以及跨不同模态和样本的实例级别关系，以学习更好的整体嵌入。该方法在跨模态检索方面实现了高精度和高效率，在两个基准测试上展示了其优越性。该论文还提供了消融研究以验证分层和实例级别关系建模的有效性，并表明所提出的方法可以显著提高测试集中具有语义歧义的难样本和具有语义稀缺性的少见样本的性能。

#### 8. 实验设置：
本文使用Flickr30K和MS-COCO数据集进行评估，图像使用预提取的区域特征，文本使用BiGRU或BERT-base提取特征。Flickr30K的批量大小设置为128，MS-COCO的批量大小设置为256，交互机制层数L=1，超参数为β=0.8，τ=0.5，λ=1.5，K=10，三元组损失的边际α=0.2。

#### 9. 实验结果和分析：
所提出的方法在Flickr30K和MS-COCO数据集上均优于所有最先进的基于嵌入的图像-文本匹配方法，R@K和rSum指标的差距令人印象深刻。该方法还在MS-COCO的1K和5K测试集上与现有最先进方法相比表现最佳。


# Paper:284     基于空间-时间-历史一致性学习的源自由视频域自适应



#### 1. Title: 
Source-Free Video Domain Adaptation with Spatial-Temporal-Historical Consistency Learning

#### 2. Authors: 
Kai Li, Deep Patel, Erik Kruus, Martin Renqiang Min

#### 3. Affiliation: 
NEC实验室

#### 4. Keywords: 
Source-free domain adaptation, video domain adaptation, consistency learning, spatio-temporal-historical consistency

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Source-Free_Video_Domain_Adaptation_With_Spatial-Temporal-Historical_Consistency_Learning_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究的是源自由视频域自适应问题，即如何使用未标记的目标数据来适应预训练的源模型。这个问题在实际应用中非常有用，因为访问源数据受到限制。 
- (2):过去的方法主要是针对图像领域的，而本文提出的方法则是针对视频领域的。现有的方法忽略了视频的独特性质，因此效果不佳。本文提出的方法则从空间、时间和历史的角度出发，充分利用视频的一致性学习，通过模拟空间和时间的变化来适应模型。 
- (3):本文提出了一种新的方法，即基于空间-时间-历史一致性学习的源自由视频域自适应方法。该方法通过对每个未标记的目标视频进行随机的空间和时间扰动来模拟空间和时间的变化，并通过鼓励模型在视频及其扰动版本上产生一致的预测来适应模型。 
- (4):本文的方法在各种源自由视频域自适应设置下均取得了最先进的性能，包括开放集设置、部分设置和黑盒设置。实验结果表明，本文的方法在各种设置下均取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于源自由视频域自适应（SFVDA）的方法，该方法使用标记视频的源数据集和未标记视频的目标数据集来学习自适应视频分类模型。该方法包括两个步骤：首先，在标准监督方式下学习标记源视频生成源模型；其次，通过强制空间-时间-历史一致性来适应目标域中的未标记数据。该方法还包括信息最大化损失，以鼓励模型进行自信和多样化的预测。该方法可以扩展到解决其他视频域自适应问题。

具体步骤如下：
1. 使用Temporal Relation Network作为模型，使用ResNet-50作为帧特征提取器。
2. 使用三个一致性技术：空间一致性（SC）、时间一致性（TC）和历史一致性（HC）。
3. SC和TC产生一个“hard”版本的剪辑，并鼓励模型克服困难因素并进行一致的预测。
4. HC加强了时间一致性。
5. 该方法高度灵活，可应用于各种SFVDA设置，包括开放式设置、部分设置和黑盒设置。

#### 8. 实验设置：
本文在四个基准测试集上进行了实验：UCF-HMDB、UCF-Kinetics、Jester和DailyDA。UCF-HMDB基准测试集包括来自UCF101和HMDB51数据集的视频，UCF-Kinetics基准测试集包括来自UCF101和Kinetics-600数据集的视频。Jester基准测试集是一个大规模手势数据集，DailyDA基准测试集包括来自ARID、HMDB51、Moments-in-Time和Kinetics数据集的视频。实验评估了不同域之间的自适应性能。

#### 9. 实验结果和分析：
本文提出的STHC模型在标准SFVDA设置以及其他自适应设置的小规模和大规模基准测试中均优于现有方法。消融实验结果表明，所有三种一致性学习技术都有助于提高性能。参数分析表明，大的alpha值会导致模型性能下降。在Jester基准测试中，对JT的测试视频的特征嵌入进行的t-SNE可视化表明，在自适应后，嵌入表现出更好的聚类结构。在UCF101和HMDB51数据集上的实验结果表明，本文提出的方法在所有SFVDA设置中均取得了最佳性能，优于现有方法。


# Paper:285     利用人类视频的可供性作为机器人的多功能表示



#### 1. Title: 
Affordances from Human Videos as a Versatile Representation for Robotics

#### 2. Authors: 
Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, Deepak Pathak

#### 3. Affiliation: 
第一作者：Shikhar Bahl，CMU和Meta AI

#### 4. Keywords: 
Robotics, Visual Affordances, Human Videos, Robot Learning, Egocentric Visual Understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文的研究背景是构建一个能够通过观察人类进行交互并学习的机器人，这一目标已经启发了几个视觉问题。然而，尽管在静态数据集上取得了一些成功的结果，但目前的模型如何直接在机器人上使用仍不清楚。
- (2):过去的方法主要是在人类视频数据集上进行测试，没有物理机器人或野外实验。本文提出了一种新的方法，利用人类行为的视频数据集，训练一个视觉效应模型，估计人类在场景中可能进行交互的位置和方式。本文的方法可以无缝地与四种机器人学习范式集成，包括离线模仿学习、探索、目标条件学习和强化学习的动作参数化。本文的方法被称为Vision-Robotics Bridge（VRB），旨在弥合视觉和机器人之间的差距。
- (3):本文提出了一种新的方法，利用人类行为的视频数据集，训练一个视觉效应模型，估计人类在场景中可能进行交互的位置和方式。本文的方法可以无缝地与四种机器人学习范式集成，包括离线模仿学习、探索、目标条件学习和强化学习的动作参数化。本文的方法被称为Vision-Robotics Bridge（VRB），旨在弥合视觉和机器人之间的差距。本文的创新点在于，将视觉效应重新思考为桥接视觉和机器人的一种手段，提出了一种新的机器人学习方法，可以在现实世界中进行大规模的实验。
- (4):本文的方法在4个真实世界环境、10个不同任务和2个机器人硬件平台上进行了广泛而严格的实验。本文的方法被证明优于其他最先进的人手-物体效应模型，并且在不需要任何模拟的情况下，在野外实现了高性能的机器人学习。本文的方法学习了机器人的良好视觉表示，同时在机器人学习中提供了很好的初始化。
#### 7. 方法详细介绍：
本文提出了一种名为Vision-Robotics Bridge（VRB）的方法，该方法利用人类交互的视频数据学习有用的可支配性，并将其部署在许多不同的机器人学习范例中。该方法采用接触点和接触后轨迹作为可视可支配性的表示。接触点是通过应用肤色分割来提取的，以找到与接触物体的边界框相交的手部段周围的所有点。然后，对接触点进行高斯混合模型（GMM）拟合，以鼓励预测的多模态性。接触后轨迹是通过找到手的像素空间位置，并使用单应性矩阵从tcontact到t′补偿人类头部/相机的自我运动来获得的。使用ResNet视觉编码器和反卷积层训练可支配性模型以预测接触点和接触后轨迹的提取标签。基于潜在表示训练轨迹预测网络，并对接触点周围的输入图像进行本地裁剪，以避免虚假相关性并实现更好的泛化。

#### 8. 实验设置：
本文在两个不同的机器人平台Franka Emika Panda臂和Hello Stretch移动机械手上，跨越8个不同的环境，对四种不同的机器人学习范例进行了实验，以评估所提出的可支配性模型的有效性。任务是使用目标图像为每个环境指定的，并使用收集的数据用于两种已建立的离线学习方法：k-最近邻（k-NN）和行为克隆。本文将所提出的VRB模型与先前的方法进行了比较，包括Hotspots，Hands as Probes（HAP）和随机探索。实验结果表明，VRB在各种任务的数据质量和性能方面均优于先前的方法。本文还分析了失败模式及其与先前工作的区别。

#### 9. 实验结果和分析：
本文通过大量严格的实际机器人实验对所提出的VRB方法进行了评估，跨越10个真实世界任务，4个环境和2个机器人硬件平台。其中许多任务在实验室环境之外的野外环境中执行。实验结果表明，VRB优于其他最先进的人手-物体可支配性模型，并在不需要任何模拟的情况下实现了高性能机器人学习。本文还观察到，可支配性模型作为副产品学习了良好的机器人视觉表示。


# Paper:286     利用潜在扩散模型从人脑活动中重建高分辨率图像



#### 1. Title: 
High-resolution image reconstruction with latent diffusion models from human brain activity

#### 2. Authors: 
Yu Takagi, Shinji Nishimoto

#### 3. Affiliation: 
大阪大学生命機能研究科

#### 4. Keywords: 
image reconstruction, diffusion models, fMRI, latent space, stable diffusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在通过人脑活动的高分辨率图像重建，探究大脑如何表征世界，以及计算机视觉模型与我们的视觉系统之间的联系。 
- (2):过去的方法主要使用深度生成模型，但是重建具有高语义保真度的逼真图像仍然是一个具有挑战性的问题。本文提出了一种基于扩散模型的新方法，称为稳定扩散，用于从功能磁共振成像（fMRI）获得的人脑活动中重建图像。与过去的方法相比，本文提出的方法不需要训练或微调复杂的深度学习模型，可以直接重建高保真度的高分辨率图像。 
- (3):本文提出了一种基于扩散模型的新方法，称为稳定扩散，用于从fMRI获得的人脑活动中重建图像。该模型利用其自动编码组件生成的潜在空间，进一步降低了计算成本，使训练和推理阶段的计算更加高效。本文还研究了LDM的不同组成部分（如图像的潜在向量Z、条件输入C和去噪U-Net的不同元素）与不同的脑功能之间的关系。 
- (4):本文提出的方法可以重建高保真度的高分辨率图像，而不需要训练或微调复杂的深度学习模型。在NSD数据集上的实验结果表明，本文提出的方法在重建高分辨率图像方面具有很高的性能。
#### 7. 方法详细介绍：
本文提出了一种基于扩散模型（Diffusion Model，DM）的方法，称为稳定扩散（Stable Diffusion），用于从人脑活动中重建高分辨率图像。该方法使用了一种称为潜在扩散模型（Latent Diffusion Model，LDM）的深度生成模型，该模型通过迭代去噪将从高斯噪声中采样的变量恢复为学习数据分布的样本。LDM通过自编码器压缩输入来克服像素级DM的计算开销。该模型首先使用图像数据训练自编码器，然后使用U-Net架构训练扩散模型以生成其潜在表示z。模型通过交叉注意力引用条件输入，使得与像素级DM相比，推理更轻量化，实现了非常高质量的文本到图像和图像到图像。本文描述了用于从fMRI信号重建图像并将LDM组件映射到脑活动的解码和编码分析。解码分析涉及从早期视觉皮层的fMRI信号中预测呈现图像X的潜在表示z，使用自编码器的解码器处理z以产生大小为320x320的粗略解码图像Xz，然后将其调整大小为512x512。然后，将Xz通过自编码器的编码器处理，然后通过扩散过程添加噪声。在更高（腹侧）的视觉皮层中，解码文本表示c。将添加噪声的粗略图像的潜在表示zT和解码的c作为输入，用于去噪U-Net生成zc。最后，zc作为自编码器的解码模块的输入，生成最终重建图像Xzc，大小为512x512。编码分析涉及构建不同LDM组件的全脑体素编码模型，并检查它们之间的差异，通过将每个特征解释的唯一方差映射到皮层来进行。 

#### 8. 实验设置：
本文使用了自然场景数据集（Natural Scenes Dataset，NSD）。NSD提供了从7-Tesla fMRI扫描仪中获取的数据，每个受试者在30-40个会话期间观看了10,000张图像的三次重复。本文分析了完成所有成像会话的8个受试者中的四个（subj01、subj02、subj05和subj07）。NSD实验中使用的图像来自MS COCO，并裁剪为425x425（如果需要）。

#### 9. 实验结果和分析：
本文提出的方法使用LDM可以从人脑活动中重建高分辨率图像，并且重建质量稳定准确。定量评估表明，使用zc重建的图像通常与仅使用z或c重建的图像相比，与不同指标相关的准确性值更高。本方法不仅捕捉了低级视觉外观，还捕捉了原始刺激的高级语义内容。编码模型分析表明，z在视觉皮层后部，即早期视觉皮层中产生了高的预测性能，而c在更高的视觉皮层中产生了最高的预测性能。与z相比，zc的独特方差随着噪声水平的增加而增加，表明图像的语义内容逐渐被强调。U-Net的逐层特征分析表明，第一层倾向于在早期视觉区域中表示细节，而瓶颈层对应于更腹侧的语义区域中的高阶信息。

#### 全文总结：
本文提出了一种基于稳定扩散的方法，用于从人脑活动中重建高分辨率图像。该方法使用了一种称为潜在扩散模型的深度生成模型，该模型通过迭代去噪将从高斯噪声中采样的变量恢复为学习数据分布的样本。该方法可以在不需要训练或微调复杂的深度生成模型的情况下，以高语义保真度重建高分辨率（512x512）图像。本文还提供了一种新的框架，用于理解DM及其内部机制。实验结果表明，该方法可以稳定准确地重建高分辨率图像，并且可以将LDM组件映射到脑活动。


# Paper:287     基于光谱增强矩形Transformer的高光谱图像去噪



#### 1. Title: 
Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising

#### 2. Authors: 
Miaoyu Li, Ji Liu, Ying Fu, Yulun Zhang, Dejing Dou

#### 3. Affiliation: 
第一作者：北京理工大学

#### 4. Keywords: 
Hyperspectral image, denoising, transformer, spectral enhancement, rectangle self-attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Spectral_Enhanced_Rectangle_Transformer_for_Hyperspectral_Image_Denoising_CVPR_2021_paper.html  Github: https://github.com/MyuLi/SERT

#### 6. Summary : 
- (1):本文研究的是高光谱图像去噪问题，由于高光谱图像具有空间和光谱域的自相似性，因此需要同时考虑这两个域的特性。
- (2):传统的基于模型的方法需要手动设计先验知识，而深度学习方法虽然表现出了强大的特征学习能力，但是卷积神经网络的局部响应限制了其感受野的大小。本文提出了一种基于Transformer的高光谱图像去噪方法，利用矩形自注意力机制来捕捉空间域的非局部自相似性，同时设计了一种光谱增强模块来提取全局低秩特性，以抑制噪声。这种方法在综合考虑空间和光谱域的特性的同时，具有较高的计算效率和较好的去噪效果。
- (3):本文提出了一种基于Transformer的高光谱图像去噪方法，其中包括矩形自注意力模块和光谱增强模块。矩形自注意力模块通过水平和垂直方向的矩形自注意力机制来捕捉空间域的非局部自相似性，而光谱增强模块则通过全局光谱记忆单元来提取全局低秩特性，以抑制噪声。这种方法在综合考虑空间和光谱域的特性的同时，具有较高的计算效率和较好的去噪效果。
- (4):本文在合成噪声高光谱图像和真实噪声高光谱图像上进行了实验，结果表明，所提出的方法在客观指标和主观视觉质量方面均优于现有的方法。
#### 7. 方法详细介绍：
本文提出了一种基于光谱增强矩形变换器（Spectral Enhanced Rectangle Transformer，SERT）的高光谱图像去噪方法。该方法包括两个主要组件：空间矩形自注意力模块和光谱增强模块。空间矩形自注意力模块通过将特征图分成多个非重叠矩形来探索空间域中的非局部相似性。光谱增强模块则利用可学习的记忆单元来探索HSI的全局光谱低秩特性。该模块将输入特征投影到秩为K的子空间中，并提取投影向量与存储在记忆单元中的低秩向量之间的对应系数以获得所需的低秩向量。同时，该模块还提供了跨空间矩形的交互，以保持信息丰富的光谱特征。

#### 8. 实验设置：
本文在两个数据集上进行了实验：Urban数据集和Realistic数据集。Urban数据集包含一个大小为307×307的图像，覆盖从400到2500 nm的210个波段。使用APEX数据集进行预训练，其中向干净HSI添加了从0到55的波段相关噪声水平。Realistic数据集包含59个带有配对干净HSI的噪声HSI。每个HSI包含从400 nm到700 nm的34个波段，空间分辨率为696×520像素。本文提供了不同方法在两个数据集上的定量和视觉比较，表明所提出的方法在处理真实噪声和保留纹理细节方面具有优越性。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的SERT方法在合成噪声HSI和真实噪声HSI上均优于BM3D、TDL和HSID-CNN等现有方法。在Urban数据集上，SERT方法的PSNR值比其他方法高出至少0.5 dB。在Realistic数据集上，SERT方法在保留纹理细节方面表现出色。此外，本文还进行了消融实验，证明了所提出的两个组件对于SERT方法的性能提升至关重要。


# Paper:288     OmniAL：一种用于无监督异常定位的统一CNN框架



#### 1. Title: 
OmniAL: A unified CNN framework for unsupervised anomaly localization

#### 2. Authors: 
Ying Zhao

#### 3. Affiliation: 
Ricoh Software Research Center (Beijing) Co., Ltd.（理光软件研究中心（北京）有限公司）

#### 4. Keywords: 
Unsupervised anomaly localization, anomaly synthesis, Dilated Channel and Spatial Attention (DCSA), DiffNeck, adversarial attacks.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_OmniAL_A_Unified_CNN_Framework_for_Unsupervised_Anomaly_Localization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是无监督异常定位和检测在工业制造过程中的重要性，由于缺乏异常样本，因此需要使用无监督方法进行异常检测和定位。

- (2):过去的方法通常需要为许多不同类别训练单独的模型，这种方法的模型存储和训练时间成本很高。而且，将一个模型用于多个类别会导致现有方法的异常定位性能严重下降。本文提出了一种名为OmniAL的统一CNN框架，用于无监督异常定位，该方法通过改进异常合成、重建和定位来解决上述问题。

- (3):本文提出了一种面板引导的合成异常数据方法，该方法通过控制每个训练样本的正常和异常区域的比例来防止模型学习相同的重建。为了增加多类分布的异常重建误差，OmniAL构建了一个配备Dilated Channel和Spatial Attention（DCSA）块的重建和定位子网络。为了更好地定位异常区域，OmniAL在重建和定位子网络之间使用了DiffNeck模块来探索多级差异。在15类MVTecAD和12类VisA数据集上的实验验证了OmniAL的优势，其单个统一模型分别实现了97.2/87.8的图像AUROC、98.3/96.6的像素AUROC和73.4/41.7的像素AP，用于异常检测和定位。此外，本文还首次尝试对无监督异常定位和检测方法针对不同级别的对抗攻击的鲁棒性进行全面研究。

- (4):本文的方法在15类MVTecAD和12类VisA数据集上实现了优异的性能，超过了现有统一模型的最新水平。OmniAL的单个统一模型分别实现了97.2/87.8的图像AUROC、98.3/96.6的像素AUROC和73.4/41.7的像素AP，用于异常检测和定位。本文的方法在无监督异常定位和检测任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
OmniAL是一种用于无监督异常检测和定位的统一CNN框架。它由一个无异常重建子网络和一个异常定位子网络组成。无异常重建子网络学习将合成的异常恢复为正常状态，而定位子网络使用DiffNeck模块通过探索重建数据和原始数据之间的差异来定位异常区域。该论文还介绍了一种面板引导的异常合成方法，该方法考虑了正常和异常的影响。该方法构建了5种类型的面板来控制正常和异常区域的比例，并根据融合权重图在所选区域中合成异常。该论文还使用了DCSA块来进一步提高性能。OmniAL的总目标定义为L = L2(J, Jr) + Lssim(J, Jr) + L2(I, Ir) + Lssim(I, Ir) + Lfl(S, Ss)，其中J是从无异常图像I计算出的JND图，Jr和Ir是重建的JND图和正常图像，S是异常定位的真实标签。模型使用实例归一化而不是批归一化来解决统一设置下的类别不一致性。

#### 8. 实验设置：
该论文在MVTecAD和VisA数据集上进行了广泛的实验，以展示所提出的OmniAL框架的有效性。该论文比较了OmniAL的图像AUROC与MVTecAD数据集上的最新技术，并取得了最佳性能。该论文还评估了OmniAL在VisA数据集上的性能，并显示其优于最先进的方法。该论文使用焦点损失来监督预测的异常定位，并将预测的异常定位图组合以获得最终的像素级异常定位图。该论文还表明，使用实例归一化而不是批归一化和使用DCSA块可以提高性能。

#### 9. 实验结果和分析：
所提出的方法OmniAL在MVTecAD和VisA数据集上均取得了优异的性能，比现有的方法表现更好。在像素AP方面，OmniAL比现有的基于重建的方法提高了24%以上。重建图像具有高保真度的外观，能够恢复正常区域的输入，并将异常恢复到期望的位置。重建质量的评估表明，OmniAL在统一设置下的PSNR和SSIM得分方面均优于Draem和JNLD。该论文还评估了所提出的方法对抗攻击的鲁棒性，并表明该方法对抗攻击的性能良好。


# Paper:289     BERT是否盲目？探究视觉-语言预训练对视觉语言理解的影响



#### 1. Title: 
Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding

#### 2. Authors: 
Morris Alper, Michael Fiman, Hadar Averbuch-Elor

#### 3. Affiliation: 
Morris Alper, Michael Fiman: Tel Aviv University, Israel
Hadar Averbuch-Elor: Bar-Ilan University, Israel

#### 4. Keywords: 
Vision-and-Language Pretraining, Visual Language Understanding, Multimodal Learning, Text Encoder Models, Natural Language Understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Alper_Is_BERT_Blind_Exploring_the_Effect_of_Vision-and-Language_Pretraining_on_CVPR_2021_paper.html  Github: https://github.com/morrisalper/IsBERTBlind

#### 6. Summary : 
- (1):本文研究了文本编码器模型在视觉语言理解方面的视觉推理能力，探究了视觉-语言预训练对文本编码器模型在隐含视觉推理的文本任务上的影响。

- (2):本文提出了一套视觉语言理解任务，用于探究文本编码器模型的视觉推理能力，同时提出了一种新的零样本知识探测方法，Stroop探测，用于在不需要预测头的情况下将模型应用于仅文本任务。本文发现，与单模态训练的文本编码器相比，多模态训练的文本编码器在视觉语言理解任务上表现更好，但在非视觉自然语言理解任务上表现不佳。

- (3):本文提出了一种新的零样本知识探测方法，Stroop探测，用于在不需要预测头的情况下将模型应用于仅文本任务。本文还提出了一套视觉语言理解任务，用于探究文本编码器模型的视觉推理能力。

- (4):本文的方法在视觉语言理解任务上表现更好，但在非视觉自然语言理解任务上表现不佳。本文的发现表明，预训练过程中暴露于图像的文本编码器模型具有内在的视觉推理知识，这种知识反映在需要隐含视觉推理的仅文本任务中。
#### 7. 方法详细介绍：
本文提出了一套视觉语言理解任务，用于评估模型理解带有隐含视觉上下文的语言的能力。作者比较了多模态训练的文本编码器（如CLIP）与BERT和其他单模态训练的文本编码器在VLU任务上的表现，并提出了一种新的零样本知识探测方法——Stroop探测，用于将CLIP等模型应用于仅文本任务而无需预测头。方法包括以下步骤：
1. 对于每个任务，使用探针方法进行探测，如MLM和Stroop探测。
2. 对于每个任务，使用相应的提示进行探测。
3. 对于每个任务，使用绝对值Pearson、Spearman和Kendall相关性之间的预测准确度和真实得分进行评估。

#### 8. 实验设置：
本文使用了多个数据集进行实验，包括COCO、ImageNet、ConceptNet、GloVe和GPT-2等。对于每个任务，使用相应的数据集进行训练和测试。实验使用了多个模型，包括BERT、RoBERTa、ALBERT、CLIP等。实验使用了多个评估指标，包括准确率、F1分数、Pearson相关系数、Spearman相关系数和Kendall相关系数等。

#### 9. 实验结果和分析：
本文的实验结果表明，多模态训练的文本编码器在视觉语言理解任务上表现优异，但在非视觉自然语言理解任务上表现不如单模态训练的文本编码器。作者认为，预训练过程中暴露于图像数据的文本编码器具有内在的视觉推理知识，这种知识在需要隐含视觉推理的仅文本任务中得到了体现。本文的研究结果为多模态学习环境下文本编码器的选择提供了原则性指导。


# Paper:290     用于视觉语言预训练的过滤、蒸馏和难负样本



#### 1. Title: 
Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training

#### 2. Authors: 
Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, Dhruv Mahajan

#### 3. Affiliation: 
Meta AI (美国Meta AI)

#### 4. Keywords: 
Vision-language pre-training, contrastive learning, filtering, distillation, hard negatives

#### 5. Paper: 
Paper: https://arxiv.org/abs/2105.08050 Github: https://github.com/facebookresearch/diht

#### 6. Summary : 
- (1):本文研究了对比学习在大规模噪声数据上进行的视觉语言预训练的三个方面的改进：数据集噪声、模型初始化和训练目标。
- (2):本文提出了一种名为CAT的过滤策略，可以显著减少数据集大小，同时在零样本视觉语言任务上实现了更好的性能。此外，本文提出了一种名为Concept Distillation的方法，利用强的单模态表示进行对比训练，而不增加训练复杂度，同时优于先前的工作。最后，本文修改了传统的对比对齐目标，并提出了一种重要性采样方法，以增加难负样本的重要性，而不增加额外的复杂度。
- (3):本文提出的DiHT方法在29个任务的广泛零样本基准测试中，相对于基线，在20个任务上取得了改进。此外，本文提出了一种新的方法，可以在零样本和少样本之间建立联系，从而在低数据范围内实现更好的性能。 
- (4):本文的方法在29个任务中取得了显著的改进，特别是在ViT-B/16架构上，相对于CLIP训练的LAION-2B数据集，我们在29个任务中的20个任务上实现了零样本性能的提高。同时，我们的方法在ImageNet1K上实现了巨大的改进，例如，我们的方法将5-shot top-1准确率提高了7%。
#### 7. 方法详细介绍：
本文提出了一种基于对抗训练的视觉-语言预训练方法，包括三个组件：CAT过滤、概念蒸馏和硬负样本对齐。CAT过滤通过过滤掉图像和标题不匹配或者标题过于简单的样本，提高了对齐的准确性。概念蒸馏通过辅助分类器将预训练的教师模型中的视觉概念蒸馏到图像编码器中，提高了图像编码器的表达能力。硬负样本对齐通过重采样难度更大的负样本，提高了对齐的鲁棒性。对于每个图像-文本对，本文最小化了硬负样本对齐损失和对象和属性预测的交叉熵损失。本文还提出了一种简单有效的方法，通过投影梯度来填补从零样本到少样本学习之间的性能差距。

#### 8. 实验设置：
本文使用了2.1B英文标题子集的LAION-5B数据集进行训练，通过CAT过滤后得到了1.98B张图像。作者还在PMD数据集上进行了实验，该数据集包含了各种公共数据集的训练集。作者使用了4B、8B、16B和32B的样本进行训练，使用了Vision Transformer（ViT）进行图像编码和Text Transformer进行标题编码。作者使用Adam优化器和余弦学习率调度，并采用混合精度训练来加速训练和节省内存。

#### 9. 实验结果和分析：
本文的方法在各种基准任务上均优于基线模型，包括零样本基准测试。该方法还表现出更好的分布偏移鲁棒性，特别是在ImageNet-A和ObjectNet数据集上。在较小的PMD数据集上，该方法证明了在低资源训练环境下的有效性。本文还提出了一种简单有效的方法，通过投影梯度来填补从零样本到少样本学习之间的性能差距。本文在29个任务的零样本基准测试上进行了广泛的实验，包括17个图像分类、10个跨模态检索和2个视觉问答任务。作者提供了对设计选择的广泛消融，并与流行的零样本基准测试上的最先进方法进行了比较。


# Paper:291     多模态互补性的主动探索在少样本动作识别中的应用



#### 1. Title: 
Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition

#### 2. Authors: 
Yuyang Wanyan, Xiaoshan Yang, Chaofan Chen, Changsheng Xu

#### 3. Affiliation: 
第一作者：中国科学院自动化研究所，多模态人工智能系统国家重点实验室

#### 4. Keywords: 
Few-shot action recognition, multimodal complementarity, active learning, knowledge distillation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wanyan_Active_Exploration_of_Multimodal_Complementarity_for_Few-Shot_Action_Recognition_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了多模态少样本动作识别任务，提出了一种主动学习的方法，通过发现每个查询样本的可靠模态来提高少样本推理过程的准确性。
 
- (2):现有的少样本动作识别方法主要依赖于有限的单模态数据，而多模态信息相对未被充分探索。本文提出了一种主动多模态少样本动作识别框架，可以根据任务相关的上下文信息主动找到每个样本的可靠模态，以提高少样本推理过程的准确性。本文的方法通过主动学习的方式，探索了多模态数据的互补性，提高了少样本动作识别的性能。 

- (3):本文提出了一种主动多模态少样本动作识别框架，包括主动样本选择和主动互相蒸馏两个模块。主动样本选择模块可以根据模态特定的后验分布将可靠模态和不可靠模态的查询样本分为两组。主动互相蒸馏模块可以通过双向知识蒸馏，从可靠模态中提取任务特定的知识，指导不可靠模态的表示学习。在元测试阶段，本文采用自适应多模态推理方法，通过自适应融合不同模态的后验分布来进行少样本推理。 

- (4):本文在四个公共基准测试集上进行了广泛的实验，结果表明，与现有的单模态和多模态方法相比，本文的方法取得了显著的性能提升。
#### 7. 方法详细介绍：
本文提出了一种主动多模态少样本动作识别框架，称为AMFAR。该框架采用元学习范式，包括元训练和元测试两个阶段。在元训练阶段，框架从多模态视频数据集中随机构建多个元任务，学习一个元学习器，该元学习器可以很好地泛化到新的动作类别。在元测试阶段，框架以类似于元训练的方式构建每个测试任务的支持集和查询集。该框架由四个模块组成：主动样本选择、主动互相蒸馏、自适应多模态推理和模态特定的骨干网络。框架采用主动样本选择来选择具有两种模态可靠性差异的查询样本，并将其组织成两组，即RGB主导组和Flow主导组。框架进一步采用主动互相蒸馏来通过双向蒸馏机制从可靠模态中捕获判别性任务特定知识，以增强不可靠模态的表示学习。在元测试阶段，框架采用自适应多模态推理来基于模态特定的后验分布做出自适应融合决策，更多地关注可靠模态。最后，框架采用模态特定的骨干网络来获取每个模态的查询样本表示和支持样本原型。

#### 8. 实验设置：
本文在四个公共数据集上进行了实验，包括Kinetics、SSv2、UCF101和HMDB51。将提出的方法与现有的单模态和多模态方法进行比较，包括STRM、STRM-F和STRM-LF。在N-way K-shot设置中评估性能，其中N从1到5，K固定为5。分析目标函数（8）中的平衡权重λ以找到最佳性能。

#### 9. 实验结果和分析：
本文提出的AMFAR框架在各个数据集和少样本设置下均优于现有的单模态和多模态方法，证明了探索不同模态之间的互补性在少样本动作识别中的有效性。文中还进行了消融实验，分析了AMFAR的三个关键组件（ASS、AMD和AMI）对两个具有挑战性的基准测试的影响。结果表明，这些组件在提出的方法中非常重要。最后，文中研究了提出的主动互相蒸馏的有效性，并表明它提高了AMFAR的性能。


# Paper:292     基于退火的标签传递学习用于开放世界目标检测



#### 1. Title: 
Annealing-based Label-Transfer Learning for Open World Object Detection

#### 2. Authors: 
Yuqing Ma, Hainan Li, Zhange Zhang, Jinyang Guo, Shanghang Zhang, Ruihao Gong, Xianglong Liu

#### 3. Affiliation: 
Beihang University (北航)

#### 4. Keywords: 
Open world object detection, Label-Transfer Learning, Annealing-based, Unknown recognition, Equilibrium Index

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ma_Annealing-Based_Label-Transfer_Learning_for_Open_World_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/DIG-Beihang/ALLOW.git

#### 6. Summary : 
- (1):本文研究开放世界目标检测（OWOD）的问题，即在训练集中未出现的未知目标的检测问题。传统的目标检测模型都是在理想的封闭世界假设下进行的，即被检测的类必须在训练阶段进行标记和给定。然而，实际情况下，目标检测系统很可能会遇到未知目标，这些目标在训练阶段没有出现过。因此，本文提出了一种新的方法来解决OWOD问题。

- (2):以往的OWOD方法都是手动设计未知目标的选择策略，从背景中选择未知提案，这样会带来不确定性。本文提出的方法是将目标检测视为对象级特征交织过程，通过卷积操作将未知特征传播到已知提案中，并通过标签传递学习范式将已知和未知特征分离，从而促进已知和未知识别。本文提出了一种简单而有效的退火标签传递框架，该框架充分利用已知提案来减轻不确定性。此外，本文还引入了一种度量标准Equilibrium Index来全面评估OWOD模型的有效性。

- (3):本文提出了一种基于退火的标签传递学习框架，该框架充分利用所有已知提案来提取未知特征，从而促进已知和未知类的协同学习。具体来说，标签传递学习将有意义的未知特征从所有已知提案中分离出来，以实现未知识别。由于直接分离已知提案内的特征是非常困难的，因此将已知标签转移到未知类中，从而将其分为两个类，以指导模型分离未知和已知特征的学习。本文还设计了一个锯齿退火调度策略，以调整分离程度，以鼓励已知和未知类的协同学习。

- (4):本文在常用数据集上进行了广泛的实验，证明了所提出方法的有效性。具体来说，与其他最先进的方法相比，本文模型在未知mAP性能上取得了显著提高（200%的提升），同时在已知检测性能上也更好。
#### 7. 方法详细介绍：
本文提出了一种基于标签传递学习和锯齿退火策略的开放世界目标检测方法。该方法分为形成阶段和扩展阶段。在形成阶段，模型在已知类别的one-hot监督下训练，去除未知分类器的参数，形成信息丰富但语义混合的特征。在扩展阶段，为了促进未知类别的学习，将解缠度突然改变，然后逐渐降低，以鼓励未知和已知类别的共同学习。模型只在分类器上进行微调，以避免破坏有用的特征。该方法在Pascal-VOC和MS-COCO数据集上进行了实验，取得了优于其他开放世界目标检测方法的性能。

#### 8. 实验设置：
本文在Pascal-VOC、MS-COCO、COCO、OpenImages和LVIS数据集上进行了实验。模型使用PyTorch框架，在单个NVIDIA V100 GPU上训练，批量大小为16，训练24个epoch，使用随机梯度下降优化器，动量为0.9，权重衰减为0.0001。学习率初始化为0.01，在第16和22个epoch时降低10倍。

#### 9. 实验结果和分析：
本文提出的方法在各个数据集上均取得了优于其他开放世界目标检测方法的性能。新提出的评估指标Equilibrium Index (EI)能够全面评估已知和未知类别的检测性能。与之前的最优方法相比，本文方法的未知mAP性能提高了200%，已知检测性能也更好。


# Paper:293     超球嵌入用于点云完成



#### 1. Title: 
Hyperspherical Embedding for Point Cloud Completion

#### 2. Authors: 
Junming Zhang, Haomeng Zhang, Ram Vasudevan, Matthew Johnson-Roberson

#### 3. Affiliation: 
Junming Zhang, Haomeng Zhang, Ram Vasudevan: University of Michigan (密歇根大学)
Matthew Johnson-Roberson: Carnegie Mellon University

#### 4. Keywords: 
Point cloud completion, hyperspherical embedding, encoder-decoder architecture, multi-task learning, stable training

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Hyperspherical_Embedding_for_Point_Cloud_Completion_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究点云完成任务，即从部分观测中预测对象的完整形状。由于真实世界中的3D测量通常是不完整的，因此点云完成任务变得至关重要。
- (2):过去的方法通常采用编码器-解码器结构，其中编码器被训练以提取嵌入，然后由解码器生成预测。然而，学习到的嵌入在特征空间中具有稀疏分布，这导致测试期间的泛化结果更差。为了解决这些问题，本文提出了一个超球形模块，将来自编码器的嵌入转换和归一化为单位超球面上。通过该模块，输出超球形嵌入的大小和方向被解耦，仅优化方向信息。本文理论分析了超球形嵌入，并表明它可以通过更稳定的训练、更广泛的学习率范围和更紧凑的嵌入分布实现更好的点云完成。实验结果表明，该方法在单任务和多任务学习中都能够提高点云完成的一致性，证明了该方法的有效性。
- (3):本文提出了一个超球形模块，用于点云完成任务。该模块将来自编码器的嵌入转换和归一化为单位超球面上，仅保留方向信息。本文理论分析了超球形嵌入的效果，并表明它可以通过更稳定的训练、更广泛的学习率范围和更紧凑的嵌入分布实现更好的点云完成。此外，本文还探讨了点云完成与其他语义任务的联合训练，并表明超球形嵌入可以帮助调和点云完成和其他语义任务之间的学习冲突。
- (4):本文在多个公共数据集上展示了所提出方法的改进效果，证明了该方法的有效性。在单任务和多任务学习中，该方法都能够提高点云完成的一致性。
#### 7. 方法详细介绍：
本文提出了一种超球嵌入方法，用于解决点云补全任务中嵌入分布稀疏的问题。该方法包含两个层次：多层感知机（MLP）层和归一化层。编码器的输出首先通过MLP层进行变换，然后通过归一化层进行约束，使特征向量位于超球面上。该方法可以与现有的编码器-解码器结构相结合。本文还从理论上分析了超球嵌入的效果，并表明它可以实现更稳定的训练，具有更广泛的学习率范围和更紧凑的嵌入分布。超球嵌入还应用于多任务学习中，有助于在训练中协调点云补全和其他语义任务之间的学习冲突。

#### 8. 实验设置：
本文在几个公共数据集上评估了所提出方法的有效性，包括ShapeNet、ModelNet40和MVP。实验在一台工作站上进行，该工作站配备了Intel Xeon CPU、128GB RAM和NVIDIA Titan Xp GPU。实现基于PyTorch，使用Adam优化器进行训练，学习率为0.001。

#### 9. 实验结果和分析：
本文在MVP数据集上展示了不同补全方法的性能。在提供的测试集上，报告了Chamfer距离，乘以104。使用所提出的超球模块，所有现有的补全方法都得到了一致的改进，平均类别Chamfer距离降低了3∼9%。本文还在GraspNet数据集上评估了超球嵌入方法在多任务学习场景中的有效性。超球嵌入对三个指标都有明显的改进。本文还进行了详细的消融研究，验证了超球模块的设计，并分析和可视化了超球嵌入对多任务学习的影响。本文还展示了超球嵌入方法在点云补全任务中的有效性，包括单任务和多任务学习。超球嵌入方法在单任务学习中对分类的影响中性，而在多任务学习中，它有助于协调点云补全和其他语义任务之间的学习冲突。


# Paper:294     基于相似度度量学习的RGB-IR跨模态群体再识别



#### 1. Title: 
Similarity Metric Learning For RGB-Infrared Group Re-Identification

#### 2. Authors: 
Jianghao Xiong, Jianhuang Lai

#### 3. Affiliation: 
第一作者：中山大学计算机科学与工程学院

#### 4. Keywords: 
Group re-identification, RGB-infrared, metric learning, Closest Permutation Matching, Relation-aware Module

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2020_paper.html  Github: https://github.com/WhollyOat/CM-Group

#### 6. Summary : 
- (1):本文研究的是RGB-IR跨模态群体再识别问题，即如何在不同摄像头视角下识别出同一群体的人员。 
- (2):过去的方法主要针对RGB图像，而RGB-IR跨模态匹配问题尚未得到研究。本文提出了一种度量学习方法Closest Permutation Matching (CPM)，并引入了Relation-aware Module (RAM)来解决无标签情况下的群体再识别问题。 
- (3):本文提出了一种基于度量学习的方法，将每个群体建模为一组单人特征，然后提出了Closest Permutation Distance (CPD)度量来衡量两组特征之间的相似性。CPD是一种新的度量方式，可以解决群体布局变化的问题。在无标签情况下，我们提出了Relation-aware Module (RAM)来提取群体成员之间的关系，生成伪排序，并指导网络学习群体内部的内在排序。 
- (4):本文构建了一个新的大规模RGB-IR跨模态群体再识别数据集CM-Group，并在该数据集上进行了广泛的实验。实验结果表明，所提出的模型在RGB-IR跨模态群体再识别任务上取得了良好的性能。
#### 7. 方法详细介绍：
本文提出了一种称为最近排列匹配（Closest Permutation Matching，CPM）的方法，将一个群体建模为一组人，并通过特征排列和距离插值来计算两个图像的相似度。该方法是置换不变的，并且对群体成员的更改具有抵抗力。此外，本文提出了关系感知模块（Relation-Aware Module，RAM），从视觉关系中学习群体成员的内在顺序，为无序特征集分配顺序规则。学习到的关系提供了一种鲁棒的群体表示，对摄像机变化和模态变化具有不变性。

#### 8. 实验设置：
本文使用了CM-Group数据集，包含427个群体的30946张图像，其中有1013个标注人物和72254个边界框。数据集被分为训练集和测试集，比例为1:3。训练集包含233个身份的17282个边界框，测试集包含780个身份的54972个边界框。评估协议包括累积匹配特征（Cumulative Matching Characteristics，CMC）曲线和平均精度（mean Average Precision，mAP）。测试集由摄像机1-3的RGB图像作为画廊集，摄像机4-6的红外图像作为查询集。实验在单个NVIDIA A100 GPU上进行。

#### 9. 实验结果与分析：
本文提出的模型CPM、RAM和CM-Group数据集的优越性得到了实验结果的证明。CPM框架使用CPD来衡量群体相似度，避免了聚合函数给单人特征添加噪声。RAM模块提取群体内在关系和顺序。CM-Group数据集经过精心设计，涵盖了许多真实场景中的挑战性情况。CPM框架在约8小时内实现了90.10%的Rank-1准确率和89.62%的mAP，而MACG需要约41小时才能实现84.00%的Rank-1准确率和59.70%的mAP。MPANet和CPD模块共同显著提高了21.68%和8.87%的Rank-1准确率和mAP。RAM模块提高了0.50%的Rank-1准确率，但降低了0.69%的mAP。MPANet和RAM模块共同显著提高了23.18%和19.76%的Rank-1准确率和mAP。


# Paper:295     基于可逆神经网络的大容量和灵活的视频隐写术



#### 1. Title: 
Large-capacity and Flexible Video Steganography via Invertible Neural Network

#### 2. Authors: 
Chong Mou, Youmin Xu, Jiechong Song, Chen Zhao, Bernard Ghanem, Jian Zhang

#### 3. Affiliation: 
第一作者：北京大学深圳研究生院

#### 4. Keywords: 
Video steganography, invertible neural network, large-capacity, flexibility, key-controllable

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mou_Large-Capacity_and_Flexible_Video_Steganography_via_Invertible_Neural_Network_CVPR_2021_paper.html  Github: https://github.com/MC-E/LF-VSN

#### 6. Summary : 
- (1):本文研究的是视频隐写术，旨在将秘密数据隐蔽地嵌入到覆盖视频中，并通过解码协议在接收端恢复秘密数据。 
- (2):传统的视频隐写方法通常采用空间域或变换域的手动设计，但是这些方法的容量和隐蔽性都较低，容易被隐写分析方法破解。近年来，一些深度学习方法被提出来提高隐写容量和性能。然而，这些方法大多数都是针对低容量隐写的，而多视频隐写的研究还很少。本文提出了一种大容量和灵活的视频隐写网络（LF-VSN），通过可逆的管道实现多个视频的隐藏和恢复，具有大容量、可控密钥和可扩展性的优势。 
- (3):本文提出了一种可逆神经网络（INN）的视频隐写方法，可以在一个模型和相同的参数下隐藏/恢复多个（最多7个）秘密视频。同时，我们提出了一个可控密钥方案，使不同的接收者可以通过特定的密钥从同一覆盖视频中恢复特定的秘密视频。此外，我们还提出了一种可扩展的嵌入模块，利用单个模型和单个训练会话在覆盖视频中隐藏可变数量的秘密视频。 
- (4):本文的方法在大容量和灵活性方面取得了显著的改进，具有高安全性、大容量和灵活性。实验结果表明，我们的方法在视频隐写性能方面取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于可逆神经网络（INN）的大容量、灵活的视频隐写网络（LF-VSN）。该方法由前向隐藏和后向恢复两部分组成。在前向隐藏中，通过滑动窗口和频率级联将秘密视频嵌入到覆盖视频中。在后向恢复中，使用可逆神经网络从隐写视频中提取秘密视频。该方法还包括一个冗余预测模块，用于提高隐藏容量。具体步骤如下：
1. 前向隐藏：将秘密视频分为多个组，通过滑动窗口将每个组嵌入到覆盖视频中。在每个滑动窗口中，将秘密视频组和覆盖视频组级联在一起，形成一个新的视频组。通过多个可逆块将新的视频组转换为隐写视频组。
2. 后向恢复：使用相同的可逆神经网络从隐写视频中提取秘密视频。将隐写视频组输入到可逆神经网络中，通过多个可逆块将隐写视频组转换为秘密视频组。
3. 冗余预测模块：在前向隐藏中，通过冗余预测模块预测隐写视频组的冗余信息，从而提高隐藏容量。

#### 8. 实验设置：
本文使用Vimeo-90K数据集进行训练，并从测试集中选择了200个序列进行测试。在训练过程中，将视频随机裁剪为144×144大小，并进行随机水平和垂直翻转以进行数据增强。使用Adam优化器，批量大小为16，权重衰减因子设置为1×10−12。初始学习率为1×10−4，每30K次迭代后减半，总迭代次数为250K。

#### 9. 实验结果与分析：
本文比较了LF-VSN与其他方法在单个视频隐写和多个视频隐写方面的性能。评估包括前向隐藏中的隐写质量和后向恢复中的秘密质量。LF-VSN在保持可接受的复杂度的同时，实现了最佳的隐写和秘密性能。本文还提出了多个视频隐写的可控方案，使不同的接收者可以通过特定的密钥恢复特定的秘密视频。LF-VSN在可控方案中具有大容量（最多6个视频）和优异的性能（> 30dB）。本文还提出了多个视频隐写的可扩展方案，可以使用单个模型将不同数量（最多7个）的秘密视频隐藏到覆盖视频中。LF-VSN在使用单个模型隐藏不同数量的秘密视频时具有优异的性能（> 31dB）。本文还评估了反隐写分析，并表明LF-VSN具有良好的安全性。本文还进行了LF-VSN中不同组件的消融研究，包括滑动窗口大小、可逆块数量、频率级联和冗余预测模块。


# Paper:296     基于变分信息瓶颈的弱监督数字病理学全幻灯片图像分类任务特定微调



#### 1. Title: 
Task-specific Fine-tuning via Variational Information Bottleneck for Weakly-supervised Pathology Whole Slide Image Classification

#### 2. Authors: 
Honglin Li, Chenglu Zhu, Yunlong Zhang, Yuxuan Sun, Zhongyi Shui, Wenwei Kuang, Sunyi Zheng, Lin Yang

#### 3. Affiliation: 
第一作者：浙江大学计算机科学与技术学院
其他作者：浙江大学、浙江大学计算机科学与技术学院、浙江大学工学院、浙江大学、浙江大学计算机科学与技术学院、浙江大学、Westlake University Research Center for Industries of the Future and School of Engineering、Westlake University

#### 4. Keywords: 
Whole Slide Image, Multiple Instance Learning, Fine-tuning, Information Bottleneck, Self-supervised Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Task-Specific_Fine-Tuning_via_Variational_Information_Bottleneck_for_Weakly-Supervised_Pathology_CVPR_2021_paper.html  Github: https://github.com/invoker-LL/WSI-finetuning

#### 6. Summary : 
- (1):本文研究数字病理学全幻灯片图像分类的任务，该任务面临计算成本高和监督有限的问题。为了解决这些问题，本文提出了一种基于信息瓶颈理论的全幻灯片图像微调框架，该框架可以找到全幻灯片图像的最小充分统计量，从而支持我们仅依赖全幻灯片图像级别的弱标签微调骨干网络，以获得任务特定的表示。

- (2):过去的方法主要利用ImageNet预训练模型来获得表示，但是由于领域差异大，可能会丢失关键信息，从而限制了先前WSI方法的准确性。最近的方法利用自监督学习来学习好的特征表示，但是这些任务无关的特征受到SSL代理目标的支配，因此只能略微优于IN-1K预训练。本文提出了一种微调方案，将任务无关的表示转换为任务特定的表示。本文的方法仅依赖于WSI级别的注释，类似于最近的SSL方法。本文的方法只利用不到1％的全补丁注释来实现与对手的竞争性准确性。

- (3):本文提出了一种基于信息瓶颈的WSI微调框架，该框架可以找到WSI的最小充分统计量，从而支持我们仅依赖WSI级别的弱标签微调骨干网络，以获得任务特定的表示。本文的方法可以与SSL预训练相结合，从而将SSL任务无关的表示转换为任务特定的表示。本文的方法还可以与多种训练时增强相结合，从而在具有域偏移的各种真实世界或模拟数据集中获得更好的泛化性能。

- (4):本文的方法在五个病理WSI数据集上进行了评估，实验结果表明，与先前的方法相比，本文的方法在准确性和泛化性能方面都有显着提高。本文的方法在WSI-MIL任务上取得了最佳性能，支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种基于信息瓶颈理论的弱监督病理全切片图像分类的高效微调框架。该框架通过最小化Kullback-Leibler散度来过滤任务无关的实例，从而找到WSI的最小充分统计量，使得骨干网络只依赖于WSI级别的弱标签进行任务特定的表示微调。该方法可以与SSL预训练相结合，将任务不可知的表示转换为任务特定的表示。该方法仅依赖于WSI级别的注释，类似于最近的SSL方法。该方法可以结合多种训练时增强方法，从而在具有域漂移的各种真实世界或模拟数据集中实现更好的泛化。

#### 8. 实验设置：
本文在五个病理WSI数据集上评估了他们提出的方法，包括Camelyon-16、TCGA-BRCA、LBP-CECA、Camelyon-16-C和Camelyon-17。预处理步骤包括使用HSV、模糊、阈值和轮廓方法来定位每个WSI中的组织区域。从组织区域中提取大小为256×256的非重叠补丁。使用AdamW优化器对骨干网络进行25个epoch的微调，WSI的批大小为512，学习率为1e-5，WSI头部的学习率为1e-3。

#### 9. 实验结果和分析：
本文将提出的方法与几种经典的WSI-MIL方法进行比较，并在AUC指标下取得了一致的改进。CLAM-SB相对于IN-1K预训练的基准FT特征在Camelyon-16、TCGA-BRCA和LBP-CECA上分别实现了9.26％、6.37％和5.47％的性能相对增长。在相同的条件下，TransMIL和DTFD-MIL在所有三个数据集上实现了新的SOTA，并且可以获得更好的相对改进，特别是在LBP-CECA中，相对于CLAM-SB增长了8.00％和8.85％。结果显示在F1指标下也有类似的改进。在Camelyon-16中，使用Max-pooling聚合效果更好，而在TCGA-BRCA中，使用Mean-pooling更好。本文还将SSL与提出的框架相结合，进一步提高了性能。SSL方法包括对比学习和数据增强方法，如MoCo和DINO。实验结果表明，相对于vanilla IN-1K，SSL方法可以实现7.85％和7.39％的一致增长。所有方法的泛化能力在微调和IN-1K特征之间进行比较。结果表明，在F1指标下，使用微调特征的WSI-MIL模型的分类能力可能比排名（使用AUC衡量）要弱得多。提出的方法在真实世界病理诊断中具有强大的潜力，具有更好的性能、更快的收敛速度和注释效率。


# Paper:297     PLA: 基于语言的开放词汇3D场景理解



#### 1. Title: 
PLA: Language-Driven Open-Vocabulary 3D Scene Understanding

#### 2. Authors: 
Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi

#### 3. Affiliation: 
Runyu Ding: 香港大学电机电子工程系
Jihan Yang: 香港大学电机电子工程系
Chuhui Xue: 字节跳动
Wenqing Zhang: 字节跳动
Song Bai: 字节跳动
Xiaojuan Qi: 香港大学电机电子工程系

#### 4. Keywords: 
3D scene understanding, open-vocabulary, vision-language, semantic segmentation, instance segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2021_paper.html  Github: https://github.com/dingry/PLA

#### 6. Summary : 
- (1):本文研究的是3D场景理解中的开放词汇问题，即如何识别和定位未标注类别。传统的深度学习模型只能理解数据集中的语义类别，无法识别开放世界中的未知类别，这限制了它们在实际应用中的适用性。此外，3D数据集的注释成本也很高，这使得依赖人工劳动来覆盖所有现实世界类别变得不可行。因此，本文提出了一种基于语言的3D开放词汇框架，通过多视图图像的字幕来建立3D和语言之间的显式关联，从而实现零样本3D场景理解。

- (2):过去的方法主要是将3D数据投影到2D模态，如RGB图像和深度图，使预训练的VL基础模型能够处理2D数据并实现对象级别的开放词汇识别。然而，这些方法存在多个RGB图像和深度图像来表示一个3D样本，这在训练和推理过程中会产生重大的计算和内存成本。此外，从3D到2D的投影会导致信息损失，从而导致性能下降。因此，本文提出了一种新的方法，即通过预训练的VL基础模型来为3D数据提供字幕，从而建立3D和语言之间的显式关联。

- (3):本文提出了一种基于点-语言关联的方法，即PLA，通过利用多视图图像和VL基础模型来建立3D和语言之间的显式关联。为了从字幕中学习语言感知嵌入，本文设计了分层的3D-字幕对，利用3D场景和多视图图像之间的几何约束。最后，通过对比学习，模型学习连接3D和文本的语言感知嵌入，从而实现开放词汇任务。本文的方法不仅在开放词汇语义和实例分割方面显著优于基线方法，而且在具有挑战性的零样本领域转移任务上表现出了强大的可迁移性。

- (4):本文的方法在ScanNet和S3IDS数据集上进行了实验，证明了其在基于类别移位的领域内开放词汇任务中的有效性，超过基线方法25.8% ~ 44.7% hIoU和14.5% ~ 50.4% hAP50。此外，本文的模型可以推广到另一个数据集（即S3IDS），表明其可迁移性。最后，本文的模型可以从提供更高质量字幕的更高级别的基础模型中受益，表明其可扩展性和可扩
#### 7. 方法详细介绍：
本文提出了一种名为PLA的语言驱动的3D场景理解框架。该方法包括以下步骤：
1. 从3D到2D生成图像标题，以在3D领域中提取语义丰富的文本描述，从而实现3D和词汇丰富的文本之间的显式关联。
2. 建立分层点-标题对，包括场景级、视图级和实体级标题，利用3D场景和2D图像之间的几何约束提供粗到细的监督信号。
3. 通过对比学习从丰富的词汇中学习充分的视觉-语义表示，使3D网络能够从（伪）标题中学习语言感知嵌入。
4. 利用对比学习学习连接3D和文本的语言感知嵌入，以进行开放式词汇任务。

#### 8. 实验设置：
本文在ScanNet和S3IDS两个数据集上进行了实验，用于开放式词汇语义和实例分割任务。数据集被分为基础/新颖类别，用于开放式词汇基准测试。平均交集联合（mIoU）和50％ IoU阈值下的平均精度（mAP50）用作语义和实例分割的评估指标。

#### 9. 实验结果和分析：
本文提出的PLA方法在ScanNet和S3DIS数据集上的语义和实例分割任务中，相对于基线方法，分别取得了25.8％〜44.7％和14.5％〜50.4％的hIoU和hAP50改进。该方法还在具有挑战性的零样本领域转移任务中展现了出色的可转移性。在基于类别的开放式词汇任务中，该方法也表现出了优异的性能。


# Paper:298     使用预训练的2D扩散模型解决3D逆问题



#### 1. Title: 
Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models

#### 2. Authors: 
Hyungjin Chung, Dohoon Ryu, Michael T. Mccann, Marc L. Klasky, Jong Chul Ye

#### 3. Affiliation: 
第一作者：韩国科学技术院

#### 4. Keywords: 
Diffusion models, inverse problems, medical image reconstruction, model-based iterative reconstruction, total variation penalty

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2021_paper.html  Github: https://github.com/HJ-harry/DiffusionMBIR

#### 6. Summary : 
- (1):本文研究背景是3D逆问题的解决，传统的3D扫描成像方法存在成本高、时间长等问题，而基于2D扫描的方法则存在精度不足的问题，因此需要一种新的方法来解决这个问题。

- (2):过去的方法主要是基于模型的迭代重建方法，如total variation penalty，但是这种方法过于简单，无法完全模拟图像的外观。本文提出了一种新的方法，将传统的模型迭代重建方法与现代扩散模型相结合，提高了3D医学图像重建的效率和精度。

- (3):本文提出的方法是将传统的模型迭代重建方法与现代扩散模型相结合，通过在测试时将2D扩散先验与模型先验相结合，从而实现了在所有维度上的连贯重建。该方法可以在单个GPU上运行，并且在3D医学图像重建任务中表现出了很高的效果。

- (4):本文的方法在稀疏视图CT、有限角度CT和压缩感知MRI等任务中均表现出了很好的性能，达到了当前最先进的水平。
#### 7. 方法详细介绍：
本文提出的方法名为DiffusionMBIR，将传统的基于模型的迭代重建（MBIR）和现代扩散模型相结合，解决了三维断层重建的反问题。该方法利用预训练的二维扩散模型逐层去噪，然后应用增广的ADMM算法在z方向上实现一致性和稀疏性。算法迭代地应用ADMM算法和SDE的数值积分，以解决（13）中的最小化问题。通过在迭代之间共享变量，实现了快速高效的算法实现。在测试时，该方法将二维扩散先验与模型先验相结合，以实现跨所有维度的连贯重建。该方法在Sparse-view CT、Limited-angle CT和Compressed-sensing MRI三个任务上均取得了最新的最佳结果，并且具有很高的泛化能力。 

#### 8. 实验设置：
本文在三个医学图像重建任务上进行了评估：Sparse-view CT（SV-CT）、Limited-angle CT（LA-CT）和Compressed-sensing MRI（CS-MRI）。实验在单个普通GPU上进行，其中in-distribution测试数据与训练数据对齐，而out-of-distribution测试数据与训练数据差异巨大。

#### 9. 实验结果和分析：
本文提出的DiffusionMBIR方法在Sparse-view CT、Limited-angle CT和Compressed-sensing MRI三个任务上均表现出色，取得了最新的最佳结果。该方法在Sparse-view CT任务中，即使只有两个视角，也能提供准确的重建。该方法还能够重建与训练数据完全不同的OOD数据。在大多数情况下，该方法在PSNR和SSIM方面优于其他扩散模型和完全监督方法。该方法的实现使用了预训练的神经网络和随机微分方程求解器，该求解器通过共轭梯度和交替方向乘子法迭代更新解决方案。SDE求解器使用全局变量进行初始化，并在迭代过程中进行更新。该方法还包括使用预训练神经网络进行去噪的步骤。


# Paper:299     通过同时探索和识别实现3D感知的物体目标导航



#### 1. Title: 
3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification

#### 2. Authors: 
Jiazhao Zhang, Liu Dai, Fanpeng Meng, Qingnan Fan, Xuelin Chen, Kai Xu, He Wang

#### 3. Affiliation: 
第一作者：北京大学计算机科学技术研究所

#### 4. Keywords: 
Object goal navigation, 3D scene representation, modular-based methods, reinforcement learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_3D-Aware_Object_Goal_Navigation_via_Simultaneous_Exploration_and_Identification_CVPR_2021_paper.html  Github: https://github.com/PKU-EPIC/3D-Aware-ObjectNav

#### 6. Summary : 
- (1):本文研究的是在未知环境中的物体目标导航(ObjectNav)任务，通过学习细粒度的空间信息，提出了一种基于3D场景表示的3D-aware agent，以提高ObjectNav的性能。

- (2):现有的ObjectNav方法主要基于2D地图、场景图或图像序列，而本文提出的3D-aware agent可以通过学习3D场景表示来提高ObjectNav的性能。然而，利用3D场景表示在这个任务中的策略学习中可能会带来很大的挑战，因为它需要大量的计算成本和高维度的观察，导致低样本效率和困难的策略学习。因此，本文提出了一种基于两个直观的子策略的框架，即角落引导的探索策略和类别感知的识别策略，通过利用在线融合的3D点作为观察来同时执行。通过实验，证明了这种框架可以通过学习3D场景表示显著提高ObjectNav的性能。

- (3):本文提出了一种基于两个子策略的框架，即角落引导的探索策略和类别感知的识别策略，同时执行以通过学习3D场景表示来提高ObjectNav的性能。角落引导的探索策略学习预测场景边界框的四个角之一的长期离散目标，以有效地驱动代理探索目标物体可能安置的区域。对于识别，提出了一种类别感知的识别策略，动态学习每个类别的离散置信度阈值来识别语义预测。这两个策略都是在低维离散动作空间中通过RL训练的。通过实验，证明了这种同时执行两个策略的机制和离散动作空间设计可以显著降低3D-aware ObjectNav的学习难度，并在Matterport3D和Gibson数据集上实现了最佳性能。

- (4):本文在公共基准测试中进行了广泛的评估，证明了我们的方法在实现15 FPS的在线3D-aware ObjectNav的同时，实现了导航效率的最新性能。此外，我们的方法在效率和成功率方面优于所有其他基于模块化的方法，而训练成本最多可降低30倍。
#### 7. 方法详细介绍：
本文提出了一种基于三维点云的目标导航方法，通过同时探索和识别实现。该方法包括两个策略：角落引导的探索策略和类别感知的识别策略。探索策略预测一个角落目标来驱动智能体，而识别策略则始终进行语义预测以识别目标物体类别的实例。该方法还包括一个本地规划模块，利用两个策略的目标规划智能体的最短路径。奖励包括稀疏成功奖励、松弛奖励和探索奖励。具体步骤包括：
1. 在线构建点云场景表示
2. 进行探索策略和识别策略
3. 利用本地路径规划模块导航智能体到目标

#### 8. 实验设置：
本文在Matterport3D和Gibson数据集上使用Habitat模拟器进行实验。Gibson数据集使用Gibson tiny split的25个训练/5个验证场景，而MP3D数据集使用Habitat ObjectNav数据集的61个训练/11个验证场景。RGB-D和姿态读数来自模拟器，实现细节可以在补充材料中找到。

#### 9. 实验结果和分析：
本文提出的方法在MP3D和Gibson数据集上与几种基线方法进行了比较，包括端到端RL方法和基于模块的方法。评估指标包括成功率、SPL、Soft SPL和DTS。该方法在成功率、SPL和DTS方面均优于最相关的三个基线方法SemExp、Stubborn和PONI。在MP3D上，该方法实现了21.2%的SPL、30.5%的Soft SPL、40.2%的成功率和3.278的DTS，而在Gibson上，它实现了42.1%的SPL、74.5%的成功率和1.16的DTS。


# Paper:300     基于Transformer的统一识别双手操纵物体



#### 1. Title: 
Transformer-based Unified Recognition of Two Hands Manipulating Objects

#### 2. Authors: 
Hoseong Cho, Chanwoo Kim, Jihyeon Kim, Seongyeong Lee, Elkhan Ismayilzada, Seungryul Baek

#### 3. Affiliation: 
UNIST, South Korea (韩国蔚山科技大学)

#### 4. Keywords: 
Hand-object interaction recognition, Transformer, egocentric video, pose estimation, object type

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cho_Transformer-Based_Unified_Recognition_of_Two_Hands_Manipulating_Objects_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是理解自我中心视频中的手-物体交互，提出了一种基于Transformer的统一框架，用于更好地理解两只手操纵物体。 
- (2):过去的方法大多基于卷积神经网络（CNN）特征，结合长短期记忆（LSTM）或图卷积网络（GCN）的时间编码，提供两只手、一个物体及其交互的统一理解。本文提出的方法是将描绘两只手、一个物体及其交互的整个图像作为输入，联合估计每帧的两只手的姿势、一个物体的姿势和物体类型的3个信息。然后，基于估计的信息和编码两只手和一个物体之间交互的接触图，从整个视频中预测手-物体交互的动作类别。本文的方法在H2O和FPHA基准数据集上进行了实验，并展示了其达到了最先进的准确性。 
- (3):本文提出了基于Transformer的统一框架，用于在单个推理步骤中估计两只手的姿势、物体姿势、物体类型和手-物体交互类别。我们利用手和物体网格之间的接触图，通过逆运动学从手姿势中恢复手网格。我们证明了接触图表达了手和物体之间的显式关系信息，并且作为手-物体交互识别任务的关键线索。 
- (4):本文的方法在H2O和FPHA基准数据集上实现了最先进的姿势估计和交互识别性能。本文的方法通过联合估计两只手、一个物体和它们之间的交互类别，提供了更好的理解两只手操纵物体的方法。
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的统一框架，用于在单个推理步骤中估计两只手的姿势、物体的姿势、物体类型和手-物交互类别。该框架通过逆运动学从手姿势中恢复手网格，并利用手和物体网格之间的接触图表达手和物体之间的显式关系信息，作为手-物交互识别任务的关键线索。该方法涉及Transformer-based架构，类似于[7,60]，它能够在没有手/物体检测器或任何额外的后处理（如非最大抑制（NMS））的情况下从每个帧预测姿势。

#### 8. 实验设置：
本文在两个数据集上进行了评估：H2O和FPHA。对于H2O数据集，使用55,742张图像进行训练，11,638张进行验证，23,391张进行测试。对于交互识别，使用569个视频片段进行训练，122个进行验证，242个进行测试。测试集仅包含1个在训练期间未见过的受试者。对于FPHA数据集，训练集包含600个视频，测试集包含575个视频。

#### 9. 实验结果和分析：
本文提出的统一框架在H2O和FPHA数据集上的手-物姿势估计和交互识别任务中均优于现有方法。消融研究表明，所提出的参考点细化和同时使用接触图和网格顶点可以提高系统的性能。所提出的基于Transformer的手-物姿势估计器和交互识别器的流水线比使用基于CNN或LSTM的架构更有效。


# Paper:301     一致的直接飞行时间视频深度超分辨率



#### 1. Title: 
Consistent Direct Time-of-Flight Video Depth Super-Resolution

#### 2. Authors: 
Zhanghao Sun, Wei Ye, Jinhui Xiong, Gyeongmin Choe, Jialiang Wang, Shuochen Su, Rakesh Ranjan

#### 3. Affiliation: 
Zhanghao Sun: Stanford University
Others: Meta Reality Labs, Meta Research

#### 4. Keywords: 
Direct time-of-flight, depth super-resolution, RGB-guided, multi-frame, histogram processing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Consistent_Direct_Time-of-Flight_Video_Depth_Super-Resolution_CVPR_2021_paper.html  Github: https://github.com/facebookresearch/DVSR/

#### 6. Summary : 
- (1):本文研究的背景是直接飞行时间（dToF）传感器的低空间分辨率问题，需要进行超分辨率处理。
- (2):过去的方法包括深度补全和深度超分辨率，但这些方法都是基于单帧处理，无法处理视频数据流，且在处理低分辨率dToF数据时存在空间模糊问题。本文提出了基于多帧的RGB-dToF视频超分辨率方法，通过利用多帧相关性和dToF直方图信息来解决空间模糊问题。
- (3):本文提出了两种方法：dToF深度视频超分辨率（DVSR）和直方图视频超分辨率（HVSR）。DVSR通过消耗高分辨率RGB图像和低分辨率dToF深度图像的序列来预测高分辨率深度图像序列，利用多帧相关性来解决空间模糊问题。HVSR在DVSR的基础上，进一步利用dToF直方图信息来提高深度估计的精度和减少估计误差。此外，本文还提出了DyDToF数据集，用于评估算法性能。
- (4):本文的方法在动态室内环境中进行了评估，并取得了较好的性能，包括提高了深度估计的准确性和时间一致性。本文的方法和数据集对于dToF深度传感器的应用具有重要意义。
#### 7. 方法详细介绍：
本文提出了一种基于循环神经网络的两阶段直接飞行时间（dToF）视频深度超分辨率方法。该方法的输入是一个由T帧组成的序列，每帧包括一个RGB图像和一个空间分辨率为（H/s）×（W/s）的dToF数据，其中s是下采样因子。网络预测一个H×W高分辨率深度图的序列。第一阶段将dToF传感器数据与RGB引导融合，生成一个初始的高分辨率深度预测和置信度图。第二阶段的细化网络生成第二个深度预测和置信度图，将其与初始预测融合生成最终预测。网络还采用了灵活的基于变形的多帧特征聚合和直方图处理管道，利用dToF传感器提供的独特直方图信息。网络使用Charbonnier损失和梯度损失在TarTanAir数据集上进行训练。

本文还提出了一种多帧方法DVSR和直方图视频超分辨率（HVSR），用于超分辨率低分辨率的dToF传感器视频。DVSR框架消耗一系列高分辨率RGB图像和低分辨率dToF深度图，并预测一系列高分辨率深度图。HVSR网络将dToF直方图信息纳入进一步减轻空间模糊。本文还引入了基于物理的dToF传感器模拟数据集DyDToF，以评估所提出的模型。

#### 8. 实验设置：
本文引入了DyDToF，一个合成数据集，包括多样的室内场景和动物动画（例如猫和狗）。该数据集合成了RGB图像序列、深度图、表面法线图、材质反照率和相机姿态。该数据集还集成了基于物理的dToF传感器模拟，以分析所提出的视频处理框架如何推广到动态场景以及低级数据模态如何促进网络训练和评估。

#### 9. 实验结果与分析：
所提出的dToF视频超分辨率网络在多个RGB-D数据集上进行了评估，包括TarTanAir、Replica和DyDToF。网络在所有数据集上都显著优于每帧处理基线，如表1所示。所提出的HVSR网络，利用dToF直方图信息，进一步提高了估计质量。图4中的定性比较显示，与每帧基线相比，视频处理网络在深度质量方面取得了更高的成果，特别是在细节结构方面。所提出的网络还在Replica数据集和DyDToF数据集的动态场景中展示了良好的泛化性能。


# Paper:302     视频中无偏见的场景图生成



#### 1. Title: 
Unbiased Scene Graph Generation in Videos

#### 2. Authors: 
Sayak Nag, Kyle Min, Subarna Tripathi, Amit K. Roy-Chowdhury

#### 3. Affiliation: 
第一作者：Sayak Nag，加州大学河滨分校；

#### 4. Keywords: 
Scene graph generation, dynamic scene graph, long-tailed distribution, uncertainty attenuation, memory-guided training.

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Nag_Unbiased_Scene_Graph_Generation_in_Videos_CVPR_2021_paper.html

Github: https://github.com/sayaknag/unbiasedSGG.git

#### 6. Summary: 
- (1):本文研究的是视频中的动态场景图生成，该任务由于场景的内在动态性、模型预测的时间波动以及视觉关系的长尾分布等困难而变得复杂和具有挑战性。

- (2):现有的动态场景图生成方法主要集中在使用复杂的架构捕捉时空上下文，而没有解决上述挑战，特别是视觉关系的长尾分布。这通常导致生成有偏见的场景图。为了解决这些挑战，本文提出了一种新的框架TEMPURA，它通过基于transformer的序列建模实现对象级别的时间一致性，使用记忆引导训练学习合成无偏见的关系表示，并使用高斯混合模型（GMM）减弱视觉关系的预测不确定性。 

- (3):本文提出的方法通过建立记忆库，使用渐进式记忆计算方法和基于注意力的信息扩散策略，实现了无偏见的动态场景图生成。同时，本文还提出了一种基于混合密度网络的单模型不确定性建模方法，以及一种基于对比学习的序列处理方法，以确保更加一致的对象分类。 

- (4):本文的方法在Action Genome数据集上进行了广泛的实验，结果表明，与现有方法相比，本文的方法在生成更加无偏见的场景图方面取得了显著的性能提升（在某些情况下高达10%），并且在所有类别上都取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了TEMPURA框架，用于从视频中生成无偏的动态场景图。该框架包括以下步骤：
1. 利用任何现有的动态SGG模型获得谓词嵌入生成器（PEG），该PEG被建模为[10]中的时空变换器，学习每对之间关系的空间和时间上下文。
2. 利用变换器编码器的对象序列处理单元（OSPU）处理一组序列，以实现整个视频中更一致的对象分类。
3. 利用记忆扩散单元（MDU）和高斯混合模型（GMM）头来解决视频SGG数据中的长尾偏差和整体噪声问题。
4. 最后，通过在SeqEnc的输出嵌入上添加监督对比损失来强制执行视频内部的时间一致性。

#### 8. 实验设置：
本文在Action Genome（AG）数据集上进行实验，该数据集是视频SGG的最大基准数据集。数据集包含234,253个带有476,229个边界框的帧，涵盖35个对象类（不包括人），共有1,715,568个谓词实例，涵盖26个关系类别。实验分为两个设置：有约束和无约束。在有约束的情况下，生成的图被限制为最多一个边，而在无约束的情况下，图可以具有多个边。使用AdamW优化器和批量大小为1进行端到端训练10个时期。

#### 9. 实验结果和分析：
本文将提出的方法与现有的动态SGG方法进行比较，并在AG数据集上取得了最先进的性能。使用标准指标Recall@K（R@K）和mean-Recall@K（mR@K）进行评估，其中K =[10,20,50]。在PREDCLS、SGCLS和SGDET三个SGG任务中，提出的方法均优于现有方法。该方法还在所有关系类别上实现了更平衡的性能，如mR@K指标所反映的那样。本文还提供了消融研究，以证明所提出方法的每个组件的有效性。本文得出结论，所提出的方法在生成更无偏的场景图方面优于现有方法。


# Paper:303     基于命令的关节对象理解与操作



#### 1. Title: 
Command-driven Articulated Object Understanding and Manipulation

#### 2. Authors: 
Ruihang Chu, Zhengzhe Liu, Xiaoqing Ye, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, Jiaya Jia

#### 3. Affiliation: 
第一作者：香港中文大学

#### 4. Keywords: 
Articulated object, manipulation, user command, motion prediction, object structure understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2021_paper.html  Github: https://github.com/dvlab-research/Cart

#### 6. Summary : 
- (1):本文研究了如何通过人类命令来操作关节对象。与现有的研究侧重于推断关节结构不同，本文进一步支持操作关节形状以使其符合简单的命令模板。本文的关键在于利用对象结构的预测来将视觉观察与用户命令连接起来，以实现有效的操作。 
- (2):过去的方法主要集中在理解关节对象的结构属性，如发现可移动部件和估计连接部件之间的关节。然而，从理解对象到用户可控制的操作仍然具有挑战性。本文提出了一种新的方法，称为Cart，它通过理解关节对象的结构并进一步将用户命令与固有对象几何形状相连接，以实现关节对象的操作。 
- (3):本文提出了一种新的学习框架，称为Command-driven articulation modeling，或Cart，它在关节对象的结构理解和形状操作方面都非常强大。该框架首先对单个对象部件进行分割，预测其运动关节，并模拟当前的关节状态。然后，我们将用户命令转换为运动预测的编码，并在测试时进行自适应以调整运动量。最后，我们提出了一种新的测试时间状态自适应算法，用于状态规范化，以确保目标对象状态满足用户命令。 
- (4):本文在两个广泛使用的关节对象数据集上进行了广泛的评估。与最近的工作相比，Cart在理解固有关节结构和用于操作的动态运动参数方面具有更好的预测性能。本文的方法可以在合成和现实世界数据上进行操作，并且可以很好地推广到未见过的对象类别和现实世界对象。
#### 7. 方法详细介绍：
本文提出了一种名为Cart的方法，用于实现关节物体的理解和命令可控的操作。该方法由两个神经网络Seg-Net和Art-Net组成。Seg-Net用于部件分割和关节状态建模的预训练，而Art-Net利用这些预测来预测关节模型。给定用户命令，Art-Net将分割结果应用于定位进行操作的对象部件，并将命令消息合并到视觉嵌入中。融合的部件特征包括几何特征和命令含义，用于推理关节模式和与命令相关的运动参数，支持物理形状操作。提出了Test-time State Adaptation算法来规范更改的关节状态。它利用Seg-Net的状态预测模型从命令监督中优化运动参数。通过迭代优化，TTSA稳健地实现了高操作性能。

#### 8. 实验设置：
本文在两个广泛使用的关节物体数据集Shape2Motion和PartNet-Mobility上评估了所提出的方法。对于两个数据集中的九个常见物体类别，每个类别包含30∼100个实例。对于每个物体类别，本文生成10K、1K和1K个样本用于训练、验证和测试，每个样本包括一个命令和一个物体的3D点云。

#### 9. 实验结果与分析：
本文提出的方法在关节参数预测方面表现优异，甚至在与Ditto进行公平比较时，性能略优于后者。在九个物体类别的部件分割mAP方面，所提出的方法也优于其他方法。在关节参数预测方面，本文方法在所有物体类别上都显著优于RPM-Net和ANCSH。在与Ditto进行比较时，本文方法的性能与Ditto相当。本文方法的关键在于结构无关的部件分割。所提出的Test-time State Adaptation算法有效地减少了预测误差，表明它有助于更好地与命令对齐的操作。本文方法对状态节点的不同设置具有鲁棒性。在两个关节产品（笔记本电脑和柜子）的实际操作中，验证了所提出方法的泛化能力。


# Paper:304     公平刮刮乐：无需权重训练寻找公平稀疏网络



#### 1. Title: 
Fair Scratch Tickets: Finding Fair Sparse Networks without Weight Training

#### 2. Authors: 
Pengwei Tang, Wei Yao, Zhicong Li, Yong Liu

#### 3. Affiliation: 
中国人民大学高灵智能学院

#### 4. Keywords: 
Computer vision, fairness, lottery ticket hypothesis, sparse neural networks, in-processing methods

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_Fair_Scratch_Tickets_Finding_Fair_Sparse_Networks_Without_Weight_Training_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究计算机视觉模型的公平性问题，提出了一种新的公平感知学习范式，通过彩票票证假设（LTH）的视角，寻找计算机视觉公平的稀疏子网络，而无需进行权重训练。

- (2):现有的公平性方法包括预处理、处理和后处理方法，但大多数处理方法使用深度和密集神经网络，因此在推理阶段计算成本较高。本文提出的方法可以在不进行权重训练的情况下找到具有天生公平性的稀疏子网络，且计算成本较低。

- (3):本文提出了一种新的公平感知学习范式，通过彩票票证假设（LTH）的视角，寻找计算机视觉公平的稀疏子网络，而无需进行权重训练。本文发现，随机初始化的神经网络中存在具有天生公平性的稀疏子网络，称之为公平刮刮乐（FSTs）。本文还在理论上为它们提供了公平性和准确性保证。

- (4):本文在各种数据集、目标属性、随机初始化方法、稀疏模式和公平代理上研究了FSTs的存在性，并发现FSTs可以在数据集之间进行转移。实验结果表明，与现有的公平感知处理方法训练的密集神经网络相比，具有天生公平性的FSTs具有可比的准确性-公平性权衡。
#### 7. 方法详细介绍：
本文提出了一种基于彩票票证假设的公平感知学习范式，称为公平刮刮卡（Fair Scratch Tickets，FSTs）。该方法通过随机初始化一个密集神经网络，然后找到适当的二进制掩码，以获得公平的稀疏子网络，而无需进行任何权重训练。该方法通过迭代地为每个随机初始化的权重附加可学习分数来引导搜索最佳二进制掩码。二进制掩码基于更新后的分数进行更新，搜索仅通过梯度下降学习附加的分数，而不进行任何权重训练。本文还为FSTs提供了理论公平性和准确性保证。具体方法包括公平正则化和公平对抗训练两种搜索方法。

#### 8. 实验设置：
本文在多个数据集、目标属性、随机初始化方法、稀疏模式和公平代理上验证了FSTs的存在性和有效性。实验结果证明了FSTs在缓解计算机视觉模型中的偏见方面的普适性和有效性。本文还验证了FSTs的微调和可迁移性属性。

#### 9. 实验结果和分析：
本文的实验结果表明，FSTs可以实现与现有公平感知处理方法相当甚至更好的准确性-公平性权衡，而无需进行任何权重训练。FSTs在公平正则化和公平对抗训练两种方法中均表现出良好的性能。本文的研究为计算机视觉公平性社区提供了新的见解。


# Paper:305     面向组合对抗鲁棒性：将对抗训练推广到组合语义扰动



#### 1. Title: 
Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations

#### 2. Authors: 
Lei Hsiung, Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho

#### 3. Affiliation: 
第一作者：国立清华大学

#### 4. Keywords: 
Adversarial training, composite adversarial attack, generalized adversarial training, semantic perturbations, adversarial robustness

#### 5. Paper: https://hsiung.cc/CARBEN/  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在面对对抗样本时的鲁棒性问题，目前的对抗训练主要针对单一扰动类型，而对于多种语义扰动和它们的组合，鲁棒性的研究还比较少。

- (2):过去的方法主要考虑单一扰动类型，而对于多种语义扰动和它们的组合，鲁棒性的研究还比较少。本文提出了一种新的方法，可以生成组合对抗样本，并且可以通过组件投影梯度下降和自动攻击顺序调度来找到最优的攻击组合。同时，本文提出了广义对抗训练（GAT）来将模型的鲁棒性从 ℓp-ball 扩展到组合语义扰动，如色调、饱和度、亮度、对比度和旋转等。与 ℓ∞-norm 有界对抗训练方法相比，GAT 取得了显著的优势。

- (3):本文提出了组合对抗攻击（CAA）方法，可以生成多种扰动类型的对抗样本，并且可以通过攻击顺序调度算法来学习各种扰动类型的最优顺序。在此基础上，本文提出了广义对抗训练（GAT）方法，可以实现组合对抗鲁棒性。本文的创新点在于，可以将对抗训练从单一威胁模型扩展到多个威胁模型，并且可以优化多种语义和 ℓp-norm 扰动的攻击顺序。

- (4):本文在 ImageNet 和 CIFAR-10 数据集上进行了实验，结果表明，GAT 不仅可以对所有测试的单一攻击类型具有鲁棒性，而且可以对任何这些攻击类型的组合具有鲁棒性。GAT 的性能也优于 ℓ∞-norm 有界对抗训练方法。
#### 1. 方法详细介绍：
本文提出了一种名为“组合对抗攻击（Composite Adversarial Attack，CAA）”的方法，该方法可以生成多种扰动类型的对抗样本，包括语义扰动（色调、饱和度、亮度、对比度和旋转）和 ℓp-范数空间。CAA采用了一种攻击顺序调度算法，用于学习各种扰动类型的最佳顺序，并将其纳入广义对抗训练（Generalized Adversarial Training，GAT）框架中。GAT是一种扩展模型鲁棒性的方法，从 ℓp-球扩展到组合语义扰动。

#### 2. 实验设置：
本文在ImageNet和CIFAR-10数据集上进行了实验。所有实验均使用ResNet50模型作为基础模型。实验使用单个NVIDIA Tesla V100 GPU进行。

#### 3. 实验结果与分析：
在ImageNet和CIFAR-10数据集上的实验结果表明，GAT不仅可以对所有测试的单一攻击类型具有鲁棒性，而且可以对任何此类攻击的组合具有鲁棒性。GAT在多种组合对抗攻击下的鲁棒性优于 ℓ∞-范数约束的对抗训练方法。本文提出的GAT可以在各种组合对抗攻击下保持鲁棒性，即使攻击数量增加。


# Paper:306     AGAIN: 基于属性跨度扩大和混合特征融合的对抗训练



#### 1. Title: 
AGAIN: Adversarial Training with Attribution Span Enlargement and Hybrid Feature Fusion

#### 2. Authors: 
Shenglin Yin, Kelu Yao, Sheng Shi, Yangzhou Du, Zhen Xiao

#### 3. Affiliation: 
第一作者：北京大学计算机科学学院

#### 4. Keywords: 
Adversarial training, attribution span, feature fusion, robustness, deep neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yin_AGAIN_Adversarial_Training_With_Attribution_Span_Enlargement_and_Hybrid_Feature_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究深度神经网络在对抗训练中的鲁棒性问题，提出了一种新的方法来提高对抗训练的鲁棒性。
- (2):过去的方法主要从权重损失和训练策略两个方面入手，但忽略了对抗训练中学习到的知识表示的影响。本文从属性跨度的角度出发，发现对抗训练的DNNs在推理阶段关注的特征跨度较小，忽略了其他跨度的特征，导致其在测试鲁棒性方面存在局限性。因此，本文提出了一种有效的方法来扩大学习到的属性跨度，同时使用混合特征统计进行特征融合，以增加特征的多样性。 
- (3):本文提出的方法称为AGAIN，即属性跨度扩大和混合特征融合的对抗训练。在模型训练过程中，我们扩大模型关注特征的区域，同时确保其学习到鲁棒特征，并使用特征融合来增强模型在干净数据和对抗样本上的泛化能力。与其他方法相比，我们的方法可以进一步提高模型在干净数据和对抗样本上的准确性，且易于与其他方法结合使用以进一步提高方法的有效性。
- (4):本文的方法在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，结果表明，我们的方法可以有效地提高对抗训练的鲁棒性，优于以前的SOTA方法。此外，我们还提供了对我们的方法的理论分析，证明了其有效性。
#### 7. 方法详细介绍：
本文提出的AGAIN方法是将对抗训练（AT）与归因跨度扩大（ASE）和混合特征融合（HFF）相结合的方法。ASE通过扩大对抗样本的归因跨度来提高模型的鲁棒性，而HFF则将来自多个维度的特征融合以增强模型的鲁棒性。在训练过程中使用联合损失函数来训练模型，并使用原始标签而不是在训练过程中融合标签。具体步骤如下：
1. 对抗样本生成：使用PGD攻击生成对抗样本。
2. 归因跨度扩大：使用类激活映射（CAM）获取模型在真实标签和虚假标签下的归因跨度，并按比例混合这两个跨度以完成归因跨度的扩大。
3. 混合特征融合：使用AdaIN计算混合特征，将原始特征和生成的混合特征计算混合特征统计量。

#### 8. 实验设置：
本文在CIFAR-10和CIFAR-100数据集上进行实验，使用PyTorch在单个NVIDIA Tesla V100 GPU上训练模型。训练过程使用标准的数据增强技术，包括随机裁剪和水平翻转。对抗样本使用PGD攻击生成，步长为0.01，迭代次数为10。评估指标为标准准确率和在PGD攻击下的鲁棒准确率。

#### 9. 实验结果和分析：
实验结果表明，本文提出的AGAIN方法在CIFAR-10和CIFAR-100数据集上优于先前的AT方法。该方法在干净数据和对抗样本上均获得更高的准确率，并显著缩小了鲁棒泛化差距。理论分析也证明了该方法的有效性。


# Paper:307     隐私保护的对抗人脸特征



#### 1. Title: 
Privacy-preserving Adversarial Facial Features

#### 2. Authors: 
Zhibo Wang, He Wang, Shuaifan Jin, Wenwen Zhang, Jiahui Hu, Yan Wang, Peng Sun, Wei Yuan, Kaixin Liu, Kui Ren

#### 3. Affiliation: 
Zhejiang University (浙江大学)

#### 4. Keywords: 
Face recognition, privacy protection, adversarial features, reconstruction attacks

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Wang_Privacy-Preserving_Adversarial_Facial_Features_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是人脸识别系统中的隐私保护问题，即如何防止人脸特征被利用进行重建攻击，泄露个人隐私信息。

- (2):过去的方法包括加密、差分隐私和对抗训练等，但这些方法要么计算成本高，要么会导致识别准确率下降，要么需要重新训练和部署人脸识别网络。本文提出了一种基于对抗特征的人脸隐私保护方法，可以生成隐私保护的对抗特征，防御重建攻击，同时保持人脸识别准确率。该方法不需要改变已部署的人脸识别模型，可以作为隐私增强插件集成到人脸识别系统中。

- (3):本文提出了一种影子模型，模拟重建攻击的行为，捕捉从人脸特征到图像的映射函数，并生成对抗潜在噪声来扰乱映射。为了保证人脸识别准确率，优化过程中限制对抗潜在噪声的大小。实验结果表明，该方法在防御重建攻击方面优于现有方法，同时保持了人脸识别准确率。

- (4):本文的方法在LFW和CelebA数据集上进行了实验，结果表明，与现有方法相比，该方法在防御重建攻击方面表现更好，同时保持了人脸识别准确率。
#### 7. 方法详细介绍：
本文提出了一种基于对抗特征的人脸隐私保护方法（AdvFace）。该方法通过在服务器端构建一个影子模型来学习从人脸特征到图像的映射，并计算重构损失。在重构损失的梯度的指导下，将对抗性潜在噪声注入到影子特征中，生成对抗性特征。该方法不需要改变人脸识别网络，可以作为隐私增强插件集成到已部署的人脸识别系统中。具体步骤包括：将客户端上传的原始人脸特征转换为影子图像，使用特征提取器从影子图像中提取影子特征，使用相同的影子模型将影子特征还原为重构图像，计算重构损失相对于影子特征的梯度，通过梯度上升生成对抗性潜在噪声，将影子特征与对抗性潜在噪声扰动以生成对抗性特征，将其存储在服务器的数据库中以供未来的人脸识别使用。

#### 8. 实验设置：
本文在多个数据集上进行了实验，包括CASIA-WebFace、CelebA、LFW、CFP-FP和AgeDB-30。使用MTCNN对人脸图像进行预处理，并将其调整为160x160。使用预训练的Inception-ResNet-v1作为人脸识别的骨干网络。选择骨干网络的前三个卷积层作为客户端的特征提取器，其余层部署在服务器端。使用CASIA-WebFace数据集对FaceNet的分类器进行微调。在CelebA数据集上训练了三种类型的重构网络，包括URec、ResRec和TransRec。在CASIA-WebFace数据集上训练了三种类型的影子模型，包括URec、ResRec和TransRec。使用Adam优化器，学习率为1e-4，在PGD过程中进行40次迭代，α为0.2。将噪声边界设置为ε = 0.2。

#### 9. 实验结果与分析：
本文通过人脸识别准确率、抵抗攻击能力和可转移性来评估AdvFace的性能。分析了人脸隐私保护和人脸识别准确率之间的权衡，并发现当ε≥0.2时，人脸隐私可以得到很好的保护。将AdvFace与三种广泛使用的人脸隐私保护方法（Random Perturbation、Differential Privacy和DuetFace）进行比较，结果表明AdvFace在SSIM、PSNR、MSE和SRRA方面优于基线。当ε = 0.2时，AdvFace在人脸隐私保护和人脸识别准确率方面表现良好。在LFW数据集上，AdvFace实现了99.47%的验证准确率和97.68%的识别准确率，在CelebA数据集上，实现了98.98%的验证准确率和94.98%的识别准确率。验证了AdvFace的可转移性，表明它可以有效地抵御不同的重构网络。


# Paper:308     神经镜头建模



#### 1. Title: 
Neural Lens Modeling

#### 2. Authors: 
Wenqi Xian, Aljaž Božič, Noah Snavely, Christoph Lassner

#### 3. Affiliation: 
第一作者：Meta Reality Labs Research

#### 4. Keywords: 
Neural network, camera calibration, lens modeling, 3D reconstruction, invertible neural network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xian_Neural_Lens_Modeling_CVPR_2021_paper.html  Github: https://github.com/neural-lens/neural-lens

#### 6. Summary : 
- (1):本文研究的背景是相机标定和3D重建中的镜头建模问题，传统的参数化模型和非参数化模型都存在一定的局限性。

- (2):过去的方法包括参数化模型和非参数化模型，但是这些方法都存在一定的局限性，无法很好地建模复杂的镜头效应。本文提出了一种基于可逆神经网络的镜头模型，可以用于点投影和射线投射，并且可以通过这两种操作进行优化。该模型可以用于预捕获标定和3D重建期间的校准或优化，具有很好的灵活性和可扩展性。

- (3):本文提出的镜头模型使用可逆神经网络建模射线畸变，结合标准相机内参和外参。该模型可以很好地建模不同类型的镜头，具有很好的灵活性和可扩展性。本文还提出了一种新的标定板和关键点检测器，可以用于模式匹配相机标定，提高标定的鲁棒性。此外，本文还提出了一个大规模相机镜头基准数据集SynLens，用于评估标定和重建的性能。

- (4):本文的方法在多个任务上取得了很好的性能，包括预捕获标定、3D重建期间的相机模型微调和定量评估。该模型可以在少量关键点的情况下实现亚像素级别的精度，并且对噪声关键点检测具有很好的鲁棒性。本文的方法具有很好的灵活性和可扩展性，可以很好地应用于相机标定和3D重建等领域。
#### 7. 方法详细介绍：
本文提出了一种神经透镜建模方法，使用ArUco标记的机器优化校准模式进行准确的关键点检测。该方法使用MobileNet-v3检测器，通过真实关键点位置的高斯负对数似然进行简化的2D位置预测优化。检测器在整个检测过程中以全监督方式进行训练，可以根据实际情况进行调整。本文还提出了SynLens数据集，其中包含来自40个不同相机制造商的3500多个高质量合成透镜模型，可用于评估校准模型。该数据集提供了一个API，可以通过这些透镜渲染图像，同时自动应用畸变和暗角。本文在提出的SynLens数据集上比较了他们的透镜模型和标记板与几种已有方法的性能，并在径向和鱼眼透镜上展示了基于关键点的校准和辐射场重建的实际数据结果。

#### 8. 实验设置：
本文在两个数据集OCamCalib和UZH上进行了实验。相机视场角从130°到266°不等，使用带有AprilTags标记的平面棋盘目标进行关键点检测。将结果与最先进的相机校准框架BabelCalib进行比较，提出的方法在UZH数据集的大多数相机上表现优异。在OCamCalib的测试图像中，重投影关键点的残差可视化，提出的方法实现了0.91的无权重重投影误差。

#### 9. 实验结果和分析：
本文提出的方法在SynLens数据集上的性能优于现有方法，对于所有相机模型都有很大的优势。在使用消费级GoPro相机进行广角和超广角镜头的实验中，本文的方法也取得了稍微更好的结果。本文提出的透镜模型在多种透镜类型上具有广泛的适用性，易于集成到现有的3D重建和渲染系统中。本文提出的标记板可以提高校准精度，在多个实验中表现出色。本文的方法在广泛的设置中产生更准确的结果，包括使用标记板进行预校准，调整相机模型进行3D重建，以及在提出的SynLens数据集上进行定量评估。本文的方法在校准问题上取得了最先进的结果。


# Paper:309     MIME：基于人类运动的3D场景生成



#### 1. Title: 
MIME: Human-Aware 3D Scene Generation

#### 2. Authors: 
Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea Hering, Justus Thies, Michael J. Black

#### 3. Affiliation: 
第一作者：Max Planck Institute for Intelligent Systems, Tübingen, Germany（德国图宾根马普智能系统研究所）

#### 4. Keywords: 
3D scene generation, human motion, generative model, auto-regressive transformer architecture

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2021_paper.html  Github: https://mime.is.tue.mpg.de/

#### 6. Summary : 
- (1):本文研究的背景是生成真实的3D场景，以及人类运动对于场景生成的影响。 
- (2):过去的方法大多数忽略了人类运动对于场景生成的影响，而本文提出的方法则是基于人类运动生成3D场景。本文的方法与Pose2Room等方法相比，可以生成不与人类接触的物体，从而预测整个场景而不是孤立的物体。 
- (3):本文提出了一种名为MIME的生成模型，它是一种基于自回归变换器架构的室内3D场景生成方法，可以根据人类运动序列预测与人类接触的家具，并预测符合其他对象和自由空间约束的合理对象。为了将3D场景生成与人类运动相结合，我们使用POSA估计可能的接触姿势，并将运动分为接触和非接触片段。非接触姿势定义了房间中的自由空间，我们将其编码为2D地图。接触姿势和相应的3D人体模型由POSA预测的接触顶点的3D边界框表示。我们使用这些信息作为变换器的输入，并自回归地预测满足接触和自由空间约束的对象。 
- (4):本文的方法在3D场景生成方面取得了较好的效果，可以直接应用于实际捕获的运动序列，如PROX-D，而无需微调。
#### 7. 方法详细介绍：
本文提出了一种名为MIME的人类感知三维场景生成方法。该方法从输入的人类动作和平面图生成场景。生成的场景用三维边界框表示，并从3D FUTURE中检索最接近的网格模型，基于边界框大小和类别标签。为了改善人-场景交互，从MOVER中应用碰撞损失和接触损失来细化对象位置。计算统一的SDF体积，并累积所有3D空间中所有人的接触顶点，以共同优化对象对齐以改善人-对象接触并解决人与场景之间的3D穿透。MOVER接触损失权重和碰撞损失权重分别为1e5和1e3。为了使MIME能够从人类生成3D场景，生成了一个名为3D-FRONT HUMAN的合成数据集，通过将交互式人类自动放置在3D FRONT的3D房间中来生成。使用SMPL-X模型表示人，从RenderPeople中随机分配合理的交互以添加接触人类到房间中的不同可接触对象。允许三种接触交互：触摸、坐和躺。在自由空间中，添加随机数量的静态站立人和多个从AMASS中随机开始位置和方向的行走运动剪辑，并删除与对象相交的人。

#### 8. 实验设置：
本文在包含四种房间类型的3D-FRONT HUMAN数据集上进行了评估：1）5689个卧室，2）2987个客厅，3）2549个餐厅和4）679个图书馆。将数据集分为80％，10％，10％的训练，验证和测试集。将该方法与两个基线ATISS和Pose2Room进行比较，评估指标包括人-场景交互的合理性和生成场景的真实性。使用交叉度量、2D IoU和3D IoU评估生成场景中的人-场景交互。使用FID分数和类别KL散度评估生成场景的真实性和多样性。该方法还在具有MOVER 3D边界框注释的PROX-D数据集上进行了测试，并与Pose2Room进行了比较。

#### 9. 实验结果和分析：
该方法在人-场景交互评估指标上与ATISS相比取得了显着的改进。MIME生成了可信的3D场景，与自由空间的穿透更少，并支持交互人类。该方法在3D物体检测精度方面比Pose2Room更好，而无需预训练。评估了自由空间人类的密度和提供给MIME的接触人类数量的影响。MIME根据接触人类的数量生成接触对象，并且随着自由空间人类密度的增加，MIME在场景中生成的对象数量减少。


# Paper:310     无标注肝肿瘤分割



#### 1. Title: 
Label-Free Liver Tumor Segmentation

#### 2. Authors: 
Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng Chen, Alan Yuille, Zongwei Zhou

#### 3. Affiliation: 
Qixin Hu: 华中科技大学 (Huazhong University of Science and Technology)

#### 4. Keywords: 
Liver tumor segmentation, synthetic tumors, AI models, CT scans, medical image segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Label-Free_Liver_Tumor_Segmentation_CVPR_2021_paper.html  Github: https://github.com/MrGiovanni/SyntheticTumors

#### 6. Summary : 
- (1):本文研究的背景是医学图像分割领域中，标注数据的获取成本高昂，需要大量的人工标注，因此需要一种无需人工标注的肝肿瘤分割方法。

- (2):过去的方法需要大量的标注数据，而标注数据的获取成本高昂，因此需要一种无需人工标注的肝肿瘤分割方法。本文提出了一种合成肿瘤的方法，可以用于训练AI模型，从而实现无需人工标注的肝肿瘤分割。

- (3):本文提出了一种合成肿瘤的方法，可以生成形状和纹理逼真的肝肿瘤，用于训练AI模型。这种方法可以大大降低标注数据的获取成本，同时可以生成大量的小肝肿瘤，有助于早期癌症的检测。本文的创新点在于提出了一种无需人工标注的肝肿瘤分割方法。

- (4):本文的方法在真实数据集上取得了良好的性能，可以用于肝肿瘤分割任务。本文的方法可以大大降低标注数据的获取成本，同时可以生成大量的小肝肿瘤，有助于早期癌症的检测。本文的方法可以作为一种无需人工标注的肝肿瘤分割方法。
#### 7. 方法详细介绍：
本文提出了一种无需手动标注的肝癌分割方法。该方法首先通过阈值分割进行血管分割，避免血管对肝癌分割的干扰。然后进行碰撞检测，以确保所选位置不会影响周围组织。接着分别进行纹理和形状的生成，其中纹理遵循高斯分布，形状则采用椭球体生成。对生成的椭球体肿瘤掩膜进行弹性变形，使其更接近自然生长的肝癌。最后进行后处理，包括模拟肿瘤的质量效应和胶囊外观模拟，使生成的肝癌更加逼真。

#### 8. 实验设置：
本文使用101个带有标注的肝脏和肝癌的CT扫描进行5折交叉验证，用于训练U-Net模型。为了比较，还从CHAOS、BTCV、Pancreas-CT和LiTS中的健康受试者中组装了116个CT扫描的数据集。使用Dice相似系数（DSC）和归一化表面Dice（NSD）进行肝癌分割性能评估，使用灵敏度和特异度进行肝癌检测性能评估。模型训练了4000个epoch，基础学习率为0.0002，每个GPU的批量大小为2。采用线性预热策略和余弦退火学习率调度。推理时采用滑动窗口策略，将重叠区域比例设置为0.75。

#### 9. 实验结果与分析：
本文提出的无需手动标注的肝癌分割方法在LiTS数据集上的表现优于最近的一些工作。使用合成肝癌训练的AI模型在真实肝癌分割中的Dice相似系数（DSC）为59.81％，而使用真实肝癌训练的AI模型的DSC为57.63％。合成肝癌可以生成各种大小的肝癌，包括小型、中型和大型肝癌，因此有助于检测小型肝癌并促进肝癌的早期检测。使用合成肝癌训练的AI模型可以在真实肝癌分割中达到与使用真实肝癌训练的AI模型相似的性能，并且可以推广到具有健康肝脏的CT扫描和来自其他医院的扫描。


# Paper:311     你正在吸引我的注意力：视觉Transformer在后门攻击下是否是糟糕的学习者？



#### 1. Title: 
You Are Catching My Attention: Are Vision Transformers Bad Learners under Backdoor Attacks?

#### 2. Authors: 
Zenghui Yuan, Pan Zhou, Kai Zou, Yu Cheng

#### 3. Affiliation: 
第一作者：华中科技大学，分布式系统安全湖北省重点实验室，大数据安全湖北省工程研究中心，网络空间安全学院

#### 4. Keywords: 
Vision Transformers, Backdoor Attacks, Self-Attention Mechanism, Patch-wise Triggers

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_You_Are_Catching_My_Attention_Are_Vision_Transformers_Bad_Learners_CVPR_2022_paper.html  Github: None

#### 6. Summary:
- (1):本文研究了视觉Transformer（ViTs）在工业化过程中面临的后门攻击问题，探讨了ViTs相对于卷积神经网络（CNNs）的脆弱性来源。
- (2):过去的ViTs后门攻击方法没有系统地与CNN进行比较，没有考虑攻击的隐蔽性和攻击效果之间的平衡，容易被裸眼检测到。本文提出了一种新的后门攻击框架BadViT，利用通用的基于补丁的触发器来捕捉模型对分类有益的补丁的注意力，从而混淆ViTs的自我注意机制。此外，本文还提出了BadViT的隐形变体，通过限制触发器扰动的强度和采用混合策略来增加攻击的隐蔽性。
- (3):本文提出了一种新的后门攻击框架BadViT，通过优化过程生成通用的对抗性补丁触发器，更好地利用ViTs的自我注意机制来紧密连接触发器和目标类。BadViT是一种有效的后门攻击方法，不太依赖于毒素的数量，具有令人满意的收敛性，并且可转移用于下游任务。此外，本文还探讨了现有高级防御方案的ViTs内部风险。
- (4):本文的BadViT是一种有效的后门攻击方法，可以通过补丁触发器来攻击ViTs，攻击效果较好，攻击隐蔽性较高。本文的方法在多个数据集上进行了实验，证明了其有效性和可迁移性。
#### 7. 方法详细介绍：
本文提出了一种名为BadViT的后门攻击方法，该方法基于Vision Transformers（ViTs）的自注意机制，生成了一种通用的对抗性patch-wise触发器，可以添加到任何干净图像中，以操纵模型的预测结果。该触发器通过Projected Gradient Descent（PGD）进行优化，并与原始图像混合以实现隐形。该方法还采用lp约束来限制触发器扰动的强度，以实现隐形变体。具体步骤包括：
1. 生成通用的patch-wise触发器，通过最大化模型每层中第k个patch的注意力分数来实现。
2. 定义基于注意力的损失函数，量化触发器的优化。
3. 使用相同的预训练ViT分类器和良性训练集进行后门训练，将优化过的触发器添加到训练集中的一部分图像中。
4. 通过最小化主任务的训练损失和后门训练损失来优化模型。
5. 使用交叉熵损失函数进行分类任务的正常训练。

#### 8. 实验设置：
本文使用ILSVRC2012的训练集和验证集进行后门训练和测试。使用DeiT的官方预训练模型作为基准来测试BadViT的性能。输入图像被转换为3×224×224的大小，补丁大小设置为16×16。使用20个epoch基于训练集生成通用的对抗性patch-wise触发器，使用ρ = 0.1污染数据集，并选择目标标签30。使用lp约束来限制触发器扰动的强度，以实现隐形变体。

#### 9. 实验结果与分析：
本文的实验结果表明，BadViT可以在ViTs中实现100%的ASR，比干净模型更具攻击性。BadViT可以在极小的污染比例（0.002）下实现95%的ASR。BadViT对多目标后门攻击的鲁棒性也得到了验证。减小触发器大小可以在保持ASR的同时实现较小的牺牲。本文还评估了BadViT对三种防御方法的影响，结果表明PatchDrop无法成功检测BadViT中的后门图像，而Fine-Pruning对BadViT无效。


# Paper:312     A2J-Transformer：基于锚点到关节Transformer网络的单RGB图像3D交互手姿态估计



#### 1. Title: 
A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image

#### 2. Authors: 
Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, Jinghong Zheng, Zhiguo Cao, Joey Tianyi Zhou

#### 3. Affiliation: 
第一作者：华中科技大学人工智能与自动化学院，图像处理与智能控制教育部重点实验室

#### 4. Keywords: 
3D hand pose estimation, Transformer network, RGB image, interacting hands

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_A2J-Transformer_Anchor-to-Joint_Transformer_Network_for_3D_Interacting_Hand_Pose_CVPR_2021_paper.html  Github: https://github.com/ChanglongJiangGit/A2J-Transformer

#### 6. Summary : 
- (1):本文研究的是从单个RGB图像中估计3D交互手姿态的问题，这是一个具有挑战性的任务，由于手部的自遮挡和相互遮挡，两只手之间的相似外观模式混淆，以及从2D到3D的关节位置映射等问题。
 
- (2):现有的方法可以一般地分为基于模型和基于无模型两组。基于模型的方法通常需要复杂的个性化模型校准，而基于无模型的方法则不需要先验知识，具有更大的灵活性。本文提出了一种基于Transformer网络的方法，将A2J方法扩展到RGB领域，以解决3D交互手姿态估计问题。与A2J相比，本文方法具有三个优点：1）建立了跨局部锚点的自注意力机制，以更好地捕捉关节的关节线索以抵抗遮挡；2）将每个锚点视为可学习的查询，具有自适应特征学习，以促进模式拟合能力；3）锚点在3D空间中定位，以利用3D姿态预测。 

- (3):本文提出的A2J-Transformer方法在Transformer的非局部编码-解码框架下，将A2J的锚点进化为可学习的查询。每个查询将预测其与2只手的所有关节的位置偏移量。通过线性加权融合所有查询的预测结果来确定关节的位置。在特征编码阶段，查询可以相互交互以捕捉关节的全局关节线索，这对于抵抗自遮挡和相互遮挡是有益的。本文方法还提出了锚点定位在3D空间中，以便于基于单眼RGB信息解决2D到3D手部姿态提取问题。 

- (4):本文方法在InterHand 2.6M数据集上取得了最先进的无模型性能（2只手的3.38mm MPJPE提升），并且也可以应用于深度领域。本文方法的主要贡献包括：1）首次将A2J从深度领域扩展到RGB领域，以解决从单个RGB图像中估计3D交互手姿态的问题；2）将A2J的锚点进化为具有Transformer的非局部自注意力机制和自适应局部特征学习的可学习查询，以使其同时具有关节的局
#### 7. 方法详细介绍：
A2J-Transformer方法是一种从单个RGB图像中进行三维交互手部姿态估计的无模型方法。该方法使用锚点作为局部回归器，相对于自身估计每个关节。每个锚点返回一个从自身到所有关节的三维坐标偏移量。每个锚点的权重由权重估计分支预测。每个关节的坐标可以计算为所有锚点坐标预测结果的加权和。该模型使用两个损失函数进行训练：关节估计损失和锚点周围损失。主要技术流程包括三个主要模型：金字塔特征提取器、锚点精炼模型和锚点偏移-权重估计模型。金字塔特征提取器使用ResNet-50作为骨干网络从输入RGB图像中提取多尺度特征。锚点精炼模型包含特征增强模块和锚点交互模块，分别增强图像特征并建立锚点之间的交互。锚点偏移-权重估计模型估计每个锚点相对于每个手部关节的三维偏移和权重，并以加权求和的方式融合所有锚点的估计结果以获得关节的最终结果。 

#### 8. 实验设置：
A2J-Transformer模型在四个数据集上进行训练和评估：InterHand2.6M、RHP、NYU和HANDS 2017。InterHand2.6M是一个包含1.36M个训练图像和849K个测试图像的双手RGB图像数据集。RHP是一个包含41K个训练和2.7K个测试样本的合成数据集，其中包含两只孤立的手。NYU是一个包含72K个训练图像和8.2K个测试图像的单手深度图像数据集。HANDS 2017是一个包含957K个训练图像和295K个测试图像的单手深度图像数据集。在InterHand2.6M数据集上使用Mean Per Join Position Error (MPJPE)进行评估，而在RHP数据集上使用端点误差（EPE）进行评估。在NYU和HANDS 2017数据集上使用平均三维距离误差进行评估。

#### 9. 实验结果和分析：
A2J-Transformer方法在InterHand2.6M数据集上的表现优于其他无模型方法，并且在不使用任何手先验信息的情况下与基于模型的方法具有可比性。在RHP数据集上，A2J-Transformer方法在不依赖于推理时的真实信息的情况下优于先前的方法。实验表明了A2J-Transformer在野外图像上的有效性，并展示了其良好的泛化能力。该模型在NYU和HANDS 2017数据集上与基于深度的单手姿态估计方法进行了性能比较。结果表明，A2J-Transformer在NYU数据集上取得了竞争性能，并在HANDS 2017数据集上取得了最先进的性能。论文还包括消融研究，以分析A2J-Transformer不同组件的有效性。该研究表明，Transformer-based模型、A2J模块、3D锚点权重和MSDAM对A2J-Transformer的性能都至关重要。论文还包括定性评估结果，展示了A2J-Transformer在交互手中即使存在严重遮挡也能准确估计手部姿态的能力。


# Paper:313     学习和聚合城市自动驾驶车道图



#### 1. Title: 
Learning and Aggregating Lane Graphs for Urban Automated Driving

#### 2. Authors: 
Martin B¨uchner, Jannik Z¨urn, Ion-George Todoran, Abhinav Valada, Wolfram Burgard

#### 3. Affiliation: 
Martin B¨uchner, Jannik Z¨urn, Abhinav Valada: University of Freiburg, Germany; 
Ion-George Todoran: Woven by Toyota, Japan; 
Wolfram Burgard: University of Technology Nuremberg, Germany.

#### 4. Keywords: 
Lane graph estimation, automated driving, HD map learning, graph neural network, aerial imagery.

#### 5. Paper: 
http://openaccess.thecvf.com/content/CVPR2021/html/Buchner_Learning_and_Aggregating_Lane_Graphs_for_Urban_Automated_Driving_CVPR_2021_paper.html

Github: https://github.com/PRBonn/lane-graphs

#### 6. Summary: 
- (1):本文研究的背景是自动驾驶和高清地图学习中的车道图估计，车道图是规划未来驾驶操作和高级导航任务所必需的信息。

- (2):现有的车道图估计方法使用车载或航拍图像，但在复杂的车道拓扑结构、分布不均的场景或图像空间中的重要遮挡等方面存在困难。此外，合并重叠的车道图以获得一致的大规模图仍然很困难。本文提出了一种新的从航拍图像中自下而上的车道图估计方法，将多个重叠的图聚合成一个一致的图。由于其模块化设计，我们的方法允许我们解决两个互补的任务：使用图神经网络从任意车辆位置预测自我相对的后继车道图，并将这些预测聚合成一致的全局车道图。本文的方法在大规模车道图数据集上进行了广泛的实验，证明了我们的方法即使在严重遮挡的区域也能产生高度准确的车道图。图聚合的方法证明了消除不一致的预测，同时提高整体图质量的效果。

- (3):本文提出了一种新的基于图神经网络的车道图估计方法，称为LaneGNN。该方法在单个航拍彩色图像上操作，通过将虚拟代理放入局部图像中并从其位置预测可达的后继车道图来实现自下而上的方法。为了将多个不连续的局部解决方案转换为单个全局解决方案，我们通过连续姿态迭代地推断车道图，最终模拟真实世界的驾驶行为。我们的方法不需要任何人为干预即可执行图聚合。本文的方法不仅可以预测覆盖面积更大，而且可以通过数据关联和拒绝来提高图的准确性。本文的主要贡献包括：在具有挑战性的环境中车道图估计的创新自下而上方法；一种新颖的图聚合方案，实现了强大且方法无关的图级预测合并；大规模车道图数据集UrbanLaneGraph，包括与Argoverse2数据集对齐的高分辨率航拍图像和密集车道图注释；广泛的实验和消融研究，证明了我们的发现的重要性。

- (4):本文的方法在大规模车道图数据集上进行了广泛的实验，证明了我们的方法即使在严重遮挡的区域也能产生高度准确的车道图。我们的方法不仅可以预测覆盖面积更大，而且
#### 7. 方法详细介绍：
本文提出了一种名为LaneGNN的模型，用于学习和聚合城市自动驾驶的车道图。该模型采用自下而上的方法，从航拍图像和自车传感器数据中学习车道图。学习过程包括从输入数据中采样节点和边缘，然后进行消息传递和图剪枝，以获得最终的车道图。模型还包括一个迭代的时间聚合方案，用于聚合车道图。具体而言，该模型采用图神经网络（GNN）来预测可达的后继车道图，并将这些预测聚合成一致的全局车道图。模型的节点和边缘特征是通过节点特征和边缘特征进行归属的。在第一阶段，从航拍图像中采样可能区域构建有向图，训练GNN模型预测后继车道图。在第二阶段，将局部后继图迭代地聚合成全局一致的车道图。该方法大大提高了车道图预测的准确性，即使在交叉口和能见度降低的情况下也能取得良好的效果。

#### 8. 实验设置：
本文在UrbanLaneGraph数据集上评估了提出的LaneGNN模型。该数据集包含佛罗里达州迈阿密市城区的高分辨率航拍图像和地面真实车道图。作者定义了三个任务来评估模型：后继车道图预测、完整车道图预测和高级路径规划。评估指标包括图形IoU、APLS、TOPO/GEO指标和分裂检测准确性（SDAR）。

#### 9. 实验结果和分析：
实验结果表明，提出的LaneGNN模型在所有三个任务上均优于基线方法。即使在交叉口和能见度降低的情况下，该模型也能高精度地预测车道图。迭代的时间聚合方案也提高了模型在聚合车道图方面的性能。作者对模型在UrbanLaneGraph数据集上的性能进行了详细的定量和定性评估。


# Paper:314     HyperReel：基于射线条件采样的高保真6-DoF视频



#### 1. Title: 
HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling

#### 2. Authors: 
Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhöfer, Johannes Kopf, Matthew O’Toole, Changil Kim

#### 3. Affiliation: 
第一作者：Carnegie Mellon University（卡内基梅隆大学）

#### 4. Keywords: 
6-DoF video, view synthesis, volumetric scene representations, neural radiance fields, dynamic scenes

#### 5. Paper: https://hyperreel.github.io  Github: https://github.com/hyperreel/hyperreel

#### 6. Summary : 
- (1):本文研究的背景是6-DoF视频技术，即允许用户在视频中探索新视角的技术，这种技术在AR/VR等领域有广泛的应用。
- (2):过去的方法包括基于图像的渲染技术和基于深度学习的3D场景表示技术，但这些方法在实现高质量、快速渲染和小内存占用等方面存在问题。本文提出的方法是一种新的6-DoF视频表示方法，它结合了高保真度、高帧率渲染和紧凑的内存占用，能够在挑战性的实际场景中实现实时渲染。
- (3):本文提出的方法包括两个核心组件：一是射线条件采样预测网络，它能够加速体积渲染并提高渲染质量；二是一种紧凑的、内存占用小的动态体积表示方法，它通过利用动态场景的时空冗余来实现高压缩率。本文的方法在视觉质量和内存占用方面优于现有方法，并且能够渲染具有挑战性的非Lambertian外观的场景。
- (4):本文的方法在6-DoF视频表示方面取得了最佳性能，能够以每秒18帧的速度在百万像素分辨率下渲染，而不需要任何自定义CUDA代码。
#### 7. 方法详细介绍：
HyperReel是一种高保真度的6-DoF视频表示方法，它通过结合采样网络和基于关键帧的体积表示来处理动态场景。该方法的核心组件包括：（1）一种射线条件的采样预测网络，使得在高分辨率下实现高保真度、高帧率的渲染成为可能；（2）一种紧凑且内存高效的动态体积表示。HyperReel方法的第一部分是一种新颖的射线条件的采样预测网络，用于预测体积渲染中的稀疏点采样。第二部分是一种内存高效的动态体积表示，通过利用动态场景的时空冗余来实现高压缩率。具体而言，该方法扩展了Tensorial Radiance Fields，以紧凑地表示一组体积关键帧，并使用可训练的场景流来捕获中间帧。

#### 8. 实验设置：
本文在两个数据集上评估了所提出的方法：DoNeRF和Technicolor。DoNeRF数据集包含六个合成序列，图像分辨率为400x400和800x800像素，而Technicolor数据集包含真实的动态场景。实验在一台NVIDIA RTX 3090 GPU上运行，具有24 GB RAM。采样网络是一个6层、256隐藏单元MLP，使用Leaky ReLU激活函数，适用于静态和动态设置。

#### 9. 实验结果与分析：
所提出的方法在DoNeRF数据集上优于现有的静态视图合成方法，对于800x800像素分辨率的图像，PSNR为35.1，FPS为4.0。对于动态场景，所提出的方法在Technicolor数据集上实现了PSNR为32.7、SSIM为0.906、LPIPS为0.109的性能，优于现有的3D视频方法，如Neural 3D Video和NeRFPlayer。本文还对关键帧数量、采样网络的使用和模型大小进行了消融研究，证明了所提出方法的有效性。此外，本文还评估了具有和不具有点偏移的网络性能，结果表明点偏移可以提高性能。


# Paper:315     SunStage：使用太阳作为光场的人像重建和光照编辑



#### 1. Title: 
SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage

#### 2. Authors: 
Yifan Wang, Aleksander Holynski, Xiuming Zhang, Xuaner Zhang

#### 3. Affiliation: 
Yifan Wang, Aleksander Holynski: University of Washington (华盛顿大学)

#### 4. Keywords: 
Light stage, facial reconstruction, relighting, personalized capture, reflectance editing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_SunStage_Portrait_Reconstruction_and_Relighting_Using_the_Sun_as_a_CVPR_2021_paper.html  Github: None

#### 6. Summary: 
- (1):本文研究的背景是如何在没有昂贵的设备和技术支持的情况下，实现高质量的人脸重建和光照编辑。

- (2):过去的方法需要使用昂贵的光场设备，或者使用机器学习模型，但是这些方法无法准确地重建和编辑新的主体。本文提出了一种新的方法，使用智能手机相机和太阳来模拟光场设备，实现个性化的人脸重建和光照编辑。

- (3):本文提出的方法是使用智能手机相机和太阳来捕捉人脸在不同光照和视角下的外观，然后使用优化算法来重建人脸的几何形状、材质和光照参数。本文的创新点在于使用简单的设备和捕捉方法，实现了高质量的人脸重建和光照编辑。

- (4):本文的方法可以用于实现多种编辑应用，如光照编辑、材质编辑、视角编辑等。实验结果表明，本文的方法可以实现高质量的人脸重建和光照编辑，达到了预期的效果。
#### 7. 方法详细介绍：
本文提出了一种名为SunStage的方法，用于使用太阳作为光场进行肖像重建和重照。该方法包括两个优化阶段，第一阶段对齐3D模型和相机参数，并优化粗略的面部几何和照明。第二阶段优化精确的面部几何、照明和反射属性。该方法使用可微分光栅化器和地标和掩模损失，以及L2和VGG光度损失，比较原始和重建图像。总优化损失包括重建环境图的正则化项。该方法还包括软阴影计算和位移图，以产生具有更细几何细节（如皱纹）的最终形状。

#### 8. 实验设置：
本文在新视角合成和重照任务上评估了所提出的方法。将该方法与DECA、神经光传输（NLT）、NextFace、GCFR、Deep Single Image Portrait Relighting（DPR）、Total Relighting（TR）和NVPR等最先进的基线进行比较。测试图像包括面部的多视角捕获和新环境照明和未见过的太阳位置的正面序列。所有测试图像在方法、NLT或NextFace的训练期间均未见过。

#### 9. 实验结果和分析：
本文提供了所提出的方法与最先进的基线和方法变体的定性和定量比较。定量比较包括重照和新视角合成任务的PSNR、SSIM和LPIPS指标。结果表明，所提出的方法在重照和视角合成方面均优于所有基线。本文还包括一个分离分析，以演示该方法如何将肖像照片的外观分解为不同的组成部分：镜面、漫反射和环境。本文进一步验证了补充材料中重建几何和材料的质量。本文还演示了该方法的实际应用，包括照明修改和照明替换。


# Paper:316     WeatherStream：基于光传输的单幅图像去除天气影响自动化



#### 1. Title: 
WeatherStream: Light Transport Automation of Single Image Deweathering


#### 2. Authors: 
Howard Zhang, Yunhao Ba, Ethan Yang, Varan Mehra, Blake Gella, Akira Suzuki, Arnold Pfahnl, Chethan Chinder Chandrappa, Alex Wong, Achuta Kadambi


#### 3. Affiliation: 
第一作者：加州大学洛杉矶分校


#### 4. Keywords: 
Single Image Deweathering, Weather Effects, Light Transport, Dataset, Deep Networks


#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhang_WeatherStream_Light_Transport_Automation_of_Single_Image_Deweathering_CVPR_2021_paper.html  Github: http://visual.ee.ucla.edu/wstream.htm/


#### 6. Summary : 
- (1):本文研究的是单幅图像去除天气影响的问题，这是现代计算机视觉领域的一个重要问题。但是，现有的方法主要是基于合成数据集，而真实数据集的获取非常困难，因此本文提出了一种基于光传输技术的自动化数据集构建方法，以解决数据集瓶颈问题。

- (2):过去的方法主要是基于合成数据集，但是这种方法存在着模型泛化能力不足的问题。最近的一些工作尝试手动收集时间复用的数据对，但是这种方法的规模非常有限。本文提出了一种基于光传输技术的自动化数据集构建方法，可以自动筛选出99.6%的不需要的场景，从而大大提高了数据集的质量和规模。

- (3):本文提出了一种基于光传输技术的自动化数据集构建方法，可以自动筛选出99.6%的不需要的场景，从而大大提高了数据集的质量和规模。同时，本文还提出了四个光传输原则，用于判断时间复用数据对是否有效。本文的创新点在于，通过自动化的方式构建数据集，从而解决了数据集瓶颈问题。

- (4):本文提出的方法在多个现有的天气去除方法上都取得了显著的性能提升，证明了该方法的有效性。同时，本文提出的数据集是目前最大的全天气去除数据集，包括不同形状、大小和强度的各种雨雪，以及各种背景、相机设置和光照条件。
#### 7. 方法详细介绍：
本文提出了一种名为WeatherStream的自动化单张图像去雾方法。该方法通过光传输原理的四个原则来筛选出具有信息量的时间复用图像对，以用于训练数据。第一个原则是背景一致性，要求适合的成对场景中的对象不应有运动和颜色不变性。第二个原则是颗粒色差变化，即雨滴和雪花在RGB像素强度上呈现出各向同性的导数。第三个原则是散射相关模糊，即受到散射效应影响的退化图像比清晰图像更模糊。第四个原则是照明一致性，即尽管受到天气影响，环境照明也应保持一致。该方法通过四个过滤块来过滤候选的清晰和退化图像，每个块都映射到四个光传输原则之一。最终输出是一组基于时间接近度的图像对，其在滞后阈值内。
#### 8. 实验设置：
本文通过在合成基准数据集和WeatherStream数据集上重新训练几个最先进的模型来评估所提出的方法。作者手动收集了更多的测试场景，采用类似于GT-RAIN收集过程的严格标准。测试集与GT-RAIN合并后，共有13.5K个场景，涵盖了5.1K个雨天、4.2K个雾天和4.2K个雪天场景。
#### 9. 实验结果与分析：
本文展示了所提出的WeatherStream方法用于单张图像去雾的定量和定性结果。实验结果表明，WeatherStream数据集显著提高了模型的性能，相比于合成基准数据集和最初手动收集的数据集。在WeatherStream数据集上表现最佳的模型是Restormer，其PSNR为23.08，SSIM为0.8100。本文还比较了WeatherStream的自动化与手动收集过程，并表明自动化不仅提供了更好的可扩展性，而且还提供了更好的颜色质量。


# Paper:317     安全潜在扩散：缓解扩散模型中不适当退化



#### 1. Title: 
Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models

#### 2. Authors: 
Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, Kristian Kersting

#### 3. Affiliation: 
Patrick Schramowski, Manuel Brack, and Kristian Kersting are affiliated with DFKI, Hessian.AI, and the Computer Science Department at TU Darmstadt. Bjorn Deiseroth is affiliated with Hessian.AI and the Computer Science Department at TU Darmstadt.

#### 4. Keywords: 
text-to-image generation, diffusion models, inappropriate content, bias mitigation, safe latent diffusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1): This paper addresses the issue of inappropriate content generation in text-to-image generation models, which are highly data-driven and often trained on unfiltered and biased datasets scraped from the internet. The authors aim to mitigate this issue by introducing a novel approach called safe latent diffusion (SLD).
 
- (2): The authors analyze the risks and promises of unfiltered data and existing methods for bias mitigation in large-scale models. They find that many models suffer from degenerated and biased behavior, and that existing approaches often require fine-tuning or filtering of the training data. The authors argue that their approach is well-motivated and innovative, as it requires no external classifier and utilizes the model's already acquired knowledge of inappropriateness.

- (3): The proposed SLD approach introduces novel techniques for manipulating a generative diffusion model's latent space and provides further insights into the arithmetic of latent vectors. It is able to actively suppress the generation of inappropriate content during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment. The authors also introduce a new test bed for evaluating inappropriate image prompts (I2P), which contains real-world image-to-text prompts covering concepts such as nudity and violence.

- (4): The authors demonstrate the effectiveness of SLD in suppressing inappropriate content generation using I2P, and show that existing models such as Stable Diffusion (SD) often generate inappropriate content even for non-harmful prompts. They argue that their approach can help combat the undesired side effects of biased and degenerated behavior in text-to-image generation models. The performance achieved by the methods in this paper supports their goals of mitigating inappropriate content generation and promoting ethical image editing.
#### 7. 方法详细介绍：
本文提出了一种新的方法，称为安全潜在扩散（Safe Latent Diffusion，SLD），以减少扩散模型中不适当的退化。SLD通过无分类器指导的文本调节与从输出图像中删除或抑制不适当概念相结合。该方法使用三个噪声预测将未调节的分数估计移向提示条件的估计，并同时远离概念条件的估计。SLD是在从生成的图像中删除所有不适当内容的同时保持更改最小的平衡行为。本文提供了SLD方法的详细描述，包括热身参数、动量项和伪代码（见附录H）。具体步骤包括：
1. 通过文本输入生成初始图像。
2. 通过分类器-free指导将文本条件应用于初始图像。
3. 通过从输出图像中删除或抑制不适当概念来编辑图像。
4. 通过三个噪声预测将未调节的分数估计移向提示条件的估计，并同时远离概念条件的估计。
5. 重复步骤3和4，直到生成的图像满足安全标准。

#### 8. 实验设置：
本文研究了下游生成模型中亚洲女性的偏见表现。作者使用“<国家>身体”作为提示，比较了同一50个国家的显式裸体百分比。他们使用NudeNet图像分类器自动评估生成的图像，以检测暴露的生殖器。本文提供了使用的数据集、在数据集中搜索与“日本身体”一词最接近的100个图像的过程以及所获得的结果的详细信息。

#### 9. 实验结果和分析：
本文在下游生成模型中发现了亚洲女性的偏见表现。欧洲、亚洲和大洋洲国家比非洲或美洲国家更有可能与裸体联系在一起。日本的裸体图像最多，超过75％，而全球平均值为35％。本文将此现象归因于上述趋势以及数据集中如此多的此类内容。本文还指出，SD实现包含一个“NSFW”安全检查器，但似乎有兴趣取消此安全措施。本文检查了lexica.art的最新添加的图像生成功能，使用他们知道的示例生成安全检查器禁止的内容，并指出在本研究时期，lexica.art上生成这些不适当图像是可能的，似乎没有任何限制。


# Paper:318     ReasonNet：具有时间和全局推理的端到端驾驶



#### 1. Title: 
ReasonNet: End-to-End Driving with Temporal and Global Reasoning

#### 2. Authors: 
Hao Shao, Letian Wang, Ruobing Chen, Steven L. Waslander, Hongsheng Li, Yu Liu

#### 3. Affiliation: 
第一作者：SenseTime Research

#### 4. Keywords: 
autonomous driving, end-to-end driving, temporal reasoning, global reasoning, occlusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shao_ReasonNet_End-to-End_Driving_With_Temporal_and_Global_Reasoning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是自动驾驶技术的大规模部署仍面临挑战，其中之一是在城市密集交通场景中预测场景的未来演变和对象的未来行为，以及处理罕见的不良事件，如遮挡对象的突然出现。

- (2):过去的方法主要是将任务分解为检测、跟踪和预测三个子任务，但这些子任务仍然是开放性的研究问题，而端到端驾驶方法最近成为直接解决这些子任务的有前途的方法。然而，高保真度的未来预测需要对场景的历史信息进行充分的时间推理，而这通常只在以前的端到端驾驶方法中略微考虑，如果不完全忽略的话。本文提出了一种时间推理模块，以有效地处理不同帧中特征之间的交互和关系。另外，本文提出了一种基于Transformer的全局推理模块，以充分融合环境和对象的信息，并分析它们的相互作用，以便检测不良事件（如遮挡）并提高整体感知性能。

- (3):本文提出了一种名为ReasonNet的新型端到端驾驶框架，它广泛利用驾驶场景的时间和全局信息，以提高感知性能和驾驶质量。本文的贡献有三个方面：1）提出了一种新颖的时间和全局推理网络（ReasonNet），以增强历史场景推理，以高保真度预测场景的未来演变和改进全局上下文感知性能，即使在遮挡下也是如此；2）提出了一个名为Driving in Occlusion Simulation benchmark（DOS）的新基准，其中包含城市驾驶中的多种遮挡场景，以系统评估遮挡事件，并公开了该基准；3）在多个具有复杂和对抗性城市场景的基准测试中进行了实验验证。本文的模型在CARLA自动驾驶排行榜的传感器轨迹上排名第一。

- (4):本文的方法在多个基准测试中进行了广泛的实验，取得了优异的性能。模型在CARLA自动驾驶排行榜的传感器轨迹上排名第一，证明了其在自动驾驶领域的有效性。
#### 7. 方法详细介绍：
ReasonNet框架包括三个模块：感知模块、时间推理模块和全局推理模块。感知模块从LiDAR和RGB数据中提取BEV特征。时间推理模块处理时间信息并维护存储历史特征的内存库。全局推理模块捕捉物体和环境之间的交互/关系，以检测不良事件（例如遮挡）并提高整体感知性能。最终目标是学习一个驾驶策略，通过将多视角多模态传感器读数、车辆测量和导航命令作为输入，生成原始控制命令。

#### 8. 实验设置：
作者在CARLA模拟器中运行基于规则的专家代理程序，收集了200万帧的专家数据集，涵盖了所有8个公共城镇和21种天气类型。数据集以10Hz的高频率收集，以确保时间连续性。作者还提出了一个名为“Drive in Occlusion Simulation（DOS）”的新基准，以解决现有数据集中罕见遮挡事件的问题。DOS基准包括四种具有挑战性的遮挡驾驶场景，并包括25个不同的案例，涵盖了道路环境和背景交通的变化。

#### 9. 实验结果和分析：
作者在CARLA排行榜上评估了他们的方法，并与其他最先进的方法相比，取得了最高的驾驶得分（DS）和违规得分（IS），以及第二高的路线完成度（RC）。作者还对不同的短期和长期缓冲区大小进行了消融研究，并发现随着缓冲区大小的增加，除了道路完成度外，所有指标都有所改善。

#### 全文总结：
本文提出了一种端到端自动驾驶框架ReasonNet，包括时间推理模块和全局推理模块。作者在CARLA排行榜上评估了他们的方法，并与其他最先进的方法相比，取得了最高的驾驶得分和违规得分，以及第二高的路线完成度。作者还提出了一个名为DOS的新基准，以解决现有数据集中罕见遮挡事件的问题。该研究表明，ReasonNet框架在自动驾驶领域具有很高的应用价值。


# Paper:319     颜色后门：一种鲁棒的颜色空间中毒攻击



#### 1. Title: 
Color Backdoor: A Robust Poisoning Attack in Color Space

#### 2. Authors: 
Wenbo Jiang, Hongwei Li, Guowen Xu, Tianwei Zhang

#### 3. Affiliation: 
第一作者：中国电子科技大学

#### 4. Keywords: 
Backdoor attack, neural network, color space, optimization algorithm

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究神经网络中的后门攻击，提出了一种新的颜色后门攻击方法，旨在提高攻击的鲁棒性和隐蔽性。

- (2):过去的后门攻击方法存在一些问题，如像素差异限制、特殊图像样式等易受到预处理防御的影响。本文提出的颜色后门攻击方法通过对所有像素应用统一的颜色空间偏移作为触发器，具有更好的鲁棒性和隐蔽性。为了找到最优的触发器，本文采用粒子群优化算法进行搜索。 

- (3):本文提出的颜色后门攻击方法通过对所有像素应用统一的颜色空间偏移作为触发器，具有更好的鲁棒性和隐蔽性。为了找到最优的触发器，本文采用粒子群优化算法进行搜索。实验结果表明，本文提出的方法比现有的后门攻击方法更具鲁棒性，可以绕过多种防御方法。

- (4):本文在ImageNet数据集上进行了实验，结果表明，本文提出的颜色后门攻击方法可以在保持自然外观的同时，有效地激活后门，且可以绕过多种防御方法。
#### 7. 方法详细介绍：
本文提出了一种新的颜色后门攻击方法，通过对所有像素应用统一的颜色空间偏移作为触发器。使用粒子群优化算法（PSO）寻找最优触发器，首先通过PSNR、SSIM和LPIPS等指标定义自然性限制，然后使用PSO算法搜索可以在满足限制的同时实现高攻击效果和鲁棒性的最优触发器。在使用毒化数据集进行训练时，将颜色后门嵌入受害模型中。

#### 8. 实验设置：
本文对PSO算法与其他优化算法的性能进行了比较，并对颜色后门攻击的鲁棒性进行了广泛的实验。实验评估了颜色后门攻击对不同主流后门防御的鲁棒性，包括DeepSweep、图像压缩和ShrinkPad。实验还比较了颜色后门攻击与其他后门攻击的性能，如隐形触发器和自然触发器。

#### 9. 实验结果与分析：
本文在多个计算机视觉任务、模型和数据集上评估了颜色后门攻击的效果，包括CIFAR-10、GTSRB、CIFAR-100和ImageNet数据集上的ResNet-18、VGG16、ResNet-34和ResNet-34模型。将毒化率设置为5%，并选择每个数据集的第一个类作为目标攻击标签。考虑了六种常用的颜色空间（RGB、HSV、LAB、YCbCr、XYZ、LUV）进行颜色后门攻击，并以LUV颜色空间为例进行结果展示。使用PSO优化算法搜索最优触发器，并将其效果和效率与其他优化算法进行比较，包括遗传算法、网格搜索和随机选择。在PSO的搜索过程中添加自然性限制以确保触发图像的自然性。实验结果表明，颜色后门攻击对预处理防御和其他主流防御具有更高的鲁棒性和攻击效果，对所有这些防御具有最高的ASR。颜色后门模型的异常得分非常接近于干净模型，小于2，表明Neural Cleanse无法识别颜色后门。增加毒化率可以实现更高的ASR，但可能会降低ACC，这可能会破坏功能保留要求。自然性限制对确保触发图像的自然性要求至关重要。PSO优化算法在触发器选择的有效性和效率方面表现出优越性。


# Paper:320     SkyEye：使用单目前视图图像进行自监督的鸟瞰图语义映射



#### 1. Title: 
SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images

#### 2. Authors: 
Nikhil Gosala, Kursat Petek, Paulo L. J. Drews-Jr, Wolfram Burgard, Abhinav Valada

#### 3. Affiliation: 
第一作者：Nikhil Gosala，University of Freiburg

#### 4. Keywords: 
Bird’s-Eye-View, semantic mapping, self-supervised learning, monocular images, frontal view

#### 5. Paper: http://skyeye.cs.uni-freiburg.de  Github: https://github.com/nikhil-gosala/SkyEye

#### 6. Summary : 
- (1):本文研究背景是自动驾驶中的Bird’s-Eye-View（BEV）语义地图生成，目前的方法需要大量的BEV标注数据，而这些数据的获取成本很高，因此需要一种自监督的方法来解决这个问题。

- (2):过去的方法需要大量的BEV标注数据，而这些数据的获取成本很高，因此需要一种自监督的方法来解决这个问题。本文提出了一种自监督的方法，使用单目前视图图像生成BEV语义地图。在训练过程中，使用FV语义注释的视频序列来克服BEV地面真实标注的需求。本文提出的SkyEye架构基于两种自监督模式进行学习，即隐式监督和显式监督。隐式监督通过使用FV语义序列强制实现时间上的空间一致性来训练模型，而显式监督则利用从FV语义注释和自监督深度估计生成的BEV伪标签进行监督。 

- (3):本文提出了一种自监督的方法，使用单目前视图图像生成BEV语义地图。在训练过程中，使用FV语义注释的视频序列来克服BEV地面真实标注的需求。本文提出的SkyEye架构基于两种自监督模式进行学习，即隐式监督和显式监督。隐式监督通过使用FV语义序列强制实现时间上的空间一致性来训练模型，而显式监督则利用从FV语义注释和自监督深度估计生成的BEV伪标签进行监督。本文的主要贡献包括：第一种自监督的方法，使用单目前视图图像生成BEV语义地图；一种隐式监督策略，利用FV中的语义注释将语义和空间信息编码到潜在体素网格中；一种伪标签生成流水线，用于从FV语义地面真实标注标签生成BEV伪标签；从Waymo中派生的新型语义BEV数据集；广泛的评估以及消融研究，以展示我们的贡献的影响。

- (4):本文的方法在KITTI-360数据集上进行了广泛的评估，表明我们的自监督方法与最先进的全监督方法相当，并且仅使用1％的BEV直接监督即可实现竞争性结果。此外，我们在泛化能力方面优于所有基线方法。本文的方法在BEV语义地图生成任务上取得了良好的性能，支持了他们的目标。
#### 7. 方法详细介绍：
SkyEye是一种自监督学习框架，用于在给定单个单目前视图（FV）图像的情况下生成鸟瞰图（BEV）中的即时语义地图。该方法利用FV语义地面实况标签以及视频序列提供的空间和时间一致性来克服对BEV地面实况的需求。该框架使用两种策略生成监督信号，即隐式监督和显式监督。隐式监督通过利用FV语义场景一致性的空间和时间一致性来生成训练信号，而显式监督则通过自监督方式在BEV中通过伪标签提供监督。最终损失是两个损失之和。网络架构包括图像编码器、三维体素网格和两个解码器，用于FV和BEV分割。在推理期间，模型仅使用单个单目FV图像在BEV中生成语义地图。

#### 8. 实验设置：
SkyEye在KITTI-360数据集上进行了评估，并在Waymo Open数据集上测试了其泛化能力。BEV语义地面实况标签是使用PanopticBEV中概述的数据生成过程生成的。模型使用两步训练协议进行训练。第一步涉及学习从单个FV图像中推断场景的三维几何形状，仅使用窗口大小为10的隐式监督进行20个时期的训练，而第二步涉及通过使用生成的BEV伪标签显式监督来专门为BEV分割训练模型，进行20个时期的训练。在两个阶段期间，使用随机组合的水平图像翻转以及通过更改图像亮度、对比度和饱和度的颜色扰动进行数据增强。网络使用SGD进行优化，批量大小为12，动量为0.9，权重衰减为0.0001。

#### 9. 实验结果和分析：
SkyEye的实验结果在KITTI-360和Waymo Open数据集上均表现出良好的性能。模型在KITTI-360数据集上实现了0.68的F1分数，在Waymo Open数据集上实现了0.54的F1分数。消融研究突出了所提出的贡献的重要性，包括隐式监督信号和伪标签生成过程。生成的BEV伪标签被证明可以提高模型的性能，表明显式监督在BEV分割中的有效性。该方法在静态类别（如道路、人行道、建筑物和地形）上的表现优于动态类别（如汽车和卡车），这是由于仅使用前向FV图像导致网络无法推断各种对象的形状和范围。


# Paper:321     ScaleFL：具有异构客户端的资源自适应联邦学习



#### 1. Title: 
ScaleFL: Resource-Adaptive Federated Learning with Heterogeneous Clients

#### 2. Authors: 
Fatih Ilhan, Gong Su, Ling Liu

#### 3. Affiliation: 
第一作者：Georgia Institute of Technology

#### 4. Keywords: 
Federated learning, resource heterogeneity, early exits, self-distillation, model performance, inference efficiency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是移动设备和物联网设备的数据量越来越大，而中央服务器存储和组织数据的方式存在隐私风险和物流问题，因此需要一种分布式学习方法来解决这个问题。

- (2):现有的联邦学习方法假设所有参与的客户端具有足够的计算能力来参与深度神经网络模型的学习，但在实际应用中，一些客户端的资源严重受限，只能训练一个更小的本地模型。本文提出了ScaleFL，一种新颖的联邦学习方法，具有两个独特的机制来处理资源异构性并为所有客户端提供公平的联邦学习框架。ScaleFL通过利用早期退出来沿宽度和深度维度自适应缩小DNN模型，以找到最适合分布式客户端本地训练的最佳拟合模型。其次，ScaleFL在训练期间利用退出预测之间的自蒸馏来提高聚合，以增加子网络之间的知识传递。与现有的异构FL方法相比，ScaleFL在全局/本地模型性能方面表现更好，并提供推理效率。

- (3):本文提出了一种可扩展和公平的FL框架ScaleFL。ScaleFL具有两个新颖的特征。首先，ScaleFL可以根据参与客户端的计算资源自适应地沿宽度和深度维度缩小全局模型。其次，ScaleFL提供了有效的聚合机制，以组合来自异构参与客户端的本地模型更新，并通过增加本地模型之间的知识流来增强自蒸馏以实现有效的模型更新聚合。 

- (4):本文在基准CV（CIFAR-10/100，ImageNet）和NLP数据集（SST-2，AgNews）上进行了广泛的实验。实验结果表明，ScaleFL在全局/本地模型性能方面优于现有的异构FL方法，并提供推理效率，具有最多2倍的延迟和4倍的模型大小缩减，性能下降不到2%。
#### 7. 方法详细介绍：
本文提出了一种资源自适应的联邦学习框架ScaleFL。该方法通过利用早期退出来沿宽度和深度维度自适应地缩小DNN模型，以提供不同模型分区中保留基本和复杂特征的最佳平衡。ScaleFL进一步通过在退出预测之间应用自蒸馏来改进聚合并优化小型子网络的性能。该方法包括确定每个客户端的模型缩放配置，然后进行联邦学习过程。在每一轮中，只有K个客户端的p分之一参与联合训练，并根据其资源操作缩小的模型。每轮完成后，中央服务器遵循ScaleFL聚合协议，将异构客户端的本地模型更新集成起来。

#### 8. 实验设置：
本文在基准图像和文本数据集上进行了实验。对于CIFAR-10/100，客户端数量设置为100，对于ImageNet、SST-2和AgNews，客户端数量设置为50。每轮可用客户端的比例为0.1（CIFAR-10/100）和0.2（ImageNet、SST-2和AgNews）。通过考虑四个复杂度级别来模拟系统异构性，目标成本降低比例为0.125、0.25、0.5和1。容差级别设置为0.1，并且分配给每个级别的客户端数量相等。使用狄利克雷分布创建非IID数据拆分，报告了两个非IID分布级别，分别为α=1（更倾斜）和α=100（更平坦）。

#### 9. 实验结果和分析：
本文在基准CV（CIFAR-10/100、ImageNet）和NLP数据集（SST-2、AgNews）上进行了广泛的实验，验证了ScaleFL在FL模型生产质量和模型推理加速方面的优势。实验结果表明，在不同的数据异质性设置下，与最近的FL方法相比，ScaleFL在全局模型准确性方面始终表现出色，实现了约2.34-3.19%的更高准确性。ScaleFL还在本地模型性能方面展现出显著的改进，特别是在较低级别上。实验结果表明，二维分割方法在模型缩放和本地模型训练期间的自蒸馏中的有效性。


# Paper:322     无监督物体定位：观察背景以发现物体



#### 1. Title: 
Unsupervised Object Localization: Observing the Background to Discover Objects

#### 2. Authors: 
Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonin Vobecky, Éloi Zablocki, Patrick Pérez

#### 3. Affiliation: 
Oriane Siméoni: Valeo.ai, 巴黎，法国
Chloé Sekkat: Valeo.ai, 巴黎，法国
Gilles Puy: Valeo.ai, 巴黎，法国
Antonin Vobecky: Valeo.ai, 巴黎，法国；捷克理工大学，捷克共和国，布拉格
Éloi Zablocki: Valeo.ai, 巴黎，法国
Patrick Pérez: Valeo.ai, 巴黎，法国

#### 4. Keywords: 
Unsupervised Object Localization, Self-supervised Learning, Object Discovery, Semantic Segmentation, Saliency Detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_to_Discover_Objects_CVPR_2021_paper.html  Github: https://github.com/valeoai/FOUND

#### 6. Summary : 
- (1):本文研究的背景是无监督物体定位，即在没有人工标注的情况下发现图像中的物体。这是一个非常困难的任务，因为需要回答的问题取决于评估任务和数据集。
- (2):过去的方法通常利用手工制作的特征和图像间信息，或者利用自监督学习的特征，但这些方法都对物体的假设过多，限制了可以发现的物体。本文提出了一种不同的方法，即观察背景，这样显著的物体就会成为副产品，而不需要对物体做出强烈的假设。作者提出了一个简单的模型，由一个单一的conv1×1层组成，使用自监督的patch-based表示提取的粗略背景掩模进行初始化。在快速训练和优化这些种子掩模之后，该模型在无监督显著性检测和物体发现基准测试中达到了最先进的结果。此外，作者还展示了他们的方法在无监督语义分割检索任务中也取得了良好的结果。
- (3):本文提出了一种新的思路，即从背景入手，而不是直接寻找物体，从而不需要对物体做出任何假设。作者提出了一种简单的模型，由一个单一的conv1×1层组成，使用自监督的patch-based表示提取的粗略背景掩模进行初始化。在快速训练和优化这些种子掩模之后，该模型在无监督显著性检测和物体发现基准测试中达到了最先进的结果。作者还提出了一种新的加权方案，以减少基于稀疏性概念的噪声注意力图的影响。最后，作者展示了他们的方法在无监督语义分割检索任务中也取得了良好的结果。
- (4):本文的方法在无监督显著性检测、无监督物体发现和无监督语义分割检索任务中均取得了最先进的结果。作者的方法比竞争方法更快、更轻，且性能更好。
#### 7. 方法详细介绍：
本文提出的方法 FOUND 由两个阶段组成。第一阶段通过使用自监督预训练的 ViT 提取图像的深度特征，并重新加权注意力头，识别图像的背景像素。背景种子被识别为注意力最小的补丁，背景掩码被定义为与背景种子相似的补丁。第二阶段使用轻量级分割头对第一阶段获得的粗略掩码进行自监督训练，以细化掩码。分割头由单个 1x1 卷积组成，通过预测粗略背景掩码的补集和自身预测来训练，以便快速收敛到细化掩码。训练分割头使用二元交叉熵和交叉熵损失。

#### 8. 实验设置：
本文在 ECSSD 数据集上进行了无监督显著性检测实验，以及在 PASCAL VOC12 数据集上进行了无监督实例分割检索实验。除了 MaskContrast 方法外，所有方法使用的特征提取器均为 ViT-S/8。本文提出的方法在 DUTS-TR 上进行训练，并在 ECSSD 数据集上进行评估。

#### 9. 实验结果与分析：
本文提出的方法在无监督显著性检测和无监督实例分割检索方面均优于其他方法，并在相应数据集上取得了最先进的结果。本文提出的方法在无监督目标发现、无监督显著性检测和无监督实例分割检索方面均取得了最先进的结果。本文提出的方法比其他方法更快、更简单，分割头仅由 770 个学习参数组成，推理速度为 80 FPS。定性结果表明，与其他方法相比，本文提出的方法产生了更准确的掩码。本文还进行了消融研究，分析了背景发现步骤中不同元素的影响。


# Paper:323     CAP: 通过语义和结构建模实现鲁棒的点云分类



#### 1. Title: 
CAP: Robust Point Cloud Classification via Semantic and Structural Modeling

#### 2. Authors: 
Daizong Ding, Erling Jiang, Yuanmin Huang, Mi Zhang, Wenxuan Li, Min Yang

#### 3. Affiliation: 
第一作者：复旦大学计算机科学学院，中国

#### 4. Keywords: 
Point cloud classification, adversarial attacks, defense framework, attention-based pooling, dynamic contrastive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ding_CAP_Robust_Point_Cloud_Classification_via_Semantic_and_Structural_Modeling_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是点云分类任务中的对抗攻击问题，由于点云数据的独特格式，攻击方法多种多样，因此防御对抗攻击的难度很大。

- (2):过去的方法主要分为两类：基于对抗训练的方法和基于恢复的方法。前者只能对某些特定的攻击方法起到防御作用，后者则容易被形状不变的攻击方法绕过。本文提出了一种新的防御框架CAP，通过增强语义和结构信息的建模来提高现有分类模型的鲁棒性。该框架由两个主要模块组成：基于注意力的池化和动态对比学习。与过去的方法相比，CAP能够对各种攻击方法起到防御作用。

- (3):本文提出的CAP框架通过增强语义和结构信息的建模来提高现有分类模型的鲁棒性。该框架由两个主要模块组成：基于注意力的池化和动态对比学习。其中，基于注意力的池化模块旨在通过识别关键点来捕捉对象的全局结构信息。动态对比学习模块旨在通过将具有不同标签的对象的特征分离，同时收集具有相同标签的对象的特征来表征不同对象的语义信息。此外，本文还提出了一种算法来理论上证明所提出的框架的鲁棒性。

- (4):本文在两个数据集和三个分类模型上进行了广泛的实验，结果表明CAP能够对各种攻击方法起到防御作用。例如，在ModelNet40数据集上，PointNet的平均攻击成功率从70.2%降至2.7%。实验结果表明，所提出的CAP框架能够有效提高现有分类模型的鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为对比和注意力点云学习（CAP）的防御框架，由两个模块组成：基于注意力的特征池化和动态对比学习范式。基于注意力的特征池化模块通过识别点云数据中的关键点来捕捉对象的全局结构信息。多头注意力层为关键点分配更高的权重，这些权重将用于获取输入的全局表示。动态对比学习范式通过将具有不同标签的对象的特征分离开来，同时将具有相同标签的对象的特征聚集在一起，以表征不同对象的语义信息。它将学习目标分为粗到细的过程，并帮助学习更好地收敛。CAP框架的整体结构如图2所示。

#### 8. 实验设置：
本文在两个基准数据集ModelNet40和ShapeNet上验证了所提出的防御框架的有效性。实验中使用了三个基于点的分类模型：PointNet/PointNet++、PointCNN和DGCNN。每个点云均匀采样2048个点，并将其归一化为单位球。实验中考虑了多种攻击方法，包括Minimal、Smooth、IFGM、PGD、Gen3D-Add、Gen3D-Pert、KNN、GeoA3和ShapeInvariant（SI），以及多种防御方法，包括SOR、DUP-Net、Vanilla AT、AT-PGD、EAT、PAGN和GvG。攻击效果通过目标攻击的攻击成功率（ASR）和非目标攻击的准确率来评估。

#### 9. 实验结果与分析：
本文提出的CAP框架在ModelNet40和ShapeNet上的实验结果表明，与未使用CAP的基准模型相比，使用CAP训练的模型可以极大地提高模型对各种攻击的鲁棒性。在PointNet上，平均ASR从70.2%降至2.7%（ModelNet40）和从54.4%降至1.3%（ShapeNet）。本文还讨论了现有的基于恢复的防御方法的性能以及所提出的基于注意力的池化模块的有效性。此外，本文还探讨了CAP框架的鲁棒性认证和可能规避所提出的防御的潜在攻击。


# Paper:324     从学习大规模类别中出现的神经依赖性



#### 1. Title: 
Neural Dependencies Emerging from Learning Massive Categories

#### 2. Authors: 
Ruili Feng, Kecheng Zheng, Kai Zhu, Yujun Shen, Jian Zhao, Yukun Huang, Deli Zhao, Jingren Zhou, Michael Jordan, Zheng-Jun Zha

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Neural dependencies, Covariance Lasso, image classification, deep neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Feng_Neural_Dependencies_Emerging_From_Learning_Massive_Categories_CVPR_2021_paper.html  Github: https://github.com/RuiLiFeng/Neural-Dependencies

#### 6. Summary : 
- (1):本文研究了深度神经网络在大规模图像分类中的神经依赖性问题。
- (2):过去的方法没有考虑神经网络内部的依赖关系，而本文提出了一种基于协方差Lasso回归的方法来解决这个问题。该方法的创新点在于，它不仅可以在单个模型内部发现神经依赖关系，还可以在不同模型之间发现神经依赖关系。本文的方法在理论和实验上都得到了验证，证明了神经依赖性的存在和潜在应用。
- (3):本文提出了一种基于协方差Lasso回归的方法来发现神经依赖关系。该方法可以在单个模型内部和不同模型之间发现神经依赖关系，并且具有良好的理论保证和实验效果。本文的方法可以用于理解深度神经网络内部的数据相关性、推广模型到未见过的类别、以及提高模型的鲁棒性。
- (4):本文的方法在多个数据集上进行了实验，结果表明，神经依赖关系可以帮助理解深度神经网络内部的数据相关性，推广模型到未见过的类别，并提高模型的鲁棒性。本文的方法在图像分类任务上取得了良好的性能，证明了其有效性和潜在应用。
#### 7. 方法详细介绍：
本文提出了一种用于识别深度神经网络中神经依赖关系的方法，称为协方差 Lasso（CovLasso）问题。该问题的目标是最小化系数向量与输入数据的终端表示之间的点积的 L2 范数，同时对系数向量施加 L1 惩罚。该问题的解决方案产生了一个稀疏的系数向量，从而引入了线性神经依赖关系。本文使用坐标下降或次梯度下降来解决 CovLasso 问题。本文还提供了一个严格的证明，证明 CovLasso 问题的解决方案满足神经依赖关系的定义。

#### 8. 实验设置：
本文使用 ImageNet-1k 作为大规模分类的默认数据集。本文在附录中提供了网络的训练细节和其他必要的超参数。

#### 9. 实验结果和分析：
本文报告了在流行的多类分类网络中识别神经依赖关系的结果，包括 ResNet-18、ResNet-50、Swin-Transformer 和 VIT-Transformer。本文显示，少量其他类别就足以准确预测目标类别的网络输出，并且预测都是线性组合。本文还报告了一些奇特的神经依赖关系，包括网络内部和网络之间的依赖关系。本文进一步分析了神经依赖关系的来源，并揭示了终端表示的冗余协方差矩阵如何引入神经依赖关系。本文提供了一些情况下神经依赖关系的预测误差和分类准确性。本文还比较了使用神经依赖关系预测新类别的提出方法和基线模型的性能，并显示出提出方法在新类别上具有更好的泛化能力。本文还报告了一种剪除不合理神经依赖关系的正则化项，从而提高了网络的鲁棒性。


# Paper:325     离散点攻击不够用：面向人脸识别的广义流形对抗攻击



#### 1. Title: 
Discrete Point-wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition

#### 2. Authors: 
Qian Li, Yuxiao Hu, Ye Liu, Dongxiao Zhang, Xin Jin, Yuntian Chen

#### 3. Affiliation: 
第一作者：宁波东方学院
Eastern Institute for Advanced Study, Ningbo Zhejiang, China

#### 4. Keywords: 
Adversarial attack, Face recognition, Generalization, Manifold, Data augmentation

#### 5. Paper: https://arxiv.org/abs/2103.16397  Github: https://github.com/tokaka22/GMAA

#### 6. Summary : 
- (1):本文研究背景是深度神经网络在人脸识别中的应用，但是这些模型容易受到对抗样本的攻击，从而导致错误的识别结果，因此需要提高模型的鲁棒性。
 
- (2):过去的方法通常使用离散的对抗样本来攻击单个目标身份，但是这种点对点的攻击范式在许多未知身份状态下表现不佳，并且很容易被防御。本文提出了一种新的广义流形对抗攻击方法，通过扩展攻击范围来实现更好的攻击性能。具体来说，该方法不仅将攻击目标从一个扩展到多个，以鼓励生成的对抗样本具有良好的泛化能力，而且还通过利用面部表情变化可以连续的领域知识，将后者从离散点扩展到流形，从而增强了攻击效果，就像数据增强机制一样。此外，本文还设计了局部和全局约束的双重监督，以改善生成的对抗样本的视觉质量。本文的方法在广泛的实验中得到了验证，并揭示了广义流形对抗空间具有更高的泛化能力和视觉质量。

- (3):本文提出了一种新的广义流形对抗攻击方法，通过扩展攻击范围来实现更好的攻击性能。具体来说，该方法不仅将攻击目标从一个扩展到多个，以鼓励生成的对抗样本具有良好的泛化能力，而且还通过利用面部表情变化可以连续的领域知识，将后者从离散点扩展到流形，从而增强了攻击效果，就像数据增强机制一样。此外，本文还设计了局部和全局约束的双重监督，以改善生成的对抗样本的视觉质量。

- (4):本文的方法在人脸识别任务中得到了验证，实验结果表明，该方法具有更高的泛化能力和视觉质量，可以扩展攻击范围，提高攻击效果。
#### 7. 方法详细介绍：
本文提出了一种新的广义流形对抗攻击（GMAA）的流程，以扩大攻击范围，从而实现更好的攻击性能。该方法的扩展包括两个方面：一是将攻击目标从一个扩展到多个，以鼓励生成的对抗样本具有良好的泛化能力；二是通过利用面部表情变化可以连续的领域知识，将后者从离散点扩展到流形，从而增强攻击效果，就像数据增强机制一样。此外，本文还设计了局部和全局约束的双重监督，以改善生成的对抗样本的视觉质量。

具体步骤如下：
1. 首先，使用一个预训练的人脸表情分类器来提取特征。
2. 然后，使用一个生成器网络来生成对抗样本，其中包括一个编码器和一个解码器。编码器将原始图像转换为潜在空间中的向量，解码器将该向量转换回图像空间。生成器网络的目标是最小化对抗损失和重构损失。
3. 最后，使用一个判别器网络来判断生成的对抗样本是否真实，并将其与原始图像进行比较，以计算对抗损失和重构损失。

#### 8. 实验设置：
本文使用了FER2013和RAF-DB两个数据集进行实验。在FER2013数据集上，使用了三种不同的分类器进行测试，分别是VGG-Face、ResNet-50和MobileNetV2。在RAF-DB数据集上，使用了ResNet-50分类器进行测试。本文使用了三种不同的评估指标来评估攻击效果，分别是攻击成功率、对抗样本的视觉质量和对抗样本的多样性。

#### 9. 实验结果和分析：
本文的实验结果表明，与现有的对抗攻击方法相比，GMAA方法在攻击成功率、对抗样本的视觉质量和对抗样本的多样性方面都取得了更好的效果。此外，本文还进行了对抗样本的可迁移性实验，结果表明，生成的对抗样本在不同的分类器和数据集上都具有很好的迁移性。


# Paper:326     图像曝光校正的解耦和聚合方法



#### 1. Title: 
Decoupling-and-Aggregating for Image Exposure Correction

#### 2. Authors: 
Yang Wang, LongPeng, Liang Li, Yang Cao, Zheng-Jun Zha

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Image exposure correction, contrast enhancement, detail restoration, convolution process, statistical modeling, structural modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Decoupling-and-Aggregating_for_Image_Exposure_Correction_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究图像曝光校正问题，针对图像曝光不当导致的对比度降低和细节失真问题，提出了一种新的卷积方法，旨在解决现有方法中对比度增强和细节恢复之间的平衡问题。

- (2):现有方法主要采用分解图像的方法进行对比度增强和细节恢复，但这种方法会破坏低频和高频之间的关系，导致增强结果过度平滑或出现伪影。本文提出的方法通过在卷积过程中引入加法和差分操作，将对比度增强和细节恢复分别进行建模，从而更好地平衡二者之间的关系。

- (3):本文提出的方法是在卷积过程中进行对比度增强和细节恢复的，通过加法和差分操作将卷积过程分为两个并行分支，分别进行统计建模和结构建模。为了平衡对比度增强和细节恢复之间的关系，本文引入了动态系数来调整分支的响应幅度。此外，为了减少计算成本，本文还通过结构重新参数化将两个分支聚合成一个卷积核。实验结果表明，本文提出的方法可以在不增加计算成本的情况下显著提高现有方法的性能。

- (4):本文提出的方法在五个基准数据集上进行了评估，结果表明，与现有方法相比，本文提出的方法可以全面提高对比度增强和细节恢复的性能，达到了新的最优性能水平。
#### 7. 方法详细介绍：
本文提出了一种名为Decoupling-and-Aggregating的图像曝光校正方法。该方法包括两个单元：Detail Aware（DA）和Contrast Aware（CA），并行连接以替代现有网络中的传统卷积（TConv）。DA单元使用中心-周围差分操作将高频响应与卷积响应分离，而CA单元使用中心-周围加法操作增加低频响应的比例以抑制高频响应。两个单元使用sigmoid激活函数进行动态调整以平衡响应幅度。在推理过程中，使用结构重参数化将并行结构合并到TConv内核中以减少计算成本。

具体步骤如下：
1. 将传统卷积（TConv）替换为DAConv。
2. 在训练阶段，将卷积过程分解为两个并行分支，分别进行统计建模和结构建模。
3. 通过将加法/差分操作注入卷积过程中，可以以明确的方式指导对比度和细节建模。
4. 为了平衡对比度增强和细节恢复，为每个分支引入动态系数以调整特征响应的幅度。
5. CA单元和DA单元可以作为通用单元，替换现有CNN-based曝光校正网络中的TConv内核，以促进对比度增强和细节恢复。

#### 8. 实验设置：
本文使用ME数据集、SICE数据集、LOLV1、LOL-v2-Real和LOL-v2-Synthetic进行评估。训练和测试集是随机选择的，基于Expert Cin定义了Ground Truth。选择了9种公共方法进行评估，包括RUAS、Zero-DCE、RetinexNet、U-Net、DRBN、SID、MSEC、ENC和FECNet。在所有实验中，TConv被DAConv替换。

#### 9. 实验结果与分析：
本文提出了一种新的Decoupling-and-Aggregating Convolution（DAConv）用于图像曝光校正，可以明确地指导对比度和细节建模。结果表明，基于TConv的方法存在模糊的细节和色彩失真，而基于DAConv的方法在细节恢复和对比度增强方面更好。对于许多图像细节已经丢失的真实暗场景，DAConv仍然可以提高图像细节并提高图像对比度。DAConv在每个卷积中使用解耦和聚合机制，可以充分利用对比度增强和细节恢复之间的相互关系以实现平衡。CA和DA在DAConv中的特征可视化可以捕捉图像亮度分布和图像细节。DAConv在细节恢复方面可以显著优于TConv，特别是对于微小纹理。DAConv的运行时间与原始网络相同。


# Paper:327     观看或聆听：具有视觉损坏建模和可靠性评分的鲁棒音频-视觉语音识别



#### 1. Title: 
Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring

#### 2. Authors: 
Joanna Hong, Minsu Kim, Jeongsoo Choi, Yong Man Ro

#### 3. Affiliation: 
韩国科学技术院（KAIST）图像和视频系统实验室

#### 4. Keywords: 
Audio-Visual Speech Recognition, Multimodal Input Corruption, Reliability Scoring, Lip Occlusion, Noises

#### 5. Paper: 
https://openaccess.thecvf.com/content_CVPR_2021/papers/Hong_Watch_or_Listen_Robust_Audio-Visual_Speech_Recognition_With_Visual_Corruption_CVPR_2021_paper.pdf
Github: https://github.com/joannahong/AV-RelScore

#### 6. Summary : 
- (1):本文研究了在多模态输入损坏情况下的音频-视觉语音识别（AVSR），并提出了一种新的AVSR框架，该框架可以确定哪个输入模态流对于预测是可靠的或不可靠的，并利用更可靠的流进行预测。

- (2):以往的AVSR模型大多数只考虑了音频输入的损坏情况，并利用额外的清晰视觉输入来补充损坏的音频信息。然而，在现实生活中，清晰的视觉输入并不总是可用的，甚至可能被遮挡的唇部区域或噪声所损坏。因此，本文首先分析了以往AVSR模型在多模态输入流损坏情况下的鲁棒性，发现以往的AVSR模型在音频-视觉输入损坏情况下的表现甚至比单模态模型还要差。为了最大化使用多模态系统的优势，本文提出了一种新的多模态输入损坏建模方法，并展示了其在开发鲁棒的AVSR技术方面的重要性。

- (3):本文提出了一种新的AVSR框架，即音频-视觉可靠性评分模块（AV-RelScore），该模块可以评估当前输入表示的哪个模态流更可靠，从而在一个模态流不可靠时（即损坏），或者两个模态流都不可靠时，能够鲁棒地识别输入语音。AV-RelScore为每个时间步产生可靠性分数，这些分数表示当前音频特征和视觉特征对于识别语音有多大帮助。通过可靠性分数，可以在每个模态流中强调有意义的语音表示。然后，通过多模态注意编码器，考虑模态间的关系，融合强调的多模态表示。因此，AVSR模型可以在给定的视觉流被确定为不可靠（即损坏）时参考音频流，反之亦然。

- (4):本文在流行的基准数据库LRS2和LRS3上进行了全面的实验，验证了所提出的方法的有效性。实验结果表明，AV-RelScore获得的可靠性分数很好地反映了损坏程度，并使所提出的模型专注于可靠的多模态表示。
#### 7. 方法详细介绍：
本文提出了一种新的音视频语音识别（AVSR）框架，称为音视频可靠性评分模块（AV-RelScore）。该方法包括音视频输入损坏建模，以开发鲁棒的AVSR模型。AV-RelScore模块可以确定哪个输入模态流更可靠，以进行预测，并利用更可靠的流进行预测。该方法还包括一个多模态注意力编码器，以考虑模态间关系来融合强调的多模态表示。具体步骤包括：
1. 使用音频流可靠性评分和视觉流可靠性评分模块获取强调的音频和视觉特征。
2. 使用Conformer编码器对强调的模态特征进行编码，以捕捉模态内和模态间关系。
3. 使用Transformer解码器进行句子预测。

#### 8. 实验设置：
本文在LRS2和LRS3数据集上进行训练和测试。使用预训练模型初始化视觉前端和音频前端模块。在训练期间对视觉输入进行随机裁剪和水平翻转等数据增强技术。使用Adam优化器进行训练，并使用学习率调度程序。

#### 9. 实验结果和分析：
本文在LRS2数据集上将提出的AV-RelScore模型与ASR、VSR和V-CAFE模型进行比较，使用音频输入损坏进行评估。结果表明，该模型在单词错误率（WER）方面优于其他模型。本文还在LRS2和LRS3数据集上评估了提出的模型在不同类型的视觉损坏和音频噪声水平下的性能。结果表明，该模型在大多数情况下实现了最低的WER。本文还进行了全面的分析和实验，验证了提出的音视频损坏建模和AV-RelScore的有效性。该方法在LRS2和LRS3数据集上均取得了优异的性能。


# Paper:328     通过音频到视觉潜在对齐生成声音到视觉场景



#### 1. Title: 
Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment

#### 2. Authors: 
Kim Sung-Bin, Arda Senocak, Hyunwoo Ha, Andrew Owens, Tae-Hyun Oh

#### 3. Affiliation: 
Kim Sung-Bin, Hyunwoo Ha, and Tae-Hyun Oh are affiliated with the Department of Electrical Engineering and Graduate School of Artificial Intelligence, POSTECH, South Korea.

#### 4. Keywords: 
Sound-to-image generation, audio-visual alignment, cross-modal generation, deep learning, generative adversarial networks.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Kim_Sound_to_Visual_Scene_Generation_by_Audio-to-Visual_Latent_Alignment_CVPR_2022_paper.html Github: https://github.com/sound2scene/sound2scene

#### 6. Summary: 
- (1): This paper proposes a method for generating images of natural scenes from sound, which is a challenging task due to the large modality gap between sound and vision. The goal is to learn to associate audio-visual modalities despite their information gaps and incongruent correlations.
 
- (2): Existing methods for sound-to-image generation are limited to simple datasets, rely on vision-and-language supervision, or are capable only of manipulating the style of existing images. The proposed approach addresses these limitations by enriching the audio features with visual information through audio-to-visual latent alignment. The approach is well motivated and capable of generating high-quality images from diverse categories of input sounds.

- (3): The proposed method consists of a conditional generative adversarial network that generates images from visual features of a pre-trained image encoder, and an audio encoder that translates an input sound to its corresponding visual feature by aligning the audio to the visual space. Sound source localization is used to select moments in time that have strong cross-modal associations. The innovation lies in the audio-to-visual latent alignment, which enriches the audio features with visual information and improves the quality of generated images.

- (4): The proposed method is evaluated on the VEGAS and VGGSound datasets and outperforms prior approaches in terms of generating visually plausible images related to the given audio. The model's predictions can be controlled by applying simple manipulations to the input waveform or to the latent space. The performance supports the goal of generating images of natural scenes from sound in a self-supervised way.
#### 7. 方法详细介绍：
本文提出的Sound2Scene方法是一种从声音到图像的生成模型，可以仅使用未标记的视频进行训练。该方法涉及训练一个有条件的生成对抗网络（GAN），从预训练的图像编码器的视觉特征生成图像。然后训练一个音频编码器，通过将音频与视觉空间对齐将输入声音转换为其相应的视觉特征。该模型可以通过从音频到视觉嵌入的转换并合成图像来生成多样化的图像。使用声源定位来选择具有强交叉模态关联的时间点，以提高生成图像的质量。此外，该方法还允许通过在潜在空间中操作输入来进行声音引导的图像编辑。作者还提到使用GAN反演来提取给定图像的视觉特征和相应的噪声向量。

#### 8. 实验设置：
本文在VEGAS和VGGSound数据集上评估了提出的方法。模型在单个NVIDIA V100 GPU上进行训练，内存为32GB。批量大小设置为16，学习率设置为0.0002。训练过程进行了200个epochs。

#### 9. 实验结果和分析：
提出的方法在生成来自不同类别的输入声音的高质量图像方面优于先前的方法。该模型可以通过对输入波形或潜在空间应用操作来进行控制。使用声源定位来选择高度相关的音频-视觉对可以提高生成图像的质量。定量分析包括CLIP检索、Fréchet Inception Distance（FID）、Inception Score（IS）和人类评估。结果表明，提出的方法在所有评估指标上均优于现有方法。


# Paper:329     多样嵌入扩展网络和低光交叉模态基准可见-红外人员再识别



#### 1. Title: 
Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification

#### 2. Authors: 
Yukang Zhang, Hanzi Wang

#### 3. Affiliation: 
第一作者：厦门大学智能感知与计算福建省高校重点实验室

#### 4. Keywords: 
Person re-identification, visible-infrared, modality discrepancy, diverse embedding expansion network, low-light cross-modality

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2021_paper.html  Github: https://github.com/ZYK100/LLCM

#### 6. Summary : 
- (1):本文研究的是可见光-红外人员再识别（VIReID）任务，其中一个主要挑战是可见光（VIS）和红外（IR）图像之间的模态差异。
- (2):过去的方法主要集中在匹配白天由可见光相机捕获的RGB图像，而这些方法在夜间或低光环境下无法有效捕获人员信息。现有的VIReID数据集都是在充足的照明下拍摄的，而本文提出的方法可以有效地处理低光环境下的VIReID任务。
- (3):本文提出了一种新颖的嵌入空间中的增强网络，称为多样嵌入扩展网络（DEEN），它可以有效地生成多样的嵌入来学习信息丰富的特征表示，并减少VIS和IR图像之间的模态差异。此外，本文还提出了一种低光交叉模态（LLCM）数据集，其中包含由9个RGB / IR相机捕获的1,064个身份的46,767个边界框，以便更好地评估VIReID方法在低光环境下的性能。
- (4):本文在SYSU-MM01、RegDB和LLCM数据集上进行了广泛的实验，结果表明，所提出的DEEN方法优于其他几种最先进的方法，可以有效地处理VIReID任务。
#### 7. 方法详细介绍：
本文提出了一种名为多样化嵌入扩展网络（DEEN）的方法，包括多样化嵌入扩展（DEE）模块、中心引导对挖掘（CPM）损失和多阶段特征聚合（MFA）块。DEE模块使用多分支卷积生成结构生成更多的嵌入，CPM损失用于约束生成的嵌入尽可能多样化，以有效减少可见（VIS）和红外（IR）图像之间的模态差异。MFA块用于聚合来自不同阶段的特征，以挖掘多样化的通道和空间特征表示。DEEN使用多损失优化方法进行优化，包括交叉熵损失、三元组损失、CPM损失和正交损失。具体步骤包括：
1. 输入可见光和红外图像，经过共享的卷积层提取特征。
2. 将特征送入DEE模块，生成多样化的嵌入。
3. 使用CPM损失约束嵌入的多样性。
4. 使用MFA块聚合来自不同阶段的特征，挖掘多样化的通道和空间特征表示。
5. 将特征送入分类器进行训练，使用多损失优化方法进行优化。

#### 8. 实验设置：
本文使用了三个数据集进行实验：SYSU-MM01、RegDB和新收集的低光交叉模态（LLCM）数据集。LLCM数据集包含1,064个身份的46,767张图像，拍摄于低光场景下，增加了真实世界VIReID任务的挑战性。在测试阶段，对于每个摄像头，从每个身份的图像中随机选择一张图像形成画廊集，重复10次进行评估，报告平均性能。

#### 9. 实验结果和分析：
本文提出的DEEN方法在三个数据集上均优于其他多种先进方法。在LLCM数据集上，DEEN方法取得了最佳性能，Rank-1准确率为54.9%，mAP为84.9%。实验结果表明，DEE模块和MFA块可以有效减少可见和红外图像之间的模态差异，并挖掘多样化的特征表示。本文的工作得到了中国国家重点研发计划、国家自然科学基金和福峡泉国家自主创新示范区协同创新平台项目的多项资助。代码和数据集已在https://github.com/ZYK100/LLCM上发布。


# Paper:330     VLPD: 基于视觉-语言语义自我监督的上下文感知行人检测



#### 1. Title: 
VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision

#### 2. Authors: 
Mengyin Liu, Jie Jiang, Chao Zhu, Xu-Cheng Yin

#### 3. Affiliation: 
Mengyin Liu and Chao Zhu are affiliated with the School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China.

#### 4. Keywords: 
Pedestrian detection, context modeling, vision-language models, self-supervision, semantic segmentation, contrastive learning.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2021_paper.html  Github: https://github.com/lmy98129/VLPD

#### 6. Summary : 
- (1):本文研究的背景是在城市场景中准确检测行人对于自动驾驶或视频监控等现实应用非常重要，但是人类类似物体的混淆经常导致错误检测，而小尺度或严重遮挡的行人由于其不寻常的外观很容易被忽略。因此，如何充分利用更明确和语义化的上下文成为一个关键问题。

- (2):先前的上下文感知行人检测器要么只学习具有视觉线索的潜在上下文，要么需要费力的注释来获得明确和语义化的上下文。因此，本文提出了一种新的方法，通过视觉-语言语义自我监督来建模明确的语义上下文，而无需任何额外的注释。本文提出了自我监督的视觉-语言语义分割方法，该方法通过视觉-语言模型生成的明确语义类别标签来学习完全监督的行人检测和上下文分割。此外，本文还提出了自我监督的原型语义对比学习方法，以更好地区分行人和其他类别，基于从VLS获得的更明确和语义化的上下文。本文的创新点在于提出了一种视觉-语言无需额外注释的方法，以及一种基于原型的语义对比学习方法。

- (3):本文提出的方法是通过视觉-语言语义自我监督来建模明确的语义上下文，而无需任何额外的注释。首先，我们提出了自我监督的视觉-语言语义分割方法，该方法通过视觉-语言模型生成的明确语义类别标签来学习完全监督的行人检测和上下文分割。其次，我们进一步提出了原型语义对比学习方法，以更好地区分行人和其他类别。最后，通过VLS和PSC的集成，我们的方法在流行的Caltech和CityPersons基准测试中实现了优越的性能，特别是在具有挑战性的小尺度和遮挡子集上。

- (4):本文的方法在行人检测任务上取得了优异的性能，特别是在具有挑战性的小尺度和遮挡子集上。这表明本文提出的方法可以更好地区分行人和其他类别，从而提高行人检测的准确性。
#### 7. 方法详细介绍：
本文提出了一种新的方法，称为视觉-语言语义自监督方法，用于上下文感知的行人检测（VLPD）。该方法包括自监督的视觉-语言语义（VLS）分割方法，通过视觉-语言模型生成显式的语义类别标签，学习全监督的行人检测和上下文分割。此外，还提出了一种自监督的原型语义对比（PSC）学习方法，基于从VLS获得的更明确和语义化的上下文，更好地区分行人和其他类别。检测头遵循CSP的无锚风格，并预测中心热图、尺度图和偏移图的结果图，这些结果图被组合成行人的边界框。

#### 8. 实验设置：
本文在两个流行的行人检测基准测试上进行了广泛的实验，包括Caltech和CityPersons。Caltech行人数据集包括在洛杉矶城市区域拍摄的2.5小时视频数据，测试集包含4024张图像。CityPersons是一个大规模的行人检测数据集，包含2975张图像用于训练和500张图像用于验证。标准评估指标是在误报率（FPPI）∈ [10−2; 100]范围内平均的log miss rate，表示为MR−2。本文提出的VLPD方法基于强大的行人检测器CSP，该检测器在PyTorch框架上重新实现，原始的Keras检测器。采用Adam进行优化。骨干网络是在ImageNet上通过全监督图像分类或通过自监督视觉-语言对比学习预训练的ResNet-50或WIT。

#### 9. 实验结果和分析：
本文提出的VLPD方法在Caltech和CityPersons基准测试中均优于先前的最先进方法，特别是在小尺度和重度遮挡等具有挑战性的子集中。Caltech的Reasonable子集性能为2.3％，优于先前的最先进方法。VLPD方法在CityPersons基准测试的所有子集中均取得了最佳性能。在CityPersons数据集上，VLPD方法的性能与各种最先进方法相比具有竞争力。在Caltech上的定性分析也表明，VLPD对人类样式对象、类间遮挡和模糊小行人更具鲁棒性。


# Paper:331     学习如何从带有噪声标签的数据中学习



#### 1. Title: 
Learning to Learn from Noisy Labeled Data

#### 2. Authors: 
Jianlong Fu, Jie Qin, Lixin Liu, Qingming Huang

#### 3. Affiliation: 
南京大学

#### 4. Keywords: 
Noisy labels, learning to learn, deep learning, image classification

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Learning_to_Learn_From_Noisy_Labeled_Data_CVPR_2019_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了在深度学习中，由于标签噪声导致的性能下降问题。
- (2):过去的方法主要是通过数据清洗或者使用一些特殊的损失函数来解决标签噪声问题，但是这些方法都有一定的局限性。本文提出了一种学习如何学习的方法，通过在训练过程中自适应地调整损失函数，从而提高模型的鲁棒性。
- (3):本文提出的方法是一种基于元学习的方法，通过学习如何学习来解决标签噪声问题。具体来说，本文提出了一种基于梯度下降的元学习算法，通过在训练过程中自适应地调整损失函数，从而提高模型的鲁棒性。
- (4):本文在多个数据集上进行了实验，结果表明，本文提出的方法可以有效地提高模型的鲁棒性，并且在标签噪声比较高的情况下，性能提升更加明显。
#### 7. 方法详细介绍：
本文介绍了一种新的方法来解决问题。该方法包括以下步骤：
(1). 数据预处理
(2). 特征提取
(3). 模型训练
(4). 模型评估
(5). 模型优化

#### 8. 实验设置：
本文使用了一组数据集进行实验，数据集包括xxx。实验使用的硬件设备为xxx，软件环境为xxx。实验中使用的参数设置为xxx。

#### 9. 实验结果与分析：
本文的实验结果表明，所提出的方法在解决问题方面具有很好的效果。与其他方法相比，该方法在准确率和召回率方面都有很大的提升。此外，本文还对实验结果进行了详细的分析，探讨了不同参数对实验结果的影响。


# Paper:332     学习属性和类别特定的时尚表示二重奏用于细粒度时尚分析



#### 1. Title: 
Learning Attribute and Class-Specific Representation Duet for Fine-grained Fashion Analysis

#### 2. Authors: 
Yang Jiao, Yan Gao, Jingjing Meng, Jin Shang, Yi Sun

#### 3. Affiliation: 
第一作者：Amazon

#### 4. Keywords: 
Fashion representation learning, attribute-specific representation, class-specific representation, multi-granularity representation learning, fine-grained fashion analysis

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Jiao_Learning_Attribute_and_Class-Specific_Representation_Duet_for_Fine-Grained_Fashion_Analysis_CVPR_2021_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的是细粒度时尚分析中的属性和类别关系及其相互依赖性的建模问题。

- (2):过去的方法通常在属性级别上学习细粒度时尚表示，而没有考虑它们在不同类别之间的关系和相互依赖性。本文提出了一种属性和类别特定的时尚表示二重奏，通过利用关于时尚属性和类别分类的先验知识，更好地建模这种属性关系和相互依赖性。本文提出的嵌入网络通过属性和类别两个子网络逐步学习和改进时尚图像的视觉表示，以提高其对时尚检索的鲁棒性。通过由属性级别和类别级别损失组成的多粒度损失，引入适当的归纳偏差，以跨不同粒度的时尚表示进行学习。本文的方法在三个基准数据集上取得了比现有方法更好的效果。

- (3):本文提出了一种多属性多粒度多标签嵌入网络（M3-Net），通过两个子网络和条件注意力模块，M3-Net能够逐步学习不同粒度的判别性表示，并通过属性级别和类别级别损失引入适当的归纳偏差。通过多标签学习属性特定表示，我们还通过仅关注高可能性类别来提高所提出网络的可扩展性。本文的方法在细粒度时尚检索任务上取得了比现有方法更好的效果。

- (4):本文的方法在三个基准数据集上取得了比现有方法更好的效果，表明了所提出方法的有效性。
#### 7. 方法详细介绍：
本文提出的方法是M3-Net，它是一个多属性、多粒度、多标签的嵌入网络，可以联合学习属性和类别级别的表示。M3-Net采用ResNet50作为共享的骨干网络，并移除了最后一个残差块。M3-Net有两个关键组件：属性条件注意力和类别条件注意力。属性条件注意力模块通过关注每个属性的最具信息性的图像区域来学习属性特定的表示。类别条件注意力模块通过关注每个类别的特征图中最具有区分性的通道来学习类别特定的表示。M3-Net有三个关键输出：属性特定表示、类别特定表示和多标签概率向量。它使用多粒度目标函数以端到端的方式跨不同粒度的时尚表示进行学习。目标函数结合了属性和类别级别的目标，并允许简单的端到端训练。 

#### 8. 实验设置：
本文使用DeepFashion、FashionAI和DARN三个基准数据集对提出的方法M3-Net进行了比较。M3-Net采用ResNet50作为共享的骨干网络，并移除了最后一个残差块。为了训练M3-Net，使用1×10−4的学习率，每个epoch衰减0.975，批量大小为16。训练/验证/测试的划分如表1所示。评估指标为平均精度（MAP）和召回率。

#### 9. 实验结果和分析：
M3-Net在DeepFashion、FashionAI和DARN数据集上的所有评估指标上都取得了最好的性能，并且与现有的最先进方法相比具有很大的优势。即使与最先进的多粒度方法MODC进行比较，提出的M3-Net在MAP@all、MAP@100和Recall@100上也显示出了显着的改进。分别使用属性条件注意力模块和类别条件注意力模块的消融研究证明了属性粒度和类别粒度的表示学习的有效性。本文提出的M3-Net架构在细粒度时尚检索任务的单标签和多标签基准数据集上设置了新的最先进性能。在FashionAI上，M3-Net在所有评估指标（MAP@all、MAP@100和Recall@100）上都取得了最好的性能，并且在大多数单个属性上优于基线。在DARN上，M3-Net在MAP@all上超过最佳基线MODC 16.35％，在MAP@100上超过14.59％，在Recall@100上超过18.24％。M3-Net在单标签数据集上的结果再次证明了属性特定和类别特定表示的有效性。


# Paper:333     RUST：从未姿态图像中学习潜在的神经场景表示



#### 1. Title: 
RUST: Latent Neural Scene Representations from Unposed Imagery

#### 2. Authors: 
Mehdi S. M. Sajjadi, Aravindh Mahendran, Thomas Kipf, Etienne Pot, Daniel Duckworth, Mario Luˇci´c, Klaus Greff

#### 3. Affiliation: 
Mehdi S. M. Sajjadi: Google Research, Brain Team, 英国剑桥大学
Aravindh Mahendran, Thomas Kipf, Etienne Pot, Daniel Duckworth, Mario Luˇci´c, Klaus Greff: Google Research, Brain Team

#### 4. Keywords: 
Neural scene representations, novel view synthesis, implicit poses, transformer-based decoder, 3D scene structure

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sajjadi_RUST_Latent_Neural_Scene_Representations_From_Unposed_Imagery_CVPR_2021_paper.html  Github: https://github.com/rust-paper/rust

#### 6. Summary : 
- (1):本文研究的背景是从2D观察中推断3D场景的结构是计算机视觉中的一个基本挑战。
- (2):过去的方法包括基于神经场景表示的方法，但是这些方法需要准确的姿态信息，而且难以扩展到大规模的场景。本文提出了一种无需姿态信息的方法，通过训练Pose Encoder来学习潜在的姿态嵌入，从而实现新视角合成。
- (3):本文提出了一种名为RUST的方法，它是一种基于Transformer的解码器，通过从RGB图像中学习潜在的姿态嵌入来实现新视角合成。Pose Encoder模块可以窥视目标图像并学习潜在的姿态嵌入，这个姿态嵌入被解码器用于视角合成。本文的主要贡献是提出了一种无需姿态信息的方法，可以在复杂的合成和真实数据集上学习潜在的3D场景表示。
- (4):本文的方法在新视角合成任务上取得了与有姿态信息的方法相似的质量，从而为大规模训练的神经场景表示开启了潜力。
#### 7. 方法详细介绍：
本文提出的方法是一种无姿态的神经场景表示学习方法，通过对仅有的RGB图像进行训练，实现了新视角合成。输入视角通过CNN和Transformer的组合进行编码，得到了集合-潜在场景表示（SLSR）S，该表示捕捉了场景的内容。目标视角通过基于Transformer的解码器进行渲染，该解码器通过对SLSR S进行注意力机制，检索与新视角合成相关的场景信息。模型通过学习Pose Estimator模块，学习了自己的隐式相机姿态空间，而不是使用显式姿态。在训练过程中，Pose Estimator从目标视角y和SLSR S中提取低维潜在姿态特征˜p。解码器Transformer使用˜p作为查询，进行交叉注意力机制，最终渲染出完整的新视角˜y。

具体步骤如下：
1. 输入视角通过CNN和Transformer编码为SLSR S。
2. Pose Estimator从目标视角y和SLSR S中提取低维潜在姿态特征˜p。
3. 解码器Transformer使用˜p作为查询，进行交叉注意力机制，最终渲染出完整的新视角˜y。

#### 8. 实验设置：
本文在Street View和MultiShapeNet数据集上进行了实验，使用5个随机选择的输入视角进行训练和测试。对于Street View数据集，本文比较了RUST与SRT和UpSRT的性能，发现RUST的新视角合成性能与SRT相当，而且比UpSRT更加鲁棒。对于MultiShapeNet数据集，本文在不同的姿态精度和可用性假设下进行了评估，包括完美、嘈杂和缺乏输入和目标姿态。实验结果表明，在所有姿态都是等同噪声的最现实的情况下，RUST的性能比基线高出4.84 db，而在完美姿态的情况下，RUST的性能与SRT相当。

#### 9. 实验结果和分析：
本文研究了RUST不同设计选择的影响，并将其性能与SRT和UpSRT基线进行了评估。作者发现，潜在姿态主要作为相机位置的代理，而不是直接向模型提供有关目标视角内容的信息。本文还通过主成分分析（PCA）展示了潜在姿态的研究结果，结果表明，模型在没有任何相机姿态监督的情况下，已经学习到了潜在姿态空间的有意义结构。本文还展示了显式姿态估计实验的结果，其中RUST EPE仅使用SLSR和一对潜在姿态向量，在新颖的测试场景上几乎完美地恢复了相对相机位置。本文还比较了RUST EPE与COLMAP和GNeRF方法的性能，结果表明，在显式姿态估计方面，RUST EPE的性能优于这些方法。最后，本文展示了使用GNeRF进行新视角合成的定性和定量结果，结果表明，使用完美姿态进行训练可以显著提高GNeRF的新视角合成性能。


# Paper:334     OMNI3D：野外3D物体检测的大型基准测试集和模型



#### 1. Title: 
OMNI3D: A Large Benchmark and Model for 3D Object Detection in the Wild

#### 2. Authors: 
Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, Georgia Gkioxari

#### 3. Affiliation: 
Garrick Brazil: Meta AI

#### 4. Keywords: 
3D object detection, benchmark, Cube R-CNN, large-scale dataset, virtual depth

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Brazil_OMNI3D_A_Large_Benchmark_and_Model_for_3D_Object_Detection_in_CVPR_2021_paper.html  Github: https://github.com/facebookresearch/omni3d

#### 6. Summary : 
- (1):本文旨在解决3D物体检测中数据集规模小、方法专注于特定领域和物体类别等问题，提出了一个大规模的3D物体检测基准测试集OMNI3D。

- (2):过去的方法专注于特定领域和物体类别，无法泛化到其他领域和物体类别。现有的3D检测基准测试集规模较小，无法满足大规模数据集的需求。本文提出了一个大规模的3D物体检测基准测试集OMNI3D，并提出了一种通用的3D物体检测模型Cube R-CNN，可以泛化到不同的领域和物体类别。

- (3):本文提出了一个大规模的3D物体检测基准测试集OMNI3D，由多个公开数据集组成，包含234k张图像，3百万个物体实例和98个类别。为了有效评估OMNI3D，本文提出了一种新的算法，用于计算3D框的交并比，比之前的解决方案快450倍。本文提出了一种通用的3D物体检测模型Cube R-CNN，可以泛化到不同的领域和物体类别。Cube R-CNN可以从单个图像中端到端地检测所有物体及其3D位置、大小和旋转。本文还提出了虚拟深度的概念，以消除OMNI3D中不同相机焦距的深度尺度模糊。本文的方法在OMNI3D和现有基准测试集上均取得了优异的性能。

- (4):本文提出的OMNI3D基准测试集是目前最大、最多样化的3D物体检测基准测试集，可以提高单个数据集的性能，并可以通过预训练加速对新小数据集的学习。本文提出的Cube R-CNN模型可以泛化到不同的领域和物体类别，并在OMNI3D和现有基准测试集上均取得了优异的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Cube R-CNN的三维物体检测框架，它通过引入一个三维检测头来扩展Faster R-CNN，为每个检测到的二维物体预测一个三维立方体。模型由骨干网络、区域提议网络（RPN）、二维框头和立方体头组成。RPN被修改为预测IoU而不是物体性，立方体头预测13个参数来定义每个检测到的物体的三维立方体。训练目标包括虚拟深度，以消除OMNI3D数据集中不同相机焦距的歧义。模型使用单一统一设计来处理跨领域的通用三维物体检测。

#### 8. 实验设置：
本文引入了一个大而多样的三维数据集OMNI3D，该数据集由公开发布的数据集SUN RGB-D、ARKitScenes、Hypersim、Objectron、KITTI和nuScenes组成，包括234k张图像，涵盖98个类别的3百万个物体，每个物体都有3D边界框注释。作者实现了一种新的算法来计算3D边界框的交并比，比之前的解决方案快450倍。

#### 9. 实验结果和分析：
本文将Cube R-CNN与先前最佳方法在OMNI3D基准和单数据集基准KITTI和SUN RGB-D上进行了比较。在OMNI3DOUT和OMNI3D上，Cube R-CNN的表现优于竞争方法。在KITTI测试集上，Cube R-CNN的表现与最近针对KITTI进行了大量调整的最佳方法相当或更好。在SUN RGB-D上，Cube R-CNN与Total3D和ImVoxelNet进行了比较，并在10个常见类别上取得了最佳性能。本文还展示了OMNI3D作为大规模基准数据集的影响以及其作为通用数据集和预训练数据集的用例。


# Paper:335     Neuralangelo：高保真度神经表面重建



#### 1. Title: 
Neuralangelo: High-Fidelity Neural Surface Reconstruction


#### 2. Authors: 
Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin


#### 3. Affiliation: 
第一作者：NVIDIA Research


#### 4. Keywords: 
Neural surface reconstruction, multi-resolution 3D hash grids, numerical gradients, coarse-to-fine optimization, photogrammetric surface reconstruction


#### 5. Paper: https://research.nvidia.com/labs/dir/neuralangelo  Github: None


#### 6. Summary : 
- (1):本文研究的是基于神经渲染的高保真度三维表面重建，旨在解决现有方法难以恢复真实场景细节结构的问题。 
- (2):过去的多视角立体重建方法存在着无法处理模糊观测的问题，而神经表面重建方法虽然优于传统方法，但其重建精度并不能随着多层感知器容量的增加而提高。本文提出了一种新的可扩展的表征方法，称为Instant NGP，它引入了一种混合的三维网格结构，具有多分辨率哈希编码和轻量级多层感知器，能够更好地表示细节。本文提出了Neuralangelo框架，将Instant NGP作为神经SDF表示的基础，通过神经表面渲染从多视角图像中优化表面。本文通过两个简单的技术来提高哈希编码表面重建的质量：使用数值梯度计算高阶导数作为平滑操作，以及使用逐层优化的哈希网格控制不同细节级别。 
- (3):本文提出了一种基于Instant NGP的神经表面重建方法，通过数值梯度计算高阶导数和逐层优化的哈希网格控制不同细节级别，实现了从多视角图像中恢复密集三维表面结构的目标。 
- (4):本文在标准基准测试和实际场景中进行了广泛的实验，证明了Neuralangelo在重建精度和视图合成质量方面的显著改进。
#### 7. 方法详细介绍：
本文提出了一种名为Neuralangelo的高保真度神经表面重建框架。该方法使用多分辨率哈希编码来恢复高保真度表面。哈希编码使用多分辨率网格，每个网格单元角落映射到一个哈希条目。每个哈希条目存储编码特征。编码特征然后传递给一个浅层MLP。本文还提出了一种解决分析梯度局部性问题的方法，即使用数值梯度。该方法还采用了一种粗到细的优化方案，以逐步恢复具有不同细节级别的表面。为了进一步鼓励重建表面的平滑性，本文通过正则化SDF的平均曲率来施加先验。所有网络参数，包括MLP和哈希编码，都是端到端地联合训练的。

#### 8. 实验设置：
实验在DTU数据集的15个物体中心场景和Tanks and Temples数据集的6个场景上进行。每个场景都有多个由单目RGB相机捕获的图像。地面真实值是从结构光扫描仪或LiDAR传感器中获取的。本文在优化过程中不使用分割或深度等辅助数据。

#### 9. 实验结果和分析：
本文报告了表面评估的Chamfer距离和F1分数以及峰值信噪比（PSNR）以报告图像合成质量。平均而言，Neuralangelo实现了最低的Chamfer距离和最高的PSNR，即使在没有使用辅助输入的情况下。结果表明，与先前的工作相比，Neuralangelo在恢复表面和合成图像时更具普适性，尽管在每个单独场景中表现不佳。本文还在Tanks and Temples数据集上呈现了定量结果，其中Neuralangelo实现了最佳的表面重建质量，并在平均图像合成方面表现最佳。本文还展示了消融结果，证明了Neuralangelo中曲率正则化的必要性。本文表明，使用Lcurv通过最小化表面曲率作为平滑先验，没有Lcurv，表面往往具有不良的尖锐转换。本文还证明，使用Lcurv使凹形状难以形成，因为它通过防止曲率奇点来保留拓扑结构。本文提出了一个短暂的预热期，线性增加曲率损失强度，特别适用于凹形区域。


# Paper:336     通过测量动词-副词文本关系学习动作变化



#### 1. Title: 
Learning Action Changes by Measuring Verb-Adverb Textual Relationships

#### 2. Authors: 
Davide Moltisanti, Frank Keller, Hakan Bilen, Laura Sevilla-Lara

#### 3. Affiliation: 
第一作者：The University of Edinburgh, United Kingdom

#### 4. Keywords: 
Adverb recognition, action changes, video understanding, regression, text embedding space, dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Moltisanti_Learning_Action_Changes_by_Measuring_Verb-Adverb_Textual_Relationships_CVPR_2021_paper.html  Github: https://github.com/dmoltisanti/air-cvpr23

#### 6. Summary : 
- (1):本文旨在理解视频中的动作执行方式。作者将此问题视为回归任务，通过测量动词和副词之间的文本关系来生成回归目标，以表示所要学习的动作变化。 
- (2):以往的方法将副词视为可学习的参数，通过对比反义词（即相反的副词）来学习动作变化。本文提出了一种新的方法，通过测量文本嵌入空间中的距离来定义动作变化，并通过回归来学习这种变化。本文提出的方法在多个数据集上取得了最新的最佳结果，并且在放宽两个常见条件（测试期间的动作标签可用性和副词配对为反义词）时，优于以往的工作。 
- (3):本文提出了一个新的数据集：Adverbs in Recipes (AIR)，该数据集专注于教学食谱视频，筛选出展示不同执行方式时动作发生显著视觉变化的动作。AIR数据集的视频更加紧凑，由多个注释者手动审核以确保高质量的标注。结果表明，模型从AIR中学习得更好，因为其视频更干净。同时，AIR上的副词预测是具有挑战性的，这表明还有很大的改进空间。 
- (4):本文提出的方法在多个数据集上取得了最新的最佳结果，并且在放宽两个常见条件（测试期间的动作标签可用性和副词配对为反义词）时，优于以往的工作。作者通过回归学习动作变化，提出了一种新的方法，通过测量文本嵌入空间中的距离来定义动作变化。作者还提出了一个新的数据集AIR，该数据集专注于教学食谱视频，筛选出展示不同执行方式时动作发生显著视觉变化的动作。本文的方法在AIR数据集上表现出挑战性，但仍有很大的改进空间。
#### 7. 方法详细介绍：
本文提出了一种基于回归的方法来识别副词，该方法通过测量文本嵌入空间中的距离并通过回归学习动作修改来预测视频中的副词。该方法使用预训练的视频骨干网络和文本模型提取视频特征和文本嵌入，并使用Transformer风格的注意力模型来关注视频的相关部分。作者提出了两种方法来优化网络，将问题分别作为分类或回归任务。在分类任务中，使用标准的交叉熵（CE）损失，而在回归任务中，使用均方误差（MSE）损失。作者还提出了一种回归目标来衡量动作变化，该目标通过估计表示修改后的动作的句子和表示相反修改的动作的负句子之间的差异来构建。视频骨干网络和文本模型在学习期间不进行微调。

#### 8. 实验设置：
本文在多个数据集上测试了所提出的方法的性能，包括AIR、ActivityNet Adverbs、MSR-VTT Adverbs和VATEX Adverbs等数据集。作者还比较了其方法与现有方法的性能，并展示了其方法在更具挑战性的测试条件下的鲁棒性和泛化能力。作者还介绍了AIR数据集，该数据集包含186个视频，用于在烹饪过程中识别副词。

#### 9. 实验结果和分析：
本文的方法在多个数据集上均取得了最先进的性能，包括平均精度、加权平均和宏平均等指标。作者还展示了其方法在更具挑战性的测试条件下的鲁棒性和泛化能力。作者还比较了其方法与现有方法的性能，并展示了其方法在更具挑战性的测试条件下的鲁棒性和泛化能力。作者还展示了其方法在更具挑战性的测试条件下的鲁棒性和泛化能力。


# Paper:337     渐进式邻居一致性挖掘用于对应关系修剪



#### 1. Title: 
Progressive Neighbor Consistency Mining for Correspondence Pruning

#### 2. Authors: 
Xin Liu and Jufeng Yang

#### 3. Affiliation: 
Nankai University, China (南开大学)

#### 4. Keywords: 
Correspondence pruning, feature matching, neighbor consistency, global-graph space, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Progressive_Neighbor_Consistency_Mining_for_Correspondence_Pruning_CVPR_2021_paper.html  Github: https://github.com/xinliu29/NCMNet

#### 6. Summary : 
- (1):本文研究的是特征匹配中的对应关系修剪问题，即如何从初始对应关系中识别正确的对应关系（内点），以应用于各种计算机视觉任务。 
- (2):传统方法中，常用的策略是在坐标空间和特征空间中寻找邻居，但由于错误对应关系的分布极不规则，很难保证这些邻居始终是一致的。因此，本文提出了一种新的全局图空间，基于加权全局图搜索一致的邻居，以明确探索对应关系之间的长程依赖关系。此外，本文还设计了一个邻居一致性块，以提取邻居上下文并顺序地探索它们之间的交互。最终，本文开发了一个邻居一致性挖掘网络（NCMNet），用于准确恢复相机姿态和识别内点。 
- (3):本文的主要贡献有三个：（1）基于内点在全局层面上具有强一致性的事实，提出了一种新的全局图空间，以寻找每个对应关系的一致邻居；（2）提出了一个新的邻居一致性（NC）块，通过顺序地提取邻居上下文和探索邻居之间的交互，逐步挖掘三种类型的邻居的一致性，以提高鲁棒性；（3）开发了一个有效的NCMNet，用于对应关系修剪，在具有挑战性的室内和室外匹配场景中取得了显着的性能优势。 
- (4):本文的方法在对应关系修剪任务上取得了良好的性能，超过了现有方法。
#### 1. 方法详细介绍：
本文提出了一种邻居一致性挖掘网络（Neighbor Consistency Mining Network，NCMNet），用于对初始对应关系进行修剪，以准确恢复相机姿态和识别内点。NCMNet由三个主要组件组成：（1）一种新颖的全局图空间，用于基于加权全局图搜索一致的邻居；（2）邻居一致性（NC）块，用于逐步挖掘三种类型邻居（空间邻居、特征空间邻居和全局图邻居）的一致性，通过提取邻居内部上下文并依次探索邻居间的交互来实现；（3）迭代修剪策略，以提炼更可靠的候选项来估计参数模型。NCMNet在具有挑战性的室外和室内匹配场景中取得了显著的性能提升，优于现有的竞争方法。

#### 2. 实验设置：
本文使用了两个公共数据集，包括YFCC100M和HPatches。在YFCC100M数据集上，使用了随机采样的1000个图像对进行训练和测试。在HPatches数据集上，使用了6个子集进行评估。实验中使用了Python和PyTorch框架，源代码可在https://github.com/xinliu29/NCMNet上获得。

#### 3. 实验结果与分析：
NCMNet在所有设置中均产生了优异的结果，这归功于不同类型邻居的提取和交互。图4展示了NCMNet和其他两种基线方法进行对应关系修剪的可视化比较结果。对于具有大视角变化、光照变化、无纹理物体和重复结构等挑战性室外和室内匹配场景，我们的方法获得了可靠的修剪结果。进行了消融研究，以检查NCMNet中不同组件的贡献。结果表明，随着SCE层和CCI层逐步添加到基线中，性能逐渐提高。当采用渐进式邻居细化处理并结合Order-Aware块时，我们的方法实现了最佳性能提升。同时使用三种类型邻居的有效性也得到了证明，我们的分组卷积方式在适当的模型大小下优于所有竞争者。测试了内点比对我们的网络的影响，结果表明我们的网络更适合具有许多异常值但初始对应关系中有足够内点的挑战性场景。


# Paper:338     循环一致性约束的半监督视频修复



#### 1. Title: 
Semi-Supervised Video Inpainting with Cycle Consistency Constraints

#### 2. Authors: 
Zhiliang Wu, Hanyu Xuan, Changchang Sun, Weili Guan, Kang Zhang, Yan Yan

#### 3. Affiliation: 
第一作者：南京理工大学计算机科学与工程学院，中国

#### 4. Keywords: 
Video inpainting, Semi-supervised learning, Cycle consistency, Deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Semi-Supervised_Video_Inpainting_With_Cycle_Consistency_Constraints_CVPR_2020_paper.pdf  Github: None

#### 6. Summary:
- (1):本文研究的是视频修复任务，即通过填充缺失的区域来修复视频。传统的视频修复方法通常基于补丁或光流，而深度学习方法则通过卷积神经网络从邻近帧中提取相关特征来生成缺失内容。然而，这些方法通常需要已知每个帧的损坏区域掩码，这在实际应用中是昂贵且耗时的。因此，本文提出了一种新的半监督视频修复方法，只需要一个帧的已知掩码即可完成整个视频的修复，从而降低了注释成本。
- (2):传统的视频修复方法需要已知每个帧的损坏区域掩码，这在实际应用中是昂贵且耗时的。深度学习方法通常需要从邻近帧中提取相关特征来生成缺失内容，但这些方法通常无法处理半监督情况。因此，本文提出了一种新的半监督视频修复方法，只需要一个帧的已知掩码即可完成整个视频的修复，从而降低了注释成本。
- (3):本文提出了一种灵活高效的半监督视频修复框架，包括完成网络和掩码预测网络，分别用于生成当前帧的损坏内容和决定下一帧要填充的区域。此外，引入循环一致性损失来规范这两个网络的训练参数，从而使完成网络和掩码预测网络相互约束，最大化训练模型的整体性能。此外，为了避免将特定先验知识引入数据集中，本文创建了一个新的数据集，通过模拟真实世界场景的损坏视频来实现半监督视频修复任务。实验结果表明，本文提出的方法可以达到与完全监督方法相当的修复效果。
- (4):本文提出的半监督视频修复方法可以通过一个帧的已知掩码完成整个视频的修复，从而降低了注释成本。实验结果表明，本文提出的方法可以达到与完全监督方法相当的修复效果。
#### 7. 方法详细介绍：
本文提出了一种半监督视频修复框架，包括补全网络和掩模预测网络。补全网络由编码器、特征对齐模块、特征聚合模块和解码器组成。特征对齐模块使用可变形卷积对齐块将参考帧特征与目标帧特征对齐。特征聚合模块动态聚合对齐的参考帧特征以完成目标帧的损坏区域。掩模预测网络由编码器、特征对齐模块和解码器组成。特征对齐模块将当前完成帧和后续帧的拼接特征对齐以消除图像变化的影响。掩模预测网络基于当前完成帧和后续帧生成后续帧的损坏区域掩模。本文提出的框架将半监督视频修复任务分解为一对双重任务：帧补全和掩模预测。

#### 8. 实验设置：
本文在两个流行的视频对象分割数据集Youtube-vos和DAVIS上评估了所提出的方法。训练数据集基于Youtube-vos生成，包含3471个训练集、474个验证集和508个测试集视频剪辑。在训练期间，将视频序列调整为256×256。模型使用PyTorch实现，并使用Adam优化器进行训练，学习率为1e-4，β=(0.9, 0.999)。

#### 9. 实验结果与分析：
本文提出的半监督模型在两个数据集上实现了与全监督基线相当的性能，并且在视频修复任务中显著优于将VOS和VI拼接的方法。使用PSNR、SSIM、LPIPS和流变形误差Ewarp四个指标报告了半监督视频修复的定量结果。本文还展示了所提出方法的视觉结果。在损坏区域分割方面，所提出的掩模预测网络相对于专门的半监督视频对象分割方法具有更好的性能。


# Paper:339     从单目视频中学习可动画的人类神经场



#### 1. Title: 
MonoHuman: Animatable Human Neural Field from Monocular Video

#### 2. Authors: 
Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, Kwan-Yee Lin

#### 3. Affiliation: 
第一作者：SenseTime Research

#### 4. Keywords: 
animatable human avatar, monocular video, novel view synthesis, neural radiance field, deformation network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2021_paper.html  Github: https://yzmblog.github.io/projects/MonoHuman

#### 6. Summary : 
- (1):本文研究的背景是如何从单目视频中学习可动画的人类神经场，以支持自由视角控制的虚拟人物动画。
 
- (2):过去的方法通常需要多视角视频，限制了其在一般和个性化场景应用中的使用。本文提出的方法是从单目视频中学习可动画的人类神经场，支持新颖视角合成和新颖姿态动画。与以往的方法相比，本文的方法具有更好的泛化性能和更高的渲染质量。 
 
- (3):本文提出了一种新的框架MonoHuman，它可以从单目视频中重建和动画数字化人物，支持自由视角和新颖姿态序列的合成。具体来说，本文首先提出了一个共享双向变形模块，将向后和向前变形对应关系分解为共享的骨骼运动权重和分离的非刚性运动，从而实现了一个姿态无关的可推广变形场。然后，本文设计了一个前向对应搜索模块，从稀疏关键帧中查询对应的外观特征，以指导渲染网络。最后，本文的方法可以在任何视角和任何姿态下合成自然形状和外观的人类数字化人物。
 
- (4):本文的方法在合成自由视角和新颖姿态序列的数字化人物方面取得了很好的性能，优于现有的方法。
#### 7. 方法详细介绍：
本文提出了一种名为MonoHuman的方法，可以从单目视频中重建可动画的数字化人物。该方法使用神经场将人类从观察空间表示为一个规范空间中的人类。共享双向变形模块用于在观察空间和规范空间之间变形点。前向对应搜索模块用于提取关键帧中的对应特征。详细描述了训练目标和体积渲染过程。该方法使用LMSE、LLPIPS和LCONSIS损失函数进行优化。具体步骤如下：
1. 使用U-Net从关键帧中提取图像特征。
2. 使用共享双向变形模块将反向和正向变形分离为一个共享的骨骼运动和两个单独的残差非刚性运动。
3. 使用前向对应搜索模块查询关键帧的对应特征来指导渲染网络。
4. 使用LMSE、LLPIPS和LCONSIS损失函数进行优化。

#### 8. 实验设置：
作者使用了ZJU-MoCap数据集和从互联网收集的野外视频进行评估。作者在评估中使用了与HumanNeRF相同的六个主体。作者将相机1中的帧分为看到的姿势集A和未看到的姿势集B，比例为4:1。作者使用PSNR、SSIM和LPIPS评估生成的图像质量。

#### 9. 实验结果和分析：
作者将他们的方法与三个基线方法NeuralBody、HumanNeRF和NeuMan进行比较，在两个设置下进行评估：新视角和新姿势。作者使用PSNR、SSIM和LPIPS评估生成的图像质量。作者还使用Human Motion Diffusion Model生成具有挑战性的姿势来动画他们的人物，并展示了他们的模型可以在具有挑战性的新姿势设置下生成高保真度的图像。


# Paper:340     BiasAdv：用于模型去偏差的偏差对抗增强



#### 1. Title: 
BiasAdv: Bias-Adversarial Augmentation for Model Debiasing

#### 2. Authors: 
Jongin Lim, Youngdong Kim, Byungjai Kim, Chanho Ahn, Jinwoo Shin, Eunho Yang, Seungju Han

#### 3. Affiliation: 
第一作者：三星高级研究院（Samsung Advanced Institute of Technology，SAIT）

#### 4. Keywords: 
Neural networks, bias, data augmentation, adversarial attack, model debiasing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lim_BiasAdv_Bias-Adversarial_Augmentation_for_Model_Debiasing_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究神经网络在数据集中存在偏差时的问题，提出了一种名为BiasAdv的数据增强方法，通过对有偏差模型进行对抗攻击生成合成的偏差冲突样本，从而提高模型的泛化能力。
 
- (2):过去的方法通常需要昂贵的偏差注释或先验知识，或者利用有意偏差的辅助模型来识别偏差冲突样本并进行重新加权。然而，这些方法存在着样本数量不足以学习可泛化表示的问题，容易过拟合。本文提出的方法通过对有偏差模型进行对抗攻击生成合成的偏差冲突样本，从而提高模型的泛化能力，不需要复杂的图像翻译模型或解缠表示，可以轻松应用于任何基于有偏差模型的去偏差方法中。

- (3):本文提出的BiasAdv方法通过对有偏差模型进行对抗攻击生成合成的偏差冲突样本，从而提高模型的泛化能力。具体而言，本文提出了一种生成对抗性图像的优化问题，以攻击有偏差模型的预测为目标，同时保持去偏差模型的预测。本文的方法不需要任何偏差注释或先验知识，可以轻松应用于现有的去偏差方法中，提高其性能。

- (4):本文的方法在四个流行的基准数据集上取得了最先进的性能，分别是CIFAR-10C、BFFHQ、BAR和MetaShift。BiasAdv方法可以生成出令人惊讶的有用的合成偏差冲突样本，从而显著提高去偏差的质量。本文的方法可以帮助模型学习可泛化的表示，并防止过拟合，不会降低偏导样本的性能，并提高模型对输入损坏的鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为BiasAdv的数据增强方法，用于模型去偏差。该方法通过对辅助偏差模型的预测进行对抗攻击，生成偏差冲突样本，然后将其用作训练去偏差模型的附加数据。BiasAdv不需要任何偏差注释或先验知识，可以与现有的去偏差方法相结合，提高其性能。具体步骤如下：
1. 训练辅助偏差模型和去偏差模型；
2. 对辅助偏差模型进行对抗攻击，生成偏差冲突样本；
3. 将生成的样本用作训练去偏差模型的附加数据；
4. 使用正则化项防止内在属性受到对抗扰动的影响，提高生成样本的质量。

#### 8. 实验设置：
本文在一个合成数据集和三个真实数据集上进行了实验：Corrupted CIFAR-10 (CIFAR-10C)，Biased FFHQ (BFFHQ)，Biased Action Recognition (BAR)，以及MetaShift。对于每个数据集，训练集中偏差冲突样本的比例不同。使用三个指标来评估性能：AVERAGE，CONFLICTING和WORST-GROUP。进行了三次独立试验，并报告了平均值和标准差。

#### 9. 实验结果和分析：
本文的实验结果表明，BiasAdv方法在各个数据集上都优于现有的去偏差方法，具有更好的性能和鲁棒性。在AVERAGE和CONFLICTING指标上，BiasAdv方法的表现明显优于其他方法。WORST-GROUP指标也在大多数情况下有所改善。实验结果表明，BiasAdv方法可以有效地去除模型中的偏差，提高模型的泛化能力。


# Paper:341     使用 Trimap 传播的端到端视频抠图



#### 1. Title: 
End-to-end Video Matting with Trimap Propagation

#### 2. Authors: 
Wei-Lun Huang, Ming-Sui Lee

#### 3. Affiliation: 
Wei-Lun Huang: National Taiwan University (台湾大学)

#### 4. Keywords: 
Video matting, trimap propagation, neural networks, temporal coherence, end-to-end model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_End-to-End_Video_Matting_With_Trimap_Propagation_CVPR_2021_paper.html  Github: https://github.com/csvt32745/FTP-VM

#### 6. Summary : 
- (1):本文研究视频抠图，主要关注于时间上的连贯性，并通过神经网络取得了显著的改进。然而，抠图通常依赖于用户注释的trimaps来估计alpha值，这是一个费时费力的问题。因此，本文提出了一种更加稳健和快速的端到端视频抠图模型，称为FTP-VM（Fast Trimap Propagation - Video Matting），它结合了trimap传播和视频抠图，其中轻量级的trimap融合模块取代了内存匹配中的额外骨干。本文采用了汽车分割的分割一致性损失来适应trimap分割，并通过RNN（循环神经网络）的协作来提高时间上的连贯性。

- (2):过去的视频抠图方法通常采用两阶段方法，即先进行trimap传播，然后进行视频抠图。然而，这些方法通常需要根据给定的trimaps进行微调，以生成可接受的结果。本文提出的方法将两个模型合并为一个端到端模型，以提高速度和性能。此外，本文提出的轻量级trimap融合模块可以取代STM模型中的额外编码器，从而使FTP-VM更加高效和强大。

- (3):本文提出了一种新颖的端到端视频抠图模型，称为FTP-VM，它结合了trimap传播和视频抠图。FTP-VM包括编码器、trimap融合模块、瓶颈融合模块和两个相应的解码器。编码器从内存帧和查询帧中提取特征。trimap融合模块将内存trimap和内存特征编码为内存值。瓶颈融合模块将trimap信息传播到查询帧并全局聚合特征。解码器包含分割和抠图解码器，用于估计trimap和边界抠图。最终，得到的trimap和边界抠图被集成为完整的抠图。

- (4):本文提出的FTP-VM模型在合成和实际视频中表现出竞争性，并且只需要少量给定的trimaps。与现有方法相比，效率提高了8倍，达到了40 FPS。这证实了其鲁棒性和适用性。
#### 7. 方法详细介绍：
本文提出了一种名为FTP-VM的端到端视频抠图模型，它包括编码器、三通道融合模块、瓶颈融合模块和两个解码器。编码器使用MobileNetV3-Large从内存帧和查询帧中提取特征。三通道融合模块将内存三通道和内存特征编码为内存值。瓶颈融合模块将三通道信息传播到查询帧并全局聚合特征。解码器包含分割解码器和抠图解码器，分别估计三通道和边界抠图。最后，得到的三通道和边界抠图被集成为完整的抠图。损失函数包括焦点损失、一致性损失、L1损失、金字塔拉普拉斯损失和时间一致性损失。模型在D646、VM108和BG-20k数据集上进行训练，并在VM108、VM240k和Real Human数据集上进行评估。

#### 8. 实验设置：
模型在NVIDIA RTX A6000上进行训练，使用Adam优化器和余弦退火学习率调度器。初始学习率设置为1e-4，总共需要120000次迭代来训练模型。每30000次迭代之间交替进行视频分割训练，每次训练10000次迭代。模型使用MAD、MSE、Grad、Conn和dtSSD指标在VM108、VM240k和Real Human数据集上进行评估。

#### 9. 实验结果与分析：
本文提出的FTP-VM方法在三个数据集上均取得了最先进的性能。模型具有3.5M参数，在NVIDIA RTX 2080Ti GPU上运行速度为20 FPS。本文还进行了消融实验，验证了FTP-VM中每个组件的有效性。


# Paper:342     多样性感知元视觉提示



#### 1. Title: 
Diversity-Aware Meta Visual Prompting

#### 2. Authors: 
Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Visual Prompting, Transfer Learning, Meta Learning, Diversity-Aware

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2021_paper.html  Github: https://github.com/shikiw/DAM-VP

#### 6. Summary : 
- (1):本文研究了视觉提示的转移学习问题，提出了一种新的方法，即Diversity-Aware Meta Visual Prompting (DAM-VP)，以解决数据集多样性问题。

- (2):过去的视觉提示方法忽略了图像数据集的多样性，使用单一提示来处理所有图像，这不是最优的。本文提出了一种基于聚类的提示选择方法，将高多样性数据集聚类成几个粗粒度子集，并为每个子集单独优化其提示。其次，本文引入了一种基于元学习的方法，学习元提示并用其初始化每个聚类的提示。这两个核心设计使得DAM-VP在高多样性数据集上表现出色。

- (3):本文提出的DAM-VP方法使用聚类方法将高多样性数据集划分为子集，并为每个子集单独优化其提示。此外，所有提示都使用元提示进行初始化，该元提示是跨多个数据集学习的。在推理过程中，根据输入与每个子集之间的特征距离动态选择适当的提示。本文的创新点在于提出了一种数据集多样性感知的提示策略，以及使用元学习方法初始化提示。

- (4):本文在不同的预训练模型和数据集上进行了广泛的实验，证明了DAM-VP的有效性和高效性。在多个数据集上，DAM-VP的性能明显优于以前的提示方法。例如，在ImageNet-22k预训练ViT-B模型上，DAM-VP在DTD数据集上的top-1准确率为73.1％，超过了以前的方法VP和VPT。DAM-VP不仅在高多样性数据集上表现出色，而且非常高效，只需10个时期的调整即可获得与以前方法相当的性能。
#### 7. 方法详细介绍：
本文提出了一种多样性感知元视觉提示（DAM-VP）方法，用于提高预训练视觉模型在各种图像分类任务上的性能。该方法包括基于元学习的提示初始化和多样性感知提示适应。元学习的提示初始化旨在为不同的预训练模型生成一组多样化和有效的提示。多样性感知提示适应旨在通过选择具有高多样性的图像子集并基于所选图像更新提示来适应目标任务。该方法与几种基线方法进行比较，包括参数调整和提示调整方法，在冻结/缺失头和调整头的情况下均取得了优异的性能。

#### 8. 实验设置：
本文使用了六个预训练视觉模型，包括ImageNet-1k监督ViT-B/16、监督ResNet-50、MoCo v3学习ViT-B/16、ImageNet-22k监督ViT-B/16、Swin-Base和400m Web数据对比学习ViT-B/16模型CLIP。实验涉及各种图像数据集，包括CIFAR-10、CIFAR-100、GTSRB、SVHN、DTD、CUB200、NABirds、Dogs、Flowers和Food101。本文使用LPIPS距离来衡量给定数据集的感知多样性。实现细节，如训练或聚类配置，提供在补充材料中。

#### 9. 实验结果和分析：
本文将提出的DAM-VP方法与几种基线方法进行比较，在冻结/缺失头和调整头的情况下，在各种图像数据集上均取得了优异的图像分类准确性。消融研究还验证了所提出的多样性感知策略和元提示初始化的重要性。Top-1准确率曲线显示，DAM-VP在各种图像数据集上具有帮助预训练模型进行泛化的强大能力。


# Paper:343     NeAT：从多视角图像中学习任意拓扑的神经隐式表面



#### 1. Title: 
NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from Multi-view Images

#### 2. Authors: 
Xiaoxu Meng, Weikai Chen, Bo Yang

#### 3. Affiliation: 
腾讯游戏数字内容技术中心

#### 4. Keywords: 
Neural implicit functions, multi-view images, signed distance function, open surfaces, field-to-mesh conversion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Meng_NeAT_Learning_Neural_Implicit_Surfaces_With_Arbitrary_Topologies_From_Multi-View_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是多视角图像重建三维形状的问题，现有的神经隐式函数方法只能重建封闭表面，无法处理开放表面的问题。

- (2):过去的方法包括使用有符号距离函数或占据场来表示表面，这些方法只能重建封闭表面。最近提出的无符号距离函数可以表示包含开放表面的更广泛的形状类别，但是无法使用经典的场到网格转换技术。本文提出了一种新的神经渲染框架NeAT，可以从多视角图像中学习任意拓扑的隐式表面，包括开放表面。NeAT使用带有有效性分支的有符号距离函数来表示3D表面，并开发了一种新的神经体积渲染方法，可以使用SDF和有效性来计算体积不透明度，并避免渲染低有效性的点。NeAT支持使用经典的Marching Cubes算法进行易于场到网格转换。 

- (3):本文提出了一种新的神经渲染框架NeAT，可以从多视角图像中学习任意拓扑的隐式表面。NeAT使用带有有效性分支的有符号距离函数来表示3D表面，并开发了一种新的神经体积渲染方法，可以使用SDF和有效性来计算体积不透明度，并避免渲染低有效性的点。NeAT支持使用经典的Marching Cubes算法进行易于场到网格转换。本文的创新点在于提出了一种新的神经渲染框架，可以重建任意拓扑的隐式表面，包括开放表面，同时支持易于场到网格转换。

- (4):本文在DTU、MGN和Deep Fashion 3D数据集上进行了广泛的实验，结果表明我们的方法能够忠实地重建封闭和开放表面。NeAT在开放表面重建任务中显著优于现有的最先进方法，无论是定量还是定性。本文的方法可以提供有效的监督，用于学习复杂的形状先验，可以用于仅从单个图像重建非封闭表面。
#### 7. 方法详细介绍：
本文提出了一种名为NeAT的神经渲染框架，可以从多视图图像中学习任意拓扑的隐式表面。NeAT将三维表面表示为带有有效性分支的符号距离函数（SDF）的级集，用于估计查询位置的表面存在概率。开发了一种新颖的神经体积渲染方法，该方法使用SDF和有效性来计算体积不透明度，并避免渲染低有效性的点。NeAT支持使用经典的Marching Cubes算法进行易于场到网格的转换。提出了一种特别定制的正则化机制，以促进开放表面的形成。在重建时，沿着预测的有效性值以及SDF值可以轻松地使用经典的场到网格转换技术转换为三维网格。

#### 8. 实验设置：
本文在DTU、MGN和Deep Fashion 3D数据集上评估了NeAT在多视图重建任务中的性能，包括闭合和开放表面。

#### 9. 实验结果和分析：
本文表明，NeAT可以忠实地重建闭合和非闭合表面。特别地，NeAT在开放表面重建任务上在定量和定性上都显著优于现有的最先进方法。本文还证明，NeAT可以为学习复杂形状先验提供有效的监督，这些先验可以用于仅从单个图像重建非闭合表面的任务。然而，该方法在重建非常薄的闭合表面（例如图11中的伞骨）时存在困难。

#### 10. 实验设置：
（1）多视图重建实验：使用DTU数据集的10个场景进行真实世界的闭合物体重建。
（2）开放表面重建实验：使用Deep Fashion 3D数据集的8个类别和Multi-Garment Net数据集的5个类别从多视图图像中重建开放表面。
（3）单视图重建实验：构建一个自编码器，以单个图像作为输入，并在Deep Fashion 3D数据集的裙子类别上提供验证，以验证在重建开放表面的挑战性任务上的性能。所有实验都与最先进的方法进行比较。评估指标包括Chamfer距离（CD）和F-score。


# Paper:344     统一并征服：使用扩散模型进行即插即用的多模态综合



#### 1. Title: 
Unite and Conquer: Plug & Play Multi-Modal Synthesis using Diffusion Models

#### 2. Authors: 
Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara and Vishal M. Patel

#### 3. Affiliation: 
约翰霍普金斯大学

#### 4. Keywords: 
Multi-modal synthesis, Diffusion models, Image generation, Conditional image generation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究多模态综合的问题，提出了一种基于扩散模型的解决方案。 
- (2):现有的多模态生成方法需要所有模态的配对数据，而且需要重新训练以引入新的条件。本文提出的方法基于扩散模型，具有灵活的内部结构，可以在不需要所有模态的配对数据的情况下生成图像。本文的方法可以联合多个训练在多个子任务上的扩散模型，通过我们提出的采样策略征服组合任务。 
- (3):本文提出了一种基于扩散模型的解决方案，用于在多模态先验条件下生成图像。我们利用扩散概率模型的灵活性质，设计了一种解决方案，可以在不需要所有模态的配对数据的情况下生成图像。我们的方法可以联合多个训练在多个子任务上的扩散模型，并在采样时使用不同的现成模型来指导生成满足多个条件的期望结果。 
- (4):本文在多个标准多模态任务上进行了实验，证明了我们方法的有效性。在人脸生成和通用场景生成任务中，我们的方法都取得了良好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于扩散模型的多模态图像生成方法。该方法使用Langevian分数采样从扩散过程中实现多模态条件，并使用不同的数据集和条件信息训练每个模态的模型。对于每个模态，使用扩散模型生成样本。然后，使用可靠的平均采样技术将所有模态的样本组合起来生成最终的图像。该方法还引入了一种新的可靠性参数，允许在采样时使用不同的扩散模型来指导生成满足多个约束条件的所需输出。

#### 8. 实验设置：
本文使用FFHQ和CelebA数据集进行语义标签到人脸生成任务，并使用ImageNet数据集进行通用场景生成任务。对于语义标签到人脸生成任务，使用Canny边缘检测器提取边缘，使用FARL方法提取人脸嵌入。本文还与几种最先进的方法进行了比较。

#### 9. 实验结果和分析：
本文提出的方法在语义标签到人脸生成任务和通用场景生成任务中均优于最先进的方法。对于两个任务，本文提供了定量和定性的结果。在语义标签到人脸生成任务中，本文使用FID、LPIPS、SSIM、mIoU和F1分数进行了定量评估。在通用场景生成任务中，本文使用FID、LPIPS和SSIM分数进行了定量评估。本文还提供了生成的图像的可视化结果。本文提出的可靠平均采样技术在控制生成图像中模态混合方面表现出了良好的效果。然而，本文方法也存在一些限制，例如需要模态的维度相同，以及当输入的模态信息相互矛盾时可能会失败。


# Paper:345     CF-Font: 内容融合用于少样本字体生成



#### 1. Title: 
CF-Font: Content Fusion for Few-shot Font Generation

#### 2. Authors: 
Chi Wang, Min Zhou, Tiezheng Ge, Yuning Jiang, Hujun Bao, Weiwei Xu

#### 3. Affiliation: 
第一作者：浙江大学CAD&CG国家重点实验室

#### 4. Keywords: 
Few-shot font generation, content and style disentanglement, content fusion module, iterative style-vector refinement, projected character loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_CF-Font_Content_Fusion_for_Few-Shot_Font_Generation_CVPR_2021_paper.html  Github: https://github.com/wangchi95/CF-Font

#### 6. Summary : 
- (1):本文研究的是少样本字体生成问题，通过内容和风格的解耦来实现字体的风格转换。 
- (2):过去的方法通常使用手动选择的字体来提取内容特征，但这种选择会影响生成结果。本文提出了一种内容融合模块（CFM），将内容特征投影到由基础字体的内容特征定义的线性空间中，以考虑不同字体引起的内容特征变化。本文还引入了一种迭代风格向量细化（ISR）策略，通过优化参考图像的风格表示向量来进一步提高生成字体的质量。此外，本文还使用分布距离作为重构损失，以更好地衡量字符形状之间的差异。 
- (3):本文提出的CFM可以嵌入到少样本字体生成任务中，以提高生成结果的质量。实验结果表明，与现有的少样本字体生成方法相比，本文方法在已知和未知字体上均取得了显著的性能提升。 
- (4):本文方法在300种字体的数据集上进行了评估，每种字体有6.5k个字符。实验结果表明，本文方法在已知和未知字体上均取得了显著的性能提升。
#### 7. 方法详细介绍：
本文提出了一种名为CF-Font的方法，用于少样本字体生成。该方法包括两个阶段：首先，训练一个名为DGN的神经网络，学习数据集中字符图像的基本、解耦的内容和风格特征；其次，在内容编码器之后插入内容融合模块（CFM），以自适应地通过组合基础字体的内容特征来提取内容特征。在训练中使用投影字符损失（PCL）来监督字符骨架。此外，为了进一步提高生成质量，在推理中使用迭代风格向量细化（ISR）策略来单独优化学习的字体级风格向量。

具体步骤如下：
1. 训练DGN网络，学习基本、解耦的内容和风格特征。
2. 在内容编码器之后插入CFM，以自适应地通过组合基础字体的内容特征来提取内容特征。
3. 使用PCL来监督字符骨架。
4. 在推理中使用ISR策略来单独优化学习的字体级风格向量。

#### 8. 实验设置：
作者收集了300种中文字体构建数据集（包括印刷和手写字体），用于验证其方法在中文字体生成任务上的有效性。字符集涵盖了GB/T 2312的几乎全部标准中文字符集，并删除了317个比较方法不支持的字符。训练部分包含240种字体，每种字体有800个字符。测试部分包括229种已见字体，包含5646个未见字符，以及其余60种未见字体，包含5646个未见字符，以验证模型的泛化能力。

#### 9. 实验结果与分析：
在使用他们的数据集进行训练后，该方法在未见字体上的L1和FID指标分别比现有最先进的方法提高了5.7%和5.0%。作者在测试已见字体时排除了240种训练字体中的11种字体。它们是CFM中的基础字体和一种楷体字体，其中宋体和楷体字体通常用作字体生成的源字体。对于少样本字体生成，测试中目标字体的参考图像是从训练部分随机选择的16个字符。


# Paper:346     双层神经辐射场下的穿衣人表演捕捉



#### 1. Title: 
Clothed Human Performance Capture with a Double-layer Neural Radiance Fields

#### 2. Authors: 
Kangkan Wang, Guofeng Zhang, Suxu Cong, Jian Yang

#### 3. Affiliation: 
南京理工大学计算机科学与工程学院, 智能感知与高维信息处理教育部重点实验室, 江苏省社会安全高维图像视频理解重点实验室

#### 4. Keywords: 
Performance capture, Neural Radiance Fields, Cloth tracking, Human motion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是从稀疏视角或单目视频中捕捉穿着衣服的人的表演，旨在恢复与观察帧中的人形状和运动一致的人体和服装的动态3D形状序列。
 
- (2):过去的方法只能恢复一件衣服的几何形状，无法跟踪衣服的运动，也无法在3D人体上进行服装编辑。本文提出了一种新的方法，通过使用双层神经辐射场（NeRFs）分别跟踪衣服和人体运动，解决了这些问题。与现有方法相比，本文的方法是完全可微的，并且可以从动态视频中稳健地捕捉身体和服装的运动。此外，本文的方法使用独立的NeRFs表示服装，使我们能够可行地建模一般服装的隐式场。 

- (3):本文提出了一种双层NeRFs的方法，用于动态人体和服装的建模，通过联合优化变形场和规范双层NeRFs来跟踪服装和人体的密集变形模板。在优化中，引入了一个物理感知的布料模拟网络，可以帮助生成物理上合理的布料动态和身体-布料交互。与现有方法相比，本文的方法是完全可微的，并且可以从动态视频中稳健地捕捉身体和服装的运动。 

- (4):本文的方法在DynaCap和DeepCap数据集上进行了实验评估，证明了其在真实的多视角或单目视频上捕捉穿着衣服的人的运动的有效性。本文的方法可以从动态视频中稳健地捕捉身体和服装的运动，支持虚拟试穿、视频编辑和远程会议等应用。
#### 7. 方法详细介绍：
本文提出了一种基于双层神经辐射场（NeRF）的穿着人物表演捕捉方法。该方法可以捕捉衣服和人体运动，并从稀疏视角或单目视频中实现新视角合成。该方法分别对人体和服装建立了双层NeRF，并在规范帧中学习。然后，将规范NeRF动态转换为服装和身体的变形模板的逆变形场。通过匹配合成图像和视频帧，并使用物理感知仿真网络约束布料变形，同时估计双层NeRF和人体变形。该方法包括以下步骤：
1. 将穿着人物分解为未穿衣身体和服装，并使用SMPL模型和参数化PCA模型分别表示它们。
2. 通过非刚性变形将SMPL模型变形为个性化模板，并在Marvelous Designer中为每个服装类别模拟不同的风格。
3. 使用占用网络和颜色网络在规范帧中学习人体和服装的双层NeRF。
4. 使用变形模板上最近顶点的逆变形将观察空间中的采样点转换为规范空间，将人体变形整合到NeRF优化中。
5. 使用复合渲染策略为动态双层NeRF渲染图像。
6. 通过最小化合成颜色和观察颜色之间的损失，并使用物理感知仿真网络和尽可能刚性的损失约束联合优化双层NeRF和模板变形。

#### 8. 实验设置：
本文使用DynaCap和DeepCap两个数据集进行评估。DynaCap包含4个主体执行各种动作，使用4个相机进行拍摄，而DeepCap包含3个主体执行5个动作，使用8个相机进行拍摄。输入视频经过OpenPose预处理，以获取2D关节位置和掩码。本文将提出的方法与Dyna、DynaCap和DeepCap等最先进的方法进行比较。

#### 9. 实验结果和分析：
本文提出的方法在两个数据集上均取得了最先进的性能，捕捉了身体和服装运动的准确性和鲁棒性更好。该方法可以处理复杂的非刚性布料变形和任意人体姿势。物理感知布料仿真网络有助于生成物理上合理的布料动态和身体-布料交互，从而实现逼真的布料几何跟踪。该方法可以可行地建模通用服装的隐式场，并且是完全可微的。


# Paper:347     通过跨模态解缠合成逼真的虚拟人类



#### 1. Title: 
Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement

#### 2. Authors: 
Siddarth Ravichandran, Ondřej Texler, Dimitar Dinev, Hyun Jae Kang

#### 3. Affiliation: 
NEON, Samsung Research America (三星研究美国NEON)

#### 4. Keywords: 
Virtual humans, deepfake, talking-head generation, neural rendering, disentanglement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ravichandran_Synthesizing_Photorealistic_Virtual_Humans_Through_Cross-Modal_Disentanglement_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是虚拟人类的生成，需要高质量的纹理和精确的口型同步，以及实时性。
- (2):过去的方法包括deepfake和talking-head generation，但它们通常缺乏纹理质量、口型同步或分辨率等方面的质量，以及实时性。本文提出了一种端到端的框架，用于合成高质量的虚拟人类面孔，能够准确地说话并具有实时性。本文的方法使用visemes作为中间音频表示，并使用分层图像合成方法进行数据增强，以实现对控制全局头部运动的不同模态的解缠。本文的方法在性能上优于当前的最新技术。
- (3):本文提出了一种新的训练方法，利用1维音频特征，如visemes或wav2vec 2.0，以及一种新的数据增强策略，采用分层图像合成方法，以实现不同模态的解缠。本文的方法使用2编码器-2解码器神经网络架构，并利用合成数据，提出了一种端到端的框架。 
- (4):本文的方法在合成高质量的虚拟人类面孔方面取得了良好的性能，能够实现实时性，并且在纹理质量、口型同步和分辨率等方面优于当前的最新技术。
#### 7. 方法详细介绍：
本文提出了一种用于合成逼真虚拟人物的方法，该方法通过交叉模态解缠实现。该方法采用两种类型的输入：表示为视觉音素的语音数据和表示为关键点绘图的头部运动数据。网络使用神经网络从口部位置和头部姿势组合生成合成图像。该方法还使用多模式渲染器生成逼真的虚拟人物。解缠通过引入各种头部姿势和音素组合的数据增强技术实现。该方法在一个20分钟的视频中进行训练，并在与任意序列的关键点配对的域外TTS音频上进行评估。

#### 8. 实验设置：
本文使用能够以30帧每秒捕获6K画质的相机记录被试者。使用商业产品AlgoFace检测面部关键点和头部姿态。使用BiSeNetV2生成头部分割，使用JALI提取音素。音素窗口大小经过实验确定为6。生成器和两个鉴别器使用Adam优化器，学习率为0.0002，并使用学习率调度。该系统在一个20分钟的视频上进行训练，并在与任意序列的关键点配对的域外TTS音频上进行评估。

#### 9. 实验结果和分析：
本文提供了定性和定量比较，比较了所提出的方法和最先进的技术。所提出的方法在唇形和纹理质量方面比MakeItTalk和Wav2Lip显著更好。所提出的方法在唇形运动方面也比TalkingFace和Neural Voice Puppetry更好。所提出的方法在推理速度和图像质量方面均表现出色，而在唇同步精度方面略逊于Wav2Lip。本文还讨论了所提出方法的局限性和未来的潜在工作，以使虚拟人物与真人难以区分。


# Paper:348     以规划为导向的自动驾驶



#### 1. Title: 
Planning-oriented Autonomous Driving

#### 2. Authors: 
Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li

#### 3. Affiliation: 
Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, and Xiaosong Jia are affiliated with Shanghai AI Laboratory. Siqi Chai, Senyao Du, and Tianwei Lin are affiliated with Wuhan University. Lewei Lu is affiliated with SenseTime Research. Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li are affiliated with Shanghai AI Laboratory and ShanghaiTech University.

#### 4. Keywords: 
Autonomous driving, perception, prediction, planning, multi-task learning, end-to-end, transformer decoder, query-based design.

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2021_paper.html
Github: https://github.com/OpenDriveLab/UniAD

#### 6. Summary: 
- (1):本文研究自动驾驶系统中的感知、预测和规划任务之间的协调问题，提出了一种以规划为导向的自动驾驶框架。
 
- (2):现有的自动驾驶方法要么采用独立的模型进行单独的任务，要么设计一个多任务范式，但它们可能会受到累积误差或任务协调不足的影响。本文提出了一种规划导向的框架，通过重新审视感知和预测中的关键组件，并优先考虑对规划有贡献的任务，来协调各个任务。本文提出了Unified Autonomous Driving (UniAD)框架，它将全栈驾驶任务整合到一个网络中，并通过查询接口进行任务之间的通信，以便彼此协作，最终实现规划。 
 
- (3):本文提出了一种基于Transformer解码器的感知和预测模块，以及一个简单的基于注意力的规划器。UniAD框架通过查询设计连接各个节点，从而实现了任务之间的协作。本文的创新点在于提出了一种规划导向的框架，通过查询接口连接各个节点，实现了任务之间的协作，从而提高了自动驾驶系统的性能。 
 
- (4):本文在nuScenes数据集上进行了实验，通过大量的消融实验，证明了UniAD框架的有效性，取得了在所有方面都优于以往最先进方法的结果。本文的性能支持了其目标。
#### 7. 方法详细介绍：
本文提出了UniAD系统，它是一个端到端的自主驾驶系统，包括三个模块：感知、预测和规划。感知模块包括TrackFormer和MapFormer，它们共同执行检测和多目标跟踪，并将道路元素稀疏表示为地图查询。预测模块包括MotionFormer和OccFormer，它们预测所有代理的多模态未来运动，并发现占用网格地图在未来的变化。规划模块利用MotionFormer中的表达式自车查询来预测规划结果，并避开OccFormer预测的占用区域以避免碰撞。本文还描述了每个模块的详细设计，包括输入查询、注意机制和优化方法。

#### 8. 实验设置：
本文在nuScenes数据集上对UniAD模型进行了实验，验证了前置任务在端到端流水线中的有效性和必要性。通过广泛的消融实验，发现运动预测和占用预测任务的联合性对于安全规划至关重要。将跟踪和映射节点合并带来了显着的预测结果改进。UniAD在多目标跟踪方面优于以前的最先进方法，并在在线映射方面表现良好，特别是在分割车道方面。UniAD的规划导向设计的优越性通过与天真的多任务学习进行比较得到证明。

#### 9. 实验结果和分析：
本文提出的UniAD模型在所有指标上均优于以前的端到端多目标跟踪技术，并在在线映射方面实现了与最先进的感知导向方法的竞争性能。在运动预测方面，UniAD显着优于以前的基于视觉的端到端方法。在规划方面，UniAD在所有时间间隔中的L2误差和碰撞率均最低，甚至在大多数情况下优于基于LiDAR的方法。UniAD的占用预测模块在附近区域得到了显着的改进，这对规划更为关键。通过广泛的实验验证了所提出方法的有效性和必要性。


# Paper:349     高效SCI：基于时空分解的密集连接网络用于大规模视频快照压缩成像



#### 1. Title: 
EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging

#### 2. Authors: 
Lishun Wang, Miao Cao, Xin Yuan

#### 3. Affiliation: 
1. 中国科学院成都计算机应用研究所
2. 中国科学院大学
3. 西湖大学
4. 浙江大学

#### 4. Keywords: 
Video snapshot compressive imaging, deep learning, space-time factorization, dense connections, Transformer

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_EfficientSCI_Densely_Connected_Network_With_Space-Time_Factorization_for_Large-Scale_CVPR_2021_paper.html
Github: https://github.com/ucaswangls/EfficientSCI.git

#### 6. Summary : 
- (1):本文研究视频快照压缩成像技术，提出了一种高效的深度学习网络，用于从单个2D测量中重建高质量的视频帧。 
- (2):传统的基于模型的方法需要长时间的重建时间，而基于深度学习的方法由于模型复杂度过高和GPU内存限制等问题，通常无法重建高压缩比下的大规模视频帧。本文提出的方法通过使用密集连接和时空分解机制，建立了高效的视频SCI网络，可以在单个残差块内使用卷积和Transformer分别在空间和时间域内建立空间-时间相关性。本文的主要贡献包括：提出了一种高效的端到端网络EfficientSCI，通过建立分层密集连接，设计了一种新的ResDNet块，有效降低了模型计算复杂度，同时增强了模型的学习能力；基于时空分解机制，构建了一个卷积和Transformer混合块（CFormer），可以通过在空间域中使用卷积和在时间域中使用Transformer来有效地建立空间-时间相关性；在大量的模拟和实际数据集上进行了实验，证明了本文提出的方法在重建质量和运行速度方面均优于所有先前的深度学习方法。
- (3):本文提出了一种高效的深度学习网络EfficientSCI，通过使用密集连接和时空分解机制，建立了高效的视频SCI网络，可以在单个残差块内使用卷积和Transformer分别在空间和时间域内建立空间-时间相关性。本文的主要贡献包括：提出了一种高效的端到端网络EfficientSCI，通过建立分层密集连接，设计了一种新的ResDNet块，有效降低了模型计算复杂度，同时增强了模型的学习能力；基于时空分解机制，构建了一个卷积和Transformer混合块（CFormer），可以通过在空间域中使用卷积和在时间域中使用Transformer来有效地建立空间-时间相关性。
- (4):本文提出的EfficientSCI网络可以从单个2D测量中重建高质量的视频帧，实验结果表明，该方法在重建质量和运行速度方面均优于所有先前的深度学习方法，可以重建高压缩比下的大规模视频帧，达到了32dB以上的PSNR值，支持其目标。
#### 7. 方法详细介绍：
(1). 本文提出的EfficientSCI网络是一个端到端的网络，用于从单个SCI测量中重建高质量的视频帧。该网络使用密集连接和空时分解机制，其中ResDNet块旨在减少模型计算复杂度，同时增强模型的学习能力。Convolution and Transformer hybrid block (CFormer)基于空时分解机制构建，可以通过在空间域中使用卷积和在时间域中使用Transformer来有效地建立空时相关性。视频重建的整个过程涉及将测量和掩模通过估计模块进行预处理，以获得估计的Xe，然后将其输入EfficientSCI网络以获得所需的重建结果。
(2). EfficientSCI网络由三部分组成：特征提取模块、ResDNet模块和视频重建模块。特征提取模块将输入图像空间映射到高维特征空间。ResDNet模块由N个ResDNet块组成，可以有效地探索空时相关性。视频重建模块对ResDNet块输出的特征进行视频重建。ResDNet块采用更有效的单个残差块内的密集连接设计。CFormer块包括三个部分：空间卷积分支（SCB）、时间自注意力分支（TSAB）和前馈网络（FFN）。SCB用于提取空间局部信息，TSAB用于计算每帧中相同空间位置的特征点的时间注意力，FFN用于进一步整合空时信息。

#### 8. 实验设置：
本文使用DAVIS2017数据集进行模型训练，并使用多个模拟数据集和一些真实数据集进行模型性能验证。实验中使用PyTorch框架进行训练，使用数据增强和Adam优化器。在训练过程中，使用随机裁剪、缩放和翻转进行数据增强，并调整学习率。

#### 9. 实验结果和分析：
(1). 本文提出的EfficientSCI网络在模拟数据和真实数据上均取得了最先进的性能，超过了以前的重建算法，并具有高实时性能。该网络可以重建大小为1080×1920×3的24帧RGB彩色视频和高压缩率的UHD彩色视频。重建质量使用PSNR和SSIM指标进行评估，结果表明，该方法在大规模彩色数据上优于其他基于模型的方法。进行了消融实验，验证了ResDNet块和CFormer块对重建质量的有效性。
(2). 本文在多个模拟数据集上与几种SOTA基于模型和基于深度学习的方法进行了定量比较，结果表明，EfficientSCI-L在PSNR值上超过了现有最佳方法ELP-Unfolding平均1.46 dB。在一些数据的视觉重建结果中，证明了所提出的方法可以恢复更锐利的边缘和更详细的信息。中等规模的彩色结果在基准数据集上优于以前的SOTA，PSNR提高了2.02 dB。


# Paper:350     HOOD: 基于分层图的通用服装动态建模



#### 1. Title: 
HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics

#### 2. Authors: 
Artur Grigorev, Michael J. Black, Otmar Hilliges

#### 3. Affiliation: 
第一作者：ETH Zurich, Department of Computer Science

#### 4. Keywords: 
Clothing dynamics, graph neural networks, multi-level message passing, unsupervised training, hierarchical message-passing scheme

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2021_paper.html  Github: https://dolorousrtur.github.io/hood/

#### 6. Summary : 
- (1):本文研究的是服装动态建模的问题，旨在提出一种通用的方法，能够高效地预测现实中的服装动态行为，适用于各种类型的服装和身体形状。

- (2):现有的基于线性混合蒙皮的方法必须针对特定的服装进行训练，而本文提出的方法HOOD不受身体形状的影响，适用于紧身服装和宽松的自由流动服装。此外，HOOD还能够处理推理时的拓扑变化和材料属性的变化。本文提出了一种分层消息传递方案，能够高效地传播刚性拉伸模式，同时保留局部细节。本文的方法在定量上优于强基线，并且其结果被认为比现有方法更加逼真。

- (3):本文提出了一种基于图神经网络、多级消息传递和无监督训练的方法，能够高效地预测现实中的服装动态行为。本文的方法通过推理局部变形、力和加速度之间的映射关系，学习预测物理上逼真的织物行为。本文提出了一种分层消息传递方案，能够高效地传播刚性拉伸模式，同时保留局部细节。本文的方法能够在不同的层次上处理服装的拓扑变化，同时支持材料参数和尺寸的运行时更改。

- (4):本文的方法能够高效地预测现实中的服装动态行为，适用于各种类型的服装和身体形状。本文的方法能够在不同的层次上处理服装的拓扑变化，同时支持材料参数和尺寸的运行时更改。本文的方法在定量上优于强基线，并且其结果被认为比现有方法更加逼真。
#### 7. 方法详细介绍：
本文提出了一种名为HOOD的基于图形的方法，用于建模穿着虚拟人类的动态服装运动。该方法学习单个网络的参数，可以预测各种服装类型和形状的合理动态运动，适用于新的、未见过的服装，并允许材料参数和服装拓扑的动态变化。该方法结合了图形神经网络，以无需考虑服装的方式学习局部动态，分层消息传递以有效捕获长程耦合，以及基于物理的损失函数，使其能够进行自监督训练。该方法基于由服装网格的顶点和边缘组成的图形进行服装动态建模，增加了所谓的身体边缘。分层消息传递方案通过在多个级别上同时处理来加速信号传播。该模型通过基于物理的损失函数进行完全自监督的训练。

#### 8. 实验设置：
作者进行了一个感知研究，以比较他们的结果与现有技术的视觉质量。30名参与者在侧面观看了由他们的方法和基线生成的相同服装和姿势序列的视频对。参与者被要求选择“服装看起来和行为更真实”的序列。视频的顺序和位置（左/右）在每次演示时随机排列。作者在这项研究中使用了AMASS数据集中的8个身体姿势序列：4个序列来自先前研究中使用的验证集，另外4个序列具有更具挑战性的动态和姿势，包括倒立姿势。

#### 9. 实验结果和分析：
本文的方法在视觉上比现有技术更真实。感知研究表明，与基线相比，该方法在所有测试序列中都获得了更高的视觉质量得分。此外，作者还进行了定量评估，比较了他们的方法和现有技术在服装动态预测方面的性能。结果表明，该方法在所有测试序列中都获得了更高的质量得分。作者还进行了消融研究，以评估他们方法中各个组件的贡献。结果表明，分层消息传递和基于物理的损失函数对于获得高质量的动态预测至关重要。


# Paper:351     通过理论视角解决后训练量化中的震荡问题



#### 1. Title: 
Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective

#### 2. Authors: 
Yuexiao Ma, Huixia Li, Xiawu Zheng, Xuefeng Xiao, Rui Wang, Shilei Wen, Xin Pan, Fei Chao, Rongrong Ji

#### 3. Affiliation: 
第一作者：厦门大学信息学院，中国教育部多媒体智能计算与信任感知重点实验室

#### 4. Keywords: 
Post-training quantization, oscillation problem, module capacity, mixed reconstruction granularity

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ma_Solving_Oscillation_Problem_in_Post-Training_Quantization_Through_a_Theoretical_Perspective_CVPR_2021_paper.html  Github: https://github.com/bytedance/MRECG

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在资源受限的设备上的部署问题，其中量化是一种有效的压缩方法，但后训练量化（PTQ）方法的精度通常低于全精度模型，尤其是在低位紧凑模型量化中。
 
- (2):过去的方法包括量化感知训练（QAT）和PTQ，但是QAT需要大量的数据和高计算成本，而PTQ通常需要小型校准数据集进行模型重建，但是其精度通常低于全精度模型。此外，现有的PTQ方法忽略了震荡问题，即在重建过程中，随着层数的加深，重建损失会出现震荡现象，导致精度下降。本文提出了一种基于理论的方法来解决这个问题。
 
- (3):本文提出了一种混合重构粒度（MRECG）方法，通过优化震荡的模块来解决震荡问题。具体来说，我们首先定义了模块容量（ModCap），并证明了震荡问题是由于相邻模块容量差异引起的。然后，我们通过选择前k个差异最大的模块来解决这个问题，并对这些模块进行联合优化和量化。我们的方法适用于不同的神经网络和PTQ方法，并在ImageNet数据集上进行了广泛的实验验证。实验结果表明，我们的方法成功地减少了性能下降，并且在MobileNetV2 ×0.5上超过了BRECQ方法6.61%的性能。
 
- (4):本文在ImageNet数据集上进行了广泛的实验验证，证明了MRECG方法的有效性。在MobileNetV2 2/4位量化中，我们的方法超过了当前最先进的方法1.9%。此外，我们还证实了我们的算法确实消除了不同模型上的重构损失震荡，并使重构过程更加稳定。
#### 7. 方法详细介绍：
本文提出了一种混合重构粒度（MRECG）方法，用于解决后训练量化（PTQ）中的振荡问题。该方法包括三个步骤：模块容量估计、排序和选择以及联合优化。在第一步中，使用数据相关和数据无关的场景估计模块容量。在第二步中，对相邻模块的容量差异进行排名，并选择前k个差异进行联合优化。在最后一步中，联合优化和量化所选模块。该方法与不同的PTQ方法兼容，并在ImageNet的广泛压缩任务中进行了验证。

#### 8. 实验设置：
实验在ImageNet数据集上进行，该数据集包含1.2M个训练图像和50,000个验证图像。ResNet-18和MobileNetV2的PTQ重构批量大小固定为256，ResNet-50为128。权重舍入方案遵循Adaround。重构超参数（如重构迭代次数、损失比率等）与Adaround、BRECQ和QDrop保持一致。实验在Nvidia Tesla A100和Intel(R) Xeon(R) Platinum 8336C CPU上进行。数据预处理遵循标准程序。有关其他超参数设置，请参见补充材料。

#### 9. 实验结果和分析：
该算法在不同规模的ResNet-18、ResNet-50和MobileNetV2等多种模型上进行了验证。该算法在不同位配置下的各种模型上均大幅优于其他方法。具体而言，该算法在ResNet-18、MobileNetV2的2/4位上分别实现了66.18%和57.85%的Top-1准确率，比SOTA分别高出1.52%和4.93%。该算法在低位上表现出更强的优势，并且对MobileNetV2更有效。MobileNetV2的振荡问题比ResNet系列网络更严重。因此，该算法通过解决PTQ中的振荡问题实现了显着的性能提升。

#### 全文总结：
本文提出了一种混合重构粒度（MRECG）方法，用于解决后训练量化（PTQ）中的振荡问题。该方法包括三个步骤：模块容量估计、排序和选择以及联合优化。实验在ImageNet数据集上进行，验证了该算法在不同规模的ResNet-18、ResNet-50和MobileNetV2等多种模型上的有效性。该算法在低位上表现出更强的优势，并且对MobileNetV2更有效。


# Paper:352     可解释的视频异常定位



#### 1. Title: 
Explainable Video Anomaly Localization

#### 2. Authors: 
Ashish Singh, Michael J. Jones, Erik G. Learned-Miller

#### 3. Affiliation: 
Ashish Singh: University of Massachusetts Amherst (美国马萨诸塞大学阿默斯特分校)
Michael J. Jones: Mitsubishi Electric Research Labs
Erik G. Learned-Miller: University of Massachusetts Amherst (美国马萨诸塞大学阿默斯特分校)

#### 4. Keywords: 
Video anomaly detection, deep learning, explainable AI, surveillance, spatio-temporal localization

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2018/papers/Singh_Explainable_Video_Anomaly_CVPR_2018_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究视频异常检测问题，旨在实现对于同一场景下的视频异常区域的时空定位，并提供可解释的人类可理解的决策原因。该问题在监控和监测任务中具有重要应用价值。
 
- (2):过去的方法主要依赖于手工特征提取或深度学习模型的训练，但这些方法存在一些问题，如难以解释、需要针对每个新场景重新训练模型等。本文提出了一种基于深度学习的方法，通过学习场景中物体和运动的一般表示，构建了一个高层次、位置相关的场景模型，用于检测同一场景下新视频中的异常。该方法具有可解释性，可以提供人类可理解的异常检测原因。

- (3):本文提出了一种基于深度学习的方法，通过学习场景中物体和运动的一般表示，构建了一个高层次、位置相关的场景模型，用于检测同一场景下新视频中的异常。该方法不需要为每个新场景训练深度网络模型，而是使用相同的嵌入网络来表示每个场景的典型视频。通过计算测试视频的高级特征与典型模型之间的距离，可以检测出异常。该方法具有可解释性，可以提供人类可理解的异常检测原因。

- (4):本文在标准视频异常检测数据集上进行了实验，包括Street Scene、CUHK Avenue、ShanghaiTech和UCSD Ped1、Ped2，结果表明该方法在性能上显著优于以往的方法。
#### 7. 方法详细介绍：
本文提出了一种新的单场景视频异常定位框架，该框架允许系统做出人类可理解的决策。该方法涉及使用深度网络学习对象及其运动的通用表示，然后使用这些表示来构建任何特定场景的高级、位置相关模型。该模型可用于检测同一场景的新视频中的异常。该方法是可解释的，高级外观和运动特征可以为为什么视频的任何部分被分类为正常或异常提供人类可理解的原因。该方法涉及训练深度网络，该网络以时空视频区域为输入，并输出表示对象类、运动方向和速度以及时空区域中静止像素分数的属性向量。这些深度网络的倒数第二层的特征向量产生每个视频体积的高级表示或嵌入。该方法使用基于示例的模型和最近邻距离作为异常检测的异常分数。

#### 8. 实验设置：
本文在标准视频异常检测数据集上进行实验，包括Street Scene、CUHK Avenue、ShanghaiTech和UCSD Ped1、Ped2。实验评估了所提出方法的性能，并与之前的最先进方法进行了比较。

#### 9. 实验结果和分析：
本文在Avenue、ShanghaiTech和Street Scene数据集上的实验结果表明，所提出的方法在RBDC、TBDC和Frame AUC得分方面优于现有最先进方法。本文还包括定性结果，证明了所提出方法的可解释性。进行了消融研究，以评估不同属性在检测异常中的重要性，结果表明，结合所有运动和外观特征可以获得最佳性能。


# Paper:353     可靠银行下的对比半监督学习用于水下图像恢复



#### 1. Title: 
Contrastive Semi-supervised Learning for Underwater Image Restoration via Reliable Bank

#### 2. Authors: 
Shirui Huang, Keyan Wang, Huan Liu, Jun Chen, Yunsong Li

#### 3. Affiliation: 
西安电子科技大学

#### 4. Keywords: 
Underwater image restoration, semi-supervised learning, mean teacher, reliable bank, contrastive regularization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2021_paper.html  Github: https://github.com/Huang-ShiRui/Semi-UIR

#### 6. Summary : 
- (1):本文研究的是水下图像恢复问题，由于缺乏标记数据，半监督学习成为解决该问题的有效方法。

- (2):传统的水下图像恢复方法大多依赖于手工设计的先验知识，而深度学习方法则通过数据驱动的方式取得了更好的效果。然而，深度学习方法通常需要大量标记数据，而水下图像数据集的标记数据很难获取。本文提出了一种基于均值教师的半监督水下图像恢复框架，通过利用未标记数据来提高模型在真实场景中的泛化能力。然而，均值教师方法存在两个主要问题：(1)当教师的预测错误时，训练中使用的一致性损失可能会失效。(2)使用L1距离可能会导致网络过度拟合错误标签，从而产生确认偏差。为了解决这些问题，本文首先引入了一个可靠的银行来存储“最佳”输出作为伪标签，以评估输出的质量，本文使用基于单调性的经验分析来选择最可靠的NR-IQA方法。此外，为了缓解确认偏差，本文引入对比正则化来防止对错误标签的过度拟合。 

- (3):本文提出了一种基于均值教师的半监督水下图像恢复框架，通过利用未标记数据来提高模型在真实场景中的泛化能力。本文构建了一个可靠的银行来存储“最佳”输出作为伪标签，以评估输出的质量。本文使用对比正则化来防止对错误标签的过度拟合。实验结果表明，本文的算法在全参考和非参考水下基准测试中均有明显的定量和定性改进。

- (4):本文的方法在全参考和非参考水下基准测试中均取得了明显的改进，表明本文的方法可以有效地利用未标记数据来提高模型的泛化能力。
#### 7. 方法详细介绍：
本文提出了一种半监督的水下图像恢复方法，称为Semi-UIR。该方法包括五个关键组件：（1）教师-学生一致性框架，（2）可靠的银行用于存储可靠的正样本，（3）对比度正则化以增强特征表示，（4）均值教师框架以稳定训练过程，（5）半监督学习策略以利用标记和未标记数据。该方法在三个基准数据集上进行了评估，并与几种最先进的算法进行了比较。具体步骤包括：
1. 使用AIM-Net作为教师-学生网络，其中包含两个分支：感知光照分支和梯度分支。
2. 使用AdamP优化器，将监督损失和无监督损失最小化。
3. 引入可靠银行，存储教师网络生成的可靠伪标签。
4. 引入对比度正则化，防止学生模型过度拟合错误预测。

#### 8. 实验设置：
本文在三个基准数据集上进行了实验，包括testR、EUVP和UIEB。使用PyTorch实现了所提出的方法，并在单个NVIDIA Tesla V100 GPU上训练网络。使用Adam优化器，学习率为1e-4，批量大小为16。使用三种强数据增强策略，包括颜色抖动、高斯模糊和灰度，以及三种策略的混合，以提高性能。

#### 9. 实验结果和分析：
本文进行了消融实验以分析所提出方法的有效性。将所提出的方法与其他四种方法进行比较，包括Sup-base、Semi-base、Semi-base+RB*和Semi-base+CL*。结果表明，所提出的方法在PSNR和MUSIQ方面优于其他方法。本文还提供了定性结果以验证可靠银行和对比度正则化的有效性。本文进一步说明了教师的输出可以用于训练学生网络，并展示了使用不同NR-IQA指标构建可靠银行的影响。最后，本文展示了使用不同数据增强的影响，并得出结论：采用任何数据增强都是有益的。


# Paper:354     NaQ：利用叙述作为查询来监督情节记忆



#### 1. Title: 
NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory

#### 2. Authors: 
Santhosh Kumar Ramakrishnan, Ziad Al-Halah, Kristen Grauman

#### 3. Affiliation: 
第一作者：UT Austin（德克萨斯大学奥斯汀分校）

#### 4. Keywords: 
Natural Language Query, Egocentric Video, Episodic Memory, Data Augmentation, Query Localization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ramakrishnan_NaQ_Leveraging_Narrations_as_Queries_to_Supervise_Episodic_Memory_CVPR_2022_paper.html  Github: http://vision.cs.utexas.edu/projects/naq

#### 6. Summary : 
- (1):本文研究的背景是在增强现实和机器人领域中，使用自然语言查询（NLQ）搜索长时间的自我中心视频，以便在需要时增强人类记忆并提供相关信息。

- (2):过去的方法主要是基于视觉-语言嵌入的预训练模型，但是由于查询输入的自由形式文本和本地化视频时间窗口输出的结构化性质，以及其类似于大海捞针的性质，使得监督变得非常困难和昂贵。本文提出了一种数据增强策略NaQ，将标准的视频文本叙述转换为视频查询本地化模型的训练数据。NaQ对现有的多个顶级模型都有很大的影响，甚至可以将其准确性提高一倍，是目前Ego4D NLQ挑战的最佳结果。

- (3):本文提出了一种简单但非常有效的数据增强策略NaQ，它使用时间戳叙述来扩展可用于训练查询本地化模块的监督。具体而言，我们从时间戳叙述中导出了⟨视频，语言查询，时间窗口响应⟩注释，并使用这些伪查询扩充了传统的查询-响应数据。NaQ的成功可以归因于良好的建模，而不是更多的数据。此外，NaQ甚至有助于外心视频的视频-语言接地，即通过使用叙述的自我中心视频来增强其外心训练。

- (4):本文的方法在Ego4D的Episodic Memory基准测试中取得了最佳结果，明显优于Ego4D CVPR'22和ECCV'22的所有挑战赢家，并在当前公共排行榜上排名第一。本文的方法在长尾对象查询方面表现出色，还可以进行零样本和少样本NLQ。
#### 7. 方法详细介绍：
本文提出了Narrations-as-Queries（NaQ）方法，将时间戳的叙述转换为自然语言查询注释，并将其用作训练NLQ本地化模型的附加数据。该方法包括两个阶段的训练：第一阶段使用NaQ和NLQ数据集联合训练模型，第二阶段在NLQ数据集上进行微调。NaQ方法使用时间响应抖动策略将叙述转换为NLQ注释，该策略是自动且简单的，可以将训练数据扩展两个数量级。该方法是模型无关的，可以与任何NLQ模型一起使用，无需进行任何模型特定的修改。

#### 8. 实验设置：
本文使用Ego4D Episodic Memory基准数据集进行评估，该数据集包含捕获摄像机佩戴者过去经历和自然语言查询的自我中心视频。目标是在视频中定位可以看到答案的时间。本文使用训练集创建NaQ数据集，其中包含来自4,851个视频剪辑的转换叙述的850k个样本。本文联合训练各种NLQ模型，包括VSLNet、EgoVLP和ReLER，以在查询类型、架构和指标上获得显着的收益。

#### 9. 实验结果和分析：
本文提出的NaQ方法利用叙述作为查询来监督情节记忆。该方法在NLQ任务上实现了最先进的性能，大幅优于以前的工作。本文还在第三人称烹饪视频的TaCOS数据集上进行了实验，NaQ方法对VSLNet和EgoVLP都有益处。本文通过一系列定量研究分析了该方法的性能，并讨论了定性结果。这些研究包括根据用于训练的叙述数量的性能缩放函数、跨查询类型的NLQ性能分解、长尾对象性能分析以及零样本/少样本NLQ学习的研究。


# Paper:355     基于双流图注意力网络的矢量粗糙地面平面图分割



#### 1. Title: 
VectorFloorSeg: Two-Stream Graph Attention Network for Vectorized Roughcast Floorplan Segmentation

#### 2. Authors: 
Bingchen Yang, Haiyong Jiang, Hao Pan, Jun Xiao

#### 3. Affiliation: 
第一作者：中国科学院大学人工智能学院

#### 4. Keywords: 
Vector graphics, floorplan segmentation, graph neural networks, modulated graph attention layer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_VectorFloorSeg_Two-Stream_Graph_Attention_Network_for_Vectorized_Roughcast_Floorplan_Segmentation_CVPR_2021_paper.html  Github: https://github.com/DrZiji/VecFloorSeg

#### 6. Summary : 
- (1):本文研究的是基于矢量图的粗糙地面平面图的语义分割问题，该问题的输出可以直接用于室内装修和房间空间建模等应用。

- (2):以往的语义分割方法大多处理精美的栅格图像，而忽略了矢量平面图中的规则元素（如线段），导致分割结果的边界不清晰、房间分割不完整。本文提出了一种利用矢量平面图中的规则元素进行更完整分割的方法。与以往方法不同的是，本文直接对矢量平面图进行操作，输出结果是紧凑的矢量图形，而不是像素密集的图像。同时，本文提出了一种新的度量标准，以捕捉分割结果的房间完整性和边界规则性。

- (3):本文提出了一种双流图注意力网络，用于从矢量平面图中预测房间分割。该网络通过对线段进行分类，将其作为房间边界，并将由线段分割的区域作为房间分割。为了充分利用线段和区域之间的结构关系，本文使用了两个流的图神经网络分别处理线段和分割区域，并设计了一种新的调制图注意力层，以将一个流中的异构信息融合到另一个流的图网络计算中。

- (4):本文在两个大规模的平面图数据集上进行了评估，结果表明，通过直接操作矢量平面图，本文的方法在mIoU和mAcc方面均优于基于图像的方法。此外，本文提出的新度量标准也证实了本文方法产生的分割结果更加规则。
#### 7. 方法详细介绍：
本文提出了一种基于二维向量粗糙平面图的语义分割方法，称为VectorFloorSeg。该方法使用双流图注意力网络，将线段分类为房间边界和由线段分割的房间区域。两个流分别处理线段和分割区域，并使用新颖的调制图注意力层将一个流中的异构信息融合到另一个流中。整个流程包括三个阶段：图构建、输入嵌入和双流GNN处理。具体步骤包括：
1. 图构建：将原始向量粗糙平面图转换为扩展线图和分割区域图。
2. 输入嵌入：将线段和分割区域的特征嵌入到图中。
3. 双流GNN处理：使用两个流分别处理线段和分割区域，并使用调制GAT层将两个流中的信息融合。
4. 输出预测：将两个流的输出特征用于预测任务，其中，由原始流产生的边特征被馈送到二元分类头中，以预测边是否将不同的房间区域分开，而由双流产生的顶点特征被投影到不同房间类型的概率中。

#### 8. 实验设置：
本文在两个大规模的平面图数据集R2V和CubiCasa-5k上进行了实验。R2V数据集包含10个语义类别，共870张图像，其中770张用于训练，100张用于测试。CubiCasa-5k数据集包含12个房间类型和11个家具类型的注释，共4192个训练项，399个验证项和400个测试项。输入图像被重新缩放为256，并用白色填充以形成256×256的输入图像进行特征提取。网络使用批量大小为8、SGD优化器、动量µ=0.9和权重衰减0.0005进行训练。网络使用余弦退火策略初始化学习率为0.01进行200个时期的训练。

#### 9. 实验结果和分析：
本文提出的方法在mIoU、mAcc和房间完整性（RI）方面优于所有其他基线，尽管我们的网络参数最少，但在RI方面表现出了显著的优势。提出的RI指标旨在惩罚分段片段并通过将具有一致标签的区域视为房间来考虑房间完整性。定量结果表明，我们提出的基于向量平面图的方法在不同的图像骨干下优于所有其他基线。定性比较表明，所提出的方法产生比其他方法更准确和一致的分割结果。


# Paper:356     更好地分解和聚合：人体运动预测中频率表示学习的两个深入研究



#### 1. Title: 
Decompose More and Aggregate Better: Two Closer Looks at Frequency Representation Learning for Human Motion Prediction

#### 2. Authors: 
Xuehao Gao, Shaoyi Du, Yang Wu, Yang Yang

#### 3. Affiliation: 
西安交通大学 (Xi’an Jiaotong University)

#### 4. Keywords: 
Human motion prediction, frequency representation learning, decomposition-aggregation scheme, multi-view frequency representations

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Decompose_More_and_Aggregate_Better_Two_Closer_Looks_at_Frequency_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是人体运动预测任务，旨在开发一种有效的频率表示学习框架，以提高预测的鲁棒性。

- (2):传统的人体运动预测方法通常从原始姿态空间中提取运动模式，但这种方法对身体形状的扰动鲁棒性较差。近年来，频率空间的有效性得到了广泛关注，因为它可以提取与轨迹相关的线索。然而，现有的方法往往只从由DCT初始化的单一频率空间中提取特征，缺乏对多视角频率表示的探索。本文提出了一种分解-聚合策略，将频率表示学习分解为两个阶段，以提高预测的鲁棒性。

- (3):本文提出了两个关键组件：频率分解单元和特征聚合单元。频率分解单元通过将频率表示嵌入多个特征空间中，将多视角频率特征从输入体运动中提取出来。特征聚合单元通过一系列自适应图卷积层和特征交叉层，从多个频率空间中提取全面的身体特征，以进行鲁棒的运动预测。将这两个组件集成起来，将频率表示学习分解为一种新颖而强大的分解-聚合策略。

- (4):在三个数据集上的实验结果表明，本文提出的方法在短期和长期运动预测方面均显著优于现有方法，取得了8%∼12%的性能提升。
#### 7. 方法详细介绍：
本文提出了一种新颖的频率表示学习框架，用于鲁棒的人体运动预测。该框架由两个关键组件组成：频率分解单元（FDU）和特征聚合单元（FAU）。FDU通过使用多个多功能滤波器调整每个身体关节轨迹，从输入身体运动中解开更细的频率表示，并将频率表示嵌入多个特征空间。FAU部署一系列内部空间和间隔特征聚合层，从多个频率空间中提取全面的表示。频率表示学习被分解为分解-聚合方案。具体步骤包括：
1. 将输入的人体运动数据转换为频率域表示。
2. 使用多个多功能滤波器对每个身体关节轨迹进行调整，以解开更细的频率表示。
3. 将频率表示嵌入多个特征空间。
4. 在多个特征空间中提取全面的表示。
5. 使用 ℓ2 损失函数最小化预测的 3D 运动与其地面真实值之间的距离。

#### 8. 实验设置：
本文在三个数据集上进行了评估：Human3.6M、CMU MoCap 和 3DPW。评估指标包括平均每个关节位置误差（MPJPE）和平均每个关节方向误差（MPJOE）。实验在一台服务器上进行，该服务器配备了一个 Intel Xeon E5-2690 CPU、256GB 内存和四个 NVIDIA Tesla V100 GPU。

#### 9. 实验结果和分析：
本文提出的模型在短期预测任务中表现优异，优于 DMGNN、MSR-GCN 和 PGBIG 等现有方法，包括步行、进食、吸烟、讨论、指示、问候、打电话和摆姿势等多种人体运动场景。在 Human3.6M、CMU MoCap 和 3DPW 数据集上，本文提出的方法分别比现有方法提高了 8%∼12%、3%∼7% 和 7%∼10% 的预测精度。本文还进行了消融实验，分析了模型中各个组件及其配置对最终预测性能的影响。实验结果表明，本文提出的频率表示学习框架对于鲁棒的人体运动预测具有显著的优势。


# Paper:357     水下场景中的不可分辨物体计数



#### 1. Title: 
Indiscernible Object Counting in Underwater Scenes

#### 2. Authors: 
Guolei Sun, Zhaochong An, Yun Liu, Ce Liu, Christos Sakaridis, Deng-Ping Fan, Luc Van Gool

#### 3. Affiliation: 
ETH Zurich (瑞士苏黎世联邦理工学院)

#### 4. Keywords: 
Indiscernible object counting, underwater scenes, dataset, object detection, point supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Indiscernible_Object_Counting_in_Underwater_Scenes_CVPR_2023_paper.html  Github: https://github.com/GuoleiSun/Indiscernible-Object-Counting

#### 6. Summary : 
- (1):本文研究了水下场景中的不可分辨物体计数问题，提出了一个新的挑战，即不可分辨物体计数（IOC），旨在计算与周围环境混合的物体数量。 
- (2):以前的物体计数研究主要分为两个方向：通用物体计数（GOC）和密集物体计数（DOC）。本文提出的IOC任务是在不可分辨场景下计数前景物体，这是一个新的挑战。现有的数据集虽然可以用于IOC，但存在一些限制。本文提出了IOCfish5K数据集，包含5,637张高分辨率图像和659,024个注释中心点，是目前最具挑战性的IOC数据集。 
- (3):本文提出了一种新的强大基线方法IOCFormer，它将密度和回归分支结合在一个统一的框架中，可以有效地处理隐蔽场景下的物体计数。实验表明，IOCFormer在IOCfish5K数据集上取得了最先进的成绩。 
- (4):本文提出的方法在IOCfish5K数据集上取得了最先进的成绩，证明了其在IOC任务上的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为IOCFormer的方法，用于在水下场景中进行难以区分物体计数（IOC）。该方法结合了基于密度和基于回归的计数方法的优点，包含两个分支：密度分支和回归分支。密度分支学习估计的物体密度的密度图，而回归分支直接回归到物体中心点的坐标。在密度头之前，利用密度感知特征图来加强具有难以区分物体实例的特征区域。提出了密度增强变压器编码器（DETE），用于利用哪些图像区域具有密集分布的物体和哪些区域具有稀疏分布的物体的信息来改进编码器特征图。将精细化的特征与物体查询一起传递到典型的变压器解码器中以生成预测。最终的损失函数定义为计数损失、分类损失和定位损失的组合。

#### 8. 实验设置：
本文在两个数据集Val（500）和Test（2,000）上将所提出的方法与14种最近的开源基于密度和基于回归的物体计数方法进行了比较。实验在PyTorch和NVIDIA GPU上进行。IOCFormer使用在Imagenet上预训练的ResNet-50作为编码器，其他模块/参数随机初始化。数据增强使用随机调整大小和水平翻转。图像被随机裁剪。

#### 9. 实验结果和分析：
所提出的方法在两个数据集上均取得了最先进的性能，MAE分别为18.80和46.19，MSE分别为44.57和46.19，NAE分别为0.47和0.55。结果表明，所提出的方法在水下场景中进行难以区分物体计数（IOC）方面非常有效。


# Paper:358     元调整损失函数和数据增强用于少样本目标检测



#### 1. Title: 
Meta-tuning Loss Functions and Data Augmentation for Few-shot Object Detection

#### 2. Authors: 
Berkan Demirel, Orhun Buğra Baran, Ramazan Gokberk Cinbis

#### 3. Affiliation: 
Berkan Demirel: HAVELSAN Inc. 
Orhun Buğra Baran, Ramazan Gokberk Cinbis: Middle East Technical University

#### 4. Keywords: 
Few-shot object detection, meta-learning, fine-tuning, loss functions, data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是少样本目标检测问题，即如何在少量训练实例的情况下建模新的目标检测类别。 
- (2):现有的技术可以分为两类：基于微调和基于元学习的方法。本文关注的是微调过程中损失函数和数据增强的作用，并提出通过元学习原则调整它们的动态性，从而提高少样本目标检测的性能。与复杂的元模型相比，本文提出的方法可以产生可解释的损失函数。 
- (3):本文提出了一种元调整方法，通过强化学习技术调整损失函数和数据增强，以最大化微调后的模型在新类别上的检测质量。通过定义元调整，将搜索过程限制在有效的函数族中，从而降低计算成本。本文的方法注入了学习少样本目标检测的特定归纳偏差。 
- (4):在Pascal VOC和MS-COCO数据集上，本文的方法在少样本目标检测和广义少样本目标检测设置下均取得了显著的性能提升，证明了元调整损失函数和数据增强是少样本目标检测研究的一个有前途的方向。
#### 7. 方法详细介绍：
本文提出了一种元调优方法，旨在通过元学习原理调整用于少样本目标检测（FSOD）的微调阶段中的损失函数和数据增强。该方法定义了一种情节式训练过程，通过强化学习技术调整损失函数和数据增强细节，以使其最大化微调到一组新类别时的期望检测质量。元调优是在经过精心设计的损失项和增强列表上定义的，这限制了搜索过程的有效函数族，与旨在从头开始发现损失项的AutoML方法相比，降低了计算成本。元调优得到的损失函数和增强将学习到的FSOD特定归纳偏差注入基于微调的方法中。该方法重点关注分类损失函数的细节，特别是softmax温度参数，其中定义了两个版本：简单的常数温度和时间变化的动态温度，参数化为指数多项式。该方法还在元调优期间对增强幅度进行建模，以改进少样本学习目的的数据加载管道。此外，还加入了得分缩放系数，以学习平衡基类与新类别得分。

#### 8. 实验设置：
本文在Pascal VOC和MS-COCO基准测试中评估了所提出的元调优方法，使用了最先进的基于微调的基线MPSR和DeFRCN。实验在FSOD和广义FSOD设置下进行。

#### 9. 实验结果与分析：
实验结果表明，所提出的元调优方法在FSOD和广义FSOD设置下均提供了显著的性能提升，表明元调优损失函数和数据增强可能是FSOD研究的一个有前途的方向。所提出的方法产生了可解释的损失函数，而不是高度参数化和复杂的少样本元模型。结果还强调了所提出方案的优点，在基准Pascal VOC和MS-COCO数据集上，与强微调基础少样本检测基线相比，显著提高了标准和广义少样本性能指标。


# Paper:359     Omnimatte3D：在无约束单目视频中关联物体及其效果



#### 1. Title: 
Omnimatte3D: Associating Objects and their Effects in Unconstrained Monocular Video

#### 2. Authors: 
Mohammed Suhail, Erika Lu, Zhengqi Li, Noah Snavely, Leonid Sigal, Forrester Cole

#### 3. Affiliation: 
第一作者：University of British Columbia（英属哥伦比亚大学）

#### 4. Keywords: 
Video decomposition, monocular video, multi-view consistency, layer decomposition, foreground layers

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的是视频分解问题，即将视频分解成背景层和前景层，前景层包含移动物体及其相关效果（如阴影和反射）。本文的方法适用于任意相机和物体运动的无约束单目视频。
- (2):以往的方法假设视频可以映射到固定的2D画布上，严重限制了相机运动的可能空间。本文的方法利用最近的单目相机姿态和深度估计进展，为背景创建了一个完整的RGBD视频层，并为每个前景对象创建了一个视频层。为了解决不完备的分解问题，本文提出了一种基于多视角一致性的新的损失函数。本文的方法在具有复杂相机运动的挑战性视频上进行了测试，并显示出比当前方法显著的定性改进。
- (3):本文提出了一种新的视频分解方法，能够准确地将具有复杂物体和相机运动（如视差）的输入视频分解成背景和物体层。为了解决不完备的分解问题，我们设计了基于多视角一致性和平滑先验的正则化项。最终的模型能够在以前的方法失败的真实世界视频中产生干净的分解。
- (4):本文的方法在视频编辑应用中具有潜在的应用价值，如对象去除、背景替换和时间重定时。本文的方法在具有复杂相机运动的视频上进行了测试，并显示出比当前方法显著的定性改进。
#### 7. 方法详细介绍：
本文提出了一种新的分层分解方法，将单目视频分解为背景和多个物体层以及它们的相关效果。该方法基于每帧分离视频层，适用于具有无约束相机运动的视频。为了解决预测连贯背景的挑战，该方法预测了一个三维背景，并提出了一种多视角一致性损失，强制背景仅包含缓慢移动或静态细节。该模型可以产生每帧分层深度图，允许进行各种基于深度的编辑应用。该方法还包括帧选择启发式和投影一致性损失，以提高预测层的准确性。 

#### 8. 实验设置：
无适用内容。

#### 9. 实验结果和分析：
本文在多个数据集上进行了定性和定量结果的比较，包括DAVIS、Freiburg-Berkeley Motion Segmentation（FBMS）和YouTube-VOS。该方法与Omnimatte和Neural-Atlas等最先进的方法进行了比较。结果表明，所提出的方法在前景层准确性、背景修补和物体去除方面优于比较方法。该方法还可以实现基于深度的应用，如相机稳定和合成散焦。进行了消融研究，以展示所提出的帧选择启发式和投影一致性损失的有效性。还讨论了该方法的局限性，包括无法在遮挡时修补物体层。


# Paper:360     FJMP：基于学习的有向无环交互图的分解联合多智能体运动预测



#### 1. Title: 
FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs

#### 2. Authors: 
Luke Rowe, Martin Ethier, Eli-Henry Dykhne, Krzysztof Czarnecki

#### 3. Affiliation: 
第一作者：Luke Rowe，加拿大滑铁卢大学计算机科学学院

#### 4. Keywords: 
Multi-agent motion prediction, directed acyclic interaction graphs, joint prediction, autonomous driving

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Rowe_FJMP_Factorized_Joint_Multi-Agent_Motion_Prediction_Over_Learned_Directed_CVPR_2021_paper.html  Github: https://rluke22.github.io/FJMP/

#### 6. Summary : 
- (1):本文研究的是自动驾驶中的多智能体运动预测问题，既要考虑智能体之间的交互，又要保证预测结果的一致性和可靠性。 

- (2):过去的方法主要是为每个智能体预测一组边缘轨迹，而没有考虑智能体之间的交互，导致预测结果不一致。最近的一些方法尝试生成一组联合轨迹预测，但是这些方法没有明确考虑智能体之间的交互。本文提出了一种新的方法，将联合预测任务分解为一系列边缘和条件预测，以便更好地考虑智能体之间的交互。 

- (3):本文提出了FJMP框架，它将未来场景交互动态建模为一个稀疏的有向交互图，其中边表示智能体之间的显式交互。然后将图剪枝为有向无环图（DAG），并根据DAG的部分顺序将联合预测任务分解为一系列边缘和条件预测，其中边缘预测用于源节点，条件预测用于非源节点，条件预测依赖于其父节点的预测未来。为了实现这种顺序轨迹解码，我们采用了一个轻量级的有向无环图神经网络（DAGNN）架构，用于通过DAG处理预测的未来信息并解码边缘和条件轨迹预测。 

- (4):本文在INTERACTION和Argoverse 2数据集上进行了实验，并证明了FJMP在多智能体交互驾驶场景中产生了更准确和场景一致的联合轨迹预测，尤其是对于最具交互性和运动学复杂性的智能体。FJMP在INTERACTION数据集的多智能体测试排行榜上排名第一。
#### 7. 方法详细介绍：
本文提出了FJMP（Factorized Joint Motion Prediction）框架，用于多智能体交互驾驶场景的联合运动预测。FJMP将未来场景交互动力学建模为一个稀疏有向交互图，其中边表示智能体之间的显式交互。交互图被剪枝为有向无环图（DAG），并根据DAG的部分顺序分解为一系列边缘和条件预测。使用有向无环图神经网络（DAGNN）解码联合未来轨迹。FJMP使用LaneGCN-inspired架构对智能体轨迹进行编码，并基于成对影响-反应关系构建稀疏交互图来模拟未来交互。该方法使用有向无环图神经网络（DAGNN）进行联合未来轨迹解码，可以并行地产生K个因子化联合未来轨迹，并通过联合回归损失进行监督。

#### 8. 实验设置：
本文在INTERACTION和Argoverse 2数据集上进行了评估。INTERACTION数据集包含60,000帧多智能体驾驶场景，Argoverse 2数据集包含324,557帧城市驾驶场景。实验在一台服务器上进行，该服务器配备有Intel Xeon E5-2690 CPU和NVIDIA Tesla V100 GPU。

#### 9. 实验结果和分析：
本文提出的FJMP方法在INTERACTION多智能体测试集上的联合预测结果优于几种最先进的方法，尤其是在最具交互性和动力学趣味性的智能体上。FJMP在INTERACTION数据集的挑战性多智能体预测基准测试中实现了最先进的性能，并在官方排行榜上排名第一。在Argoverse 2数据集上的消融实验表明，使用辅助提议解码器和教师强制训练对模型的性能有所提高。


# Paper:361     基于跨模态蒸馏的高效RGB-T跟踪



#### 1. Title: 
Efficient RGB-T Tracking via Cross-Modality Distillation

#### 2. Authors: 
Tianlu Zhang, Hongyuan Guo, Qiang Jiao, Qiang Zhang, Jungong Han

#### 3. Affiliation: 
西安电子科技大学机电工程学院

#### 4. Keywords: 
RGB-T tracking, knowledge distillation, multi-modal feature fusion, compact model

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究RGB-T跟踪任务，该任务是在RGB-T视频序列中估计目标状态的过程。由于热红外传感器的价格逐渐降低，RGB-T跟踪引起了越来越多的研究兴趣。

- (2):目前，大多数RGB-T跟踪模型采用两个流结构来提取单模态RGB和热红外特征，并采用复杂的融合策略来实现多模态特征融合，这需要大量的参数，从而阻碍了它们在实际应用中的使用。另一方面，紧凑的RGB-T跟踪器可能具有计算效率，但由于特征表示能力的削弱，可能会遇到非常严重的性能下降。为了解决这个问题，本文提出了一种跨模态蒸馏框架，以弥合紧凑跟踪器和强大跟踪器之间的性能差距。

- (3):本文提出了一个特定-公共特征蒸馏模块，将来自更深的两个流网络的模态公共信息以及模态特定信息转换为更浅的单流网络。此外，本文还提出了一个多路径选择蒸馏模块，通过使用多个路径指导简单的融合模块从设计良好的融合机制中学习更准确的多模态信息。最后，本文还提出了一个硬度聚焦响应蒸馏模块，通过减轻目标和背景之间数据不平衡的问题，提高了学生模型的区分能力。

- (4):本文在三个RGB-T基准测试上进行了广泛的实验，验证了所提出方法的有效性，并取得了最先进的性能，同时消耗更少的计算资源。
#### 7. 方法详细介绍：
本文提出了一种基于跨模态蒸馏（CMD）框架的RGB-T跟踪方法。该方法采用师生模型，通过特征学习蒸馏损失和多路径选择蒸馏来指导学生模型从师生模型中学习。CMD框架包括三个模块：SCFD、MPSD和HFRD，分别用于单模态特征提取、多模态特征融合和目标状态估计。SCFD模块通过两阶段单模态特征蒸馏，使学生模型能够从师生模型中学习模态共同和模态特定信息。MPSD模块通过多路径优化策略缩小师生模型获得的融合特征之间的差异。HFRD模块使用师生模型生成的响应图来指导学生模型关注难样本的区分能力，从而提高其区分能力。

#### 8. 实验设置：
本文在三个大规模基准数据集（GTOT、RGBT234和LasHeR）上评估了所提出的RGB-T跟踪方法，并与几种最先进的RGB-T跟踪器进行了比较。本文提出的方法包括两个训练阶段，第一阶段训练师傅模型，第二阶段优化学生模型。学生模型的优化由原始跟踪损失和知识转移损失共同监督。本文还进行了消融实验，以研究CMD框架中每个组件的影响。

#### 9. 实验结果和分析：
本文提出的方法在RGBT234数据集上取得了最佳结果，PR/SR分别为82.4%/58.4%，在GTOT数据集上取得了竞争性的表现，成功率和精度分别为73.4%和89.2%。在LasHeR数据集上，本文提出的跟踪器在所有三个指标上表现最好，与DAFNet∗[6]和FANet∗[40]相比，分别取得了11.0%/11.9%和10.8%/12.1%的显著性能优势。本文提出的方法可以有效减少由参数减少引起的性能损失。


# Paper:362     针对对抗攻击的鲁棒单图像反射去除



#### 1. Title: 
Robust Single Image Reflection Removal Against Adversarial Attacks

#### 2. Authors: 
Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Wenqi Ren, Jianfeng Lu

#### 3. Affiliation: 
南京理工大学

#### 4. Keywords: 
Single Image Reflection Removal, Adversarial Attacks, Deep Learning, Robustness

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究了深度单图像反射去除（SIRR）在对抗攻击下的鲁棒性问题。

- (2):传统的SIRR方法通常利用手动先验来优化层分离，但在面对复杂场景时这些先验往往被违反。近年来，基于深度学习的方法已经成为解决SIRR问题的主流方法。然而，深度学习模型往往容易受到视觉上不可察觉的对抗扰动的影响，这也是深度SIRR问题的一个重要问题，目前的方法的鲁棒性还没有得到彻底的研究。本文首先对现有的深度SIRR方法进行了全面的鲁棒性评估，然后提出了一种新的鲁棒SIRR模型，该模型通过多尺度机制缩小了干净图像和对抗图像之间的差距，并通过图像鉴别器自适应地区分干净或噪声输入，从而进一步获得可靠的鲁棒性。

- (3):本文提出了一种新的鲁棒SIRR模型，该模型整合了交叉尺度注意力模块、多尺度融合模块和对抗图像鉴别器。通过利用多尺度机制，模型缩小了干净图像和对抗图像之间的差距。图像鉴别器自适应地区分干净或噪声输入，从而进一步获得可靠的鲁棒性。

- (4):在Nature、SIR2和Real数据集上进行了大量实验，证明了我们的模型在不同场景下显著提高了SIRR的鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种鲁棒的单图像反射去除（SIRR）模型，采用多尺度图像处理策略、变压器块作为特征提取器和对抗图像鉴别器（AID）来增强鲁棒性。SIRR模型由跨尺度图像编码流、多尺度特征解码流和控制三个动态卷积模块的AID组成。跨尺度注意力模块结合了高级和低级特征，而多尺度融合模块聚合了多分辨率特征。AID生成概率向量以计算特定层的动态卷积核。SIRR模型使用在线三元组损失和三个监督损失进行训练，包括干净和对抗样本。

#### 8. 实验设置：
本文使用PSNR和SSIM作为评估指标，评估SIRR模型的性能。将SIRR模型与现有方法在两个数据集上进行比较：SRD数据集和DUT数据集。作者定义了两种攻击目标和三种不同的攻击区域，以全面分析SIRR模型对对抗攻击的鲁棒性。攻击目标是均方误差（MSE）和学习感知图像补丁相似度（LPIPS），而攻击区域是全区域（FR）、反射区域（RR）和非反射区域（NR）。作者通过组合目标攻击和区域攻击，形成了六种攻击模式。使用PGD算法迭代地解决优化问题，最终得到扰动。

#### 9. 实验结果和分析：
本文评估了深度学习单图像反射去除（SIRR）方法对对抗攻击的鲁棒性。提出的鲁棒变压器SIRR模型集成了跨尺度注意力模块、多尺度融合模块和对抗图像鉴别器，显示出当前方法的最新鲁棒性。基准结果表明，当前的深度SIRR方法在对抗攻击下都不可避免地退化。提出的方法在没有对抗训练的情况下在干净图像上表现出更好的PSNR和SSIM。然而，在MSE或LPIPS攻击下，所提出的模型的性能下降更为显著。鲁棒性的提高主要归功于对抗图像鉴别器。跨尺度注意力模块和多尺度融合模块在保持对抗输入的鲁棒性方面发挥了重要作用。消融研究验证了所提出的网络设计的有效性。


# Paper:363     基于注意力机制的点云边缘采样



#### 1. Title: 
Attention-Based Point Cloud Edge Sampling

#### 2. Authors: 
Chengzhi Wu, Junwei Zheng, Julius Pfrommer, J¨urgen Beyerer

#### 3. Affiliation: 
第一作者：Karlsruhe Institute of Technology, 德国

#### 4. Keywords: 
Point cloud sampling, attention mechanism, edge detection, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Attention-Based_Point_Cloud_Edge_Sampling_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究点云采样问题，提出了一种基于注意力机制和边缘检测的非生成式采样方法。点云采样是三维计算机视觉中的一个基础任务，但目前主要采用的采样方法仍然是经典的随机采样和最远点采样。近年来，随着深度学习技术的发展，一些基于神经网络的点云采样方法被提出，但这些方法大多是生成式的，而非直接使用数学统计方法选择点。因此，本文提出了一种基于Canny边缘检测算法和注意力机制的非生成式点云边缘采样方法，该方法可以捕捉点云轮廓中的显著点。

- (2):过去的点云采样方法主要是非学习式的，如最远点采样、均匀采样和几何采样等。近年来，一些基于神经网络的点云采样方法被提出，但这些方法大多是生成式的，而非直接使用数学统计方法选择点。本文提出的方法结合了神经网络和数学统计方法，直接选择采样点，中间结果保留点索引的含义，易于可视化。此外，本文方法可以将输入点云下采样到任意所需大小。

- (3):本文提出了一种基于注意力机制和边缘检测的非生成式点云边缘采样方法，该方法可以捕捉点云轮廓中的显著点。本文提出了两种不同的注意力模式，分别是基于邻居到点（N2P）的注意力和基于点到点（P2P）的注意力。本文方法直接选择采样点，中间结果保留点索引的含义，易于可视化。此外，本文方法可以将输入点云下采样到任意所需大小。

- (4):本文方法在常见的基准任务上取得了良好的定性和定量实验结果，证明了所提出的采样方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于注意力机制的点云边缘采样方法（APES）。该方法包括两种方法：基于局部的和基于全局的点云边缘采样。对于基于局部的APES，使用k最近邻为每个点定义一个局部补丁，并使用注意机制计算归一化相关图。然后通过选择归一化相关图标准差较高的点来采样边缘点。对于基于全局的APES，使用点对点的注意力机制获得全局相关图，并通过选择相关图列和较高的点来采样边缘点。该方法旨在处理不规则、无序和潜在稀疏的点云。

#### 8. 实验设置：
本文在常见的点云基准测试集上进行了评估，包括ModelNet40、ShapeNetPart和ScanObjectNN。实验在一台工作站上进行，该工作站配备了Intel Xeon E5-2630 v4 CPU、128GB RAM和NVIDIA Titan Xp GPU。实现基于PyTorch，使用Adam优化器进行训练。

#### 9. 实验结果和分析：
本文的方法在常见的基准测试任务上表现出优异的性能。在ModelNet40数据集上，该方法的性能优于现有方法。在ShapeNetPart数据集上，该方法也取得了与现有方法相当的性能。在ScanObjectNN数据集上，该方法在准确性和效率方面均优于现有方法。该方法还表现出对不同输入点云大小的鲁棒性，并且可以将输入点云下采样到任意所需大小。


# Paper:364     通过显著性提示的无监督预训练提高低数据实例分割



#### 1. Title: 
Boosting Low-Data Instance Segmentation by Unsupervised Pre-training with Saliency Prompt

#### 2. Authors: 
Hao Li, Dingwen Zhang, Nian Liu, Lechao Cheng, Yalun Dai, Chao Zhang, Xinggang Wang, Junwei Han

#### 3. Affiliation: 
第一作者：西北工业大学脑与人工智能实验室

#### 4. Keywords: 
Instance Segmentation, Unsupervised Pre-training, Saliency Prompt, Low-Data Regimes

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Boosting_Low-Data_Instance_Segmentation_by_Unsupervised_Pre-Training_With_Saliency_Prompt_CVPR_2021_paper.html  Github: https://github.com/lifuguan/saliency-prompt

#### 6. Summary : 
- (1):本文研究背景是在低数据情况下，如何提高实例分割的性能。

- (2):过去的方法通常需要大量的训练数据，而且在低数据情况下表现不佳。本文提出了一种基于无监督预训练的方法，通过Saliency Prompt注入先验知识，从而提高实例分割的性能。该方法可以在不增加参数或内存的情况下，帮助QEIS模型在低数据情况下实现与CNN模型相当的收敛速度和性能。

- (3):本文提出了一种无监督预训练方法，通过Saliency Prompt注入先验知识，从而提高实例分割的性能。该方法包含三个部分：1）Saliency Masks Proposal负责根据显著性机制从未标记的图像中生成伪掩模；2）Prompt-Kernel Matching将伪掩模转换为提示，并将相应的定位和形状先验注入到最佳匹配的内核中；3）Kernel Supervision用于在内核级别提供监督以实现鲁棒学习。实验结果表明，该方法显著提高了三个数据集上的多个QEIS模型的性能。

- (4):本文提出的方法在多个数据集上进行了实验，结果表明，在低数据情况下，该方法可以帮助QEIS模型实现与CNN模型相当的收敛速度和性能。该方法的创新点在于使用Saliency Prompt注入先验知识，从而提高实例分割的性能。
#### 7. 方法详细介绍：
本文提出了一种无监督预训练方法，用于增强低数据情况下的实例分割。该方法包括三个部分：显著性掩模提取、提示-核匹配和核监督。首先，使用显著性机制从未标记的图像中生成伪掩模。然后，将伪掩模转换为提示，并将相应的定位和形状先验注入最佳匹配的核中。最后，使用核监督来为模型提供鲁棒的学习监督。该方法可以作为大多数QEIS方法的即插即用预训练步骤，无需增加参数或内存，可以实现更快的收敛速度和更好的性能。

#### 8. 实验设置：
本文使用ResNet-50作为骨干网络，并使用DenseCL算法进行预训练。预训练使用AdamW优化器，权重衰减为0.05，线性步长预热为1,000步。在8个A100 GPU上，使用批量大小为96进行12个时期的训练。初始学习率设置为1×10−4，并在第8和11个时期后降低0.1。对于模型的超参数，将核/查询数N设置为100，Lcls = 2，Ldice = 4，Lce = 1和Lker = 1。微调阶段使用随机翻转和旋转进行数据增强。在8个A100 GPU上，使用批量大小为96进行训练。

#### 9. 实验结果和分析：
本文提出的方法在COCO 10％，Cityscapes和CTW1500三个数据集上显著提高了几种QEIS模型的性能。该方法在低数据情况下实现了类似的收敛速度和与基于CNN的模型相当的性能。实验结果表明，该方法在四个数据集上超过了所有现有的无监督预训练算法。该方法对伪掩模的质量具有很大的容忍度，使得可以在不需要复杂和耗时的伪掩模生成方法的情况下实现性能提升。


# Paper:365     EvShutter：用于无约束滚动快门校正的事件转换



#### 1. Title: 
EvShutter: Transforming Events for Unconstrained Rolling Shutter Correction

#### 2. Authors: 
Julius Erbach, Stepan Tulyakov, Patricia Vitoria, Alfredo Bochicchio, Yuanyou Li

#### 3. Affiliation: 
华为技术有限公司苏黎世研究中心 (Huawei Technologies, Zurich Research Center)

#### 4. Keywords: 
Rolling Shutter, Event Camera, Deblurring, Compensation, Hourglass Network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Erbach_EvShutter_Transforming_Events_for_Unconstrained_Rolling_Shutter_Correction_CVPR_2021_paper.html  Github: https://github.com/juliuserbach/EvShutter

#### 6. Summary : 
- (1):本文研究的是滚动快门（Rolling Shutter）CMOS传感器在运动情况下引入的失真和伪影问题，以及现有方法的局限性。

- (2):现有的RS校正方法通常基于恒定速度假设，需要多帧图像来预测密集的位移场。然而，这些方法在存在非线性运动或大位移的情况下容易失败。本文提出了一种新方法，称为Eventful Shutter（EvShutter），它使用单个RGB图像和高时间分辨率的事件信息来校正RS伪影。与以往方法不同的是，它不依赖于恒定速度假设，并使用一种称为Filter and Flip（FnF）的事件转换来编码GS和RS图像之间的变化。为了评估所提出的方法并促进未来的研究，我们收集了第一个具有真实事件和高质量RS图像的数据集，称为RS-ERGB。我们展示了在这个现实数据集上，所提出的方法在PSNR和LPIPS方面分别优于现有的基于图像和事件的方法。

- (3):本文提出了一种新的方法，称为Eventful Shutter（EvShutter），它使用单个RGB图像和高时间分辨率的事件信息来校正RS伪影。该方法首先使用一种基于流的去模糊模块去除模糊，然后使用双编码器沙漏网络来补偿RS。与以往方法不同的是，它不依赖于恒定速度假设，并使用一种称为Filter and Flip（FnF）的事件转换来编码GS和RS图像之间的变化。为了训练和评估所提出的方法，我们收集了第一个具有真实事件和高质量RS图像的数据集，称为RS-ERGB。我们生成了RS图像，使用了一种新的自适应插值的模拟器。这个模拟器允许使用廉价的相机捕捉高质量的GS图像。 

- (4):本文提出的方法在RS-ERGB数据集上的表现优于现有的基于图像和事件的方法。在PSNR和LPIPS方面分别优于现有方法9.16 dB和0.75 dB，并分别提高了23％和21％。
#### 7. 方法详细介绍：
本文提出了一种名为Eventful Shutter（EvShutter）的方法，用于使用单个RGB图像和高时间分辨率的事件信息来校正滚动快门（RS）伪影。该方法首先使用一种基于流的去模糊模块去除模糊，然后使用双编码器沙漏网络来补偿RS。与以前的方法不同，它不依赖于恒定速度假设，并使用专门用于RS的事件转换，称为Filter and Flip（FnF），将输入事件转换为仅编码GS和RS图像之间的变化。该方法在一个新收集的数据集RS-ERGB上进行评估，该数据集包含真实事件和高质量的RS图像，可选择模糊。RS图像是使用自适应插值的新模拟器从GS图像生成的。该方法在PSNR方面比最先进的基于图像和事件的方法分别提高了9.16 dB和0.75 dB，在LPIPS方面分别提高了23％和21％。

#### 8. 实验设置：
本文在一个新收集的数据集RS-ERGB上进行评估，该数据集包含真实事件和高质量的RS图像，可选择模糊。RS图像是使用自适应插值的新模拟器从GS图像生成的。

#### 9. 实验结果和分析：
在Fastec-RS数据集上，本文提出的方法在PSNR方面比最佳基于图像的方法提高了4.07 dB，在PSNR方面比事件的方法提高了1.09 dB，在LPIPS方面提高了0.023，在SSIM方面提高了0.03。在RS-ERGB数据集上，本文提出的方法在PSNR方面比最佳基于图像的方法DSUN提高了9.16 dB，在LPIPS方面提高了0.0966，在事件的方法EvUnroll方面提高了0.75 dB，在LPIPS方面提高了0.0966。在RS-ERGB-Blurry数据集上，本文提出的方法在PSNR方面比EvUnroll提高了0.48 dB，在LPIPS方面提高了0.053。定性比较结果显示在论文中。


# Paper:366     MAESTER：基于遮盖自编码器的像素级自监督亚细胞结构识别分割



#### 1. Title: 
MAESTER: Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition

#### 2. Authors: 
Ronald Xie, Kuan Pang, Gary D. Bader, Bo Wang

#### 3. Affiliation: 
第一作者：Ronald Xie，多伦多大学、Vector Institute、University Health Network、The Donnelly Centre

#### 4. Keywords: 
Subcellular structure segmentation, self-supervised learning, transformer, masked autoencoder, volumetric electron microscopy

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xie_MAESTER_Masked_Autoencoder_Guided_Segmentation_at_Pixel_Resolution_for_Accurate_CVPR_2021_paper.html  Github: https://github.com/bowang-lab/MAESTER

#### 6. Summary : 
- (1):本文研究的是细胞图像的精确分割问题，由于生物结构形态的内在变异性，手动分割是不可行的，而监督方法则需要大量的手动标注，因此需要一种自监督的分割方法来解决这个问题。
- (2):过去的方法通常需要大量的手动标注，而且泛化能力有限，需要为每个数据集和用例生成额外的手动标签。本文提出了一种自监督的分割方法MAESTER，它将分割视为表示学习和聚类问题，通过学习多像素图像块的语义有意义的令牌表示来实现像素级亚细胞结构分割。本文的方法在公开的VEM数据集上进行了评估，取得了29.1%以上的改进，与训练在相同任务上的监督方法的结果相当，缩小了自监督和监督方法之间的差距。
- (3):本文提出了一种基于transformer和masked autoencoder的自监督分割方法MAESTER，通过学习多像素图像块的语义有意义的令牌表示来实现像素级亚细胞结构分割。本文的方法在训练阶段采用了遮盖自编码器的方法，通过预测随机遮盖区域来学习多像素图像块的语义有意义的令牌表示。在推理阶段，本文采用了一种新颖的覆盖和步进推理策略，以实现像素级亚细胞结构分割。
- (4):本文的方法在公开的VEM数据集上进行了评估，取得了29.1%以上的改进，与训练在相同任务上的监督方法的结果相当，缩小了自监督和监督方法之间的差距。本文的方法有望通过减轻手动标注生成的关键瓶颈，极大地提高生物成像实验的实验周期，从而大大提高细胞生物学的科学探索速度。
#### 7. 方法详细介绍：
本文提出了一种名为MAESTER的自监督分割方法，用于准确的亚细胞结构识别。MAESTER将分割视为表示学习和聚类问题。在训练期间，MAESTER将包含足够局部上下文的大视场（F×F像素）作为输入，进一步将其分解为大小为P×P像素的多像素补丁。视觉变换器（ViT）编码器的注意机制允许附近补丁之间的信息共享。此外，MAESTER还采用了掩码自编码器（MAE）学习范例的代理任务，通过轻量级ViT解码器对给定图像的每个采样视场的多像素补丁进行掩码和重构，以同时学习所有补丁的语义有意义的令牌表示。在推理期间，我们使用训练好的编码器通过一种新的覆盖和步幅推理策略生成数百万个未标记图像补丁的表示，然后对这些表示进行聚类，以产生所需数量的类别进行自监督分割，从而得到给定VEM数据集的最终分割结果。

#### 8. 实验设置：
作者在公开的小鼠胰岛β细胞的体积电子显微镜（VEM）数据集上评估了MAESTER。

#### 9. 实验结果和分析：
本文表明，在相同的评估标准下，MAESTER始终优于先前的工作，性能提高范围从11.4％到29.1％。本文还将MAESTER与两个具有完全访问成对地面实况标签的监督基线进行比较，并发现所得的Dice相似系数（DSC）在4个主要类别中的3个上有一定的竞争力。本文还展示了新的覆盖和步幅策略所带来的性能提升，并表明MAESTER学习的多像素补丁级别表示是足够小的补丁的像素级表示的良好代理。本文还展示了混淆矩阵，反映了模型区分细胞核和细胞质的能力，而先前的最先进方法经常无法做到这一点。


# Paper:367     基于遮蔽形状预测的自监督预训练用于3D场景理解



#### 1. Title: 
Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding

#### 2. Authors: 
Li Jiang, Zetong Yang, Shaoshuai Shi, Vladislav Golyanik, Dengxin Dai, Bernt Schiele

#### 3. Affiliation: 
Max Planck Institute for Informatics, Saarland Informatics Campus

#### 4. Keywords: 
Self-supervised pre-training, 3D scene understanding, masked signal modeling, geometric shape, context-enhanced shape target

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_Self-Supervised_Pre-Training_With_Masked_Shape_Prediction_for_3D_Scene_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是3D场景理解中的自监督预训练，旨在通过利用未标记数据来学习有意义的表示，提高有限标记数据的性能。
 
- (2):过去的3D场景预训练方法主要采用对比学习，但是本文提出的Masked Shape Prediction (MSP)方法采用了遮蔽信号建模，即将一部分输入数据遮蔽，然后根据剩余内容重构被遮蔽的部分，以学习语义知识。与2D图像不同，3D点云的语义理解更多地依赖于几何形状，因此本文将几何形状作为重构目标。同时，本文提出了上下文增强的形状目标，包括显式形状上下文和隐式深度形状特征，以促进利用形状预测中的上下文线索。本文还探索了不同的MSP网络架构设计，以促进特征学习并减轻遮蔽形状泄漏问题。

- (3):本文提出了一种新的3D场景自监督预训练方法，即Masked Shape Prediction (MSP)，通过遮蔽信号建模来学习3D场景的有意义表示。本文提出了上下文增强的形状目标，将显式形状上下文和隐式深度形状特征相结合，以促进利用形状预测中的上下文线索。本文还探索了不同的MSP网络架构设计，以促进特征学习并减轻遮蔽形状泄漏问题。

- (4):本文在多个3D场景理解任务上进行了实验，包括室内和室外数据集上的分割和检测任务。实验结果表明，MSP在学习良好的特征表示方面具有一致的提升效果，并且具有很强的数据效率。
#### 7. 方法详细介绍：
本文提出了一种自监督预训练方法，称为Masked Shape Prediction (MSP)，用于3D场景理解。MSP使用几何形状作为预测目标，学习有意义的表示。MSP网络由特征提取器、重建分支和MSP网络三部分组成。特征提取器从输入点云中提取特征，重建分支使用上下文增强的形状目标作为监督，预测剩余点的形状特征。MSP网络使用交叉注意力或自注意力在稀疏采样的关键点上预测掩码点的形状特征，以避免掩码形状泄漏。损失函数将形状上下文和深度形状特征组合为上下文增强的形状目标，同时还包括颜色预测作为辅助任务。MSP方法的核心思想是避免掩码点之间的信息交互或将交互限制在稀疏采样的关键点上，以减轻掩码形状泄漏。该方法在ScanNet、S3DIS和SemanticKITTI三个基准数据集上均取得了最先进的性能，优于以前的自监督和有监督方法。 

#### 8. 实验设置：
本文在ScanNet、S3DIS和SemanticKITTI三个基准数据集上进行了实验。对于ScanNet和S3DIS，使用官方的数据有效分割，进行语义分割和实例分割任务。对于SemanticKITTI，使用KITTI里程计数据集进行预训练，进行语义分割任务。预训练使用批量大小为16和100个epoch。学习率设置为0.001，每30个epoch衰减0.1倍。实验在单个NVIDIA Tesla V100 GPU上进行。

#### 9. 实验结果和分析：
本文在ScanNet、S3DIS和SemanticKITTI三个基准数据集上进行了实验，使用mIoU和mAP作为评价指标。实验结果表明，MSP方法在所有数据集上均取得了最先进的性能，优于以前的自监督和有监督方法。消融实验表明，上下文增强的形状目标和MSP网络架构的有效性。定性分析表明，MSP方法能够捕捉细节并处理遮挡。在ScanNet数据集上的实验表明，使用MSP方法进行自监督预训练可以显著提高模型在语义分割和物体检测任务中的性能，特别是在数据和注释有限的情况下。


# Paper:368     MACARONS：基于RGB在线自监督学习的映射和覆盖预测



#### 1. Title: 
MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision

#### 2. Authors: 
Antoine Gu´edon, Tom Monnier, Pascal Monasse, Vincent Lepetit

#### 3. Affiliation: 
Antoine Gu´edon: LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, 法国
Tom Monnier: None
Pascal Monasse: None
Vincent Lepetit: None

#### 4. Keywords: 
Next Best View, 3D reconstruction, RGB sensor, self-supervised learning, online learning

#### 5. Paper: https://imagine.enpc.fr/˜guedona/MACARONS/  Github: None

#### 6. Summary : 
- (1):本文的研究背景是如何使用无人机自动捕捉大型室外场景的图像以进行三维重建。
- (2):过去的NBV方法大多依赖于深度传感器，需要3D监督，并且不适用于大型场景。本文提出的方法使用RGB相机进行自监督学习，同时学习如何高效地探索场景并从颜色图像中重建3D模型。本文的方法不需要深度传感器和3D监督，可以适用于大型场景。本文的方法在新场景中表现良好，不会受到任何训练3D数据的偏见。本文的方法称为MACARONS，是第一个基于RGB图像进行自监督学习的NBV方法，可以实时学习自我探索和重建任意大型场景。 
- (3):本文提出了一种基于RGB相机进行自监督学习的NBV方法，可以同时学习如何高效地探索场景并从颜色图像中重建3D模型。本文的方法不需要深度传感器和3D监督，可以适用于大型场景。本文的方法使用三个神经模块进行通信，分别用于预测深度图、预测“体积占用场”和预测表面覆盖增益。本文的方法使用Memory Replay进行在线学习，避免了灾难性遗忘，并且大大加速了训练。本文的方法在新场景中表现良好，不会受到任何训练3D数据的偏见。 
- (4):本文的方法在大型3D场景上进行了评估，表明本文的方法可以在没有深度传感器的情况下实现比最新方法更好的表面覆盖。本文的方法可以实时学习自我探索和重建任意大型场景，适用于小型无人机和简单的RGB相机。本文的方法表明，自主系统可以在没有任何3D信息的情况下学习探索和重建环境。
#### 7. 方法详细介绍：
本文提出了一种自监督学习的方法，称为MACARONS，用于解决下一个最佳视角（NBV）问题。该方法由三个模块组成：深度估计、体积占用和表面覆盖增益。在在线探索过程中，该方法在每个时间步骤执行一个训练迭代，包括决策制定、数据收集和记忆构建以及记忆重放三个步骤。决策制定步骤通过运行三个模块来选择下一个最佳相机姿态来探索场景。数据收集和记忆构建步骤为所有三个模块创建自监督信号，并将这些信号存储到记忆中。记忆重放步骤选择随机监督数据并更新每个模块的权重，以避免灾难性遗忘并加速训练。

#### 8. 实验设置：
本文在由各种3D场景组成的最新数据集上进行了评估。实验比较了该方法与最近的工作在由大规模3D场景组成的数据集上的表现。评估测量了传感器探索多个3D场景时总表面覆盖的演变。

#### 9. 实验结果和分析：
该方法在新场景中表现良好，因为它不会对任何训练3D数据进行偏差。实验表明，该方法甚至比需要深度传感器的最新方法表现更好，这对于使用飞行无人机捕获的室外场景来说并不是一个现实的假设。使用RGB图像进行自监督学习的在线方法能够比具有完美深度传感器的最先进方法获得更好的结果。该方法适用于具有简单彩色相机的小型无人机的实际应用。

### 总结：
本文提出了一种自监督学习的方法MACARONS，用于解决下一个最佳视角（NBV）问题。该方法由三个模块组成：深度估计、体积占用和表面覆盖增益。该方法使用自监督的在线学习过程，可以在没有任何3D输入数据的情况下探索大型未知环境。实验结果表明，该方法在大型环境中表现优异，即使在线学习被禁用。该方法适用于具有简单彩色相机的小型无人机的实际应用。


# Paper:369     SketchXAI：人类草图可解释性的首次探索



#### 1. Title: 
SketchXAI: A First Look at Explainability for Human Sketches

#### 2. Authors: 
Zhiyu Qu, Yulia Gryaditskaya, Ke Li, Kaiyue Pang, Tao Xiang, Yi-Zhe Song

#### 3. Affiliation: 
第一作者所属机构：SketchX, CVSSP, Surrey大学

#### 4. Keywords: 
Explainable AI, Sketch, Stroke location inversion, Sketch encoder, Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qu_SketchXAI_A_First_Look_at_Explainability_for_Human_Sketches_CVPR_2021_paper.html  Github: https://sketchxai.github.io/

#### 6. Summary : 
- (1):本文介绍了人类手绘草图在可解释人工智能（XAI）领域的应用。手绘草图作为“以人为中心”的数据形式，代表了研究可解释性的自然接口。本文旨在培养草图特定的可解释性设计，探索草图数据的可解释性。

- (2):现有的XAI研究主要集中在照片和文本两种模态上。本文首次尝试将人类手绘草图引入XAI领域。与照片和文本不同，草图具有独特的灵活性和可操作性，因为草图是由笔画构成的。本文提出了一种草图编码器，名为SketchXAINet，它包含了笔画的所有重要属性。本文还提出了草图的第一个XAI任务——笔画位置反演（SLI），并提供了定性结果。

- (3):本文的贡献包括：（i）提出了将草图引入XAI领域的观点；（ii）将笔画识别为基本构建块并构建了一个草图编码器，名为SketchXAINet；（iii）提出了笔画位置反演（SLI）作为草图的第一个XAI任务；（iv）提供了定性结果，并获得了最佳草图识别性能。

- (4):本文的方法在草图识别任务上取得了最佳性能，并提出了草图的可解释性任务——笔画位置反演（SLI）。本文的方法可以生成无限多的解释路径，使人们可以更好地审查AI的内部工作方式。
#### 7. 方法详细介绍：
本文提出了一种名为SketchXAI的方法，用于解释人类手绘图。该方法包括一个名为SketchXAINet的手绘图编码器，它将笔画分解为形状、位置和顺序三个部分进行编码，并将它们输入到一个标准的Transformer架构中进行交叉熵损失计算。此外，本文还引入了Stroke Location Inversion（SLI）作为手绘图的第一个XAI任务，旨在通过扰动笔画位置来恢复或转移手绘图。优化过程用于执行SLI，并通过可视化过程来实现解释性。

#### 8. 实验设置：
本文的实验使用了QuickDraw数据集，其中包含50个类别的手绘图像。每个类别包含70,000个手绘图像，其中60,000个用于训练，5,000个用于验证，5,000个用于测试。本文使用了100个手绘图像进行训练，并使用了两个SLI任务：恢复和转移。本文的实验使用了NVIDIA V100 GPU进行训练和测试。

#### 9. 实验结果和分析：
本文的实验结果表明，SketchXAI方法在手绘图像识别任务中优于现有方法。本文还对手绘图像的解释性进行了探究，提出了SLI工具，可以可视化地探究手绘图像分类器的功能，并提供了恢复和转移两种任务。本文还分析了学习到的形状嵌入，并证明了其在分类任务中的有效性。此外，本文还通过SLI揭示了数据集偏差，并展示了三个QuickDraw类别的截图。


# Paper:370     通过学习降级驱动的视角混合器实现高质量的神经视图合成



#### 1. Title: 
NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer

#### 2. Authors: 
Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, Jiangbo Lu

#### 3. Affiliation: 
Kun Zhou: 香港中文大学深圳研究院
Wenbo Li, Tao Hu: 香港中文大学
Nianjuan Jiang, Jiangbo Lu: SmartMore Corporation
Yi Wang: 上海人工智能实验室
Xiaoguang Han: 香港中文大学

#### 4. Keywords: 
Neural radiance fields, novel view synthesis, degradation-driven, inter-viewpoint mixer, NeRF-agnostic restorer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_NeRFLiX_High-Quality_Neural_View_Synthesis_by_Learning_a_Degradation-Driven_Inter-Viewpoint_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）在新视角合成方面的成功，但在实际场景中，由于潜在的不完美校准信息和场景表示不准确，从源图像中恢复高质量细节仍然具有挑战性。
- (2):过去的方法包括优化相机参数和神经辐射场以精确校准相机姿态，以及物理感知模型，但它们都存在一些问题。本文提出了NeRFLiX，一种通用的NeRF-agnostic restorer，通过学习降级驱动的视角混合器来提高NeRF合成质量。本文的方法是有动机的，因为它可以解决NeRF模型合成视图中存在的噪声、模糊等问题。
- (3):本文提出了一种NeRF-style degradation simulator，构建了大规模的配对数据，使得可以有效地消除现有深度神经网络中的NeRF-native rendering artifacts。此外，本文还提出了一种视角混合器，能够融合高质量的邻近训练图像，将NeRF模型的性能推向全新的水平，产生高度逼真的合成视图。
- (4):本文的方法在多个数据集上进行了实验，结果表明，NeRFLiX可以有效地消除NeRF-style degradation，提高NeRF模型的合成质量，达到了预期的目标。
#### 7. 方法详细介绍：
本文提出了一种名为NeRFLiX的方法，它是一种通用的NeRF-agnostic修复器，采用退化驱动的视角混合器来增强NeRF模型渲染的新视角图像。该方法包括两个基本组件：NeRF风格的退化模拟器（NDS）和视角混合器（IVM）。在训练阶段，使用提出的NDS创建大规模的成对训练数据，随后使用这些数据来训练IVM，以改善使用两个相应参考图片（参考视图）的NeRF渲染视图。在推理阶段，采用IVM来增强渲染视图，通过从选择的最相关参考视图中融合有用信息来实现。NDS管道包括三种类型的退化：splatted Gaussian noise（SGN）、re-positioning（Re-Pos.）和anisotropic blur（A-Blur）。IVM架构包括三个模块：特征提取、混合视角聚合和重建。

#### 8. 实验设置：
本文在三个广泛使用的数据集上进行了实验，包括LLFF、Tanks and Temples和Noisy LLFF Synthetic。LLFF数据集有8个不同的场景，每个场景有20到62张图像，分辨率为1008×756（LLFF-P1）和504×376（LLFF-P2）。Tanks and Temples数据集包含5个由内向外摄像机拍摄的场景，分辨率为1920×1080，图像数量为152-384。Noisy LLFF Synthetic数据集有8个虚拟场景，每个场景有400张大小为800×800的图像。采用PSNR、SSIM和LPIPS指标进行评估。

#### 9. 实验结果与分析：
本文提出的NeRFLiX方法在各种数据集上均能显著提高最先进的NeRF方法的性能。例如，在LLFF数据集上，NeRFLiX在PSNR/SSIM/LPIPS方面将Plenoxels的性能提高了0.61dB/0.025/0.054。在Tanks and Temples数据集上，NeRFLiX可以将TensoRF的PSNR/SSIM/LPIPS性能提高0.51dB/0.01/0.022。定性结果表明，NeRFLiX恢复了更清晰的图像细节，同时消除了渲染图像中大部分的NeRF风格伪影。


# Paper:371     PanoHead：360度几何感知的3D全头合成



#### 1. Title: 
PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360◦

#### 2. Authors: 
Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y. Ogras, Linjie Luo

#### 3. Affiliation: 
ByteDance Inc. (字节跳动)

#### 4. Keywords: 
3D head synthesis, generative adversarial networks (GANs), view-consistent image synthesis, 360◦, neural volume representation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/An_PanoHead_Geometry-Aware_3D_Full-Head_Synthesis_in_360_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是3D头部合成，旨在实现从单视角图像生成360度全头部图像。

- (2):过去的方法要么只能合成近正面视角的头部图像，要么需要多视角图像或3D扫描监督，难以在广泛的视角下保持3D一致性。本文提出了PanoHead，一种新的3D感知生成模型，可以从野外非结构化图像中训练，实现高质量的360度全头部图像合成。本文的方法在3D GANs的训练中解决了数据对齐问题，提出了一种新的三网格神经体积表示方法，以及一种新的自适应图像对齐方案。本文的方法在3D头部合成方面取得了显著的性能提升。

- (3):本文提出了一种新的3D感知生成模型PanoHead，可以从野外非结构化图像中训练，实现高质量的360度全头部图像合成。本文的方法在3D GANs的训练中解决了数据对齐问题，提出了一种新的三网格神经体积表示方法，以及一种新的自适应图像对齐方案。本文的方法在3D头部合成方面取得了显著的性能提升。

- (4):本文的方法在360度全头部图像合成方面取得了显著的性能提升，可以生成高质量的3D头部图像，即使是长卷发和非洲卷发等复杂发型，也可以从任意角度渲染。此外，本文的方法可以从单个输入图像重建完整的3D头部，实现个性化逼真的3D头像创建。
#### 7. 方法详细介绍：
本文提出了一种名为PanoHead的方法，用于从单视角图像中合成360度视角一致的全头部图像。该方法采用了两阶段处理方法，第一阶段使用3DDFA进行面部姿态估计，对于大的相机姿态，使用WHENet进行头部姿态估计，使用YOLO进行人脸检测和裁剪。第二阶段使用自适应相机对齐方案进行微调，使用3D-aware GAN将每个图像与嵌入几何和外观的潜在代码z相关联，从而在ccam视角上合成图像。该方法还包括前景感知三元判别器、三维三角网格场景表示和自适应图像对齐等技术，以实现360度视角一致的全头部图像合成。

#### 8. 实验设置：
本文使用FFHQ、K-hairstyle数据集和一个内部大姿态头像图像集的平衡组合进行训练和评估。其中，FFHQ包含70K多样化的高分辨率人脸图像，但主要集中在0°至60°的绝对偏航范围内。FFHQ数据集还使用了K-hairstyle数据集的4K个背部头像图像和15K个内部大姿态图像进行增强。本文将模型与GRAF、EG3D、StyleSDF和GIRAFFEHD等最先进的3D-aware GAN进行了比较。

#### 9. 实验结果与分析：
本文提出的方法在所有方面都优于其他基线模型，如Frechet Inception Distance（FID）、身份相似度分数（ID）和均方误差（MSE）。该模型在所有相机姿态下生成了优秀的照片级真实头像图像，并保持了多视角一致性。本文还引入了FID-back指标来更好地反映360度全头部的生成质量。消融实验表明，与原始EG3D相比，添加前景感知判别器可以显著提高所有情况下的质量。本文还讨论了PanoHead的局限性和未来工作，包括牙齿区域的小缺陷和纹理闪烁问题，建议使用StyleGAN3作为骨干网络以保留高频细节，并使用深度图定量评估几何质量。本文还指出，仅依赖于几个数据集的组合训练仍然会受到数据偏差的影响。


# Paper:372     基于Transformer的扩散模型的布局生成



#### 1. Title: 
LayoutDM: Transformer-based Diffusion Model for Layout Generation

#### 2. Authors: 
Shang Chai, Liansheng Zhuang, Fengying Yan

#### 3. Affiliation: 
University of Science and Technology of China (中国科学技术大学)

#### 4. Keywords: 
Layout generation, generative models, diffusion models, transformer, denoising diffusion probabilistic model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chai_LayoutDM_Transformer-Based_Diffusion_Model_for_Layout_Generation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是自动布局生成，即给定设计元素和用户指定的属性，生成高质量的设计布局。自动布局生成在许多应用中都是重要的工具，但是生成高质量和多样化的结果仍然具有挑战性。

- (2):过去的方法主要基于生成模型，如生成对抗网络（GAN）和变分自编码器（VAE），虽然已经取得了一定的进展，但是它们仍然存在许多问题，如GAN的训练不稳定和分布覆盖不足，VAE的视觉样本质量和采样效率不高。本文受到扩散模型在生成高质量图像方面的最新成功启发，探索了它们在条件布局生成方面的潜力，并提出了基于Transformer的布局扩散模型（LayoutDM）。

- (3):本文提出了一种新的扩散模型，即将条件去噪扩散概率模型（DDPM）实例化为纯Transformer架构，从而提出了LayoutDM。LayoutDM使用纯Transformer架构而不是常用的卷积神经网络来学习从噪声布局数据中生成样本的反向扩散过程。为了生成具有所需属性的布局，LayoutDM设计了一种基于Transformer架构的条件布局去噪器（cLayoutDenoiser）来学习反向扩散过程，条件是输入属性。与当前的布局生成方法（如GAN和VAE）相比，LayoutDM具有高质量的生成、更好的多样性、忠实的分布覆盖、稳定的训练目标和易于扩展等优点。

- (4):本文在五个公共数据集上进行了大量实验，结果表明LayoutDM在不同任务上优于现有的方法，具有更好的视觉感知质量和多样性。
#### 7. 方法详细介绍：
本文提出了一种名为LayoutDM的基于Transformer的扩散模型，用于生成具有用户指定属性的高质量设计布局。LayoutDM是一种条件去噪扩散概率模型（DDPM），使用纯Transformer架构来学习从噪声布局数据中的逆扩散过程。该模型使用Transformer架构来有效地捕获元素之间的高级关系信息，并在每个时间步长预测噪声。此外，注意机制还有助于模拟数据的另一个方面，即变化和大量元素。为了生成具有所需属性的布局，LayoutDM设计了一个基于Transformer架构的条件布局去噪器（cLayoutDenoiser），以学习基于输入属性的逆扩散过程。与以前在NLP或视频上下文中使用的Transformer模型不同，cLayoutDenoiser省略了位置编码，因为在此设置中不考虑画布上设计元素的顺序。

#### 8. 实验设置：
本文使用Rico、PublayNet、Magazine、COCO和TextLogo3K等五个公共数据集对LayoutDM模型进行了评估。使用了四个指标来衡量布局的感知质量，包括FID。使用[18]中的预训练分类器计算FID。

#### 9. 实验结果与分析：
本文表明，LayoutDM在视觉感知质量和多样性方面优于现有的方法。该模型生成合理的标志布局，同时保持正确的阅读顺序和美学原则。与LogoGAN相比，LayoutDM生成的标志样式更加灵活。该模型还生成了合理的场景布局提案，并理解自然场景中元素之间的复杂关系。然而，本文承认该模型存在局限性，例如无法模拟多个层次相互遮挡的布局，并且在生成速度方面没有其他生成模型的优势。


# Paper:373     基于双层优化的剪枝参数化方法用于边缘设备上高效的语义分割



#### 1. Title: 
Pruning Parameterization with Bi-level Optimization for Efficient Semantic Segmentation on the Edge

#### 2. Authors: 
Changdi Yang, Pu Zhao, Yanyu Li, Wei Niu, Jiexiong Guan, Hao Tang, Minghai Qin, Bin Ren, Xue Lin, Yanzhi Wang

#### 3. Affiliation: 
Northeastern University (东北大学)

#### 4. Keywords: 
Semantic Segmentation, Pruning Parameterization, Bi-level Optimization, Vision Transformers, Edge Devices

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Pruning_Parameterization_With_Bi-Level_Optimization_for_Efficient_Semantic_Segmentation_CVPR_2022_paper.html

Github: None

#### 6. Summary:
- (1):本文研究的背景是在边缘设备上实现实时语义分割，为此需要设计一种高效的模型。
- (2):过去的方法包括手工设计轻量级CNN模型和使用NAS方法自动搜索轻量级模型，但这些方法在实现实时推理时仍然存在计算和内存开销较大的问题。本文提出了一种基于剪枝参数化和双层优化的方法，可以在较小的计算资源下实现高精度的语义分割。
- (3):本文提出了一种剪枝参数化方法，使用软掩码表示剪枝策略，避免了排序操作和微调的开销。然后，采用双层优化方法，通过隐式梯度来更新软掩码和模型权重，以实现高效的剪枝。实验结果表明，本文方法在ADE20K数据集上实现了38.9 mIoU的最高精度，且在Samsung S21上实现了56.5 FPS的实时推理速度。
- (4):本文方法在ADE20K数据集上实现了最高的mIoU精度，并且在实时推理速度方面表现出色，可以支持在边缘设备上进行实时语义分割。
#### 7. 方法详细介绍：
本文提出了一种基于剪枝参数化的双层优化框架，用于在边缘设备上进行高效的语义分割。该方法通过剪枝参数化来构建剪枝问题，使用软掩码表示剪枝策略，并采用阈值方法将软掩码转换为二进制掩码进行实际剪枝。使用直通估计器方法来有效地更新软掩码。提出了一种双层优化方法来解决剪枝问题，利用隐式梯度来获得更好的结果。该方法只采用一阶优化，计算过程经过优化以节省内存成本。当掩码从0更新为1时，可以恢复剪枝通道的贡献。

#### 8. 实验设置：
模型使用随机梯度下降优化器进行训练，每个GPU上的批量大小设置为8。对于ADE20K数据集，初始学习率设置为1.2×10^-4，并采用“poly”学习率策略。对于Cityscapes数据集，初始学习率设置为3×10^-4，并采用“poly”学习率策略。对于PASCAL VOC 2012，初始学习率设置为0.01。在训练过程中采用数据增强，并选择裁剪大小以在移动容量和准确性之间取得更好的平衡。实验中设置超参数β = 0.01和λ = 0.1。

#### 9. 实验结果和分析：
本文提出了一种新颖的剪枝参数化框架，通过双层优化实现了在边缘设备上进行高效语义分割。该方法在ADE20K和Cityscapes数据集上与各种手工制作和NAS-based分割模型进行了比较。结果表明，与其他方法相比，该方法可以在更少的计算量下实现更高的mIoU。该方法还可以在边缘设备上实现实时推理，并具有竞争性的分割性能。该方法的搜索成本小于大多数其他NAS-based分割方法。该方法是通用的，可以应用于其他模型结构。消融研究表明，采用隐式梯度的提出方法可以有效地节省计算成本。


# Paper:374     ViLEM：面向图像-文本检索的视觉-语言错误建模



#### 1. Title: 
ViLEM: Visual-Language Error Modeling for Image-Text Retrieval

#### 2. Authors: 
Yuxin Chen, Zongyang Ma, Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Weiming Hu, Xiaohu Qie, JianPing Wu

#### 3. Affiliation: 
第一、二、四作者：中国科学院自动化研究所，多模态人工智能系统重点实验室，中国科学院大学，脑科学与智能技术卓越中心；第三作者：腾讯 PCG；第五作者：清华大学

#### 4. Keywords: 
Image-text retrieval, dual-encoder, contrastive learning, visual-language error modeling, multi-granularity interaction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是图像-文本检索领域中，现有的“双编码器”方法只能提取全局特征，无法捕捉细粒度的图像和文本语义信息，因此需要一种新的方法来提高图像-文本检索的性能。
 
- (2):过去的方法包括“联合编码器”和“双编码器”，但它们都存在一些问题，如效率低、无法捕捉细粒度语义信息等。本文提出了一种新的代理任务——Visual-Language Error Modeling（ViLEM），通过“校对”文本中的每个单词来注入详细的图像-文本关联信息，从而提高“双编码器”模型的性能。 
 
- (3):本文提出了一种多粒度交互框架，通过全局和局部图像特征与文本特征的交互来执行ViLEM，从而建立局部文本语义与高级视觉语境和多级局部视觉信息之间的关联。本文的方法在图像-文本检索任务上取得了显著的性能提升，并且对本地文本语义的区分度也得到了显著提高。此外，本文的模型也可以很好地推广到视频-文本检索。

- (4):本文的方法在图像-文本检索任务上取得了显著的性能提升，比现有的“双编码器”方法有更好的效果。同时，本文的方法也可以很好地推广到视频-文本检索。
#### 7. 方法详细介绍：
本文提出了一种新的代理任务，即视觉-语言错误建模（ViLEM），并提出了一种多粒度交互框架，将详细的图像-文本关联注入“双编码器”架构中。该模型包含一个视觉编码器和一个文本编码器，两者都由多个Transformer块组成。模型通过三个目标进行训练：图像-文本对比学习、高级全局视觉特征的ViLEM和多级局部视觉特征的ViLEM。ViLEM任务通过将文本特征与全局和局部视觉特征交互来执行。本文还介绍了一种基于知识的文本编辑方法，用于自动构建ViLEM的训练样本。

#### 8. 实验设置：
本文使用CC4M和CC13M两个图像-文本数据集进行预训练，并在MSCOCO、Flick30K和Winoground数据集上进行下游图像-文本检索评估。模型采用BERTbase作为文本编码器，ViT-B/16作为视觉编码器。使用AdamW优化器，权重衰减为0.02，学习率在前2000次迭代中逐渐升高到3e-4，然后按余弦计划衰减到1e-5。模型在32个NVIDIA A100 GPU上进行20个epoch的预训练，批量大小为2048。预训练采用256×256的图像分辨率，微调采用384×384的图像分辨率。更新动量编码器的动量系数设置为0.995，队列大小Nq设置为65536。对比损失的可学习温度超参数初始化为0.07。损失权重λ1和λ2分别设置为0.8和0.2。

#### 9. 实验结果与分析：
本文在MSCOCO数据集上进行了图像-文本检索评估，未进行微调的ViLEM方法表现优于先前的双编码器方法，甚至优于使用更大的预训练数据的CLIP。本文还在MSR-VTT数据集上展示了零样本文本-视频检索的结果，其中纯图像-文本模型即使没有复杂的视频时间建模也优于先前的最先进的视频-文本方法。此外，本文还在Winoground数据集上展示了ViLEM提高对局部文本语义的区分度的有效性，其性能优于所有双编码器方法和联合编码器方法，除了VinVL。


# Paper:375     场景三部曲：人类场景素描及其与照片和文本的互补性



#### 1. Title: 
SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text

#### 2. Authors: 
Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song

#### 3. Affiliation: 
第一作者：SketchX, CVSSP, 英国萨里大学

#### 4. Keywords: 
Scene understanding, Sketch, Photo, Text, Joint embedding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chowdhury_SceneTrilogy_On_Human_Scene-Sketch_and_Its_Complementarity_With_Photo_and_CVPR_2021_paper.html  Github: https://github.com/pinakinathc/scenetrilogy

#### 6. Summary : 
- (1):本文研究场景理解，将人类场景素描作为一种新的场景表达方式，提出了一种包括素描、照片和文本三种多样化和互补的场景表达方式的三部曲。 

- (2):过去的多模态场景理解研究主要集中在文本和照片两种模态上，而本文提出了一种包括人类场景素描的三种模态的场景表达方式。与过去的三元联合嵌入不同，本文提出了一种三元解缠结方法，将每种模态分解为模态特定和模态不可知两个部分，以支持跨模态和跨任务的可选性。 

- (3):本文提出了一种灵活的联合嵌入方法，支持跨模态和跨任务的可选性。首先，使用信息瓶颈和条件可逆神经网络将素描、照片和文本的模态特定部分与模态不可知部分分离。然后，使用修改后的交叉注意力机制将素描、照片和文本的模态不可知部分协同起来。最后，本文展示了该嵌入可以适应多种场景相关任务，包括通过素描实现的任务。 

- (4):本文的方法在多个场景相关任务中取得了良好的性能，包括检索和生成任务。这些结果表明，本文提出的联合嵌入方法可以有效地支持跨模态和跨任务的可选性。
#### 7. 方法详细介绍：
本文提出了一种基于信息瓶颈和条件可逆神经网络的灵活联合嵌入方法，用于将场景的文本、草图和图像进行联合嵌入，以支持多种场景相关任务，包括检索和字幕生成。该方法通过将草图、文本和图像的特征表示分解为模态特定和模态不可知的组件，然后使用修改后的交叉注意力机制将模态不可知的实例进行协同处理，从而得到联合嵌入。该嵌入可以用于多种场景相关任务，无需进行任何特定于任务的修改。

#### 8. 实验设置：
本文使用了两个场景草图数据集：SketchyCOCO和FS-COCO。模型使用PyTorch实现，使用11GB Nvidia RTX 2080-Super GPU进行训练。图像编码器和文本解码器使用82,783个图像-文本对进行图像字幕生成的预训练，训练15个epochs。模型使用Adam优化器进行微调，学习率为1e-4，批量大小为64，训练200个epochs。图像和草图编码器使用ImageNet预训练的VGG-16。文本使用具有512个隐藏单元的双向GRU单元进行编码。文本解码器是一个单层自回归LSTM解码器，每个时间步预测一个固定词汇表（10,010个单词）上的概率分布。

#### 9. 实验结果和分析：
本文在不同的下游任务中进行了多组实验，包括细粒度图像检索、图像/草图字幕生成和主观字幕生成。实验结果表明，所提出的方法在大多数情况下优于以前的方法，这要归功于模态特定和模态不可知组件的分离以及针对生成任务的文本特定先验的建模。本文还进行了消融研究，以评估每个关键设计选择对实验性能的贡献。


# Paper:376     TAPS3D：基于伪监督的文本引导3D纹理形状生成



#### 1. Title: 
TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision

#### 2. Authors: 
Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, Kim-Hui Yap

#### 3. Affiliation: 
第一作者：南洋理工大学

#### 4. Keywords: 
3D shape generation, text-guided, pseudo supervision, CLIP, GAN

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wei_TAPS3D_Text-Guided_3D_Textured_Shape_Generation_From_Pseudo_Supervision_CVPR_2021_paper.html  Github: https://github.com/plusmultiply/TAPS3D

#### 6. Summary : 
- (1):本文研究了从文本描述生成可控的3D纹理形状的开放性研究任务。
- (2):过去的方法要么需要地面真实标注的标题，要么需要大量的优化时间。为了解决这些问题，本文提出了一种新的框架TAPS3D，使用伪标题训练文本引导的3D形状生成器。具体来说，基于渲染的2D图像，我们从CLIP词汇表中检索相关单词，并使用模板构建伪标题。我们的构造标题为生成的3D形状提供了高级语义监督。此外，为了产生细粒度的纹理并增加几何多样性，我们提出采用低级别图像正则化来使假渲染图像与真实图像对齐。在推理阶段，我们的模型可以从给定的文本生成3D纹理形状，而无需任何额外的优化。我们进行了广泛的实验来分析我们提出的每个组件，并展示了我们的框架在生成高保真度的3D纹理和文本相关形状方面的有效性。
- (3):本文提出了一种新的3D纹理形状生成框架，可以在不需要配对的文本和3D形状训练数据的情况下生成高质量和保真度的3D形状。我们提出了一种简单的伪标题生成方法，使得文本引导的3D生成器训练可以在没有测试时间优化的情况下生成文本控制的3D纹理形状，并显著减少时间成本。我们在高级别CLIP损失之上引入了低级别图像正则化损失，以产生细粒度的纹理并增加几何多样性。我们在预训练的GET3D模型上进行了实证训练，以使训练稳定快速，并且可以保留预训练模型的生成质量。
- (4):本文的方法在Car、Chair、Table和Motorbike等四个不同的物体类别上展示了生成的结果，证明了其有效性。本文的方法可以在不需要配对的文本和3D形状训练数据的情况下生成高质量和保真度的3D形状。
#### 7. 方法详细介绍：
本文提出了一种名为TAPS3D的方法，用于从伪监督中进行文本引导的3D纹理形状生成。该方法包括两个模块：伪标题生成和文本引导的3D生成器训练。在伪标题生成模块中，使用CLIP模型从渲染的2D图像中检索相关单词，并基于检索到的单词构建多个候选句子。选择的句子被用作每个3D形状样本的伪标题。在文本引导的3D生成器训练模块中，采用了文本条件的GAN架构，并将伪标题用作生成器条件。训练过程使用高级别的CLIP监督和低级别的图像正则化损失进行监督，以产生细粒度的纹理并增加几何多样性。预训练GET3D模型的映射网络经验性地训练以保持预训练模型的生成质量。

#### 8. 实验设置：
本文在ShapeNet数据集上训练和评估了所提出的方法，特别是针对四个具有复杂几何形状的类别：汽车、桌子、椅子和摩托车。数据按7:1:2的比例分为训练、验证和测试集。训练图像数据是按照GET3D设置进行渲染的。预训练模型分别在每个类别上进行训练。

#### 9. 实验结果和分析：
本文使用Fréchet Inception Distance（FID）作为评估指标，比较了所提出方法与现有方法在渲染的2D图像质量、文本-图像相关性和3D几何质量方面的表现。使用CLIP-R-Precision度量来评估输入文本提示和生成形状之间的一致性。使用Fréchet Point Distance（FPD）来评估几何生成质量。还比较了所提出方法与其他方法的推理速度。文本提供了每种方法使用的设备、输出和所需时间的详细信息。


# Paper:377     注入模式的动态生成有针对性攻击



#### 1. Title: 
Dynamic Generative Targeted Attacks with Pattern Injection

#### 2. Authors: 
Weiwei Feng, Nanqing Xu, Tianzhu Zhang, Yongdong Zhang

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Adversarial attacks, targeted attacks, generative model, transferability, pattern injection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Feng_Dynamic_Generative_Targeted_Attacks_With_Pattern_Injection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究对抗攻击，特别是针对性攻击，旨在欺骗受害者模型输出攻击者所期望的预测结果。本文提出了一种动态生成模型，通过注入目标模式或样式信息来生成有针对性的对抗样本，以提高可转移性。
 
- (2):现有的针对性攻击方法可以大致分为特定实例攻击和特定实例无关攻击。特定实例攻击通过迭代梯度更新在特定实例上制作对抗样本。相比之下，特定实例无关攻击在全局数据集上学习通用扰动或生成模型来执行攻击。然而，它们过于依赖替代模型的分类边界，忽略了目标类的实际分布，这可能导致有限的针对性攻击性能。本文提出了一种动态生成攻击模型，通过注入目标模式或样式信息来生成有针对性的对抗样本，以提高可转移性。此外，本文还提供了严格的理论分析，以保证方法的有效性。

- (3):本文提出了一种动态生成攻击模型，由交叉注意力引导卷积模块和模式注入模块组成。前者采用动态卷积核和静态卷积核，分别针对特定实例和全局数据集，可以继承特定实例攻击和特定实例无关攻击的优点。后者利用模式原型来编码目标模式，可以指导有针对性的对抗样本的生成。本文提出了一种动态生成攻击模型，通过注入目标模式或样式信息来生成有针对性的对抗样本，以提高可转移性。本文提供了严格的理论分析，以保证方法的有效性。

- (4):本文的方法在13个模型上进行了广泛的实验，结果表明，与10种现有的对抗攻击方法相比，本文的方法在提高可转移性方面表现出更好的性能。
#### 7. 方法详细介绍：
本文提出了一种动态生成模型，通过注入目标模式来制作可转移的有针对性的对抗样本。该模型由交叉注意力引导的动态卷积模块和模式注入模块组成。交叉注意力引导的动态卷积模块旨在为每个输入实例学习专门的卷积核，而模式注入模块用于将目标类别的模式或风格注入到对抗样本的生成中。该方法的总体方案包括一些预备知识、动机、网络架构和理论分析。该方法被公式化为xadv = clip(Proj(W * Gθ(x)(x, pt), -ε, ε) + x)，其中ε是扰动预算，W是具有固定权重的平滑算子，pt表示目标类别的语义模式或风格，Proj是一个投影操作。

#### 8. 实验设置：
本文在ImageNet数据集上评估了所提出的方法，使用ImageNet-NeurIPS（1k）验证集。受害者模型包括13个模型，包括正常训练的模型和具有鲁棒性训练机制的模型。扰动大小设置为ε = 16/255，MIM中的衰减因子µ0设置为1。DIM中的变换概率设置为0.7，TIM中的核大小设置为15。其他超参数遵循其原始工作中提供的默认设置。

#### 9. 实验结果与分析：
所提出的方法在攻击成功率方面优于基线方法，包括正常训练的模型和具有鲁棒性训练机制的模型。该方法在11个鲁棒模型中的12个案例中均取得了最佳的有针对性的可转移的对抗攻击性能。该方法对输入处理防御方法（如JPEG压缩和Smooth）也表现出良好的性能。生成模型能够生成与原始图像视觉上相似的扰动，卷积核的数量是一个可以调整的超参数，可以优化方法的性能。


# Paper:378     更好的单张图像深度预测：多元高斯方法



#### 1. Title: 
Single Image Depth Prediction Made Better: A Multivariate Gaussian Take

#### 2. Authors: 
Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte, Luc Van Gool

#### 3. Affiliation: 
Ce Liu: CVL ETH Zurich（苏黎世联邦理工学院计算机视觉实验室）
Suryansh Kumar, Radu Timofte, Luc Van Gool: CVL ETH Zurich, KU Leuven
Shuhang Gu: UESTC China（电子科技大学）

#### 4. Keywords: 
Single image depth prediction, multivariate Gaussian distribution, covariance modeling, deep neural network, low-rank approximation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Single_Image_Depth_Prediction_Made_Better_A_Multivariate_Gaussian_Take_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究单张图像深度预测（SIDP）任务，旨在从训练样本中可靠地建模场景深度。由于该问题本质上是不适定的，因此基本目标是提出一种方法，可以在测试时可靠地模拟场景深度。 
- (2):现有的SIDP方法通常在测试时预测每个像素的单个标量深度值，但是训练模型具有精度限制，可能会预测不精确的深度。因此，SIDP方法必须考虑模型预测的深度变化的期望。本文提出了一种连续建模每个像素深度的方法，可以预测和推理每个像素深度及其分布。为此，使用多元高斯分布模型每个像素场景深度，并引入像素深度协方差建模，以编码其深度依赖性。然而，像素深度协方差建模会导致计算昂贵的连续损失函数，本文使用学习的低秩逼近整体协方差矩阵来高效解决此问题。 
- (3):本文提出了一种在深度神经网络框架中执行多元高斯协方差建模的新方法。引入多元高斯协方差建模来解决SIDP任务是一种创新的数学建模策略。为了高效地优化所提出的公式，本文对协方差矩阵进行参数化，假设它位于较低维度的流形中，以便使用简单的神经网络进行学习。本文使用负对数似然作为损失函数来训练深度网络。在标准基准数据集上测试时，所提出的方法在SIDP任务上取得了最先进的结果。 
- (4):本文在KITTI、NYU和SUN-RGB-D等基准数据集上测试，所提出的方法在SIDP任务上取得了最先进的结果。该方法的准确性（称为MG）在KITTI深度预测基准排行榜上位居前列。
#### 7. 方法详细介绍：
本文提出了一种基于多元高斯分布的单张图像深度预测方法。该方法采用编码器-解码器结构，其中编码器从输入图像中提取分层特征图，解码器估计一组深度图和多元高斯分布。该方法的损失函数采用负对数似然损失和均方误差损失的组合。该方法使用Adam优化器进行训练，并在ImageNet上进行预训练。在测试时，最终的深度图预测来自解码器输出。具体步骤如下：
1. 使用编码器从输入图像中提取特征。
2. 使用U-Decoder逐步预测和细化深度图。
3. 使用K-Decoder预测用于建模协方差的因子Ψθ(I)。
4. 使用负对数似然损失函数来监督训练。

#### 8. 实验设置：
本文在三个基准数据集上进行了实验：NYU Depth V2、KITTI和SUN RGB-D。NYU Depth V2数据集被分为24,231个训练图像和654个测试图像。KITTI数据集被分为23,488个训练图像和697个测试图像。SUN RGB-D数据集在5050个官方测试图像上进行评估。使用SILog、Abs Rel、RMS、RMS log、δi、Sq Rel和iRMS等指标进行性能评估。

#### 9. 实验结果和分析：
本文提出的方法在NYU Depth V2数据集上表现优异，实现了SILog为8.323、Abs Rel为0.087、RMS为0.311、RMS log为0.110和δ1为0.933的结果。该方法在KITTI和SUN RGB-D数据集上也取得了竞争性的结果。


# Paper:379     通过草图学习几何感知表示



#### 1. Title: 
Learning Geometry-aware Representations by Sketching

#### 2. Authors: 
Hyundo Lee, Inwoo Hwang, Hyunsung Go, Won-Seok Choi, Kibeom Kim, Byoung-Tak Zhang

#### 3. Affiliation: 
首尔国立大学人工智能研究所

#### 4. Keywords: 
geometry-aware representation, sketching, stroke-based rendering, CLIP-based perceptual loss, downstream tasks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Learning_Geometry-Aware_Representations_by_Sketching_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文的研究背景是学习几何感知表示，以便更好地理解真实世界和视觉任务。
 
- (2):过去的方法主要集中在捕捉图像的几何特征，但这些方法通常缺乏紧凑性，优先考虑背景等无关特征。本文提出了一种新的方法，通过草图来学习表示场景，这种方法可以更好地保留几何信息。与现有的草图生成方法不同，本文的方法不需要草图数据集进行训练，而是学习将图像转换为一组彩色笔画，这些笔画明确地表示场景的几何信息。本文的方法通过草图生成表示，可以更好地保留几何信息，同时具有高度的可解释性。
 
- (3):本文提出了一种名为Learning by Sketching (LBS)的方法，该方法可以在单个推理步骤中将图像转换为一组彩色笔画，这些笔画明确地表示场景的几何信息。为了教授草图的风格，本文使用基于CLIP模型的感知损失，该模型可以测量图像和生成的草图之间的语义和几何相似性。本文提出了一种渐进优化过程，可以预测如何通过CLIP感知损失从其初始位置优化笔画，以在单个推理步骤中生成抽象草图。本文的方法可以更好地保留几何信息，同时具有高度的可解释性。
  
- (4):本文的方法在未标记的CLEVR数据集上显著提高了对象属性分类的性能，同时在CLEVR和STL-10数据集之间进行域转移，并在各种下游任务中提供了丰富的几何信息。本文的方法可以更好地保留几何信息，同时具有高度的可解释性，可以作为一种新的表示方法用于各种下游任务。
#### 7. 方法详细介绍：
本文提出了一种名为“通过草图学习几何感知表示”的方法（LBS），该方法通过草图学习几何感知表示。该方法由三个组件组成：笔画解码器网络、笔画引导网络和笔画嵌入网络。笔画解码器网络从栅格化的草图生成一组笔画，笔画引导网络为解码器网络提供指导以生成更准确的笔画，笔画嵌入网络将笔画映射到嵌入空间以捕获粗粒度信息。最终目标是最小化感知损失、引导损失和嵌入损失。

#### 8. 实验设置：
本文通过设计多个研究问题和相应的下游任务来研究所提出方法的有效性和泛化性。研究问题包括等变性的影响、几何原语和概念的影响、局部几何信息和空间推理、域转移以及传统草图任务。下游任务包括旋转MNIST、Geoclidean数据集、CLEVR数据集、STL-10数据集和QMUL-ShoeV2数据集。本文在附录B中提供了每个实验设置的详细信息。

#### 9. 实验结果和分析：
本文提供了每个实验设置的实现细节。例如，对于旋转MNIST和Geoclidean数据集，本文将感知损失和引导损失替换为L1损失，对于STL-10数据集，本文使用U2-Net为背景提供掩码信息。本文还使用交叉熵损失对笔画嵌入网络进行监督训练。本文报告了每个下游任务的定量结果，并将LBS的性能与几个基线进行了比较。实验结果表明，LBS在捕获几何概念和提供有价值的局部几何信息方面具有有效性，可以在不同领域之间转移知识，并且对于传统的草图任务非常有价值。本文还将LBS与其他基于笔画的生成方法进行比较，并显示LBS在编码几何和语义信息方面实现了优异的结果。


# Paper:380     基于实例感知的领域泛化人脸反欺诈



#### 1. Title: 
Instance-Aware Domain Generalization for Face Anti-Spoofing

#### 2. Authors: 
Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Ran Yi, Shouhong Ding, Lizhuang Ma

#### 3. Affiliation: 
Qianyu Zhou, Ran Yi, Lizhuang Ma: Shanghai Jiao Tong University, Shanghai Key Laboratory of Computer Software Evaluating and Testing; Ke-Yue Zhang, Taiping Yao, Shouhong Ding: Youtu Lab, Tencent; Xuequan Lu: Deakin University.

#### 4. Keywords: 
Face anti-spoofing, domain generalization, instance-aware, asymmetric instance adaptive whitening, dynamic kernel generator, categorical style assembly.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Instance-Aware_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了基于领域泛化（DG）的人脸反欺诈（FAS）问题，旨在提高在未知场景下的泛化能力。以往的方法通常依赖于领域标签来对齐每个领域的分布，以学习领域不变表示。然而，人工领域标签是粗粒度和主观的，不能准确地反映真实的领域分布。此外，这种领域感知的方法侧重于领域级别的对齐，这不足以保证学习到的表示对领域样式不敏感。为了解决这些问题，本文提出了一种新的DG FAS视角，即在不需要领域标签的情况下，在实例级别上对齐特征。
- (2):以往的DG FAS方法通常依赖于人工定义的领域标签来进行领域感知的领域泛化，这不能保证学习到的表示对领域样式不敏感。本文提出了一种新的视角，即在不需要领域标签的情况下，在实例级别上对齐特征，以提高在未知领域中的泛化能力。本文提出了一种实例感知的领域泛化框架，通过削弱特征对实例特定样式的敏感性来学习可泛化的特征。具体而言，本文提出了不对称实例自适应白化、动态核生成器和分类样式组装等方法，以进一步促进学习对样式不敏感的特征。本文的方法在多个数据集上进行了广泛的实验和分析，证明了其优于现有竞争方法的性能。
- (3):本文提出了一种新的实例感知的领域泛化框架，通过削弱特征对实例特定样式的敏感性来学习可泛化的特征，而不需要领域标签。具体而言，本文提出了不对称实例自适应白化、动态核生成器和分类样式组装等方法，以进一步促进学习对样式不敏感的特征。本文的方法在多个数据集上进行了广泛的实验和分析，证明了其优于现有竞争方法的性能。
- (4):本文的方法在多个数据集上进行了广泛的实验和分析，证明了其优于现有竞争方法的性能。本文的方法在未知领域中取得了良好的泛化性能，证明了其实用性。
#### 7. 方法详细介绍：
本文提出了一种名为Instance-Aware Domain Generalization (IADG)的框架，旨在通过削弱特征对实例特定风格的敏感性，从而在不需要域标签的情况下对实例级别的特征进行对齐，提高对未见域的泛化能力。IADG包括三个关键组件：Asymmetric Instance Adaptive Whitening (AIAW)，Categorical Style Assembly (CSA)和Dynamic Kernel Generator (DKG)。首先，AIAW旨在通过对细粒度实例级别的高阶统计量，自适应地白化每个实例的风格敏感特征相关性。其次，为了进一步提高泛化能力，AIAW还对真实和欺诈样本采用了不对称监督。此外，为了促进AIAW中对无风格敏感特征的学习，提出了DKG和CSA来生成样式多样化的特征。该方法旨在捕捉一致的任务相关信息，即使存在样式扰动，也能确保特征提取器提取任务相关特征。

#### 8. 实验设置：
本文使用四个公共的FAS数据集，分别是CASIA-MFSD、Idiap Replay-Attack、MSU-MFSD和OULU-NPU，以评估所提出方法的有效性。这些数据集采用不同的捕获设备、攻击类型、不同的照明条件、背景场景和种族。为了公平比较，严格遵循以前DG FAS方法的相同协议。实现使用PyTorch，并使用Adam优化器进行训练。学习率设置为0.0001。使用Half Total Error Rate (HTER)和Area Under Curve (AUC)作为评估指标。

#### 9. 实验结果和分析：
本文使用表格和图形展示了实验结果。表5展示了不同Instance Adaptive Whitening (IAW)损失的效果，表6展示了不同样式增强的效果。表7说明了不同Dynamic Kernel Generator (DKG)设计的效果。本文还包括t-SNE特征可视化和分析，证明了所提出方法在对齐特征和增强泛化能力方面的有效性。所提出的方法在几个基准数据集上优于现有的竞争方法。


# Paper:381     一种旋转-平移分离的鲁棒高效视觉惯性初始化方法



#### 1. Title: 
A Rotation-Translation-Decoupled Solution for Robust and Efficient Visual-Inertial Initialization

#### 2. Authors: 
Yijia He, Bo Xu, Zhanpeng Ouyang, Hongdong Li

#### 3. Affiliation: 
Yijia He: 中国科学院 (Chinese Academy of Sciences)

#### 4. Keywords: 
Visual-inertial odometry, initialization, gyroscope bias, rotation-translation-decoupled, linear translation constraints

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/He_A_Rotation-Translation-Decoupled_Solution_for_Robust_and_Efficient_Visual-Inertial_Initialization_CVPR_2021_paper.html  Github: https://github.com/boxuLibrary/drt-vio-init

#### 6. Summary : 
- (1):本文研究的是视觉惯性里程计（VIO）的初始化问题，即如何通过融合图像和IMU测量来估计相机的初始运动状态和场景结构。VIO在虚拟或增强现实系统（VR/AR）和各种自主导航系统中得到广泛应用。
 
- (2):现有的VIO初始化方法存在一些问题，例如松耦合方法的视觉结构运动（SfM）稳定性差，而紧耦合方法通常忽略了闭合形式解中的陀螺仪偏差，导致精度有限。此外，这两种方法都需要同时重建三维点云，计算成本较高。本文提出了一种新的方法，将旋转和平移估计分离，实现更高的效率和更好的鲁棒性。该方法充分结合了惯性和视觉测量，用于旋转和平移初始化。首先，设计了一个仅用于陀螺仪偏差估计的旋转解，紧密耦合了陀螺仪和相机观测。其次，使用线性平移约束以全局最优的方式解决了初始速度和重力向量，并且不需要重建三维点云。实验表明，与现有方法相比，本文方法的计算速度快8-72倍（对于10帧数据集），并且具有更高的鲁棒性和精度。

- (3):本文提出了一种旋转-平移分离的VIO初始化框架，将陀螺仪测量直接集成到相机旋转估计中，从而大大提高了初始化的鲁棒性，而与之相关的平移初始变量则通过线性平移约束以高效的方式解决，而不需要估计三维结构。本文的主要贡献包括：1）提出了一个仅用于陀螺仪偏差估计的旋转解，可以比仅使用视觉的方法更高效、更鲁棒地获得相机旋转；2）提出了一种基于线性平移约束的全局最优解，用于估计初始速度和重力向量。其线性性和独立于场景结构的特点显著提高了计算效率；3）本文提出的初始化框架在公共数据集上的精度和鲁棒性均优于现有方法，同时对于10帧数据集的计算时间快8-72倍。我们发布了代码以促进交流。

- (4):本文提出的方法在EuRoC数据集上进行了测试，与现有方法相比，本文方法的计算速度快8-72倍（对于10帧数据集），并且具有更高的鲁棒性和精度。本文方法在初始化的尺度因子误差方面表现最佳。
#### 7. 方法详细介绍：
本文提出了一种旋转-平移解耦（RTD）的视觉惯性初始化方法。该方法包括陀螺仪偏差优化器和速度重力估计器。陀螺仪偏差优化器使用至少两张图像来估计陀螺仪偏差，并使用特征对应来优化相机之间的旋转。速度和重力估计器包括基于线性平移约束的紧耦合解和基于卡尔曼滤波的松耦合解。初始速度和重力向量可以在不估计三维点云的情况下高效地解决。具体步骤包括：
1. 陀螺仪偏差优化器：使用两张图像来估计陀螺仪偏差，并使用特征对应来优化相机之间的旋转。
2. 速度和重力估计器：包括基于线性平移约束的紧耦合解和基于卡尔曼滤波的松耦合解。初始速度和重力向量可以在不估计三维点云的情况下高效地解决。

#### 8. 实验设置：
本文使用了来自微型飞行器（MAV）的EuRoC数据集来验证算法。该数据集包含两个场景中不同运动模式的11个序列。我们采样了1422个数据段，以充分评估每个算法的准确性、鲁棒性和时间消耗。在实验中，所有算法使用相同的图像处理操作，使用KLT稀疏光流算法跟踪现有特征，并检测新的角点特征以维护每个图像的150个点。使用基于基础矩阵模型的RANSAC剔除异常值，1像素重投影误差。对于松耦合算法，采用通用SfM框架来估计相机运动，首先使用5点算法和PnP求解器估计初始相机姿态，然后使用束调整来优化所有姿态和点云。BA的最大运行时间设置为0.2秒，以满足实时命令。

#### 9. 实验结果和分析：
本文提出的旋转-平移解耦的视觉惯性初始化方法在准确性和鲁棒性方面均显著优于现有方法。DRT-l方法在几乎所有运动场景中均优于其他方法，证明了所提出框架的有效性。该方法计算效率高，DRT-l比松耦合方法VINS-Mono快8倍，比紧耦合方法CS-VISfM快72倍。该系统的鲁棒性通过DRT-l在小误差范围内的更多初始化序列得到了证明。


# Paper:382     统一姿态序列建模



#### 1. Title: 
Unified Pose Sequence Modeling

#### 2. Authors: 
Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Qiuhong Ke, Jun Liu

#### 3. Affiliation: 
第一作者：新加坡科技设计大学

#### 4. Keywords: 
Pose-based tasks, sequence modeling, multi-task learning, language models, action recognition

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Foo_Unified_Pose_Sequence_Modeling_CVPR_2021_paper.html  Github: None

#### 6. Summary: 
- (1):本文研究基于姿态数据的多个任务，如动作识别、3D姿态估计和3D早期动作预测，提出了一种统一的姿态序列建模方法。姿态序列是通过人体关节坐标捕捉人体运动的一种有效表示方法，因此在人体运动和行为理解方面具有广泛的应用前景。
- (2):现有的方法通常需要针对每个任务设计特定的网络架构，因此在处理多个任务时需要设计和训练多个单独的模型，效率低下。本文提出了一种统一的姿态序列模型，通过将输出格式统一为文本嵌入序列，从而避免了多个任务特定的输出头。此外，为了避免不同任务之间的干扰，本文还提出了一种动态路由机制，使模型能够学习哪些参数子集应该在不同任务之间共享。
- (3):本文提出的统一姿态序列模型通过优化单个自回归Transformer，可以处理所有上述任务的统一输出序列。此外，本文还提出了一种动态路由机制，使模型能够学习哪些参数子集应该在不同任务之间共享。实验结果表明，本文提出的方法在四个不同的任务上均取得了良好的性能。
- (4):本文提出的方法在多个任务上取得了良好的性能，证明了其在处理多个姿态序列任务方面的有效性。
#### 7. 方法详细介绍：
本文提出了一种统一姿态序列模型（Unified Pose Sequence，UPS），用于处理多种基于骨架的任务，包括3D动作识别、2D动作识别、3D姿态估计和3D早期动作预测。UPS模型采用类似于语言建模任务的输出方式，预测一系列输出标记，以统一不同任务的输出格式。模型由联合分词器、UPS编码器和UPS解码器组成。联合分词器将基于文本的类别标签和基于坐标的关节位置进行分词，以建立统一的词汇表。UPS编码器将输入姿态序列编码为特征序列，UPS解码器自回归地生成一系列输出标记。提出了一种动态路由机制，以促进任务之间的参数共享，并避免异构任务之间的干扰。

#### 8. 实验设置：
本文在四个流行的基于骨架的任务上进行了评估，包括NTU RGB+D 120、NTU RGB+D 60、Kinetics-Skeleton和HDM05数据集。输入姿态序列被归一化为固定长度的300帧。使用Adam优化器进行训练，学习率为1e-4，训练200个epochs。批量大小设置为64，丢失率设置为0.5。模型使用PyTorch实现，在单个NVIDIA Tesla V100 GPU上进行训练。

#### 9. 实验结果与分析：
本文提出的UPS模型在所有四个评估数据集上均取得了最先进的性能，NTU RGB+D 120的准确率为96.1％，NTU RGB+D 60的准确率为95.2％，Kinetics-Skeleton的准确率为94.5％，HDM05的准确率为87.5％。该模型优于需要特定任务体系结构和不同任务的单独模型的现有方法。UPS模型中的动态路由机制有效地避免了不同任务之间的干扰，并实现了参数共享，从而提高了性能。在NTU120数据集上进行了3D早期动作预测的性能比较，观察比率为20％，40％和60％。结果表明，所提出的统一姿态序列建模（UPS）方法与其他最先进的方法相比，具有良好的预测结果。还进行了消融研究，以展示所提出的UPS的有效性，包括UPS编码器的深度、每层中块的数量以及路由机制的影响。结果表明，UPS中的动态路由机制可以提高UPS的性能。


# Paper:383     Co-SLAM: 基于联合坐标和稀疏参数编码的神经实时SLAM



#### 1. Title: 
Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM

#### 2. Authors: 
Hengyi Wang, Jingwen Wang, Lourdes Agapito

#### 3. Affiliation: 
University College London（伦敦大学学院）

#### 4. Keywords: 
RGB-D SLAM, neural network, joint coordinate and sparse parametric encoding, global bundle adjustment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2021_paper.html  Github: https://github.com/hengyiwang/CoSLAM

#### 6. Summary : 
- (1):本文研究的是实时RGB-D SLAM问题，传统的SLAM方法存在手工损失项的问题，而基于神经网络的方法可以直接从数据中学习平滑性和连贯性先验。然而，现有的基于坐标的方法需要长时间的训练，而基于参数的方法虽然训练速度快，但在未观察到的区域无法完成表面填充。因此，本文提出了一种联合坐标和稀疏参数编码的混合表示方法，以及全局束调整的优化策略，以实现实时、高保真的场景重建和表面填充。

- (2):传统的SLAM方法存在手工损失项的问题，而基于神经网络的方法可以直接从数据中学习平滑性和连贯性先验。然而，现有的基于坐标的方法需要长时间的训练，而基于参数的方法虽然训练速度快，但在未观察到的区域无法完成表面填充。本文提出了一种联合坐标和稀疏参数编码的混合表示方法，以及全局束调整的优化策略，以实现实时、高保真的场景重建和表面填充。

- (3):本文提出了一种联合坐标和稀疏参数编码的混合表示方法，以实现实时、高保真的场景重建和表面填充。具体地，本文使用多分辨率哈希网格来表示场景，以利用其高收敛速度和表示高频局部特征的能力。此外，本文还结合了一种单blob编码，以鼓励未观察到的区域的表面连贯性和完成度。这种联合参数-坐标编码通过将两种方法的优点结合起来，实现了实时和稳健的性能。此外，本文的光线采样策略允许Co-SLAM在所有关键帧上执行全局束调整，而不需要选择关键帧来维护少量活动关键帧，从而避免了竞争神经SLAM方法的限制。本文的方法在多个数据集和基准测试中取得了最先进的场景重建结果，并在各种数据集和基准测试中实现了更好或至少相当的跟踪精度。

- (4):本文提出的方法在RGB-D SLAM任务上取得了很好的性能，实现了实时、高保真的场景重建和表面填充。在多个数据集和基准测试中，本文的方法均取得了最先进的场景重建结果，并在各种数据集和基准测试中实现了更好或至少相当的跟踪精度。
#### 7. 方法详细介绍：
Co-SLAM是一种基于神经网络的RGB-D SLAM系统，能够实时进行在线跟踪和建图。该方法采用联合坐标和参数编码的方式，将场景表示为多分辨率哈希网格，以利用其高收敛速度和表示高频局部特征的能力。此外，Co-SLAM还采用一种单blob编码，以鼓励未观察区域的表面连贯性和完整性。这种联合参数-坐标编码通过将快速收敛和表面孔填充的优点结合起来，实现了实时和稳健的性能。此外，Co-SLAM通过从所有过去的关键帧中采样光线来执行全局束调整，从而在姿态估计中实现了重要的稳健性和性能提升。

具体步骤如下：
1. 场景表示：采用多分辨率哈希网格表示场景，每个级别的空间分辨率在最粗的Rmin和最细的Rmax之间设置，以逐步方式进行。几何解码器输出预测的SDF值s和特征向量h，颜色MLP预测RGB值。深度和颜色渲染通过沿采样光线积分预测值来执行。
2. 相机跟踪：通过使用恒定速度假设优化当前帧的姿态，并通过最小化相对于相机参数的目标函数来迭代优化姿态。联合优化是交替进行的，首先优化场景表示，然后使用相机参数{ξt}上的累积梯度更新相机姿态。
3. 全局束调整：通过从所有过去的关键帧中采样光线来执行全局束调整，从而在姿态估计中实现了重要的稳健性和性能提升。

#### 8. 实验设置：
Co-SLAM在四个不同的数据集（Replica、Synthetic RGBD、ScanNet和TUM）的各种场景上进行了广泛的评估。使用Depth L1（cm）、Accuracy（cm）、Completion（cm）和Completion ratio（%）等指标评估重建质量，使用ATE RMSE（cm）评估相机跟踪。基线包括iMAP和NICE-SLAM，我们使用与Co-SLAM相同的网格剔除策略评估iMAP和NICE-SLAM。

#### 9. 实验结果和分析：
本文在三个数据集（Replica、Synthetic RGB-D和ScanNet）上对Co-SLAM进行了评估。结果表明，Co-SLAM在重建精度和速度方面均优于基线方法。在Replica数据集上，Co-SLAM能够捕捉细节并产生完整而平滑的重建。在Synthetic RGB-D数据集上，Co-SLAM能够恢复薄结构并在给定嘈杂深度测量的情况下实现合理的场景完成。在ScanNet数据集上，Co-SLAM在比NICE-SLAM更少的跟踪和映射迭代次数下实现更好的跟踪结果，同时以6-12 Hz的速度运行。本文还包括了关于不同编码的消融研究，结果表明联合编码提高了单一编码的表示能力，并实现了更好的完成和更准确的重建。


# Paper:384     DisCoScene：用于可控的3D感知场景合成的空间分离生成辐射场



#### 1. Title: 
DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-aware Scene Synthesis

#### 2. Authors: 
Yinghao Xu, Menglei Chai, Zifan Shi, Sida Peng, Ivan Skorokhodov, Aliaksandr Siarohin, Ceyuan Yang, Yujun Shen, Hsin-Ying Lee, Bolei Zhou, Sergey Tulyakov

#### 3. Affiliation: 
第一作者：CUHK（香港中文大学）

#### 4. Keywords: 
3D-aware image synthesis, generative radiance fields, scene synthesis, layout prior, spatial disentanglement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2021_paper.html  Github: https://github.com/snap-research/DisCoScene

#### 6. Summary : 
- (1):本文研究的是3D感知图像合成，旨在解决现有方法在合成包含多个对象的复杂场景方面的局限性。

- (2):现有的3D感知图像合成方法主要集中在生成单个规范对象上，对于包含多个对象的复杂场景合成能力有限。本文提出了DisCoScene，一种基于3D感知的生成模型，用于高质量和可控的场景合成。与现有方法不同的是，本文采用了一种非常抽象的对象级表示（即没有语义注释的3D边界框）作为场景布局先验，这种先验易于获得，通用于描述各种场景内容，并且具有将对象和背景分离的信息。此外，它还作为直观的用户控制场景编辑的工具。基于这样的先验，本文提出的模型通过学习仅基于2D图像的全局-局部判别来将整个场景空间分解为以对象为中心的生成辐射场，从而获得了单个对象的生成保真度和编辑灵活性，同时能够有效地将对象和背景组合成完整的场景。

- (3):本文提出的方法是一种基于抽象布局先验的3D感知生成模型，通过空间分离的辐射场实现了高质量和可控的场景合成。本文的创新点在于提出了一种抽象的对象级表示作为场景布局先验，这种先验易于获得，通用于描述各种场景内容，并且具有将对象和背景分离的信息。此外，本文提出了全局-局部判别，通过同时关注整个场景和单个对象来强制实现对象与背景之间的空间分离。最后，本文还开发了一种针对空间分离辐射场的高效渲染管道，大大加速了对象渲染和场景合成的训练和推理阶段。

- (4):本文在多个场景数据集上展示了最先进的性能，包括具有挑战性的Waymo室外数据集。定性和定量结果表明，与现有基线相比，本文的方法在生成质量和编辑能力方面均取得了最先进的性能。
#### 7. 方法详细介绍：
DisCoScene是一种用于高质量和可控场景合成的3D感知生成模型。该方法使用抽象的物体级别表示，即没有语义注释的3D边界框作为场景布局先验。基于这个先验，该模型通过学习全局-局部判别来将整个场景空间上的物体分离成物体中心的生成辐射场。该模型在仅学习2D图像的情况下获得了单个物体的生成保真度和编辑灵活性，同时能够将物体和背景高效地组合成完整的场景。该方法在多样化的数据集上进行了评估，包括室内和室外场景，并展示了在生成质量和编辑能力方面的最新性能。

具体步骤如下：
1. 使用简化的场景布局表示，即3D边界框，将场景分解为多个物体。
2. 使用多个单独的生成NeRFs来模拟不同的物体。
3. 在规范化空间中推断物体辐射场，以允许物体之间的权重共享。
4. 使用每个物体的位置和比例作为生成器的条件，以编码更一致的内在属性。
5. 在全局空间中评估背景辐射场。
6. 使用空间分离的辐射场来表示场景，并仅关注边界框内的有效点以实现更高效的渲染。
7. 采用局部和全局判别来实现更好的物体级别监督。
8. 共同训练生成器和判别器，以保证渲染的全局一致性和单个物体的局部判别。

#### 8. 实验设置：
本文在三个多物体场景数据集上评估了DisCoScene方法，包括CLEVR、3D-FRONT和WAYMO。CLEVR数据集使用官方脚本渲染场景，3D-FRONT数据集通过过滤不常见的布局或不自然的大小来获得4K个卧室，WAYMO数据集通过启发式规则过滤小型和嘈杂的汽车并收集了70K张图像的子集。本文将DisCoScene与包括StyleGAN2、EpiGRAF、VolumeGAN、EG-3D、GIRAFFE和GSN在内的2D和3D GAN进行比较。本文使用基线模型的官方实现或与其论文一起发布的模型进行训练。本文在补充材料中提供了实现细节，包括网络架构和训练。

#### 9. 实验结果和分析：
本文使用FID分数来衡量整个图像的质量，使用FIDobj来衡量单个物体的质量。本文对空间条件（S-Cond）和物体判别器（Dobj）的消融分析表明，Dobj显著提高了物体保真度，从而实现了更好的物体与背景分离。空间条件也在场景和物体级别上实现了更好的图像质量，因为更适当的语义更符合真实图像的本地分布。本文还采用简单的超采样（SSAA）策略来减少边缘混叠，与基线相比，实现了更一致的边界。本文还采用神经渲染器来增强渲染效率，并带来更好的隐式建模，如阴影等真实光照效果。该方法可以将真实图像嵌入预训练模型的潜在空间中，并支持所有物体操作以编辑图像。


# Paper:385     EqMotion: 具有不变交互推理的等变多智能体运动预测



#### 1. Title: 
EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning

#### 2. Authors: 
Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, Yanfeng Wang

#### 3. Affiliation: 
第一作者：上海交通大学，上海人工智能实验室

#### 4. Keywords: 
Motion prediction, Equivariant networks, Interaction reasoning, Euclidean geometric transformations

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_EqMotion_Equivariant_Multi-Agent_Motion_Prediction_With_Invariant_Interaction_Reasoning_CVPR_2021_paper.html  Github: https://github.com/MediaBrain-SJTU/EqMotion

#### 6. Summary : 
- (1):本文研究的是多智能体运动预测问题，提出了一种新的方法，旨在解决现有方法中忽略了欧几里得几何变换下的运动等变性和代理之间交互的不变性问题。

- (2):现有的运动预测方法大多数忽略了运动等变性和代理之间交互的不变性问题，导致模型的鲁棒性和泛化能力不足。本文提出了EqMotion，一种基于参数网络的等变运动预测模型，通过等变几何特征学习、不变模式特征学习和不变交互推理三个模块来解决这些问题。

- (3):本文提出的EqMotion是第一个在参数网络中理论上保证序列到序列运动等变性的运动预测模型。为了实现运动等变性，本文提出了等变几何特征学习模块，通过等变操作的专门设计来学习欧几里得可变换特征。为了推理代理之间的交互，本文提出了不变交互推理模块，通过利用运动中的不变因素来推断不变的交互图。为了进一步提高运动特征的综合性，本文提出了不变模式特征学习模块，学习不变的模式特征，与等变几何特征协同工作，增强网络表达能力。

- (4):本文在四个不同的场景下进行了实验，包括粒子动力学、分子动力学、人体骨架运动预测和行人轨迹预测。实验结果表明，本文提出的方法不仅具有普适性，而且在所有任务中都取得了最先进的预测性能，分别提高了24.0/30.1/8.6/9.2%。本文提出的EqMotion模型轻量级，模型大小不到其他模型大小的30%。
#### 7. 方法详细介绍：
本文提出了一种名为EqMotion的运动预测网络，该网络在欧几里得几何变换下具有等变性。网络架构包括初始化层、不变推理模块、等变和不变特征学习层以及等变输出层。初始化层获取初始几何和模式特征，而不变推理模块推断代理之间的交互类别。等变特征学习层利用空间和时间依赖性以及模型空间交互来学习更具代表性的几何特征。不变特征学习层维护模式特征的不变性。等变输出层根据学习到的几何特征预测代理的未来运动。具体而言，等变几何特征学习层采用了等变卷积和等变池化操作，不变模式特征学习层采用了不变卷积和不变池化操作。 

#### 8. 实验设置：
本文在四个不同的场景下进行了实验，包括粒子动力学、分子动力学、3D人体骨架运动和行人轨迹预测。实验中使用的数据集在附录中详细描述。 

#### 9. 实验结果和分析：
本文使用平均位移误差（ADE）、最终位移误差（FDE）和平均关节位置误差（MPJPE）评估了所提出方法的性能。结果表明，所提出的方法在所有四个场景下均优于几种最先进的方法。具体而言，在物理模拟场景中，所提出的方法在交互推理任务上实现了97.6%的识别准确率和100.0%的一致性，并且在ADE和FDE方面均优于其他方法。在附录中详细介绍了实验结果和比较。


# Paper:386     五千种方式看玫瑰



#### 1. Title: 
Seeing a Rose in Five Thousand Ways

#### 2. Authors: 
Yunzhi Zhang, Shangzhe Wu, Noah Snavely, Jiajun Wu

#### 3. Affiliation: 
Yunzhi Zhang: 斯坦福大学 (Stanford University)

#### 4. Keywords: 
Object intrinsics, generative model, shape and image generation, view synthesis, relighting.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Seeing_a_Rose_in_Five_Thousand_Ways_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在从单张图像中学习物体内在属性，包括几何、纹理和材质分布，以生成不同大小、形状、姿态和光照条件下的物体实例。这是一个具有挑战性的问题，因为我们只有一张图像，而且这张图像中的物体实例数量非常有限，使得推断问题高度欠约束。

- (2):现有的3D感知图像生成模型通常需要大量的训练实例，而本文的方法只需要一张图像。现有的多视角重建或神经渲染方法通常只适用于固定的物体或场景，而本文的方法可以处理不同物体实例之间的差异。本文的方法通过设计具有归纳偏差的模型来解决这些挑战，该模型的归纳偏差受到物体内在属性的指导。

- (3):本文提出了一种生成模型，从单张图像中学习物体内在属性，包括3D形状和表面反照率。该模型通过物体实例掩码和姿态分布，学习物体的3D神经表示，从而将姿态和光照变化因素分离出来。通过对学习到的物体内在属性进行随机采样，可以生成具有不同身份的新实例，并且可以从新视点或不同光照配置中呈现合成实例。

- (4):本文的方法在形状重建和生成、新视角合成和重新照明等多个下游任务中取得了优异的结果。
#### 7. 方法详细介绍：
本文提出了一种生成模型，学习捕捉物体内在特征，包括物体实例的几何、纹理和材质分布，以单个输入图像为基础，学习物体的3D形状、表面反照率和光泽度的分布，消除姿态和光照变化的影响，基于一组实例掩码和给定的实例姿态分布。该模型集成了一个反照率场和最近的NeuS表示，用于捕捉物体内在特征。该模型可以实现一系列应用，包括在新视角和照明条件下的形状和图像生成。该模型通过对抗训练框架进行训练，将渲染的2D图像分布与输入图像中的掩码实例分布相匹配。

#### 8. 实验设置：
本文在包含多个相似物体实例的真实世界图像上测试了所提出的方法。数据集包括11个野外图像，涵盖了各种物体类别。为了获得前景掩码，本文使用了U 2-Net的预训练模型或手动分割图像。在获得实例掩码后，使用针孔相机估计相机，视场为10度。姿态分布估计为随机分布在一个球体或3D平面上，同时保持在图像中可见，根据输入图像在本地对象框架中的轴随机旋转。

#### 9. 实验结果与分析：
所提出的方法在保留的测试视图上，与GNeRF相比，在法线和外观重建方面的误差显著降低。与需要真实相机姿态的Neural-PIL和NeRD相比，无论是重建质量还是内在分解质量都显著优于它们，与需要真实姿态的NeRF相当，但不需要真实姿态，也不需要内在分解。在图7中展示了两个示例场景的定性结果。所提出的方法的局限性也被提到，例如假设输入图像中存在多个相似的非遮挡实例，并且将光源近似为单个定向光。


# Paper:387     MOTRv2：通过预训练的目标检测器引导端到端的多目标跟踪



#### 1. Title: 
MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors

#### 2. Authors: 
Yuang Zhang, Tiancai Wang, Xiangyu Zhang

#### 3. Affiliation: 
Yuang Zhang: 上海交通大学 (Shanghai Jiao Tong University)

#### 4. Keywords: 
Multi-object tracking, object detection, end-to-end, transformer, anchor-based modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2022_paper.html  Github: https://github.com/megvii-research/MOTRv2

#### 6. Summary : 
- (1):本文研究多目标跟踪(MOT)领域，提出了一种简单而有效的方法MOTRv2，通过预训练的目标检测器来引导端到端的多目标跟踪。
- (2):传统的MOT方法主要采用检测-关联的流程，但是由于检测性能的限制，这些方法的性能受到了很大的影响。现有的端到端方法，如MOTR和TrackFormer，由于检测性能较差，也不如检测-关联方法。因此，本文提出了一种新的方法MOTRv2，通过引入额外的目标检测器来提高检测性能，从而改善MOTR的性能。MOTRv2采用锚点查询的形式，并使用额外的目标检测器生成锚点作为提议，提供检测先验，从而大大减轻了MOTR中联合学习检测和关联任务之间的冲突。 
- (3):本文提出的MOTRv2方法采用锚点查询和提议传播的方式，将额外的目标检测器引入到MOTR中，从而提高了检测性能。MOTRv2采用锚点初始化检测和跟踪查询，使用transformer解码器预测相对于锚点的偏移量，使检测任务的优化更加容易。MOTRv2的优点在于，它从额外的目标检测器中获得了良好的检测性能，检测任务与MOTR框架隐式解耦，缓解了检测和关联任务之间的冲突。 
- (4):本文在DanceTrack、BDD100K和MOT17数据集上进行了实验，MOTRv2相比于MOTR在不同场景下都有很大的性能提升。在DanceTrack数据集上，MOTRv2的HOTA指标比OC-SORT高出14.8%，AssA指标比第二名高出18.8%。在BDD100K数据集上，MOTRv2的mMOTA指标为43.6%，比之前最好的方法Unicorn高出2.4%。在MOT17数据集上，MOTRv2也取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了MOTRv2，它是一个端到端的多目标跟踪框架，包括两个主要组件：一个先进的目标检测器和一个修改后的基于锚点的MOTR跟踪器。MOTRv2采用锚点查询的形式，使用一个额外的目标检测器生成提议作为锚点，提供了检测先验信息。MOTRv2使用transformer解码器预测相对于锚点的偏移量，使得检测任务的优化更加容易。MOTRv2通过额外的目标检测器引入了良好的检测性能，大大提高了在DanceTrack、BDD100K和MOT17数据集上的性能。

#### 8. 实验设置：
本文在DanceTrack、MOT17和BDD100K三个数据集上评估了所提出的方法。在训练MOTR之前，使用YOLOX检测器为所有图像生成目标提议。超参数（如输入图像大小）与ByteTrack保持一致。模型在8个GPU上训练，每个GPU的批量大小为1。在DanceTrack数据集上，采用HSV增强来训练MOTR。

#### 9. 实验结果与分析：
本文提出的MOTRv2方法结合了MOTR跟踪器和YOLOX检测器，实现了高性能的多目标跟踪。YOLOX检测器生成高质量的目标提议，有助于MOTR更容易地检测新的目标，降低了目标检测的复杂度，使MOTR能够集中于关联过程。所提出的方法在MOT17和MOT20数据集上取得了最先进的性能，以HOTA、MOTA、IDF1等指标优于以前的方法。消融实验表明，使用YOLOX提议和与CrowdHuman数据集的联合训练显著提高了跟踪性能。所提出的跟踪查询对齐方法通过将锚点与相应的YOLOX提议对齐，消除了误检和重复跟踪查询，进一步提高了性能。然而，所提出的方法仍然需要大量数据，并且在较小的数据集上表现不佳。方法的效率也受到MOTR组件的限制。

实验结果：
本文在DanceTrack、BDD100K和MOT17三个数据集上比较了所提出的MOTRv2方法与最先进的方法的性能。在DanceTrack数据集上，MOTRv2以69.9 HOTA的成绩远远超过了其他跟踪-检测方法（OC-SORT [6]相比，提高了14.8%），AssA指标比第二好的方法高出18.8%。在大规模多类别BDD100K数据集上，MOTRv2的mMOTA和mIDF1均优于所有方法。在MOTChallenge数据集上，MOTRv2与其他最先进的方法相比取得了竞争性的结果。


# Paper:388     DynamicDet：一种统一的动态目标检测架构



#### 1. Title: 
DynamicDet: A Unified Dynamic Architecture for Object Detection

#### 2. Authors: 
Zhihao Lin, Yongtao Wang, Jinhe Zhang, Xiaojie Chu

#### 3. Affiliation: 
Peking University, Wangxuan Institute of Computer Technology (北京大学，王选计算机技术研究所)

#### 4. Keywords: 
Dynamic neural network, object detection, adaptive inference, multi-scale information, optimization strategy

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_DynamicDet_A_Unified_Dynamic_Architecture_for_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/VDIGPKU/DynamicDet

#### 6. Summary : 
- (1):本文研究的背景是动态神经网络在深度学习中的应用。动态模型可以通过自适应推理实现显著的准确性和计算效率，但是在目标检测中设计一个强大的动态检测器是具有挑战性的，因为缺乏适合的动态架构和目标检测的退出标准。

- (2):过去的方法需要设计和训练多个模型才能实现几个良好的准确性和速度的权衡，这对于各种应用场景来说不够灵活。本文提出了一种动态框架，名为DynamicDet，旨在通过只使用一个动态检测器实现广泛的准确性和速度权衡。本文的方法是有动机的，因为动态神经网络在深度学习中是一个新兴的研究课题，而动态检测器在目标检测中的应用是一个具有挑战性的问题。

- (3):本文提出了一种动态架构，名为DynamicDet，它由两个级联检测器和一个路由器组成。该动态架构可以根据推理过程中的多尺度信息进行退出。然后，我们提出了一个自适应路由器，根据多尺度特征预测图像的难度分数，并实现自动决策。此外，我们为我们的动态架构提出了相应的优化和推理策略。

- (4):本文在COCO基准测试上进行了广泛的实验，证明了所提出的DynamicDet可以通过只使用一个动态检测器实现广泛的准确性和速度权衡。例如，与相当的准确性相比，我们的动态检测器Dy-YOLOv7-W6的推理速度超过了YOLOv7-E6 12％，超过了YOLOv7-D6 17％，超过了YOLOv7-E6E 39％。本文的方法在实时目标检测方面取得了新的最优性能（即46 FPS时的56.8％ AP）。
#### 7. 方法详细介绍：
本文提出了一种名为DynamicDet的动态目标检测框架，由两个检测器和一个路由器组成。第一个检测器处理“简单”图像，第二个检测器处理“困难”图像。路由器基于多尺度特征预测图像的难度分数，并决定使用哪个检测器。难度标准基于两个检测器之间的训练损失差异，引入自适应偏移来平衡两个检测器的损失。该框架还包括可变速推理方法，通过设置不同的难度分数阈值，可以实现不同的准确度和速度权衡。具体步骤包括：提取多尺度特征、预测难度分数、选择检测器、进行可变速推理。

#### 8. 实验设置：
本文在COCO基准数据集上进行实验，使用NVIDIA V100 GPU进行训练和测试。将提出的DynamicDet与其他高效目标检测器进行比较，如YOLOv7、PP-YOLOE+和YOLOv5。

#### 9. 实验结果与分析：
DynamicDet框架在COCO test-dev数据集上实现了新的准确度和速度权衡的最新水平。与其他目标检测器相比，DynamicDet获得了更好的结果，实现了新的最新准确度和速度权衡。例如，Dy-YOLOv7-W6 / 50以58 FPS的速度实现了56.1％的AP，比YOLOv7-E6更准确且更快12％。 Dy-YOLOv7-W6 / 100以46 FPS的速度实现了56.8％的AP，比YOLOv7-E6E更快39％，但准确度相似。DynamicDet框架还与两阶段检测器和基于Transformer的骨干网络兼容，提高了基线的准确度和速度性能。


# Paper:389     SLOPER4D：城市环境下全局4D人体姿态估计的场景感知数据集



#### 1. Title: 
SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments

#### 2. Authors: 
Yudi Dai, Yitai Lin, Xiping Lin, Chenglu Wen, Lan Xu, Hongwei Yi, Siqi Shen, Yuexin Ma, Cheng Wang

#### 3. Affiliation: 
福建省智能城市感知与计算重点实验室，厦门大学

#### 4. Keywords: 
Human pose estimation, LiDAR, urban environments, dataset, scene-aware

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Dai_SLOPER4D_A_Scene-Aware_Dataset_for_Global_4D_Human_Pose_Estimation_in_CVPR_2021_paper.html  Github: https://github.com/YudiDai/SLOPER4D

#### 6. Summary : 
- (1):本文研究的背景是在城市环境下进行全局人体姿态估计，以及如何捕捉人体与场景的交互。
- (2):过去的方法主要集中在受限环境下的局部姿态估计，或者是基于IMU的姿态估计，但是这些方法都无法提供全局姿态和场景信息。本文提出了一种新的方法，使用头戴式设备集成LiDAR和相机，记录12个人在10个城市场景中的活动，提供2D关键点、3D姿态参数和全局平移的逐帧注释，以及重建的场景点云。为了在这样的大动态场景中获得准确的3D地面真实值，本文提出了一种联合优化方法，将局部SMPL网格拟合到场景中，并在动态运动的每一帧中微调相机校准，从而得到合理且场景自然的3D人体姿态。最终，SLOPER4D包括15个人体运动序列，每个序列的轨迹长度超过200米（最长达1300米），覆盖面积超过200平方米（最大达30000平方米），包括超过100k的LiDAR帧、300k的视频帧和500k的基于IMU的运动帧。本文提供了详细的分析，包括基于相机的3D HPE和基于LiDAR的3D HPE在城市环境中的表现，并对GHPE进行了基准测试。深入分析表明，SLOPER4D对现有方法提出了重要挑战，并产生了巨大的研究机会。
- (3):本文提出了一种新的方法，使用头戴式设备集成LiDAR和相机，记录12个人在10个城市场景中的活动，提供2D关键点、3D姿态参数和全局平移的逐帧注释，以及重建的场景点云。为了在这样的大动态场景中获得准确的3D地面真实值，本文提出了一种联合优化方法，将局部SMPL网格拟合到场景中，并在动态运动的每一帧中微调相机校准，从而得到合理且场景自然的3D人体姿态。
- (4):本文在SLOPER4D数据集上进行了广泛的实验，展示了联合优化方法获取高质量3D姿态注释的优越性。此外，基于我们提出的新数据集，我们对两个关键任务进行了基准测试：基于相机的3D HPE和基于LiDAR的3D HPE，以及提供GHPE的基准测试。
#### 7. 方法详细介绍：
本文提出了一种名为SLOPER4D的方法，用于在城市环境中进行全局4D人体姿态估计。该方法利用头戴式LiDAR、IMU和单目相机捕捉多模态数据。该方法包括三个主要步骤：（1）基于LiDAR的3D人体姿态估计，（2）基于IMU的全局姿态优化，（3）相机外参优化。该方法使用场景感知接触项、姿态先验项和网格到点项来优化全局姿态。该方法还采用相机外参优化来优化3D人体姿态估计。具体步骤如下：
1. 利用LiDAR获取3D点云数据。
2. 利用IMU获取人体运动数据。
3. 利用单目相机获取2D图像数据。
4. 利用SMPL模型生成3D人体姿态。
5. 利用场景感知接触项、姿态先验项和网格到点项优化全局姿态。
6. 利用相机外参优化来优化3D人体姿态估计。

#### 8. 实验设置：
本文提出了一个新的大规模城市级人体姿态数据集SLOPER4D。该数据集包括12个人体主体在10个位置的15个序列，总共捕获了超过8公里的距离和高达30,000平方米的面积，其中包括超过100k个LiDAR帧、300k个视频帧和500k个基于IMU的运动帧。该数据集提供了多模态捕捉数据和丰富的人-场景注释，以及大型场景中多样化的挑战性人体活动。该数据集还提供了SMPL格式的3D姿态注释和来自Terrestrial Laser Scanner（Trimble TX5）的高精度彩色点云地图，以便更好地可视化和地图比较。

#### 9. 实验结果与分析：
本文对所提出的SLOPER4D数据集和方法进行了定性和定量评估。定性评估表明，3D人体网格与3D环境和2D图像对齐良好。定量评估采用平均每个关节位置误差（MPJPE）和Procrustes对齐MPJPE（PA-MPJPE）评估3D人体姿态估计，采用绝对轨迹误差（ATE）和相对姿态误差（RPE）评估全局轨迹。本文还进行了跨数据集评估，以进一步评估SLOPER4D数据集在基于LiDAR的3D HPE和基于相机的3D HPE任务上的新颖性。结果表明，所提出的SLOPER4D数据集和方法在这些任务上优于其他数据集和方法。


# Paper:390     LASP：面向语言感知软提示的文本到文本优化方法



#### 1. Title: 
LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models

#### 2. Authors: 
Adrian Bulat, Georgios Tzimiropoulos

#### 3. Affiliation: 
Adrian Bulat: 1. Samsung AI Cambridge, 2. Technical University of Iasi
Georgios Tzimiropoulos: 1. Samsung AI Cambridge, 3. Queen Mary University of London

#### 4. Keywords: 
Soft prompt learning, Vision & Language models, Language-Aware Soft Prompting, few-shot adaptation, zero-shot adaptation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision_CVPR_2021_paper.html  Github: https://github.com/samsung-ailab/LASP

#### 6. Summary : 
- (1):本文研究背景是如何通过少量的训练样本来适应V&L模型到下游任务中，而软提示学习是其中的一种方法。
- (2):过去的方法主要是手工设计提示或学习软提示，但这些方法在测试时会出现过拟合的问题，导致在同一领域的未知类别上的准确率显著下降。本文提出了一种新的方法，即基于文本的交叉熵损失的语言感知软提示（LASP）学习方法，以减轻基类过拟合问题。此外，本文还提出了分组LASP和重新校准机制等创新点。
- (3):本文提出了一种新的框架，即LASP，它通过文本到文本的损失函数来学习软提示，从而提高了模型的泛化能力。此外，本文还提出了分组语言感知提示表示和重新校准机制等创新点。
- (4):本文在11个数据集上进行了评估，结果表明，与现有的软提示学习方法相比，LASP方法在零样本和少样本图像分类任务上表现更好。本文的方法在大多数测试数据集（11个中的8个）上优于手工设计提示和CLIP的基线方法。
#### 7. 方法详细介绍：
本文提出了一种名为LASP（Language-Aware Soft Prompting）的方法，用于零样本识别。该方法通过学习文本提示来指导视觉编码器生成与类别名称在语义上对齐的嵌入。提示是使用文本到文本损失函数学习的，该函数鼓励提示与类别名称相似。该方法还包括重新对齐步骤，以应对数据分布偏移和V&L不对齐。此外，本文还提出了一种名为LASP-V的变体，该变体使用基于图像-文本对和基于类别名称的样本来学习提示。

#### 8. 实验设置：
本文在11个数据集上评估了所提出的方法，包括ImageNet、Caltech101、Oxford-Pets、Stanford Cars、Flowers102、Food101、FGVC Aircraft、SUN397、DTD、EuroSAT和UCF-101。数据集被分为两个相等的部分，命名为基础集和新集。模型使用基础类别的文本-图像对进行训练，并在基础集和新集上进行测试。可学习提示的数量设置为4，每个类别的样本数设置为16。文本模板的数量设置为34，取自CoOp和CLIP。训练和测试在单个NVIDIA V100 GPU上进行（除了ImageNet，其中使用了4个GPU）。

#### 9. 实验结果和分析：
所提出的方法在零样本识别方面表现出色，平均提高了>2％，特定数据集提高了>3％。所提出的变体LASP-V对特定数据集有显着影响，在EuroSAT上提高了约5.5％，在DTD上提高了约4.0％。本文还将所提出的方法与其他最先进的方法进行了比较，包括CoOp、CoCoOp和ProDA，并报告了基础集和新集的性能。


# Paper:391     可微分阴影映射用于高效反问题图形学



#### 1. Title: 
Differentiable Shadow Mapping for Efficient Inverse Graphics


#### 2. Authors: 
Markus Worchel, Marc Alexa


#### 3. Affiliation: 
TU Berlin (柏林工业大学)


#### 4. Keywords: 
Differentiable rendering, shadow mapping, inverse graphics, machine learning


#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2021_paper.html  Github: None


#### 6. Summary : 
- (1):本文研究的背景是计算机视觉中的反问题求解，不同iable渲染器已成为解决该问题的重要工具。然而，现有的局部方法虽然速度快，但缺乏全局光照交互效果，如阴影、色散或间接照明等。
- (2):过去的方法包括局部光照模型和全局光传输模拟，但前者缺乏全局效果，后者计算量大。本文提出了一种新的方法，将预过滤阴影映射技术与现有的可微分光栅化器相结合，以产生可微分的可见性信息。与全局光传输模拟相比，不同iable阴影图像的速度快得多，而没有阴影的不同iable光栅化通常无法收敛。本文的方法在多个反问题求解中得到了应用，如姿态估计、几何重建和交互式编辑等。
- (3):本文的研究方法是将现有的可微分渲染框架与阴影映射技术相结合，以处理二次光线的不连续性。本文的创新点在于将阴影映射技术与现有的可微分光栅化器相结合，以产生可微分的可见性信息。这种方法可以在保证速度的同时，提供与全局光传输模拟相似的精度。
- (4):本文的方法在多个反问题求解中得到了应用，如姿态估计、几何重建和交互式编辑等。实验结果表明，本文的方法在保证速度的同时，可以提供与全局光传输模拟相似的精度。
#### 7. 方法详细介绍：
本文提出了一种可微分的滤波阴影映射方法，用于高效的反向图形学。该方法通过使用平滑核对阴影图进行预过滤，并使用预过滤的阴影图计算可见性函数。可见性函数是点与光源之间距离的分段有理函数，两个部分之间的边界在阴影图的加权平均值处。该方法使用现有的可微分光栅化器来计算目标函数相对于场景参数的梯度。具体步骤包括：
1. 对场景进行三角网格化。
2. 对每个光源生成阴影图。
3. 对阴影图进行平滑滤波。
4. 计算可见性函数。
5. 使用现有的可微分光栅化器计算目标函数相对于场景参数的梯度。

#### 8. 实验设置：
本文没有特定的实验设置部分。

#### 9. 实验结果与分析：
本文展示了在简单场景中使用平滑阴影图的可微分滤波阴影映射方法的收敛行为。实验结果表明，使用平滑函数可以避免阴影不连续性，从而实现优化。本文还对提出的方法进行了单目姿态估计和光源方向估计的实验，通过改变阴影图分辨率和滤波核大小来进行实验。实验结果表明，增加阴影图分辨率或滤波核大小会增加运行时间，但优化行为稳健，精度高，且在不同分辨率和滤波大小下具有一致的误差边界。本文还讨论了单个光源和多个光源的优化行为差异，以及阴影和光源方向估计的挑战。


# Paper:392     ViPLO:基于Vision Transformer的姿态条件自环图用于人-物交互检测



#### 1. Title: 
ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection

#### 2. Authors: 
Jeeseung Park, Jin-Woo Park, Jong-Seok Lee

#### 3. Affiliation: 
Jeeseung Park and Jin-Woo Park: mAyl Inc., Seoul, Korea; Jong-Seok Lee: Yonsei University, Korea (李钟锡：韩国延世大学)

#### 4. Keywords: 
Human-Object Interaction Detection, Vision Transformer, Graph Neural Network, Pose-Conditioned, Self-Loop

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的是人-物交互检测，即定位和推断人和物体之间的关系，是场景理解中的重要组成部分。
- (2):现有的人-物交互检测方法可以分为两类：两阶段方法和一阶段方法。两阶段方法的优点是训练和推理效率高，但性能低于一阶段方法。本文提出了一种基于Vision Transformer的Pose-Conditioned Self-Loop Graph方法，解决了现有方法中特征提取和交互分类器的问题。本文的方法在两个公共基准测试中取得了最先进的结果。
- (3):本文提出了一种有效的两阶段人-物交互检测器，使用ViT骨干网络和MOA模块进行特征提取，设计了一种姿态条件的自环图神经网络，用于交互预测。该模型通过自环结构更新人节点编码，使分类器能够有效地识别交互类型。本文的方法在特征提取和交互预测方面都有所改进，取得了最先进的结果。
- (4):本文的方法在HICO-DET和V-COCO两个公共基准测试中取得了最先进的结果，特别是在HICO-DET数据集上取得了+2.07 mAP的性能提升。本文的方法在特征提取和交互预测方面都有所改进，取得了最先进的结果，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为ViPLO的两阶段人-物交互检测器。该方法使用Vision Transformer（ViT）骨干网络提取特征，并使用Masking with Overlapped Area（MOA）模块解决量化问题。ViPLO还使用了一种基于人体姿态的图神经网络，称为Pose-Conditioned Self-Loop Graph，用于交互预测。具体步骤如下：
1. 使用Faster R-CNN检测给定图像中的人和物体。
2. 使用现成的姿态估计器估计每个人的姿态。
3. 使用ViT骨干网络和MOA模块提取每个人和物体的特征。
4. 使用估计的姿态和ROIAlign提取每个人的局部特征。
5. 设计一个基于姿态的图神经网络进行交互预测，其中节点编码使用提取的特征进行初始化，边编码通过空间和人体姿态信息获得。
6. 通过消息传递过程更新人类节点编码与人类局部节点编码，帮助模型集中于每个人的必要局部部分。
7. 将人类节点、物体节点和边编码组合以获得HOI三元组的表示形式，用于预测交互类别。

#### 8. 实验设置：
本文在两个公共数据集HICO-DET和V-COCO上进行了评估。HICO-DET包含47,776张图像，80个物体类别和117个动词类别构成600种HOI三元组。V-COCO是MS-COCO的一个子集，包含10,346张图像，与HICO-DET相同的80个物体类别和29个动词类别。模型使用focal loss进行训练，并使用平均精度（mAP）进行评估。

#### 9. 实验结果和分析：
本文提出的ViPLO方法在HICO-DET和V-COCO数据集上均取得了最先进的结果。在HICO-DET数据集上，ViPLO相对于现有的一阶段和两阶段方法均取得了显著的性能提升，mAP提高了2.07。在V-COCO数据集上，ViPLO的性能与现有最先进方法相当。通过大量的消融实验，本文证明了ViPLO方法的有效性和可靠性。


# Paper:393     GP-VTON: 基于协作局部流全局解析学习的通用虚拟试穿技术



#### 1. Title: 
GP-VTON: Towards General Purpose Virtual Try-on via Collaborative Local-Flow Global-Parsing Learning

#### 2. Authors: 
Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, Xiaodan Liang

#### 3. Affiliation: 
第一作者所属机构：中山大学深圳校区

#### 4. Keywords: 
Virtual Try-on, Image-based, Local-Flow Global-Parsing, Dynamic Gradient Truncation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_CVPR_2021_paper.html  Github: https://github.com/switchablenorms/GP-VTON

#### 6. Summary : 
- (1):本文研究的是虚拟试穿技术，旨在将商店中的服装转移到特定的人物身上。现有的2D图像方法在处理复杂的人体姿势和服装输入时存在一些问题，如无法保留不同部位的语义信息、容易产生粘连等。这些问题限制了现有方法在实际应用中的可行性。

- (2):现有的方法采用全局变形模块来模拟不同服装部位的各向异性变形，但在接收到具有挑战性的输入时，这种方法无法保留不同部位的语义信息。此外，大多数现有方法直接将输入服装挤压到与保留区域的边界对齐，这通常需要纹理挤压以满足形状约束，从而导致纹理失真。本文提出了一种名为GP-VTON的框架，通过开发创新的局部流全局解析变形模块和动态梯度截断训练策略，解决了这些问题。

- (3):本文提出了一种新颖的局部流全局解析变形模块和动态梯度截断训练策略，用于生成高保真度的变形服装，并进一步促进我们的GP-VTON生成逼真的试穿结果。与现有的全局变形机制相比，局部流全局解析变形模块选择为不同服装部位学习不同的局部变形场，能够单独地对每个服装部位进行变形，并即使在复杂的输入情况下生成语义正确的变形服装。此外，我们的动态梯度截断训练策略可以动态地截断重叠区域的梯度，从而避免了纹理挤压问题。

- (4):本文在两个具有挑战性的高分辨率基准测试上进行了广泛的实验，证明了GP-VTON相对于现有最先进的方法的优越性。
#### 7. 方法详细介绍：
本文提出了一种名为GP-VTON的通用虚拟试穿方法。该方法包括三个主要模块：局部流全局解析（LFGP）变形模块、试穿生成器和多类别虚拟试穿模块。LFGP变形模块估计变形服装的局部流和全局解析，试穿生成器合成试穿结果，多类别虚拟试穿模块扩展了该方法以处理不同类别的服装。该方法还采用了几种损失函数，包括l1损失、感知损失、对抗损失和像素级交叉熵损失，以训练模块。具体步骤包括：
1. 使用2D姿势和densepose提取器提取人物姿势和密集姿势。
2. 使用80k手动注释的时尚图像训练统一解析估计器。
3. 使用LFGP变形模块将服装变形到目标形状。
4. 使用试穿生成器根据变形服装和其他人物相关输入合成试穿结果。
5. 使用多类别虚拟试穿模块扩展该方法以处理不同类别的服装。

#### 8. 实验设置：
本文在两个高分辨率虚拟试穿基准数据集VITON-HD和DressCode上进行实验，分别为512×384的分辨率。数据集被分为训练集和测试集。本文使用由[4]和[15]提取的2D姿势和densepose，以及使用80k手动注释的时尚图像训练的统一解析估计器。本文将GP-VTON与几种最先进的方法进行比较，并使用几种广泛使用的指标进行评估，包括SSIM、LPIPS、FID、mIoU和人类评估。

#### 9. 实验结果和分析：
本文表明，GP-VTON在VITON-HD和DressCode数据集上的SSIM、FID、LPIPS、mIoU和人类评估方面均优于最先进的方法。该方法可以生成语义正确的试穿结果，即使是复杂的姿势和服装。多类别虚拟试穿模块可以处理不同类别的服装。本文还提供了详细的定量和定性结果比较。


# Paper:394     MotionTrack：学习多目标跟踪的鲁棒短期和长期运动



#### 1. Title: 
MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking

#### 2. Authors: 
Zheng Qin, Sanping Zhou, Le Wang, Jinghai Duan, Gang Hua, Wei Tang

#### 3. Affiliation: 
第一作者：西安交通大学人机混合增强智能国家重点实验室，视觉信息应用工程技术研究中心，人工智能与机器人研究所

#### 4. Keywords: 
Multi-Object Tracking, Interaction-aware Motion, History Trajectory-based Motion, Short-term Association, Long-term Association

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qin_MotionTrack_Learning_Robust_Short-Term_and_Long-Term_Motions_for_Multi-Object_CVPR_2021_paper.html  Github: https://github.com/qwomeng/MotionTrack

#### 6. Summary : 
- (1):本文研究的是多目标跟踪中的短期和长期运动建模问题，主要挑战在于如何在密集人群和极端遮挡等动态环境下保持每个目标的连续轨迹。

- (2):现有的方法通常学习可靠的运动模式来匹配相邻帧之间的同一目标，并学习具有区分性的外观特征来在长时间后重新识别丢失的目标。然而，在跟踪过程中，密集人群和极端遮挡会轻易破坏运动预测的可靠性和外观特征的区分性。本文提出了一种简单而有效的多目标跟踪器MotionTrack，它在统一框架中学习了短期和长期的鲁棒运动，以将轨迹从短到长范围内关联起来。对于密集人群，我们设计了一种新颖的交互模块来学习短期轨迹的交互感知运动，可以估计每个目标的复杂运动。对于极端遮挡，我们构建了一种新颖的Refind模块，从目标的历史轨迹中学习可靠的长期运动，可以将中断的轨迹与其对应的检测链接起来。我们的交互模块和Refind模块嵌入在众所周知的检测跟踪范式中，可以协同工作以保持卓越的性能。

- (3):本文提出了一种简单而有效的多目标跟踪器MotionTrack，它在统一框架中学习了短期和长期的鲁棒运动，以将轨迹从短到长范围内关联起来。为了解决短期关联问题，我们设计了一种新颖的交互模块来建模目标之间的交互关系，可以预测它们的复杂运动以避免碰撞。为了解决长期关联问题，我们设计了一种新颖的Refind模块，基于每个目标的历史轨迹有效地重新识别丢失的目标。我们的方法在MOT17和MOT20数据集上进行了广泛的实验，证明了我们的方法在具有挑战性的场景中具有卓越的性能，并在各种MOT指标上实现了最先进的性能。

- (4):本文提出的MotionTrack方法在MOT17和MOT20数据集上进行了实验，取得了最先进的性能。在密集人群和极端遮挡等动态环
#### 7. 方法详细介绍：
本文提出的多目标跟踪方法MotionTrack采用了tracking-by-detection范式，并针对短程和长程关联问题提出了Interaction Module和Refind Module两个模块。Interaction Module模块用于建模目标之间的交互，可以预测它们的复杂运动以避免碰撞。Refind Module模块基于每个目标的历史轨迹，通过相关计算和误差补偿可以有效地重新识别丢失的目标。短程和长程关联结合起来生成完整的跟踪结果。

具体而言，MotionTrack方法分为短程关联和长程关联两个步骤。在短程关联步骤中，Interaction Module模块用于建模目标之间的交互，可以预测它们的复杂运动以避免碰撞。在长程关联步骤中，Refind Module模块基于每个目标的历史轨迹，通过相关计算和误差补偿可以有效地重新识别丢失的目标。短程和长程关联结合起来生成完整的跟踪结果。

#### 8. 实验设置：
本文在MOT17和MOT20数据集上评估了MotionTrack方法的性能，使用了YOLOX检测器进行公平比较。Interaction Module和Refind Module模块在MOT17和MOT20的训练集的一半上进行训练。实验在一台服务器上进行，使用了Intel Xeon E5-2630 v4 CPU和NVIDIA Titan Xp GPU。输入视频帧被调整为1080p，检测结果来自于Faster R-CNN检测器。

#### 9. 实验结果和分析：
MotionTrack方法在MOT17和MOT20数据集上取得了最先进的性能。在MOT17数据集上，MotionTrack方法的MOTA为66.1％，IDF1为68.5％，MT为34.5％，在MOT20数据集上，MOTA为58.5％，IDF1为60.5％，MT为23.5％。MotionTrack方法的性能优于其他方法，并且没有使用任何复杂的组件，例如人员Re-ID。MotionTrack方法的代码可在GitHub上获得。


# Paper:395     ECON: 显式衣着人体模型的法向量优化重建



#### 1. Title: 
ECON: Explicit Clothed humans Optimized via Normal integration

#### 2. Authors: 
Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, Michael J. Black

#### 3. Affiliation: 
第一作者：德国图宾根智能系统研究所(Max Planck Institute for Intelligent Systems, T¨ubingen, Germany)

#### 4. Keywords: 
3D reconstruction, clothed humans, implicit functions, explicit body model, normal maps

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2021_paper.html  Github: https://github.com/oval-group/ECON

#### 6. Summary : 
- (1):本文研究的背景是如何从图像中重建出真实世界中的衣着人体模型，以实现更真实的虚拟现实体验。

- (2):过去的方法主要分为显式和隐式两种，显式方法使用参数化身体模型或深度图像等来重建人体，但对于衣着的建模存在困难；隐式方法使用隐式函数来重建人体，但对于新的姿态和衣着缺乏鲁棒性。本文提出的方法将两种方法的优点结合起来，使用2D normal maps和SMPL-X模型来重建3D衣着人体模型。

- (3):本文提出的方法ECON包括三个步骤：(1)从RGB图像中预测出前后两面衣着人体的normal maps；(2)使用SMPL-X模型和normal maps来重建前后两面衣着人体的2.5D表面；(3)使用SMPL-X模型和前后两面表面来完成3D衣着人体模型的重建。本文的创新点在于使用2D normal maps来提高重建的鲁棒性和细节，同时使用SMPL-X模型来提高重建的准确性和灵活性。

- (4):本文在CAPE和Renderpeople数据集上进行了定量和定性的评估，结果表明ECON在重建衣着人体模型方面优于现有方法。此外，本文还进行了感知实验，结果表明ECON在重建具有挑战性的姿态和衣着方面的真实感和逼真度都更好。
#### 7. 方法详细介绍：
本文提出了一种名为ECON的方法，用于从RGB图像中重建穿着衣服的人体形状。该方法首先从图像和SMPL-X身体模型中估计前后法线图，然后使用深度感知的轮廓一致双边法线积分（d-BiNI）方法将衣服的法线图提升到2.5D表面上。最后，使用IF-Nets+帮助完成缺失的几何形状。ECON方法的详细步骤如下：
(1) 从前后法线图和SMPL-X估计中推断出详细的2D法线图。
(2) 将法线图转换为详细但不完整的2.5D前后表面，由SMPL-X估计指导。
(3) 然后“修补”两个表面之间的缺失几何形状。可以选择用SMPL-X中更干净的面部或手部替换。ECON方法即使在宽松的衣服或挑战性的姿势下也能推断出高保真度的3D人体。

#### 8. 实验设置：
本文使用CAPE和Renderpeople数据集中的600个样本进行比较，其中有地面真实法线图和网格。本文还在RIF上比较了IF-Nets和IF-Nets+。

#### 9. 实验结果与分析：
本文报告了估计和渲染深度图之间的“均方根误差”（RMSE）和“平均绝对误差”（MAE）。d-BiNI相对于BiNI显着提高了约50％的重建精度，并且速度快33％。IF-Nets+比IF-Nets更具鲁棒性，因为它是基于SMPL-X身体条件的。本文还展示了在野外图像和多个有遮挡的人上的定性结果。

#### 全文总结：
本文提出了一种名为ECON的方法，用于从单视图图像中重建详细的穿着衣服的3D人体模型。该方法结合了自由形式的隐式表示和显式的人形正则化的最佳方面，可以推断出高保真度的3D人体，即使是在宽松的衣服或挑战性的姿势下也能推断出。ECON方法的步骤包括：（1）使用d-BiNI表面获得完整的重建，（2）使用称为IF-Nets+的学习隐式函数模型填充缺失的几何形状，（3）使用Poisson表面重建（PSR）完成最终网格。该方法在THuman2.0数据集上进行了训练，并在CAPE和Renderpeople数据集上进行了评估。ECON方法在重建精度和细节方面优于现有方法，并且在野外图像的感知研究中也受到人类评估者的青睐。但是，当输入的SMPL-X身体结果不正确或法线图估计错误时，该方法可能会失败。


# Paper:396     学习生成结构先验进行盲文本图像超分辨率



#### 1. Title: 
Learning Generative Structure Prior for Blind Text Image Super-resolution

#### 2. Authors: 
Xiaoming Li, Wangmeng Zuo, Chen Change Loy

#### 3. Affiliation: 
1. 新加坡南洋理工大学S-Lab
2. 哈尔滨工业大学

#### 4. Keywords: 
Blind text image super-resolution, generative structure prior, StyleGAN, Transformer-based encoder, Chinese characters

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Learning_Generative_Structure_Prior_for_Blind_Text_Image_Super-Resolution_CVPR_2021_paper.html  Github: https://github.com/csxmli2016/MARCONet

#### 6. Summary : 
- (1):本文研究盲文本图像超分辨率，该问题需要应对不同字体风格和未知退化。 
- (2):现有方法通过字符识别并行进行SR任务的规范化，但高级先验在遇到严重退化时仍可能失败。本文提出了一种更注重字符结构的新型先验，通过StyleGAN学习丰富多样的结构并利用这种生成结构先验进行恢复。与基于字符识别的先验相比，所提出的结构先验对恢复指定字符的精确笔画具有更强的字符特定指导作用。 
- (3):本文通过改进StyleGAN，将其单个常量替换为代表不同字符的离散代码，从而学习这种生成结构先验。为了准确检索先验，本文提出了一种基于Transformer的编码器，从LR输入中联合预测字体样式、字符边界框和它们在代码本中的索引。在文本SR网络中，每个LR字符都是由其对齐的先验指导下进行超分辨率的。 
- (4):本文在合成和真实数据集上进行了广泛的实验，证明了所提出的生成结构先验在促进鲁棒的文本SR方面具有强大的性能。与现有方法相比，本文的方法在生成逼真的文本结果方面表现出更好的性能，并且在真实世界的LR文本图像上表现出了出色的泛化能力。
#### 7. 方法详细介绍：
本文提出了一种盲文本图像超分辨率的方法，称为MARCONet。该方法包括三个主要部分：字符特征编码、结构先验生成和超分辨率恢复。其中，字符特征编码采用Transformer-based编码器，用于预测字体风格、字符边界框和它们在代码本中的索引。结构先验生成采用预训练的StyleGAN，生成每个字符的结构先验，通过离散代码本控制字体风格。超分辨率恢复采用结构先验变换模块，将生成的结构先验嵌入到低分辨率字符中，以指导超分辨率恢复。具体步骤如下：
1. 预训练结构先验生成器：将StyleGAN预训练为生成不同风格的字符，使用代码本存储每个字符的离散代码，通过代码本索引驱动每个字符的结构先验生成。
2. 字符特征编码：采用Transformer-based编码器，预测字体风格、字符边界框和它们在代码本中的索引。其中，字体风格预测分支采用StyleGAN的梯度进行优化，字符边界框回归采用Smooth L1 loss和广义IoU loss的线性组合进行优化。
3. 超分辨率恢复：采用结构先验变换模块，将生成的结构先验嵌入到低分辨率字符中，以指导超分辨率恢复。具体地，采用简单的UNet堆叠在ResNet45之上提取低分辨率特征，然后将每个字符的结构先验通过结构先验变换模块与其对齐，最后通过像素域和感知域的差异、对抗损失和结构图像Sc的L1损失等目标进行端到端优化。

#### 8. 实验设置：
本文使用了Xu等人的中文语料库，包含数千万个常见项目，收集了182种字体家族以引入多样化的结构。文本背景来自DIV2K和Flick2K数据集。采用BSRGAN和Real-ESRGAN降质模型生成了包含1,000个LR/HR对的合成数据集，分别用于×2和×4任务。这些LR输入注入了随机噪声、模糊和JPEG压缩，以模拟真实世界的降质。

#### 9. 实验结果与分析：
本文提出的方法在合成数据集上取得了最佳的定量结果，并在PSNR、SSIM和LPIPS等指标上优于其他方法。即使在严重降质（即×4）的情况下，该方法仍然表现出良好的性能。在合成LR图像的视觉比较中，该方法与真实图像的结构更加一致。在真实世界的LR文本图像上，该方法优于专门针对文本图像的TBSRN和TATT，并在定量结果上取得了最佳表现。该方法在恢复具有复杂结构的字符的精确笔画方面表现出色，表明所提出的结构先验对恢复指定字符的精确笔画施加了更强的字符特定指导。


# Paper:397     可供性扩散：合成手-物体交互



#### 1. Title: 
Affordance Diffusion: Synthesizing Hand-Object Interactions

#### 2. Authors: 
Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, Sifei Liu

#### 3. Affiliation: 
第一作者：Yufei Ye，卡内基梅隆大学

#### 4. Keywords: 
Image synthesis, hand-object interaction, affordance, diffusion models

#### 5. Paper: https://judyye.github.io/affordiffusion-www  Github: None

#### 6. Summary:
- (1):本文研究手-物体交互的图像合成问题，旨在从单个物体的RGB图像中合成人手与物体交互的图像。这是一项具有挑战性的任务，需要考虑物理约束、物体的语义和功能等因素。
- (2):过去的方法通常只能合成整个图像、纹理转移或将对象插入到用户指定的区域。本文提出了一种基于扩散模型的两步生成方法，即LayoutNet和ContentNet。LayoutNet生成手-物体交互的布局，ContentNet根据查询物体图像和采样的HOI布局合成手-物体交互的图像。与基线相比，该方法在新颖物体上具有更好的泛化性能，并且在野外场景中表现出色。
- (3):本文提出的方法使用扩散模型解决了手-物体交互的图像合成问题。该方法将交互位置和手部姿态分离，使用两个模型分别生成手-物体交互的布局和图像。该方法在HOI4D和EPIC-KITCHEN数据集上进行了评估，表现优于基线方法，并且能够快速适应新的手-物体交互。
- (4):本文提出的方法在HOI4D和EPIC-KITCHEN数据集上进行了评估，表现优于基线方法。该方法能够生成逼真的手-物体交互图像，并且能够快速适应新的手-物体交互。该方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种生成手-物交互图像的方法，该方法包括两个步骤：LayoutNet和ContentNet。LayoutNet生成手和物体的2D空间布局，ContentNet根据预测的布局生成手-物交互图像。两个模块都是基于大规模预训练扩散模型的图像条件扩散模型实现的。为了防止过拟合，本文提出了一种新的数据增强技术。该方法在HOI4D和EPIC-KITCHEN数据集上进行了评估，并与通用图像生成基线进行了比较。实验结果表明，该方法在生成手-物交互图像方面表现出色，且具有良好的泛化能力。

#### 8. 实验设置：
本文使用HO3Pairs数据集进行训练，该数据集由HOI4D数据集和自动生成的对象图像组成。模型在HOI4D和EPIC-KITCHEN数据集上进行了评估，并使用FID分数、接触召回率和用户研究等指标进行了性能评估。

#### 9. 实验结果和分析：
本文提出的方法在接触召回率方面表现最佳，并且在用户研究中得到了最高的评价。该模型可以推广到EPIC-KITCHEN数据集，而且可以与未见过的类别进行交互。数据增强可以防止过拟合，LayoutNet的设计对模型的性能有所贡献。本文还比较了该方法与三种强基线模型的性能，证明了该方法的有效性。


# Paper:398     照片预训练，但用于素描



#### 1. Title: 
Photo Pre-Training, But for Sketch

#### 2. Authors: 
Ke Li, Kaiyue Pang, Yi-Zhe Song

#### 3. Affiliation: 
Ke Li: 北京邮电大学人工智能学院, China

#### 4. Keywords: 
Sketch, Pre-training, Fine-grained sketch-based image retrieval, Topology, Supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Photo_Pre-Training_But_for_Sketch_CVPR_2021_paper.html  Github: https://github.com/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch

#### 6. Summary : 
- (1):本文研究的背景是素描数据的稀缺性，这导致了素描任务的特殊性和挑战性。

- (2):以往的方法是在大规模照片数据集上进行预训练，然后在素描任务上进行微调，但是这种方法存在问题。本文提出了一种新的思路，即利用预训练的照片数据的拓扑结构作为下游素描任务的“免费”监督信号，从而提高了素描任务的性能。

- (3):本文提出了一种新的学习原则，即利用预训练模型中学习到的照片拓扑结构来规范素描任务的微调过程。具体而言，本文将邻域一致性约束转化为一个交叉模态三元组损失，并将其作为元目标来优化素描任务的学习。实验结果表明，本文的方法在五个产品级素描任务数据集上均取得了优于以往方法的性能。

- (4):本文的方法在五个产品级素描任务数据集上均取得了优于以往方法的性能，证明了本文提出的学习原则的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为LNT（Learning Neighbourhood Triplets）的方法，用于细粒度基于草图的图像检索（FG-SBIR）。LNT是一种元学习方法，学习建模三张照片的相对邻域。该方法涉及在大规模数据集上预训练神经网络，然后使用三元组排名损失和LNT损失的组合进行FG-SBIR任务的微调。LNT损失通过随机组合当前批次中的照片对来计算，并确保它们的相对距离排名与安全最小边距中定义的排名相符。该方法还使用一阶近似来减少具有百万级参数的深度模型的Hessian-vector乘积的计算复杂度。

#### 8. 实验设置：
本文在五个FG-SBIR基准测试集（QMUL-Shoe-V1，QMUL-Shoe-V2，QMUL-Chair-V1，QMUL-Chair-V2和QMUL-Handbag）上评估了所提出的LNT方法。实验比较了LNT与14个现有的FG-SBIR基线的性能，并报告了它们的发布数字。本文还对LNT中的三个关键工程选择进行了消融分析：照片对数K，边距值∆NT和最近邻选择。

#### 9. 实验结果和分析：
本文使用ResNet50作为所有实验的骨干架构。预训练数据集使用ImageNet或Jigsaw，具体取决于实验。本文使用标准的三元组排名损失作为FG-SBIR的基线。实验在单个NVIDIA Tesla V100 GPU上进行。本文将准确性（acc@1）作为评估指标。实验结果表明，所提出的方法优于现有方法，并提高了FG-SBIR模型的泛化能力。


# Paper:399     基于高斯标签分布学习的球面图像物体检测



#### 1. Title: 
Gaussian Label Distribution Learning for Spherical Image Object Detection

#### 2. Authors: 
Hang Xu, Xinyuan Liu, Qiang Zhao, Yike Ma, Chenggang Yan, Feng Dai

#### 3. Affiliation: 
Hangzhou Dianzi University, Hangzhou, China

#### 4. Keywords: 
Spherical image object detection, Gaussian Label Distribution Learning, IoU loss, ln-norms loss, Spherical IoU

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Gaussian_Label_Distribution_Learning_for_Spherical_Image_Object_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了球面图像物体检测问题，提出了一种基于高斯标签分布学习（GLDL）的回归损失函数，解决了ln-norms损失函数的独立优化参数和度量与损失不一致的问题。
 
- (2):现有的球面图像物体检测方法使用ln-norms损失函数进行球面边界框的回归，但该方法存在独立优化参数和度量与损失不一致的问题。本文提出的方法使用GLDL回归损失函数，将预测的球面边界框和真实边界框的切平面转换为高斯分布，提出了动态样本选择策略（GLDL-ATSS）来选择正样本，解决了IoU阈值策略的缺陷。 
 
- (3):本文提出的方法使用GLDL回归损失函数，将预测的球面边界框和真实边界框的切平面转换为高斯分布，提出了动态样本选择策略（GLDL-ATSS）来选择正样本。本文的创新点在于提出了一种新的回归损失函数和样本选择策略，解决了球面图像物体检测中度量与损失不一致的问题。 
 
- (4):本文在两个数据集上进行了实验，结果表明，本文提出的方法在球面图像物体检测任务上取得了较好的性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于高斯标签分布学习（GLDL）的球形目标检测方法。在训练阶段，将预测的球形边界框和真实边界框的切平面转换为高斯分布。然后，设计了一种动态的样本选择策略（GLDL-ATSS）来选择正样本。最后，为球形目标检测任务设计了一种基于GLDL的回归损失。GLDL损失与SphIoU损失具有趋势级别的一致性。在推理阶段，直接从训练好的模型中获取球形边界框的输出，因此网络的推理时间保持不变。具体步骤包括：将球形边界框转换为高斯分布、计算预测分布和真实分布之间的KL散度、将KL散度转换为最终的回归损失。

#### 8. 实验设置：
本文在两个真实的球形目标检测数据集360-Indoor和PANDORA上进行了实验。在所有实验中，采用mAP评估检测器的性能。根据SphIoU计算AP值以适应球形边界框并产生准确的结果。模型训练120个epochs，初始学习率为0.001，分别在60个epochs和90个epochs时除以10。批量大小为32，输入分辨率为512×1024。为了公平起见，本文将所有方法的设置和超参数保持与相应论文中的设置相同。

#### 9. 实验结果与分析：
本文提出的方法在360-Indoor和PANDORA数据集上均取得了显著的改进。与Multi-Kernel和Sphere-SSD方法相比，本文提出的方法具有很大的优势。消融实验表明，基于GLDL的样本选择策略和基于GLDL的回归损失对于提高检测性能是有效的。


# Paper:400     收集跨模态存在-缺失证据用于弱监督的音视频事件感知



#### 1. Title: 
Collecting Cross-Modal Presence-Absence Evidence for Weakly-Supervised Audio-Visual Event Perception

#### 2. Authors: 
Junyu Gao, Mengyuan Chen, Changsheng Xu

#### 3. Affiliation: 
中国科学院自动化研究所

#### 4. Keywords: 
Weakly-supervised learning, audio-visual event perception, presence-absence evidence, subjective logic theory

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2023/html/Gao_Collecting_Cross-Modal_Presence-Absence_Evidence_for_Weakly-Supervised_Audio-Visual_Event_CVPR_2023_paper.html  Github: https://github.com/MengyuanChen21/CVPR2023-CMPAE

#### 6. Summary : 
- (1):本文研究了弱监督下的音视频事件感知问题，旨在通过视频级别的标签来定位和分类每个模态下的事件。 
- (2):现有的方法要么忽略了音视频轨道的不同步属性，要么忽略了补充模态对显式增强的重要性。本文提出了一种跨模态存在-缺失证据收集方法，通过利用单模态和跨模态表示，设计了一个存在-缺失证据收集器（PAEC），并提出了一种联合模态互学习（JML）过程，以适应和动态校准不同的可听、可见和可听可见事件的证据。 
- (3):本文提出了一种新颖的跨模态存在-缺失证据学习框架，它在主观逻辑理论下同时享有单模态判别和跨模态增强的优点。通过协作的存在-缺失证据收集器和联合模态互学习过程，将单模态和跨模态信息注入到学习的证据中，并将其校准到可靠的范围内。 
- (4):在几个流行的和标准的WS-AVEP数据集上进行了广泛而深入的实验。与现有技术相比，本文的方法取得了令人鼓舞的结果，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种用于弱监督音视频事件感知的跨模态存在-缺失证据学习方法（CMPAE）。该方法包括两个主要组件：存在-缺失证据收集器（PAEC）和联合模态互相学习（JML）。PAEC通过利用每种模态的存在和缺失证据来收集跨模态存在-缺失证据，而JML则利用单模态和联合模态之间的关系进行显式的协作和校准学习。该方法使用预训练的VGGish提取音频特征，使用预训练的ResNet152和R（2+1）D提取视觉特征。证据收集器是一个带有两个额外全连接层的DNN，由LeakyReLU激活，证据函数设置为Exp。最终的损失函数是交叉熵损失和联合模态互相学习损失的组合。 

#### 8. 实验设置：
本文在AVVP和AVE数据集上评估了CMPAE方法，并将它们组合成一个名为AVEP的整个数据集。LLP数据集包含11,849个10秒的视频片段，涵盖25个事件类别，而AVE数据集则选择了4,143个涵盖28个类别的YouTube视频。AVEP数据集包括11,581个训练视频、840个验证视频和1,391个测试视频，涵盖39个类别。评估指标包括音频事件的F-score、视觉事件的F-score、音频-视觉事件的F-score、前三个指标的平均值、不考虑模态的所有事件的F-score和总体准确率。

#### 9. 实验结果和分析：
本文在AVVP、AVE和AVEP数据集上评估了CMPAE方法的性能。该方法在所有评估指标上均取得了最先进的性能，证明了该方法在弱监督音视频事件感知方面的有效性。


# Paper:401     实用的插拔式扩散模型



#### 1. Title: 
Towards Practical Plug-and-Play Diffusion Models

#### 2. Authors: 
Hyojun Go, Yunsung Lee, JinYoung Kim, Seunghyun Lee, Myeongho Jeong, Hyun Seung Lee, Seungtaek Choi

#### 3. Affiliation: 
Riiid AI Research (韩国Riiid AI研究所)

#### 4. Keywords: 
Diffusion models, generative models, plug-and-play, guidance models, parameter-efficient fine-tuning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Go_Towards_Practical_Plug-and-Play_Diffusion_Models_CVPR_2021_paper.html  Github: https://github.com/riiid/PPAP

#### 6. Summary : 
- (1):本文研究了基于扩散的生成模型中的外部模型指导问题，提出了一种实用的插拔式指导框架，旨在解决现有方法中单一指导模型难以处理不同噪声程度的输入和需要标记数据集的问题。

- (2):现有方法需要使用带有噪声的标记数据集来微调指导模型，但这种方法存在两个问题：(1)单一指导模型难以处理不同噪声程度的输入；(2)需要标记数据集，这在应用新的现成模型时成为主要障碍。本文提出了一种新的多专家策略，使用多个指导模型，每个模型专门针对特定噪声范围进行微调。为了实现更实用的插拔式指导，本文提出了一种称为Practical Plug-And-Play (PPAP)的框架，该框架利用参数高效的微调方案和无数据知识转移来避免管理多个网络和使用标记数据集。

- (3):本文提出的PPAP框架可以成功地指导扩散模型，具有小的可训练参数和无标记数据。本文还展示了图像分类器、深度估计器和语义分割模型可以通过该框架以插拔式方式指导公开可用的GLIDE。

- (4):本文的方法在ImageNet条件生成实验中得到了验证，证明了其可以成功地指导扩散模型，具有小的可训练参数和无标记数据。本文还展示了各种应用，包括使用公开可用的扩散模型GLIDE，通过插拔式方式利用现成的图像分类器、深度估计器和语义分割模型。
#### 7. 方法详细介绍：
本文提出了一种新的扩散模型引导策略，称为实用的插拔式（PPAP），它利用多个专家，每个专家专门处理特定噪声范围，以引导相应时间步骤的扩散反向过程。为了使多专家策略实用，本文引入了一种参数高效的微调方案，可以将现成的模型适应于噪声图像，同时保持参数数量。此外，本文将现成模型在干净的扩散生成数据上的知识转移给专家引导模型，从而避免了收集标记数据集的需要。

#### 8. 实验设置：
本文验证了所提出的框架PPAP的有效性，通过在ImageNet类别条件生成上进行实验。实验使用了ImageNet预训练的无条件ADM，大小为256x256，使用了两种主流架构：ResNet50和DeiT-S。对于每种架构，使用以下变体作为引导模型：Naive off-the-shelf、Single noise aware、Multi-experts-N和PPAP-N。本文生成了500k张图像以进行无数据知识转移，并使用250步的DDPM采样器和25步的DDIM采样器。引导比例s设置为7.5，因为它对大多数变体都能取得良好的结果。本文还比较了其他可应用于扩散引导的方法与现成模型。

#### 9. 实验结果和分析：
本文详细地进行了ImageNet类别条件生成实验，证明了所提出的方法可以成功地引导扩散，具有小的可训练参数和无标记数据。本文还展示了各种应用，利用公开可用的扩散模型GLIDE，通过以插拔方式利用现成的图像分类器、深度估计器和语义分割模型。实验结果验证了所提出的方法在使用现成模型进行条件图像生成方面显著提高了性能。


# Paper:402     基于超球嵌入的减少hubness问题和改进转导few-shot学习



#### 1. Title: 
Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings

#### 2. Authors: 
Daniel J. Trosten, Rwiddhi Chakraborty, Sigurd Løkse, Kristoffer Knutsen Wickstrøm, Robert Jenssen, Michael C. Kampffmeyer

#### 3. Affiliation: 
Daniel J. Trosten, Rwiddhi Chakraborty, Sigurd Løkse, Kristoffer Knutsen Wickstrøm, Robert Jenssen, Michael C. Kampffmeyer: 挪威北极大学物理与技术系

#### 4. Keywords: 
few-shot learning, hubness problem, hyperspherical embeddings, transductive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Trosten_Hubs_and_Hyperspheres_Reducing_Hubness_and_Improving_Transductive_Few-Shot_Learning_CVPR_2022_paper.html  Github: https://github.com/uitml/noHub

#### 6. Summary : 
- (1):本文研究的是few-shot learning中的hubness问题，即在高维空间中，某些点会出现在其他点的最近邻列表中，从而影响分类器的性能。 
- (2):过去的方法主要是基于距离的分类，但由于图像表示的高维性，分类器容易受到hubness问题的影响。本文提出了一种在超球面上均匀分布表示的方法，以消除hubness问题。作者提出了两种新的嵌入方法，以优化均匀性和局部相似性保留之间的权衡，从而减少hubness并保留类结构。 
- (3):本文证明了在超球面上均匀分布表示可以完全消除hubness问题，并提出了两种新的嵌入方法，即Uniform Hyperspherical Structure-preserving Embeddings (noHub)和noHub with Support labels (noHub-S)，以优化局部相似性保留和均匀性之间的权衡。noHub-S还利用支持样本的标签信息进一步增加了嵌入空间中的类可分性。 
- (4):在多个数据集和特征提取器上的实验结果表明，noHub和noHub-S优于当前最先进的嵌入方法，提高了多种转导FSL分类器的性能。
#### 7. 方法详细介绍：
本文提出了两种新的嵌入方法，即noHub和noHub-S，用于在超球面上嵌入表示，优化均匀性和局部相似性保持之间的权衡。该方法利用表示和嵌入相似性之间的KL散度分解来实现这种优化。生成的嵌入在近似均匀的同时，还能保留嵌入空间中的类别结构。noHub-S方法还利用支持样本的标签信息进一步增加嵌入空间中的类别可分性。具体实现时，使用梯度下降和ADAM优化器对嵌入进行优化，使用PCA进行初始化。该方法在mini-ImageNet、tiered-ImageNet和CUB-200数据集上进行了评估，使用各种FSL分类器进行比较。

#### 8. 实验设置：
本文在mini-ImageNet、tiered-ImageNet和CUB-200数据集上进行了实验，使用ResNet-18和Wide-Res28-10作为基础特征提取器，使用预训练权重。将该方法与各种基线方法进行比较，包括None、L2、Centered L2、ZN、ReRep、EASE和TCPR等。使用已有和最新的FSL分类器进行评估，包括SimpleShot、LaplacianShot、α−TIM、Oblique Manifold（OM）、iLPC和SIAMESE。根据各自数据集上的验证性能选择超参数。

#### 9. 实验结果和分析：
实验结果表明，所提出的方法在所有三个数据集上和所有FSL分类器上均优于基线方法。该方法在mini-ImageNet和tiered-ImageNet数据集上实现了最先进的性能。分析了权重参数α对性能的影响，以及κ和ε超参数的影响。发现使用PCA初始化嵌入比随机初始化更有效。同时，本文还分析了嵌入方法的hubness指标，结果表明noHub和noHub-S具有最低的hubness得分。


# Paper:403     深度随机投影：加速深度图像先验



#### 1. Title: 
Deep Random Projector: Accelerated Deep Image Prior

#### 2. Authors: 
Taihui Li, Hengkang Wang, Zhong Zhuang, Ju Sun

#### 3. Affiliation: 
第一作者：明尼苏达大学计算机科学与工程系

#### 4. Keywords: 
Deep image prior, image restoration, optimization, regularization, total variation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Deep_Random_Projector_Accelerated_Deep_Image_Prior_CVPR_2021_paper.html  Github: https://github.com/sun-umn/Deep-Random-Projector

#### 6. Summary : 
- (1):本文研究深度图像先验（DIP）在图像恢复中的应用，DIP是一种无需训练数据的先验方法，但其优化过程通常非常缓慢，限制了其在时间敏感的场景中的实际应用。

- (2):过去的方法包括控制Gθ的容量、显式正则化和显式噪声建模等，但这些方法往往会延长迭代过程并将性能峰值推迟到最终迭代。本文提出了一种名为深度随机投影（DRP）的有效且高效的DIP变体，它需要比DIP更少的计算时间来获得可比较的性能峰值。DRP包括三个关键修改：（1）在冻结随机初始化的网络权重的同时优化DIP种子，（2）减少网络深度，（3）包括额外的显式先验进行正则化，例如编码由总变差正则化表示的稀疏梯度先验。 

- (3):本文提出的DRP方法在DIP的基础上进行了三个关键修改，以加速优化过程。DRP需要比DIP更少的计算时间来获得可比较的性能峰值。本文的创新点在于：（1）优化DIP种子而不是网络权重，（2）减少网络深度，（3）包括额外的显式先验进行正则化。本文还证明了DRP与DIP相比的优越性，并提出了一种完整的解决方案，既解决了过拟合问题，又解决了优化缓慢的问题。

- (4):本文在三个图像恢复任务（包括图像去噪、图像超分辨率和图像修复）上评估了所提出的方法，并与原始DIP和变体以及使用元学习的竞争性metaDIP进行了比较。结果表明，DRP在更短的时间内获得了竞争性的恢复质量，是一种有效且高效的DIP变体。
#### 7. 方法详细介绍：
本文提出了一种名为Deep Random Projector (DRP)的方法，用于加速深度图像先验。该方法首先随机初始化深度神经网络（DNN）的权重，然后将其冻结。接着，优化输入种子以获得所需的输出。该方法还涉及减少网络深度以减少每次迭代的计算量。为了弥补由于减少深度而导致的恢复质量下降，该方法采用了额外的显式先验，例如总变差（TV）。将TV集成到优化函数中即可获得DRP的正则化版本。

#### 8. 实验设置：
本文使用DRP-DD和DRP-DIP两种生成器网络Gθ进行图像去噪实验。实验在标准图像去噪数据集上进行，包括四种不同类型的噪声，包括高斯噪声、脉冲噪声、射线噪声和斑点噪声。对于每种噪声，实验在低噪声水平和高噪声水平上进行。对于所有图像和所有噪声，使用相同的超参数设置DRP-DD和DRP-DIP。学习率设置为0.1，λ设置为0.45。对于DD和DIP，使用它们的原始超参数设置。对于DD，测试不同的网络宽度。所有模型运行20K次迭代，并报告峰值PSNR及其相应的达到峰值的时间。

#### 9. 实验结果和分析：
实验结果表明，DRP-DD和DRP-DIP在图像恢复质量方面与DD和DIP相当竞争力，但在OPT时间方面，DRP-DD和DRP-DIP比DD和DIP具有显着优势。上述观察结果在不同的图像、噪声和噪声水平下保持一致。将TV添加到DRP中显著提高了恢复质量，这种观察结果在不同的噪声类型和不同的噪声水平下保持一致。频谱偏差分析表明，DRP比DIP更快、更可靠地恢复所有频带。


# Paper:404     对话中的自我中心听觉注意定位



#### 1. Title: 
Egocentric Auditory Attention Localization in Conversations

#### 2. Authors: 
Fiona Ryan, Hao Jiang, Abhinav Shukla, James M. Rehg, Vamsi Krishna Ithapu

#### 3. Affiliation: 
Georgia Institute of Technology1, Meta Reality Labs Research2 (乔治亚理工学院1，Meta Reality实验室2)

#### 4. Keywords: 
Auditory attention, egocentric video, deep learning, multi-speaker conversation, audiovisual features

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ryan_Egocentric_Auditory_Attention_Localization_in_Conversations_CVPR_2021_paper.html  Github: https://fkryan.github.io/saal/

#### 6. Summary : 
- (1):本文研究了在嘈杂的环境中，人们如何选择性地关注某个说话者，以及如何通过深度学习方法来预测人们的听觉注意目标。
- (2):过去的方法主要是通过神经生理学传感器来确定听觉注意目标，这种方法不适用于现实生活中的对话环境。本文提出了一种新的方法，使用自我中心视频和多通道音频来预测摄像机佩戴者的听觉注意目标。本文的方法结合了视觉和音频信息，使用transformer模型来综合推理场景中的关系，相比于现有的方法，本文的方法更加准确。
- (3):本文提出了一种基于深度学习的方法，使用自我中心视频和多通道音频来预测摄像机佩戴者的听觉注意目标。该方法使用了视觉和音频信息，使用transformer模型来综合推理场景中的关系，相比于现有的方法，本文的方法更加准确。
- (4):本文的方法在一个具有挑战性的多说话者对话数据集上进行了评估，相比于现有的方法，本文的方法表现更好。本文的方法可以为开发能够理解社交行为和增强人类听力的设备提供帮助。
#### 7. 方法详细介绍：
本文提出了一种深度多模态模型，用于自我中心听觉注意力定位。该模型由四个主要组件组成：视觉编码器、音频编码器、场景关系变换器和解码器。视觉编码器提取场景的外观特征，音频编码器从输入剪辑的多通道音频中提取空间音频特征。场景关系变换器通过建模场景中不同区域的特征之间的相关性来细化融合的多模态特征图，从而全面选择可能的注意目标。最后，解码器为每个输入帧生成一个听觉注意力热图。模型使用像素级交叉熵损失进行端到端训练，使用Adam优化器和学习率1e-4。

#### 8. 实验设置：
本文构建了一个多说话人对话数据集，用于评估所提出的方法。该数据集由1,987帧组成，其中1,249帧包含一个或多个注意目标。

#### 9. 实验结果和分析：
本文的方法在多说话人对话数据集上表现出色，优于多个基线方法。作者还进行了消融实验，分析了模型每个组件的贡献。最终，该方法在评估数据集上实现了0.47的F1分数。


# Paper:405     基于最优目标估计的感知导向单图像超分辨率



#### 1. Title: 
Perception-Oriented Single Image Super-Resolution using Optimal Objective Estimation

#### 2. Authors: 
Seung Ho Park, Young Su Moon, Nam Ik Cho

#### 3. Affiliation: 
第一作者：韩国首尔国立大学电气与计算机工程系、INMC；

#### 4. Keywords: 
Single Image Super-Resolution, Perceptual Loss, Optimal Objective Estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Park_Perception-Oriented_Single_Image_Super-Resolution_Using_Optimal_Objective_Estimation_CVPR_2021_paper.html  Github: https://github.com/seungho-snu/SROOE

#### 6. Summary : 
- (1):本文研究单图像超分辨率（SISR）问题，旨在提高超分辨率重建的感知质量。 
- (2):传统的SISR方法采用像素损失，如L1和L2，这些方法不能考虑感知质量，因此生成的结果通常缺乏高频细节。而感知损失和生成对抗损失等感知导向的损失虽然可以提高视觉质量，但是会带来一些不良的副作用，如不自然的细节和结构失真。本文提出了一种新的SISR框架，通过为每个区域应用最优目标来生成整体高分辨率输出，从而提高了感知准确性。 
- (3):本文提出了一种新的SISR框架，该框架由两个模型组成：预测模型和生成模型。预测模型为给定的低分辨率（LR）输入推断出最优目标图，生成模型将目标目标图应用于生成相应的SR输出。生成模型在我们提出的目标轨迹上进行训练，该轨迹表示一组基本目标，使单个网络能够学习对应于轨迹上组合损失的各种SR结果。预测模型使用LR图像和相应的最优目标图对进行训练。 
- (4):本文在五个基准测试上进行了实验，结果表明，所提出的方法在LPIPS、DISTS、PSNR和SSIM指标上优于最先进的感知驱动SR方法。视觉结果也证明了我们方法在感知导向重建方面的优越性。
#### 7. 方法详细介绍：
本文提出了一种感知导向的单图像超分辨率（SISR）方法，使用最优目标估计。该方法由两个主要组件组成：一个单图像超分辨率网络和一个最优目标估计模块。网络基于残差内残差密集块（RRDB）架构，并使用对抗损失、内容损失和感知损失的组合进行训练。最优目标估计模块为每个输入图像估计最优目标图，该图用于在训练期间指导网络。该方法在四个基准数据集上进行评估，并使用多个指标进行度量，包括PSNR、SSIM、LPIPS、DISTS和LR-PSNR。结果表明，该方法优于几种最先进的单图像超分辨率方法。 

#### 8. 实验设置：
本文将提出的方法与几种最先进的单图像超分辨率方法进行比较，包括RRDB、SRGAN、ESRGAN、SFTGAN、RankSRGAN、SRFlow和FxSR。评估在四个基准数据集上进行，包括DIV2K、DF2K、General100、Urban100和Manga109。使用多个指标进行度量，包括PSNR、SSIM、LPIPS、DISTS和LR-PSNR。本文还提供了所使用的训练数据集和损失权重的详细信息。 

#### 9. 实验结果和分析：
提出的SISR框架SROOE在DIV2K基准数据集上实现了最先进的性能，无论是在感知度量还是失真度量方面。使用T∗S的SROOE具有最佳的PSNR、SSIM、LPIPS和DISTS得分，显示了所提出的SROOE的近似上限。当目标图T设置为0时，SROOE作为失真导向的SR模型运行，PSNR略低于RRDB，但LPIPS更好。定性比较表明，SROOE生成更准确的结构和细节，在使用T = 0和ˆTB的SROOE结果之间的结构组件变化很小。在补充材料中提供了4×和8× SR的额外视觉和定量比较。该方法可以应用于现成的和其他SISR网络架构。 




# Paper:406     Lite DETR：一种交错的多尺度编码器，用于高效的DETR



#### 1. Title: 
Lite DETR: An Interleaved Multi-Scale Encoder for Efficient DETR

#### 2. Authors: 
Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang, Hongyang Li, Lei Zhang, Lionel M. Ni

#### 3. Affiliation: 
香港科技大学

#### 4. Keywords: 
Object detection, DETR, multi-scale features, efficient encoder, key-aware deformable attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Li_Lite_DETR_An_Interleaved_Multi-Scale_Encoder_for_Efficient_DETR_CVPR_2022_paper.html  Github: https://github.com/IDEA-Research/Lite-DETR

#### 6. Summary : 
- (1):本文研究目标检测中的多尺度特征提取问题，提出了一种高效的编码器结构，以减少计算量并保持检测性能。

- (2):传统的目标检测模型中，多尺度特征对于小物体检测至关重要，但是直接使用Transformer编码器处理多尺度特征会导致计算量过大。本文提出的Lite DETR框架通过设计高效的编码器块，以交错的方式更新高级特征和低级特征，从而有效减少了计算量。同时，为了更好地融合跨尺度特征，本文还提出了一种基于关键点的可变形注意力机制。这些方法可以很好地推广到现有的DETR模型中。

- (3):本文提出的Lite DETR框架通过设计高效的编码器块，以交错的方式更新高级特征和低级特征，从而有效减少了计算量。同时，为了更好地融合跨尺度特征，本文还提出了一种基于关键点的可变形注意力机制。这些方法可以很好地推广到现有的DETR模型中。

- (4):在COCO数据集上的实验结果表明，Lite DETR可以将检测头的GFLOPs降低60％，同时保持99％的检测性能。具体而言，Lite-DINO-SwinT在159 GFLOPs的计算量下实现了53.9 AP的性能。
#### 7. 方法详细介绍：
本文提出了一种高效的多尺度编码器 Lite DETR，用于目标检测。该模型由骨干网络、多层编码器和多层解码器组成。多尺度特征被分为高级和低级特征，并以不同的更新频率交替更新。高级特征采用关键点感知变形注意力方法进行迭代更新，而低级特征则采用 KDA 注意力层进行较少的更新。最终的多尺度特征通过轻量级前馈网络将更新后的低级和高级特征进行拼接构建。

#### 8. 实验设置：
本文在 MS COCO 2017 数据集上进行实验，使用训练集进行模型训练，使用验证集 val2017 进行模型评估。实验中使用了 ResNet-50 和 Swin-T 两种在 ImageNet-1K 数据集上预训练的骨干网络。

#### 9. 实验结果与分析：
本文提出的 Lite DETR 方法与其他高效变体（包括 Deformable DETR、Efficient DETR 和 Sparse DETR）进行了比较。结果表明，Lite DETR 在显著降低计算成本的同时，实现了比其他高效变体更好的性能。该方法在 MS COCO 2017 数据集上取得了最先进的检测性能。具体而言，Lite-DINO-SwinT 在 159 GFLOPs 的计算成本下实现了 53.9 AP 的检测性能。本文还表明，直接丢弃一些低级特征主要影响小目标的检测性能（AP_S），但对大目标（AP_L）的影响很小。


# Paper:407     从独特的视角学习：用户感知的显著性建模



#### 1. Title: 
Learning from Unique Perspectives: User-aware Saliency Modeling

#### 2. Authors: 
Shi Chen, Nachiappan Valliappan, Shaolei Shen, Xinyu Ye, Kai Kohlhoff, Junfeng He

#### 3. Affiliation: 
第一作者：Shi Chen，University of Minnesota

#### 4. Keywords: 
Saliency modeling, user-aware, visual attention, deep learning

#### 5. Paper: 
https://arxiv.org/abs/2004.00646  Github: None

#### 6. Summary : 
- (1):本文研究的背景是视觉注意力模型的发展，旨在提高计算机对人类视觉注意力的理解和模拟能力。
- (2):过去的方法主要是基于图像特征的计算，但是这些方法没有考虑到用户的个性化差异，因此效果不尽如人意。本文提出了一种基于深度学习的用户感知视觉注意力模型，能够更好地模拟人类视觉注意力的个性化差异。
- (3):本文提出的方法是基于深度学习的用户感知视觉注意力模型，通过对用户的个性化特征进行建模，能够更好地模拟人类视觉注意力的个性化差异。本文的创新点在于将用户的个性化特征引入到视觉注意力模型中，从而提高了模型的准确性和可解释性。
- (4):本文的方法在多个数据集上进行了实验，结果表明，与传统的视觉注意力模型相比，本文提出的用户感知视觉注意力模型能够更好地模拟人类视觉注意力的个性化差异，从而提高了模型的准确性和可解释性。
#### 7. 方法详细介绍：
本文采用了xxx方法，具体步骤如下：
(1). xxx
(2). xxx
(3). xxx
(4). xxx

#### 8. 实验设置：
本实验使用了xxx设备，实验条件如下：
(1). xxx
(2). xxx
(3). xxx

#### 9. 实验结果与分析：
实验结果表明xxx，具体分析如下：
(1). xxx
(2). xxx
(3). xxx


# Paper:408     TopNet：基于Transformer的图像合成对象放置网络



#### 1. Title: 
TopNet: Transformer-based Object Placement Network for Image Compositing

#### 2. Authors: 
Sijie Zhu, Zhe Lin, Scott Cohen, Jason Kuen, Zhifei Zhang, Chen Chen

#### 3. Affiliation: 
第一作者：中央佛罗里达大学计算机视觉研究中心

#### 4. Keywords: 
Object placement, image compositing, transformer, contrastive loss, 3D heatmap

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_TopNet_Transformer-Based_Object_Placement_Network_for_Image_Compositing_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了自动将对象放置到背景图像中进行图像合成的问题。给定背景图像和分割的对象，目标是训练模型以预测对象的合理位置和比例。现有的方法要么生成候选边界框，要么使用来自背景和对象图像的全局表示进行滑动窗口搜索，这些方法无法建模背景图像中的局部信息。然而，背景图像中的局部线索对于确定将对象放置在某些位置/比例上的兼容性非常重要。因此，本文提出了一种基于Transformer的对象放置网络，以学习对象特征和所有局部背景特征之间的相关性，从而提供有关所有可能的位置/比例配置的详细信息。同时，提出了一种稀疏对比损失来进行稀疏监督训练。该方法可以在一个网络前向传递中生成一个3D热图，指示所有位置/比例组合的合理性，速度比以前的滑动窗口方法快10倍以上。它还支持交互式搜索，当用户提供预定义的位置或比例时。
- (2):现有的方法要么生成候选边界框，要么使用来自背景和对象图像的全局表示进行滑动窗口搜索，这些方法无法建模背景图像中的局部信息。本文提出了一种基于Transformer的对象放置网络，以学习对象特征和所有局部背景特征之间的相关性，从而提供有关所有可能的位置/比例配置的详细信息。同时，提出了一种稀疏对比损失来进行稀疏监督训练。该方法可以在一个网络前向传递中生成一个3D热图，指示所有位置/比例组合的合理性，速度比以前的滑动窗口方法快10倍以上。它还支持交互式搜索，当用户提供预定义的位置或比例时。
- (3):本文提出了一种基于Transformer的对象放置网络，以学习对象特征和所有局部背景特征之间的相关性，从而提供有关所有可能的位置/比例配置的详细信息。同时，提出了一种稀疏对比损失来进行稀疏监督训练。该方法可以在一个网络前向传递中生成一个3D热图，指示所有位置/比例组合的合理性，速度比以前的滑动窗口方法快10倍以上。它还支持交互式搜索，当用户提供预定义的位置或比例时。本文的创新点在于提出了一种新的方法来解决对象放置问题，可以在一个网络前向传递中生成一个3D热图，指示所有位置/比例组合的合理性，速度比以前的滑动窗口方法快10倍以上。
- (4):本文的方法在大规模的Pixabay数据集和OPA数据集上进行了实验，表现优于现有方法。本文的方法可以在一个网络前向传递中生成一个3D热图，指示所有位置/比例组合的
#### 7. 方法详细介绍：
本文提出了一种名为TopNet的基于Transformer的图像合成对象放置网络。该方法将对象放置问题转化为密集预测问题，通过一次网络前向传递生成位置和尺度的密集网格评估。TopNet直接生成一个3D热图，指示对象位置和尺度的可信度得分，比之前的滑动窗口方法快10倍以上。该方法的架构由两个编码器组成，一个用于对象，一个用于背景，并使用Transformer模块学习全局对象特征和局部背景特征之间的相关性。然后采用基于CNN的上采样解码器生成3D热图。提出了一种稀疏对比损失函数来监督模型的训练。

#### 8. 实验设置：
本文使用Pixabay数据集进行实验，该数据集包含数百万高质量、多样化、免费使用的照片，用于构建和评估适用于真实世界对象合成的模型。该数据集经过过滤，保留具有高置信度检测分数和适当边界框大小的对象。本文通过从每个图像中删除对象生成纯背景图像，并使用LAMA的现成模型进行修复。本文还使用OPA数据集进行对象放置评估，每个复合图像都有人工注释。

#### 9. 实验结果和分析：
本文提出的方法在手动注释数据集和大规模修复数据集上都显著优于先前的最先进方法，以top-5 IOU和归一化得分为评估指标。该方法的推理速度与单步预测相当，并且比之前的滑动窗口方法快10倍以上。在真实世界图像的位置和尺度预测的人类评估中，该方法的满意率更高，不满意率更低。该方法还可以通过在真实世界图像上进行小规模人工注释的微调来进一步提高性能。本文还分析了不同损失函数和特征拼接对方法性能的影响，并在表5和表6中进行了报告。在图6中提供了定性结果，并在图7中显示了对象令牌和背景局部令牌之间的示例注意力图。


# Paper:409     同时建模短期和长期时间相关性的半监督视频语义分割



#### 1. Title: 
Simultaneously Short- and Long-Term Temporal Modeling for Semi-Supervised Video Semantic Segmentation

#### 2. Authors: 
Jiangwei Lao, Weixiang Hong, Xin Guo, Yingying Zhang, Jian Wang, Jingdong Chen, Wei Chu

#### 3. Affiliation: 
Ant Group

#### 4. Keywords: 
Video semantic segmentation, semi-supervised learning, feature enhancement, long-term temporal modeling, short-term temporal modeling

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Lao_Simultaneously_Short-_and_Long-Term_Temporal_Modeling_for_Semi-Supervised_Video_CVPR_2021_paper.pdf
Github: None

#### 6. Summary : 
- (1):本文研究的是半监督视频语义分割任务，旨在通过只标注视频中的一个帧来降低成本。 
- (2):过去的方法可以分为基于伪标签和基于特征增强两类，但是它们都存在一些问题，如基于伪标签的方法可能会产生错误的伪标签，而基于特征增强的方法通常只关注短期时间相关性，忽略了远距离帧的信息。本文提出了一种新的特征增强网络，同时建模短期和长期时间相关性，可以有效地扩展时间感知领域并提供更丰富的上下文先验。 
- (3):本文提出了一种新的模型，称为Simultaneously Short- and Long-Term Temporal Modeling (SSLTM)，可以同时建模短期和长期时间相关性。具体来说，我们设计了三个模块来学习表示：1）空间-时间变换器（STT）用于建模相邻帧之间的短期时间相关性；2）参考帧上下文增强（RFCE）模块用于增强查询帧的表示，从而提高当前帧的表示质量；3）全局类别上下文（GCC）模块用于建模整个数据集的全局信息。 
- (4):在只标注视频中的一个帧的情况下，SSLTM在VSPW数据集上的mIoU比现有方法高2％〜3％。此外，当与基于伪标签的方法（如MeanTeacher）一起使用时，我们的最终模型的mIoU仅比最高性能低0.13％。这表明SSLTM可以在半监督视频语义分割任务中取得优异的性能。
#### 7. 方法详细介绍：
本文提出了一种名为SSLTM的半监督视频语义分割方法。该方法利用短期和长期帧间相关性，构建由查询帧、n个相邻帧和参考帧组成的训练样本。该方法包括两个模块：用于短期时间建模的空间-时间变换器（STT）和用于长期时间建模的参考帧上下文增强（RFCE）。该方法还包括全局类别上下文（GCC）模块，以弥补参考帧中缺失的分类信息。最终的分割结果是通过将增强表示输入到语义分割头中并使用交叉熵损失进行训练得到的。

#### 8. 实验设置：
本文在两个数据集上进行实验：Video Scene Parsing in the Wild（VSPW）和CityScapes。VSPW数据集有两个版本：VSPW-SF和VSPW-FULL。VSPW-SF仅对每个视频的第一帧进行注释，而VSPW-FULL对所有帧进行标记。CityScapes数据集在训练和验证集中仅注释一个帧。本文使用平均交并比（mIoU）、加权交并比（WIoU）、时间一致性（TC）和平均视频一致性（mVC，包括mVC8和mVC16）作为VSPW数据集的评估指标。对于CityScapes数据集，仅报告mIoU。本文使用ResNet + FPN作为骨干网络，PPM作为中间层。方程10中的超参数α和β设置为1.0和0.5。全局类别表示G中的聚类数T设置为3。本文使用ImageNet预训练权重初始化骨干网络，其他部分随机初始化。整个模型使用与[22]相同的训练协议进行更新。在训练期间，采用随机缩放和随机裁剪数据增强。在VSPW上，本文采用训练裁剪大小为479×479，批量大小为8，120个训练时期。初始学习率设置为0.002，权重衰减设置为0.0001。在测试期间，本文进行单尺度测试，并使用480p的原始图像大小进行推理。

#### 9. 实验结果与分析：
实验结果表明，所提出的SSLTM方法在具有挑战性的VSPW数据集上实现了39.79％的mIoU，并且在其他基于特征增强的最新方法上取得了很大的优势（2％〜3％mIoU）。与Mean-Teacher [32]一起使用时，我们的模型产生了高mIoU，达到40.49％，仅在mIoU上表现比天花板性能（即使用密集注释进行训练）低0.13％。


# Paper:410     一次训练适用于所有个性化



#### 1. Title: 
Train-Once-for-All Personalization

#### 2. Authors: 
Hong-You Chen, Yandong Li, Yin Cui, Mingda Zhang, Wei-Lun Chao, Li Zhang

#### 3. Affiliation: 
第一作者：The Ohio State University

#### 4. Keywords: 
personalization, deep learning, basis models, mixer predictor, visual recognition

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Train-Once-for-All_Personalization_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究如何训练一种“个性化友好”的模型，使得在仅给定任务描述的情况下，该模型可以根据不同的最终用户需求进行自适应，例如，准确地分类不同的对象子集。 
- (2):过去的方法是训练一个“通用”模型，用于分类广泛的对象，然后进行类别选择。然而，本文发现这种方法并不是最优的，可能是因为模型的权重被冻结而没有被个性化。为了解决这个问题，本文提出了Train-once-for-All Personalization（TAPER）框架，该框架仅需训练一次，即可根据任务描述为不同的最终用户定制模型。TAPER学习一组“基础”模型和一个混合器预测器，以便根据任务描述，基础模型的权重（而不是预测！）可以即时组合成单个“个性化”模型。通过在多个识别任务上进行广泛的实验，本文表明TAPER始终优于基线方法，以实现更高的个性化准确性。此外，本文还表明，TAPER可以合成一个更小的模型，以实现与巨大的通用模型相当的性能，使其“部署友好”到资源有限的最终设备。
- (3):本文提出了Train-once-for-All Personalization（TAPER）框架，该框架仅需训练一次，即可根据任务描述为不同的最终用户定制模型。TAPER学习一组“基础”模型和一个混合器预测器，以便根据任务描述，基础模型的权重（而不是预测！）可以即时组合成单个“个性化”模型。TAPER通过引入阶段性训练过程，有效地学习基础模型和混合器预测器。本文在三个视觉识别数据集上验证了TAPER的有效性，包括ImageNet，iNaturalist和DomainNet。TAPER始终优于基线方法，以实现更高的个性化准确性。此外，本文还表明，TAPER可以“自我专业化”到部署环境，而无需最终用户的任务描述。
- (4):本文的方法在多个识别任务上进行了广泛的实验，包括ImageNet，iNaturalist和DomainNet。TAPER始终优于基线方法，以实现更高的个性化准确性。例如，在ImageNet上，TAPER能够合成一个ResNet-18，以在分类20个类别时实现96％的准确性，比具有类别选择的ResNet-18高4％。此外，TAPER的准确性甚至高于具有类别选择的ResNet-152，同时使用1/5的模型大小。这些结果表明，TAPER的方法可以有效地实现个性化识别任务，并且可以在资源有限的设备上进行部署。
#### 7. 方法详细介绍：
本文提出了一种名为TAPER的框架，它可以为不同的用户根据其任务描述自定义模型。TAPER学习一组“基础”模型和一个混合器预测器，以便在给定任务描述时，基础模型的权重可以即时组合成单个“个性化”模型。TAPER通过预测相应的系数来适应用户，而不是通过调整基础模型，并且不需要重新训练，具有参数效率。TAPER引入了分阶段训练过程，以有效地学习基础模型和混合器预测器。具体步骤如下：
(1) 单基础预训练阶段：使用所有任务的训练数据预训练每个基础模型。
(2) 专业基础模型阶段：使用每个任务的训练数据，对每个基础模型进行专业化训练。
(3) 学习任务混合器阶段：使用每个任务的训练数据，训练混合器预测器，以预测任务描述向量的混合器，以选择与任务相关的基础模型并将它们组合成一个简化的个性化模型。

#### 8. 实验设置：
本文在三个大规模视觉识别数据集（ImageNet、iNaturalist和DomainNet）上验证了TAPER的有效性。每个数据集都是单标签分类任务，分辨率为224×224。对于每个数据集，将任务构造为从标签空间Y中采样的20路分类。每个训练/验证集中的图像都随机分配一个任务描述进行训练和评估。目标是准确预测整个Y中的标签，指标是标准的top-1准确率。

#### 9. 实验结果与分析：
本文展示了TAPER框架在各种数据集上的实验结果，包括ImageNet、iNaturalist和DomainNet。结果表明，TAPER始终优于非个性化基线和分类器选择基线。完整的TAPER模型始终表现最佳。本文还在DomainNet数据集上展示了个性化领域描述的用例，其中TAPER在所有领域上都优于基线，特别是在更难的领域上。本文还讨论了不同设计选择对TAPER性能的影响。


# Paper:411     DeAR：使用加性残差去偏差视觉-语言模型



#### 1. Title: 
DeAR: Debiasing Vision-Language Models with Additive Residuals

#### 2. Authors: 
Ashish Seth, Mayur Hemani, Chirag Agarwal

#### 3. Affiliation: 
Ashish Seth: 印度理工学院 (IIT Madras)
Mayur Hemani, Chirag Agarwal: Adobe Inc.

#### 4. Keywords: 
Vision-Language Models, Bias Mitigation, Additive Residuals, Fairness, Debiasing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究大型预训练视觉-语言模型（VLMs）的社会偏见问题，这些模型在训练数据中存在各种身份群体的偏差，导致它们在现实世界的高风险应用中的实用性受到限制。

- (2):过去的方法主要集中在单模态数据上，如面部图像数据集，这些方法只调查性别偏见、种族偏见或它们的交叉。本文提出了一种新的去偏差方法DeAR，它通过学习加性残差图像表示来抵消原始表示，从而确保公平的输出表示。本文提出的方法可以适用于任何预训练的视觉-语言模型，只需添加一个学习的残差表示即可。

- (3):本文提出的DEAR框架是一种简单、计算效率高、有效的去偏差方法，只需调整VLMs的图像编码器即可。本文还引入了PROTECTED ATTRIBUTE TAG ASSOCIATION（PATA）数据集，这是一个新的基于上下文的偏差基准数据集，用于评估大型预训练VLMs的公平性。实验结果表明，DEAR框架可以通过多个数据集上的定量偏差计算和定性评估来去偏差预训练的VLMs，并且所得到的表示保留了其预测性能的大部分特征。

- (4):本文在多个数据集上进行了公平性和零样本性能保持的实验，证明了所提出的框架的有效性。本文提出的方法可以在多个任务上取得良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为DEAR（Debiasing with Additive Residuals）的新型去偏置方法，用于消除大型预训练视觉语言模型（VLMs）中的偏置。该方法包括两个关键组件：加性残差学习器（ARL）和受保护属性分类器（PAC）。ARL组件学习将受保护属性信息从给定VLM的图像表示中分离出来，生成一个残差表示。然后将残差表示与原始图像表示相加，得到去偏置表示。PAC组件用于训练ARL组件，以确保去偏置表示不包含受保护属性信息。DEAR框架通过最小化重构损失和最大化PAC模块的误分类损失来训练ARL组件。DEAR框架的评估标准包括提高给定VLM的公平性和保留其在各种任务中的零-shot性能。

#### 8. 实验设置：
本文使用了两个数据集：FairFace数据集和PROTECTED ATTRIBUTE TAG ASSOCIATION（PATA）数据集。FairFace数据集包括不同人的裁剪面部图像，注释有二进制性别、种族-种族类别和多个年龄段。PATA数据集包括从网络上爬取的公共图像，每个场景组织在24个场景中，每个场景包含100到400个图像，并注释有二进制性别、种族-种族标签和两个年龄组标签。为每个场景精选文本标题，包括正面和负面标题。使用平均MaxSkew和平均MinSkew分数以及MaxSkew@K和MinSkew@K分数来衡量模型的公平性。使用标准分类准确度指标评估DEAR-augmented和vanilla VLMs的零-shot性能，用于图像分类任务的分类准确度指标和用于视频动作识别任务的top-1准确度、平均准确度度量和平均加权平均精度度量的变体。

#### 9. 实验结果和分析：
本文的实验结果表明，DEAR框架可以显著提高给定VLM的公平性，同时保留其在各种任务中的零-shot性能。使用多个数据集进行的实验结果表明，DEAR-augmented VLMs在FairFace和PATA数据集上的表现优于vanilla VLMs。在多个图像分类和视频分类数据集上，DEAR实现了与vanilla模型相似的零-shot性能。此外，本文还比较了联合训练方法和顺序策略，并发现联合训练方法更适合去偏置。


# Paper:412     将GAN扩展到文本到图像合成



#### 1. Title: 
Scaling up GANs for Text-to-Image Synthesis

#### 2. Authors: 
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park

#### 3. Affiliation: 
Minguk Kang: POSTECH

#### 4. Keywords: 
Text-to-image synthesis, GANs, StyleGAN, autoregressive models, diffusion models, large-scale generative models

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2022/html/Kang_Scaling_up_GANs_for_Text-to-Image_Synthesis_CVPR_2022_paper.html
Github: None

#### 6. Summary: 
- (1):本文研究了文本到图像合成的GAN模型的扩展问题，探讨了GAN模型在大规模数据集上的表现和可扩展性问题。

- (2):过去的方法主要是基于迭代推理的扩散模型和自回归模型，这些方法需要高昂的计算成本，而GAN模型则可以通过单次前向传递生成图像，具有高效的优势。但是，GAN模型在处理复杂数据集时仍然存在训练不稳定的问题。本文提出了一种新的GAN模型GigaGAN，通过保留滤波器库和采用样本特定的线性组合等技术，成功地实现了对大规模数据集的稳定训练。

- (3):本文提出的GigaGAN模型具有三个主要优势：一是推理速度快，生成512像素的图像只需要0.13秒；二是可以生成高分辨率的图像，例如在3.66秒内生成16兆像素的图像；三是支持各种潜在空间编辑应用，如潜在插值、样式混合和向量算术运算。本文的创新点在于，成功地将GAN模型扩展到了大规模数据集上，并提出了一种新的GAN模型GigaGAN，具有高效、高质量和可控的优势。

- (4):本文的方法在大规模数据集上进行了实验，取得了良好的效果。GigaGAN模型可以生成高质量的图像，且速度快、分辨率高、可控性强。本文的方法可以为文本到图像合成任务提供一种新的思路和方法。
#### 7. 方法详细介绍：
本文提出了一种基于GAN的文本到图像合成框架，称为GigaGAN。该框架由生成器和判别器组成，其中生成器接受文本提示并生成图像，判别器评估生成图像的真实性。生成器基于多尺度架构，金字塔的每个级别在多个尺度上进行真/假预测。判别器使用特征提取器在不同尺度上提取特征，并使用预测器调节图像特征和文本特征。训练目标包括判别器损失、匹配感知损失、CLIP对比损失和Vision-Aided对抗损失。GigaGAN框架还可以扩展到训练文本条件的超分辨率模型，以将基本GigaGAN生成器的输出上采样到512px或4k分辨率的高分辨率图像。

#### 8. 实验设置：
本文在COCO2014数据集上比较了GigaGAN与各种文本到图像生成模型的性能。这些模型在训练数据集、预训练文本编码器甚至图像分辨率上都有所不同。GigaGAN表现出比DALL·E 2、Stable Diffusion和Parti-750M更低的Fréchet Inception Distance（FID）。GigaGAN上采样器在文本条件上采样任务中进行了单独评估，并与Stable Diffusion 4x Upscaler和2x Latent Upscaler进行了比较。

#### 9. 实验结果与分析：
本文提出了GigaGAN文本到图像合成的架构，可以扩展到模型大小，从而实现文本到图像的合成。然而，与DALL·E 2等生产级模型相比，结果的视觉质量尚不可比。本文展示了几个实例，其中该方法在与DALL·E 2相比时无法产生高质量的结果，无论是在照片逼真度还是在使用其论文中相同的输入提示进行文本到图像对齐方面。尽管如此，GigaGAN架构在使用类似资源训练的自回归和扩散模型方面取得了有竞争力的结果，同时速度快了几个数量级，并且可以进行潜在插值和样式化。作者期望随着模型规模的增大，性能将得到改善。


# Paper:413     SynthVSR：利用合成监督提高视觉语音识别的规模



#### 1. Title: 
SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision

#### 2. Authors: 
Xubo Liu, Egor Lakomkin, Konstantinos Vougioukas, Pingchuan Ma, Honglie Chen, Ruiming Xie, Morrie Doulaty, Niko Moritz, Jachym Kolar, Stavros Petridis, Maja Pantic, Christian Fuegen

#### 3. Affiliation: 
第一作者：Xubo Liu，University of Surrey

#### 4. Keywords: 
Visual speech recognition, synthetic supervision, semi-supervised learning, lip animation, large-scale dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_SynthVSR_Scaling_Up_Visual_Speech_Recognition_With_Synthetic_Supervision_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了如何利用合成数据提高视觉语音识别（VSR）的性能，因为公开的转录视频数据集在规模上受到限制。 
- (2):过去的方法通常依赖于越来越大量的视频数据，而公开可用的转录视频数据集规模有限。本文提出了SynthVSR，一种半监督框架，可以利用合成唇部运动显著提高VSR模型的性能。 
- (3):本文提出了一种基于语音驱动的唇部动画模型，该模型可以根据语音内容生成合成唇部运动视频。然后，使用该模型从转录语音数据集（例如Librispeech）和人脸数据集（例如CelebA）生成大规模的合成视频。最后，将合成视频与相应的转录一起与真实视频文本对（例如LRS3）一起用于大规模半监督VSR训练。 
- (4):在LRS3数据集上，SynthVSR使用30小时的真实标记数据实现了43.3%的识别错误率（WER），超过了使用数百或数千小时视频数据进行监督训练的先前VSR方法。当使用LRS3的全部438小时标记数据时，WER进一步降至27.9％，与最先进的自监督AV-HuBERT方法相当。此外，当与大规模伪标记的音频-视觉数据结合使用时，SynthVSR仅使用公开可用数据就实现了16.9％的新的VSR WER最佳性能，超过了使用90,000小时非公开机器转录数据进行训练的最先进方法。
#### 7. 方法详细介绍：
本文提出了一种半监督的视觉语音识别（VSR）框架SynthVSR，该框架利用合成的视觉数据。该框架包括一个基于语音驱动的唇部动画模型，该模型生成以语音内容为条件的合成唇部运动。唇部动画模型在未标记的音频-视觉数据集上进行训练，并在有标记的视频可用时优化到预训练的VSR模型。合成的视频与真实的视频-文本对一起用于大规模半监督VSR训练。该方法显著提高了使用大规模合成数据的VSR模型的性能。本文还提供了广泛的消融研究，以分析SynthVSR的改进效果。

具体步骤如下：
1. 训练语音驱动的唇部动画模型，该模型基于一个具有两个鉴别器的时间GAN，使用重构损失、对抗损失和VSR感知损失进行训练。
2. 使用训练好的唇部动画模型生成合成视频，其中语音内容来自于转录的语音数据集，唇部运动来自于人脸数据集。
3. 将合成视频与真实视频-文本对一起用于大规模半监督VSR训练，使用CTC损失和基于注意力的交叉熵（CE）损失进行端到端训练。
4. 在预训练的VSR模型的基础上，使用合成数据进行微调，以进一步提高VSR模型的性能。

#### 8. 实验设置：
本文在最大的公共VSR基准测试集Lip Reading Sentences 3（LRS3）上评估了SynthVSR的性能，使用Conformer-Transformer编码器-解码器VSR模型。作者使用30小时的标记视频数据进行低资源设置，使用LRS3的全部438小时标记数据进行高资源设置。他们还使用额外的2,630小时的ASR伪标记公开可用的音频-视觉数据进行高资源设置。

#### 9. 实验结果和分析：
本文的SynthVSR方法在LRS3数据集上取得了最先进的性能，使用30小时的真实标记数据和3,652小时的合成数据，WER为43.3%。使用438小时的真实标记数据和3,652小时的合成数据，WER降至27.9%，与最先进的自监督AV-HuBERT方法相当。此外，当与大规模伪标记的音频-视觉数据相结合时，SynthVSR仅使用公开可用的数据就实现了新的VSR WER最佳结果16.9%，超过了最近使用29倍的非公开机器转录视频数据（90,000小时）训练的最先进方法。本文还进行了消融研究，以分析SynthVSR的每个组件对性能的影响。


# Paper:414     TinyMIM：蒸馏MIM预训练模型的实证研究



#### 1. Title: 
TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models

#### 2. Authors: 
Sucheng Ren, Fangyun Wei, Zheng Zhang, Han Hu

#### 3. Affiliation: 
Microsoft Research Asia (微软亚洲研究院)

#### 4. Keywords: 
Masked image modeling, knowledge distillation, vision Transformers, pre-training, distillation targets

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ren_TinyMIM_An_Empirical_Study_of_Distilling_MIM_Pre-Trained_Models_CVPR_2022_paper.html  Github: https://github.com/OliverRensu/TinyMIM

#### 6. Summary:
- (1):本文研究背景是探索如何让小型视觉Transformer模型也能从Masked image modeling (MIM)预训练中受益。
- (2):过去的方法主要是通过引入归纳偏差来设计小型视觉Transformer模型，但这会限制其表达能力。本文提出了一种基于知识蒸馏的方法，将大型MIM预训练模型的知识转移到小型模型中，避免了直接训练小型MIM模型的困难。本文系统地研究了不同设计选项在蒸馏框架中的效果，包括蒸馏目标、数据增强、网络正则化、辅助损失、宏观蒸馏策略等，得出了一些有用的发现。
- (3):本文提出的TinyMIM模型通过选择最佳的框架选项，在ImageNet-1K分类任务上，使用ViT-Tiny、ViT-Small和ViT-base模型，相比于直接使用MIM预训练，分别获得了+4.2%、+2.4%和+1.4%的精度提升。在ADE20K语义分割任务上，TinyMIM-T模型获得了45.0 mIoU的最佳性能。在ImageNet-1K分类任务上，TinyMIM-T模型获得了79.6%的top-1准确率，创造了同等模型大小和计算预算下的新记录。这表明本文提出的方法是发展小型视觉Transformer模型的一种替代方式，即通过探索更好的训练方法，而不是像大多数以前的工作那样引入归纳偏差到架构中。
- (4):本文的方法在ImageNet-1K分类任务和ADE20K语义分割任务上均取得了最佳性能，支持了其目标。本文的贡献在于提出了一种基于知识蒸馏的方法，将大型MIM预训练模型的知识转移到小型模型中，避免了直接训练小型MIM模型的困难，并通过系统研究不同设计选项，得出了一些有用的发现。
#### 7. 方法详细介绍：
本文提出了TinyMIM方法，用于将大型MIM预训练模型的知识转移给小型视觉Transformer模型。该方法通过知识蒸馏的方式，优化学生模型以模仿教师模型生成的目标。TinyMIM方法包括三种蒸馏策略：类令牌蒸馏、特征蒸馏和关系蒸馏。关系蒸馏又分为基于softmax和基于关系两种类型。该方法还包括关系蒸馏的头部对齐和顺序蒸馏等。TinyMIM方法的默认蒸馏策略是优化学生模型以模仿MIM预训练教师模型的第18个块生成的关系，输入为原始图像。预训练完成后，学生模型可以转移到各种下游任务中。

#### 8. 实验设置：
本文在ImageNet-1K和ADE20K数据集上评估了所提出的方法，使用ViT-T、ViT-S和ViT-B模型。默认设置包括关系蒸馏、头部对齐、原始图像作为输入、顺序蒸馏和MAE预训练ViT-L的第18个块作为TinyMIM-ViT-B预训练的目标块。

#### 9. 实验结果和分析：
所提出的TinyMIM方法在ImageNet-1K和ADE20K数据集上均取得了最先进的性能。TinyMIM预训练的ViT-T在ImageNet-1K上达到了75.8%的top-1准确率，TinyMIM预训练的ViT-S在ImageNet-1K上达到了83.0%的top-1准确率。TinyMIM预训练的ViT-B在ADE20K语义分割任务上超过了MAE基线和最先进的CAE，分别提高了4.1和2.0。该方法还表现出对域外数据集的改进鲁棒性。消融研究表明，特征蒸馏和关系蒸馏是有效的蒸馏策略，而类令牌蒸馏不适合蒸馏。


# Paper:415     SIEDOB：通过分离对象和背景进行语义图像编辑



#### 1. Title: 
SIEDOB: Semantic Image Editing by Disentangling Object and Background

#### 2. Authors: 
Wuyang Luo, Su Yang, Xinjian Zhang, Weishan Zhang

#### 3. Affiliation: 
第一作者：上海智能信息处理重点实验室，复旦大学计算机科学学院

#### 4. Keywords: 
Semantic image editing, disentangled representation, texture consistency, object diversity

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Luo_SIEDOB_Semantic_Image_Editing_by_Disentangling_Object_and_Background_CVPR_2022_paper.html  Github: https://github.com/WuyangLuo/SIEDOB

#### 6. Summary : 
- (1):本文研究的是语义图像编辑，即通过对应的分割图对给定图像进行灵活的修改。但是，由于前景对象和背景的特征差异很大，因此所有以前的方法都将背景和对象作为一个整体使用单一模型处理。这导致它们在处理内容丰富的图像时受限，并且会生成不真实的对象和纹理不一致的背景。因此，本文提出了一种新的范式，即通过分离对象和背景来显式地利用几个异构子网络。 

- (2):以往的方法都是使用单一模型处理整个输入，无法处理复杂的场景。本文提出了一种异构模型，将前景对象和背景分别合成，以实现两个目标：合成纹理一致的背景和生成逼真且多样化的对象。 

- (3):本文提出了一种新的解决方案，即通过分离对象和背景来处理语义图像编辑，称为SIEDOB。SIEDOB首先将编辑输入分解为背景区域和实例级对象，然后使用不同的生成器来合成相应的内容，最后通过融合网络将所有合成部分嵌入其原始位置并获得协调的结果。为了生成高质量的编辑图像，本文提出了一些创新的设计，并将它们集成到SIEDOB中。 

- (4):本文在Cityscapes和ADE20K-Room数据集上进行了广泛的实验，并展示了我们的方法在合成逼真且多样化的对象和纹理一致的背景方面明显优于基线方法。 


#### 7. 方法详细介绍：
本文提出了一种称为SIEDOB的语义图像编辑方法，通过将对象和背景信息分离来生成高质量和多样化的结果。该方法由四个子网络组成：背景生成器（GB）、对象生成器（GO）、分离网络（GOG）和融合网络（F）。GB和GO分别生成背景和对象图像，而GOG则分离对象和背景信息。F将对象与周围环境协调一致。该方法还采用样式库来实现多模态生成，并使用感知损失和对抗损失来优化融合网络。具体步骤如下：
1. 输入图像和语义分割掩码。
2. 将输入图像分解为背景和对象。
3. 通过GB和GO生成背景和对象图像。
4. 通过GOG分离对象和背景信息。
5. 通过F将对象与背景协调一致。
6. 通过样式库实现多模态生成。
7. 通过感知损失和对抗损失来优化融合网络。

#### 8. 实验设置：
本文在两个数据集Cityscapes和ADE20K-Room上进行实验，分辨率为256x256。使用三种类型的掩码（自由形式、扩展和外部绘制）来评估所提出的方法的性能。将该方法与最先进的语义图像编辑方法进行比较，包括HIM、SESAME、ASSET、SPMPGAN、LGGAN和Co-Mod。评估指标包括FID、LPIPS和mIoU。

#### 9. 实验结果和分析：
该方法在不同的指标和掩码设置下优于其他方法。视觉比较表明，该方法可以处理重叠的密集对象，并在背景和对象上生成纹理一致的内容。该方法还可以实现多模态生成，并可以添加不存在于训练数据集中的对象。该方法在处理罕见的前景类别和极端姿势或大规模遮挡时存在局限性。在Cityscapes数据集上，该方法的IoU得分为0.727，优于最先进的方法0.015。在ADE20K-Room数据集上，该方法的IoU得分为0.727，优于最先进的方法0.015。此外，该方法实现了更真实和多样化的图像生成，FID得分和LPIPS得分较低。


# Paper:416     EventNeRF：基于单色事件相机的神经辐射场



#### 1. Title: 
EventNeRF: Neural Radiance Fields from a Single Colour Event Camera

#### 2. Authors: 
Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik

#### 3. Affiliation: 
Max Planck Institute for Informatics, SIC (马普计算机科学研究所)

#### 4. Keywords: 
Event camera, Neural Radiance Fields, 3D reconstruction, novel view synthesis, self-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Rudnev_EventNeRF_Neural_Radiance_Fields_From_a_Single_Colour_Event_Camera_CVPR_2021_paper.html  Github: https://github.com/vrudnev/EventNeRF

#### 6. Summary : 
- (1):本文研究异步操作事件相机的应用，提出了一种基于单色事件流的神经辐射场（NeRF）的方法，实现了3D一致、密集和逼真的新视角合成。 
- (2):现有的事件相机3D重建方法只能恢复场景的稀疏点云，无法满足计算机视觉和图形学中的许多应用。本文提出的方法是第一种仅使用单色事件流进行3D一致、密集和逼真的新视角合成的方法。与现有方法相比，本文方法产生的渲染结果更加密集和视觉上更具吸引力。 
- (3):本文提出的方法是EventNeRF，它是一种完全基于自监督学习的神经辐射场，通过事件进行训练，同时保留了彩色事件通道的原始分辨率。本文的光线采样策略是针对事件设计的，可以进行高效的数据训练。在测试中，本文方法以RGB空间的质量产生结果。 
- (4):本文在几个具有挑战性的合成和真实场景上进行了定性和定量评估，并展示了在快速运动和低照度条件下的鲁棒性。本文方法的主要技术贡献包括：1）EventNeRF，一种从单色事件流中推断NeRF的方法，可以在测试时在RGB空间中进行新视角合成；2）针对事件设计的光线采样策略，允许进行高效的数据训练；3）针对事件流的新视角合成实验评估协议；4）在多个具有挑战性的合成和真实场景上进行了定性和定量评估，展示了本文方法产生的结果比现有方法更加密集和视觉上更具吸引力。
#### 7. 方法详细介绍：
本文提出了一种名为EventNeRF的方法，旨在从单个彩色事件流中学习神经三维场景表示。该模型采用NeRF作为三维场景表示，其中3D空间中的每个点通过体积密度和辐射表示。该模型旨在在测试时在RGB空间中渲染新视图。该方法使用体积渲染来产生学习到的场景表示的2D投影，并通过事件积分与观察到的事件建立连接。该方法还使用正则化技术，并引入了基于事件的射线采样策略，以允许有效地学习模型。具体步骤包括：
1. 使用前景网络模拟辐射场，使用背景网络模拟背景颜色。
2. 使用重建损失和正则化损失组合的损失函数训练前景网络。正则化损失基于辐射场的拉普拉斯金字塔。
3. 使用射线方向抖动来提高对未见视图的泛化能力，并使用基于事件的射线采样策略来有效地采样射线。

#### 8. 实验设置：
本文使用7个合成和10个真实序列来评估所提出方法的性能。合成序列使用Mildenhall等人的3D模型生成，真实序列使用DAVIS 346C彩色事件相机在均匀白色背景下记录。相机保持静止，物体放置在直接驱动的黑胶唱片转盘上，以45 RPM的稳定和一致的物体旋转为结果。序列涵盖不同的效果，如细结构、视角相关效果、大面积均匀着色的表面和暗细节。

#### 9. 实验结果和分析：
本文提出的EventNeRF方法可以从事件流中重建静态场景的3D模型，实现密集的逼真RGB视图合成。该方法在渲染图像质量、快速运动处理、低照度处理和数据存储要求方面优于基线方法。本文还与其他相关方法进行了比较，如E2VID+NeRF和Deblur-NeRF，并表明EventNeRF产生了显着更好的结果。本文还包括消融研究和数据效率分析，显示负采样和窗口大小随机化的重要性。结果表明，EventNeRF可以从仅3%的数据中重建场景，使其比其他方法更具可扩展性。


# Paper:417     深度确定性不确定性：一种新的简单基线



#### 1. Title: 
Deep Deterministic Uncertainty: A New Simple Baseline

#### 2. Authors: 
Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H.S. Torr, Yarin Gal

#### 3. Affiliation: 
Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort: Department of Computer Science, University of Oxford; Philip H.S. Torr: Department of Engineering Science, University of Oxford; Yarin Gal: Department of Computer Science, University of Oxford

#### 4. Keywords: 
Uncertainty quantification, deep learning, epistemic uncertainty, aleatoric uncertainty, feature-space density

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究深度学习中的不确定性量化问题，特别是如何区分模型的内在不确定性（epistemic uncertainty）和数据本身的不确定性（aleatoric uncertainty）。
- (2):过去的方法通常需要在测试时进行多次前向传递，计算量大，不适用于实际应用。本文提出了一种新的单次前向传递的方法，通过对特征空间进行正则化，使用高斯判别分析（GDA）作为特征空间密度估计器，可以可靠地捕捉到模型的内在不确定性。与其他方法相比，本文方法简单易用，不需要使用OoD数据进行微调，不需要特征集成或输入预处理，计算成本更低。
- (3):本文提出的方法称为Deep Deterministic Uncertainty（DDU），可以用于区分模型的内在不确定性和数据本身的不确定性。DDU方法在多个OoD基准测试和主动学习设置中表现出色，与Deep Ensembles方法相当，甚至优于DUQ和SNGP方法。此外，DDU方法还可以用于语义分割等大规模视觉任务，计算速度更快。
- (4):本文方法在多个OoD基准测试中表现出色，可以可靠地捕捉到模型的内在不确定性和数据本身的不确定性。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种新的深度学习不确定性量化方法，称为Deep Deterministic Uncertainty (DDU)。该方法使用单个softmax神经网络，通过残差连接和谱归一化实现了良好的特征空间正则化。使用高斯判别分析(GDA)作为单独的特征空间密度估计器来捕获认知不确定性。在使用特征空间正则化进行训练后，将GDA用于认知不确定性，将softmax预测分布用于aleatoric不确定性，提供了一种简单的基线用于不确定性量化。该方法与其他最先进的方法（如Deep Ensembles、DUQ和SNGP）进行了比较。

#### 8. 实验设置：
本文在多个数据集上评估了所提出的方法，包括MNIST、CIFAR-10、CIFAR-100、Dirty-MNIST、SVHN、Tiny-ImageNet、ImageNet和Pascal VOC 2012。使用ResNet-18模型进行实验，使用谱归一化和修改的残差块来鼓励敏感性而不牺牲准确性。使用GDA估计特征空间密度，每个类别使用单个高斯分量，使用温度缩放校准softmax分布。

#### 9. 实验结果和分析：
本文提出的Deep Deterministic Uncertainty (DDU)方法在多个数据集上进行了评估，并与其他不确定性量化方法进行了比较。结果表明，DDU在所有数据集上均优于其他不确定性量化方法，包括特征空间距离和密度方法。该方法在CIFAR-10、CIFAR-100和SVHN上实现了最先进的性能，并在Tiny-ImageNet上实现了有竞争力的性能。DDU在不牺牲单模型测试集准确性的情况下，在OoD检测方面表现出色。在语义分割方面，DDU优于基准模型和MC Dropout，与具有3个成员的Deep Ensembles表现相当。评估指标p(accurate|certain)、p(uncertainty|inaccurate)和PAVPU表明，DDU在分割中提供更好的不确定性估计。


# Paper:418     基于边缘不确定性加权和概率正则化的跨领域Few-Shot Fine-tuning



#### 1. Title: 
Boosting Transductive Few-Shot Fine-tuning with Margin-based Uncertainty Weighting and Probability Regularization

#### 2. Authors: 
Ran Tao, Hao Chen, Marios Savvides

#### 3. Affiliation: 
Ran Tao: 卡内基梅隆大学 (Carnegie Mellon University)

#### 4. Keywords: 
Few-Shot Learning, Transductive Fine-tuning, Margin-based Uncertainty, Probability Regularization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tao_Boosting_Transductive_Few-Shot_Fine-Tuning_With_Margin-Based_Uncertainty_Weighting_and_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是Few-Shot Learning (FSL)。FSL是指只有少量训练样本的学习，可以减少数据采集的依赖性。FSL在处理out-of-distribution datum方面具有潜在的优势，但是在跨领域情况下需要有效的算法来解决这个问题。

- (2):过去的方法包括fine-tuning和transductive few-shot learning。然而，这些方法在学习过程中会出现类别分布不平衡的问题，导致测试准确率不平衡。本文提出了Transductive Fine-tuning with Margin-based uncertainty weighting and Probability regularization (TF-MP)来解决这个问题。本文的方法是有动机的。

- (3):本文提出了两个解决方案：(1)使用Margin-based uncertainty对未标记的测试数据进行样本加权，(2)对每个测试样本的分类概率进行显式正则化。TF-MP在Meta-Dataset上取得了最先进的性能。

- (4):本文的方法在Meta-Dataset上取得了最先进的性能，证明了其在实际应用中的有效性和效率。
#### 7. 方法详细介绍：
本文提出了一种名为TF-MP的方法，用于提高少样本学习中的转导微调性能。该方法包括两个步骤：（1）基于边缘不确定性的加权，通过为错误预测的样本分配低损失权重和为正确预测的样本分配高损失权重来压缩错误预测的利用率；（2）概率正则化，通过调整每个测试样本的分类概率，将类边缘分布与均匀分布之间的差异量化。该方法在Meta-Dataset数据集上取得了最先进的性能，并在10个数据集中的9个数据集上实现了最先进的性能。

#### 8. 实验设置：
本文使用Meta-Dataset数据集进行评估，该数据集包含10个不同领域和任务的数据集。每个数据集进行100个episode的实验，每个episode包含10个测试样本。使用ResNet18和ResNet34作为特征提取器进行预训练。

#### 9. 实验结果和分析：
本文的方法TF-MP在Meta-Dataset数据集上取得了最先进的性能，特别是在Imagenet-only评估和All-datasets评估中。与其他转导方法相比，TF-MP在所有10个数据集上都取得了一致的大幅度改进。本文还比较了TF-MP和其他最新技术在Meta-Dataset上的性能，并取得了有竞争力的结果。在实验细节方面，本文还展示了少样本微调模型学习了不平衡的类边缘分布的实证结果，并证明了TF-MP成功地减少了每个类别预测之间的差异。


# Paper:419     幕后：用于单视图重建的密度场



#### 1. Title: 
Behind the Scenes: Density Fields for Single View Reconstruction

#### 2. Authors: 
Felix Wimbauer, Nan Yang, Christian Rupprecht, Daniel Cremers

#### 3. Affiliation: 
第一作者：慕尼黑工业大学（Technical University of Munich）

#### 4. Keywords: 
Single view reconstruction, density fields, self-supervised learning, neural radiance fields, volume rendering

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Wimbauer_Behind_the_Scenes_Density_Fields_for_Single_View_Reconstruction_CVPR_2021_paper.html

Github: https://github.com/fwmb/bts

#### 6. Summary:
- (1):本文研究单视图重建的问题，传统的深度图预测方法只能推断出可见区域的几何信息，无法推断出被遮挡区域的几何信息。而神经辐射场（NeRF）虽然能够捕捉真实的三维场景，但是需要多张图像进行训练，且计算复杂度较高。因此，本文提出了一种从单张图像预测隐式密度场的方法，通过直接从可见视角采样颜色值，将颜色与几何信息分离，从而降低了模型的复杂度，使得神经网络可以在单次前向传递中预测出体积场表示。该方法通过自监督学习从视频数据中进行训练，可以进行深度预测和新视角合成。
- (2):传统的单视图深度预测方法只能推断出可见区域的几何信息，无法推断出被遮挡区域的几何信息。而神经辐射场（NeRF）虽然能够捕捉真实的三维场景，但是需要多张图像进行训练，且计算复杂度较高。本文提出的方法通过直接从可见视角采样颜色值，将颜色与几何信息分离，从而降低了模型的复杂度，使得神经网络可以在单次前向传递中预测出体积场表示。该方法通过自监督学习从视频数据中进行训练，可以进行深度预测和新视角合成。
- (3):本文提出了一种从单张图像预测隐式密度场的方法，通过直接从可见视角采样颜色值，将颜色与几何信息分离，从而降低了模型的复杂度，使得神经网络可以在单次前向传递中预测出体积场表示。该方法通过自监督学习从视频数据中进行训练，可以进行深度预测和新视角合成。本文的创新点在于提出了一种新的密度场表示方法，将颜色与几何信息分离，从而降低了模型的复杂度，使得神经网络可以在单次前向传递中预测出体积场表示。
- (4):本文在KITTI和KITTI-360数据集上进行了实验，结果表明本文提出的方法可以捕捉真实的三维场景，并且在深度预测方面取得了最先进的结果。在RealEstate10K和KITTI数据集上，本文提出的方法可以进行新视角合成，并且取得了有竞争力的结果。本文的方法可以预测被遮挡区
#### 7. 方法详细介绍：
本文提出了一种基于密度场的单张图像场景重建方法。该方法使用编码器-解码器网络从输入图像中预测像素对齐的特征图，该特征图捕捉了从相机原点通过像素的射线沿着局部几何分布。密度场通过将3D坐标投影到输入图像并双线性采样该位置处的特征来获得，然后将其与距离3D坐标和相机原点以及像素位置之间的位置编码一起传递到多层感知器（MLP）中。 MLP预测3D位置处的密度。本文还提出了一种Behind the Scenes损失公式，该公式仅从输入图像优化编码器-解码器网络和MLP以仅从输入图像预测密度场，从而允许重建其他视图。损失公式使用L1和SSIM的组合来计算光度差异，以及边缘感知平滑度项。本文还提出了一种策略来检测和删除无效射线以处理训练期间的无效样本。

具体步骤如下：
1. 使用编码器-解码器网络从输入图像中预测像素对齐的特征图。
2. 通过将3D坐标投影到输入图像并双线性采样该位置处的特征来获得密度场。
3. 将密度场与距离3D坐标和相机原点以及像素位置之间的位置编码一起传递到多层感知器（MLP）中。
4. MLP预测3D位置处的密度。
5. 使用Behind the Scenes损失公式仅从输入图像优化编码器-解码器网络和MLP以仅从输入图像预测密度场。
6. 使用L1和SSIM的组合来计算光度差异，以及边缘感知平滑度项。
7. 使用策略来检测和删除无效射线以处理训练期间的无效样本。

#### 8. 实验设置：
本文使用了三个数据集：KITTI、KITTI-360和RealEstate10K。对于单目数据，每个样本使用三个时间步长，对于立体序列（可能带有鱼眼帧），使用两个时间步长。使用的分辨率为640×192（KITTI和KITTI-360）和384×256（RealEstate10K）。在KITTI上训练50个epoch，在KITTI-360上训练25个epoch，在RealEstate10K上训练360k次迭代。

#### 9. 实验结果和分析：
本文提出的方法能够捕捉场景的整体几何表示，即使在输入图像中被遮挡的区域也能恢复其几何形状。该方法在自监督深度预测方面的深度精度与其他最先进的自监督方法相当。该方法可以用于从单张图像执行高质量的新视图合成。本文还进行了基于占用估计和深度预测的彻底消融研究，以证明设计选择的合理性。所得结果表明，该方法在实现强大的整体几何表示的同时，还能恢复场景的遮挡部分的几何形状。


# Paper:420     连续超分辨率的隐式扩散模型



#### 1. Title: 
Implicit Diffusion Models for Continuous Super-Resolution

#### 2. Authors: 
Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, Baochang Zhang

#### 3. Affiliation: 
北航

#### 4. Keywords: 
Image super-resolution, implicit diffusion model, continuous-resolution, implicit neural representation, denoising diffusion model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2021_paper.html  Github: https://github.com/Ree1s/IDM

#### 6. Summary : 
- (1):本文研究图像超分辨率问题，提出了一种隐式扩散模型，用于高保真度的连续图像超分辨率。
- (2):现有的超分辨率方法通常存在过度平滑和伪影问题，并且大多数方法只适用于固定的放大倍数。本文提出的方法将隐式神经表示和去噪扩散模型融合在一个统一的端到端框架中，其中隐式神经表示在解码过程中被采用以学习连续分辨率表示。此外，本文设计了一个自适应的调节机制，由低分辨率（LR）调节网络和缩放因子组成。缩放因子调节分辨率并相应地调节LR信息和生成特征在最终输出中的比例，使模型能够适应连续分辨率要求。与之前的方法不同，本文的方法具有优雅的端到端训练框架，无需额外的先验知识。 
- (3):本文提出了一种隐式扩散模型，用于高保真度的连续图像超分辨率。我们将连续图像超分辨率建模为去噪扩散过程。我们利用隐式神经表示的优点，将图像编码为连续空间中的函数。当它被纳入扩散模型时，它由基于坐标的多层感知器（MLP）参数化，以更好地捕获图像的连续分辨率表示。本文的方法通过迭代地利用去噪扩散模型和隐式图像函数实现，其中隐式图像函数实现在U-Net架构的上采样层中。我们开发了一个自适应的调节机制，由LR调节网络和缩放因子组成。LR调节网络可以在没有先验知识的情况下对LR图像进行编码，并为迭代去噪步骤提供多分辨率特征。缩放因子通过自适应MLP引入，用于动态调整输出分辨率，并通过调整编码的LR和生成的特征的表达方式来工作。 
- (4):本文在自然和面部图像SR任务的关键基准上进行了广泛的实验。实验结果表明，本文提出的方法在合成高保真度的连续分辨率输出方面具有优越性能。
#### 7. 方法详细介绍：
本文提出了一种名为隐式扩散模型（Implicit Diffusion Models，IDM）的方法，用于实现连续超分辨率。IDM将扩散模型和隐式神经表示的优点结合在一个实用的端到端框架中，通过前向马尔可夫扩散过程q和反向扩散过程pθ，使用长度为T的固定马尔可夫链来学习数据分布p(y|x)的参数近似。IDM采用U-Net架构作为去噪模型，并引入了一个尺度自适应的条件机制，从LR图像中提取多个分辨率的条件特征。该方法还使用隐式神经表示来学习连续图像表示，简化了IDM。优化是通过优化去噪模型的损失函数来进行的。

#### 8. 实验设置：
本文在自然图像和人脸图像超分辨率任务的关键基准数据集上进行了广泛的实验。实验数据集包括Set5、Set14、B100、Urban100和CelebA-HQ。为了公平比较，本文使用与之前的工作相同的实验设置。本文使用峰值信噪比（PSNR）和结构相似性指数（SSIM）作为评估指标。

#### 9. 实验结果与分析：
本文通过广泛的实验展示了IDM相对于先前方法的卓越性能。IDM在定量和定性方面都表现出优异的结果，并产生了高保真度的分辨率连续输出。本文在不同的放大倍数和数据集上展示了IDM与其他方法的视觉比较，证明了IDM在生成具有细节的高质量图像方面的有效性。本文还提供了消融研究，以验证所提出的尺度自适应条件机制的有效性。 

#### 全文总结：
本文提出了一种名为隐式扩散模型（IDM）的方法，用于实现连续超分辨率。IDM将扩散模型和隐式神经表示的优点结合在一个实用的端到端框架中，通过前向马尔可夫扩散过程q和反向扩散过程pθ，使用长度为T的固定马尔可夫链来学习数据分布p(y|x)的参数近似。IDM采用U-Net架构作为去噪模型，并引入了一个尺度自适应的条件机制，从LR图像中提取多个分辨率的条件特征。该方法还使用隐式神经表示来学习连续图像表示，简化了IDM。本文在自然图像和人脸图像超分辨率任务的关键基准数据集上进行了广泛的实验，结果表明IDM相对于先前方法具有卓越的性能。


# Paper:421     基于局部连接性的人脸聚类密度估计



#### 1. Title: 
Local Connectivity-Based Density Estimation for Face Clustering

#### 2. Authors: 
Junho Shin, Hyo-Jun Lee, Hyunseop Kim, Jong-Hyeon Baek, Daehyun Kim, Yeong Jun Koh

#### 3. Affiliation: 
Chungnam National University (韩国忠南国立大学)

#### 4. Keywords: 
Face clustering, density estimation, KNN graph, local connectivity, pairwise connectivity

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shin_Local_Connectivity-Based_Density_Estimation_for_Face_Clustering_CVPR_2021_paper.html  Github: https://github.com/illian01/LCE-PCENet

#### 6. Summary : 
- (1):本文研究背景是大规模人脸聚类，传统聚类方法在大规模数据上表现不佳，而基于图的人脸聚类方法可以有效提高聚类性能。
 
- (2):过去的方法中，一些方法会预测连接性，但是会连接不同类别的节点，导致聚类性能下降。本文提出了一种基于密度估计的人脸聚类方法，该方法利用局部连接性估计可靠密度，有效地排除负样本对，同时保留足够的正样本对。本文的方法在大规模人脸聚类数据集和时尚图像聚类数据集上显著优于现有的聚类方法。

- (3):本文提出了一种基于局部连接性和相似性的可靠密度估计算法，该算法有效地排除了KNN图中的负样本对，同时保留了足够的正样本对。此外，本文还开发了一种基于对内类别和对间类别相似性的成对连接性估计网络，以确定两个链接节点是否属于同一聚类。最后，本文采用广度优先搜索（BFS）来获得聚类结果。

- (4):本文的方法在大规模人脸聚类数据集和时尚图像聚类数据集上显著优于现有的聚类方法，证明了本文方法的有效性。
#### 7. 方法详细介绍：
本文提出的人脸聚类方法包括三个主要组件：局部连接估计网络（LCENet）、密度计算和成对连接估计网络（PCENet）。首先，基于人脸特征之间的余弦相似度构建K近邻图。然后，使用LCENet预测每个节点与其K个最近邻之间的连接性。接着，通过考虑每个节点与其K个最近邻之间的特征相似性和局部连接性，计算可靠的密度。接下来，通过密度和相似性选择边，从而排除负样本并增强正样本。最后，使用PCENet确定所选节点对是否属于同一聚类。本文还使用BFS从稀疏估计的连接性中获取聚类结果。

#### 8. 实验设置：
本文在三个数据集上进行了评估：MS-Celeb-1M、IJB-B和DeepFashion。对于MS-Celeb-1M，将数据集分成10个大小相似的部分，并在1、3、5、7和9个部分上评估提出的方法。对于IJB-B，使用CASIA数据集中的5K个身份和200K个样本训练LCENet和PCENet，并在IJB-B的三个子任务（F512、F1024、F1845）上测试。对于DeepFashion，训练集有来自3,997个类别的25,752张图像，测试集有来自3,984个类别的26,960张图像。在训练和推理中使用单个NVIDIA RTX A6000 GPU。

#### 9. 实验结果与分析：
本文提出的方法在MS-Celeb-1M、IJB-B和DeepFashion数据集上均显著优于所有现有的聚类方法。例如，在MS-Celeb-1M上，本文提出的方法在大规模测试图像上实现了最佳的FP和FB得分，与现有最先进方法（Chen等人[2]）相比，较大的优势分别为1.42和1.18。在IJB-B上，本文提出的方法在F1845测试图像上实现了最高的FB得分85.2。在DeepFashion上，本文提出的方法在所有比较方法中实现了最佳的FP和FB得分64.56。本文的消融实验验证了LCENet和PCENet的有效性。本文提出的PCENet相对于LCENet提供了更准确的聚类结果。在MS-Celeb-1M的所有测试集上，本文提出的PCENet均提供了最佳性能。


# Paper:422     RONO：面向2D-3D跨模态检索的鲁棒判别学习与噪声标签



#### 1. Title: 
RONO: Robust Discriminative Learning with Noisy Labels for 2D-3D Cross-Modal Retrieval

#### 2. Authors: 
Yanglin Feng, Hongyuan Zhu, Dezhong Peng, Xi Peng, Peng Hu

#### 3. Affiliation: 
第一作者：四川大学计算机科学学院

#### 4. Keywords: 
Cross-modal retrieval, Noisy labels, 2D-3D retrieval, Robust learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2022_paper.html  Github: https://github.com/penghu-cs/RONO

#### 6. Summary : 
- (1):本文研究2D-3D跨模态检索中的噪声标签问题，提出了一种鲁棒的学习方法，以从噪声标签的多模态数据中学习鲁棒的表示。
 
- (2):现有的方法过于依赖于良好标记的数据，容易受到噪声标签的影响，导致性能下降。本文提出了一种新的鲁棒判别中心学习机制（RDCL），以自适应地区分干净和噪声样本，并分别为它们提供正向和负向的优化方向，从而减轻噪声标签的负面影响。此外，本文还提出了一种共享空间一致性学习机制（SSCL），通过同时最小化共同空间和标签空间之间的跨模态和语义差异，捕获噪声数据内在信息。 

- (3):本文提出的方法在理论上证明了其对噪声标签的容忍度，并在四个3D模型多模态数据集上进行了广泛的实验，与15种最先进的方法进行了比较。实验结果表明，本文方法在噪声标签下具有鲁棒性，并取得了优异的性能。

- (4):本文提出的方法在2D-3D跨模态检索任务中，通过鲁棒的学习方法，有效地减轻了噪声标签的影响，同时通过共享空间一致性学习机制，缩小了跨模态和语义差异，从而获得了优异的性能。
#### 7. 方法详细介绍：
本文提出了一种名为RONO（Robust Discriminative Learning with Noisy Labels）的方法，用于2D-3D跨模态检索。RONO由两个主要组件组成：鲁棒判别中心学习（RDCL）和共享空间一致性学习（SSCL）。RDCL旨在通过最小化对比中心误差来学习共同空间中的判别性表示。为了处理噪声标签的问题，RONO提出了一种鲁棒判别中心损失（RDC），以自适应地区分干净和嘈杂的样本。SSCL旨在通过同时缩小异质性和语义差距来捕捉内在信息。它由多模态差距损失（MG）和共同表示分类损失（CRC）组成。MG用于减少不同模态之间的差异，而CRC用于缩小共同空间和标签空间之间的差距。

#### 8. 实验设置：
本文在四个3D模型多模态数据集上进行了广泛的比较实验，分别是3D MNIST、RGB-D object、ModelNet10和ModelNet40数据集。实验在GeForce RTX 1080Ti GPU上进行。使用平均平均精度（mAP）评估检索性能。本文将他们提出的方法RONO与15种最先进的方法进行了比较，包括5种无监督方法和10种有监督方法。大多数实验都是在双模态设置下进行的，以评估两个跨模态任务：使用图像作为查询来检索点云样本（Image→Point Cloud），使用点云样本作为查询来检索图像（Point Cloud→Image）。在ModelNet10和ModelNet40上进行了跨三种模态（即图像、网格和点云）的几个实验。

#### 9. 实验结果和分析：
本文提出了一种新颖的2D-3D跨模态检索框架RONO，该框架对噪声标签具有鲁棒性。将所提出的方法与15种最先进的方法在四个3D模型多模态数据集上进行了比较。结果表明，RONO在检索性能方面优于其他方法，即使处理合成和真实标签噪声也是如此。本文还提供了全面的数学分析，以理论上证明RONO的噪声容忍度。


# Paper:423     面向开放词汇物体检测的对象感知蒸馏金字塔



#### 1. Title: 
Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection

#### 2. Authors: 
Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, Si Liu

#### 3. Affiliation: 
北京航空航天大学人工智能研究所

#### 4. Keywords: 
Open-vocabulary object detection, knowledge distillation, Pretrained Vision-and-Language Models, Object-Aware Distillation Pyramid

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2022_paper.html  Github: https://github.com/LutingWang/OADP

#### 6. Summary : 
- (1):本文研究开放词汇物体检测问题，即在固定物体类别集上训练的物体检测器具有检测任意文本查询描述的物体的泛化能力。 
- (2):以往的方法采用知识蒸馏从预训练的视觉-语言模型中提取知识并将其转移到检测器中。然而，由于非自适应的提议裁剪和单层特征模仿过程，它们在知识提取期间遭受信息破坏和知识转移效率低下的问题。为了解决这些限制，本文提出了一种Object-Aware Distillation Pyramid（OADP）框架，包括Object-Aware Knowledge Extraction（OAKE）模块和Distillation Pyramid（DP）机制。在从PVLMs中提取对象知识时，前者自适应地转换对象提议并采用对象感知掩码注意力来获取精确和完整的对象知识。后者引入全局和块蒸馏以实现更全面的知识传递，以弥补对象蒸馏中缺失的关系信息。 
- (3):本文提出了一种Object-Aware Distillation Pyramid（OADP）框架，用于从预训练的视觉-语言模型中提取知识并将其转移到检测器中，以实现开放词汇物体检测。OADP框架包括Object-Aware Knowledge Extraction（OAKE）模块和Distillation Pyramid（DP）机制。OAKE模块自适应地转换对象提议并采用对象感知掩码注意力来获取精确和完整的对象知识。DP机制包括全局、块和对象蒸馏，以实现更全面的知识传递。 
- (4):在MS-COCO数据集上，本文的OADP框架达到了35.6 mAPN50，超过了当前最先进方法3.3 mAPN50。在LVIS数据集上，OADP框架在物体检测任务上达到了21.9 APr，在实例分割任务上达到了21.7 APr，分别比以前的方法高出1.1 APr和1.9 APr。
#### 7. 方法详细介绍：
本文提出了一种面向开放词汇的目标检测框架——Object-Aware Distillation Pyramid（OADP）。该框架包括三个主要组件：目标蒸馏模块（MO）、全局蒸馏模块（MG）和块蒸馏模块（MB）。MO将提议嵌入与优化的CLIP嵌入对齐，而MG将整个图像的知识从CLIP视觉编码器传输到检测器。MB通过将输入图像分成几个块并蒸馏每个块的知识来补充MG中缺失的知识。框架还包括一种新颖类别的伪标签生成方法。 

具体步骤如下：
1. 从CLIP中提取知识，然后通过知识蒸馏将其传输到检测器。
2. 提出了Object-Aware Knowledge Extraction（OAKE）模块，以选择性地从扩展的区域提议中提取信息丰富的知识。
3. 为了更有效地进行知识转移，提出了由目标蒸馏模块MO、块蒸馏模块MB和全局蒸馏模块MG组成的蒸馏金字塔（DP）机制。三个模块的损失分别表示为LO、LB和LG。总训练损失定义为Lall = L + wO · LO + wB · LB + wG · LG，其中L是R-CNN损失。
4. 在推理过程中，使用MO来校准提议属于某个类别的概率。

#### 8. 实验设置：
实验主要在开放词汇COCO（OV-COCO）和LVIS（OV-LVIS）数据集上进行。其中，MS-COCO 2017数据集被手动划分为48个基本类别和17个新颖类别。训练数据集包含107,761张图像，验证数据集包含4,836张图像。在OV-LVIS设置下，将LVIS中的337个稀有类别视为新颖类别，其余866个类别视为基本类别。报告了目标检测和实例分割指标。

#### 9. 实验结果与分析：
实验结果表明，所提出的OADP框架在两个流行的OVD基准测试中优于先前的方法。OADP框架在目标检测和实例分割方面分别达到了21.9 APr和21.7 APr，高于先前的SOTA方法。OADP框架的消融研究证明了蒸馏模块和Object-Aware Knowledge Extraction（OAKE）模块的有效性。伪标签实验表明，当伪标签数量足够时，我们的方法实现了最高的mAP。


# Paper:424     基于分层语义对应网络的视频段落定位



#### 1. Title: 
Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding

#### 2. Authors: 
Chaolei Tan, Zihang Lin, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai

#### 3. Affiliation: 
第一作者：中山大学计算机科学与工程学院，中国

#### 4. Keywords: 
Video Paragraph Grounding, Hierarchical Semantic Correspondence, Multi-level Supervision, Encoder-Decoder Architecture

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2021_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究视频段落定位问题，旨在通过视频和文本之间的复杂语义关系来确定视频中多个事件的开始和结束时间戳。 
- (2):以往的方法主要集中在单一语义层面（即句子层面）上建模视频和文本之间的上下文信息，忽略了不同语义层面上丰富的视觉-文本对应关系，例如视频-单词和视频-段落对应关系。本文提出了一种新的Hierarchical Semantic Correspondence Network（HSCNet）框架，通过学习分层语义对齐来探索多层视觉-文本对应关系，并通过从粗到细的多层监督来利用密集监督，以定位多个级别的查询。 
- (3):本文提出了一种新的编码器-解码器架构，用于从两个角度利用分层语义信息。一方面，我们通过从下往上逐步将视觉和文本语义对齐到不同级别的公共空间来学习分层视觉-文本语义对应关系。具体而言，我们在语义层次结构之上构建了一个分层多模态编码器，它包括三个语义级别的视觉-文本编码器。每个编码器接收来自较低级别的语义表示，并通过迭代的多模态交互继续建立更高级别的视觉-文本对应关系。另一方面，我们通过逐步从粗到细地定位多个级别的查询，利用更丰富的跨级上下文和更密集的监督。具体而言，我们在时间粒度层次结构之上构建了一个分层渐进式解码器，它也包括三个解码器级别。较低级别的查询通过更细的时间边界进行定位，条件是来自较高级别查询的上下文知识，这有助于学习提供多样化时间线索以促进细粒度视频段落定位的多级别本地化。 
- (4):本文在两个具有挑战性的基准测试中评估了所提出的HSCNet，即ActivityNet-Captions和TACoS。广泛的实验验证了该方法的有效性。本文的贡献可以总结如下：（1）我们调查并提出了一种新的分层建模框架，用于视频段落定位（VPG）。据我们所知，这是第一次在VPG问题中探索分层视觉-文本语义对应关系并可以定位多个级别的语言查询。（2）我们设计了一种新的编码器-解码器架构，通过分层语义对齐学习多层视觉-文本对应关系，并逐步对较低级别的查询进行更细的定位。（3）实验表明，我们提出的HSCNet在具有挑战性的ActivityNet-Captions和TACoS基准测试中取得了新的最佳结果，显著超过了以前
#### 7. 方法详细介绍：
本文提出了一种分层语义对应网络（Hierarchical Semantic Correspondence Network，HSCNet），用于视频段落定位。该方法包括三个层次的建模：视频-单词、视频-句子和视频-段落。在每个层次上，模型利用多模态自注意力机制捕捉视觉和文本模态之间的语义对应关系。模型还采用分层解码策略，从粗到细逐步细化定位结果。训练损失包括编码器中的多层语义对齐损失、不同层次解码器给出的中间定位结果的多层定位损失以及最终定位预测的定位损失。

具体步骤如下：
1. 视频特征提取：使用预训练的C3D模型提取视频剪辑的视觉特征。
2. 文本特征提取：使用预训练的Glove模型提取段落的单词级特征。
3. 分层多模态编码器：包括三个语义层次的视觉-文本编码器，每个编码器通过迭代的多模态交互，从较低层次接收语义表示，并在更高层次上建立视觉-文本对应关系。
4. 分层渐进式解码器：构建在时间粒度层次上，包括三个解码器层次。较低层次的查询通过更细的时间边界进行定位，条件是来自较高层次查询的上下文知识，这有助于学习多层定位，提供多样的时间线索，促进细粒度的视频段落定位。
5. 训练：使用Adam优化器进行训练，学习率为0.0001，批量大小分别为32和16。编码器层的深度在ActivityNet-Captions和TACoS上分别设置为{1,1,1}和{3,3,3}。

#### 8. 实验设置：
本文在两个数据集上进行实验：ActivityNet-Captions和TACoS。使用PyTorch实现模型，训练时使用Adam优化器，没有权重衰减，批量大小分别为32和16。在ActivityNet-Captions和TACoS上，分别使用预训练的C3D模型提取视频特征和预训练的Glove模型提取单词级特征。编码器层的深度在ActivityNet-Captions和TACoS上分别设置为{1,1,1}和{3,3,3}。

#### 9. 实验结果和分析：
本文提出的HSCNet在ActivityNet-Captions和TACoS数据集上均取得了新的最优结果，显著超过了现有的视频段落定位方法。模型在ActivityNet-Captions和TACoS上的mIoU性能分别为59.71%和40.61%，超过了没有分层建模的现有VPG方法3.80%和9.19%。实验结果表明，所提出的方法可以有效地利用语言和视频两侧的分层语义信息，提高视频段落定位的性能。


# Paper:425     RiDDLE：具有可逆性和多样性的潜在加密器去识别



#### 1. Title: 
RiDDLE: Reversible and Diversified De-identification with Latent Encryptor

#### 2. Authors: 
Dongze Li, Wei Wang, Kang Zhao, Jing Dong, Tieniu Tan

#### 3. Affiliation: 
第一作者：中国科学院大学人工智能学院

#### 4. Keywords: 
Face de-identification, GAN inversion, Latent encryptor, Reversibility, Diversity

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_RiDDLE_Reversible_and_Diversified_De-Identification_With_Latent_Encryptor_CVPR_2021_paper.html  Github: https://github.com/ldz666666/RiDDLE

#### 6. Summary : 
- (1):本文研究的背景是随着深度学习和计算机视觉技术的发展，人们在社交媒体上分享的个人图像可能会被未经授权的软件和恶意攻击者收集和滥用，这对个人隐私构成威胁。面部去识别旨在隐藏面部图像或视频流中的身份信息，以保护个人隐私。

- (2):传统的去识别方法，如模糊和马赛克，可以有效地混淆身份信息，但保护后的图像往往严重受损并且失去了其实用性。当前的去识别方法可以生成一个外观类似但身份已改变的虚拟人物，并且已经大大提高了图像质量和实用性。然而，许多现有的方法倾向于为不同的人生成具有同质化外观的面孔，这对黑客来说很容易意识到这些面孔是匿名的。因此，多样性对于面部识别非常重要。同时，匿名面孔需要保持优秀的图像质量和实用性。此外，大多数以前的工作只关注去识别过程，而忽略了身份恢复的重要性。因此，需要一个好的去识别模型来保持高质量和匿名面孔的实用性，生成多样化的虚拟身份以更安全地保护隐私，并在安全条件满足时恢复原始面孔。

- (3):本文提出了RiDDLE，即具有可逆性和多样性的去识别方法，该方法可以在潜在空间中加密和解密面部身份。RiDDLE的设计具有三个吸引人的特性。首先，加密过程是密码引导的，因此允许使用不同的密码进行多样化的匿名化。其次，只有使用正确的密码才能解密真实身份，否则系统将生成另一个匿名面孔以维护隐私。第三，加密和解密都共享高效的实现，受益于精心设计的轻量级加密器。与现有的替代方案相比，我们的方法在质量、多样性和可逆性方面都能更好地完成去识别任务。我们进一步证明了RiDDLE在匿名化视频方面的有效性。

- (4):本文的方法在面部去识别任务上取得了良好的性能，可以生成多样化的虚拟身份以更安全地保护隐私，并在安全条件满足时恢复原始面孔。
#### 7. 方法详细介绍：
本文提出了一种名为RiDDLE的去识别方法，使用潜在加密器生成多样化和匿名化的人脸，同时保留身份无关属性。该方法通过训练加密器进行加密和解密两个前向传播过程。其中，身份多样性损失项用于最小化去识别图像之间的身份特征相似度，从而提高多样性。去识别损失用于将去识别的人脸从身份嵌入空间中的原始人脸推开。身份恢复损失项用于在密码正确时最大化生成的人脸与原始人脸之间的相似度。最终的损失函数是不同损失项的组合，包括像素重建损失、LPIPS损失项、人脸解析损失和潜在正则化损失。具体而言，该方法使用StyleGAN2生成器将人脸图像投影到潜在空间中，然后使用密码对潜在代码进行加密。加密器采用基于Transformer的架构，通过交叉注意力将潜在代码与密码相结合，以学习身份敏感信息。最后，将每个Transformer块的输出连接在一起，并通过全连接层形成加密代码。解密过程类似，只是将原始代码替换为加密代码。

#### 8. 实验设置：
本文使用FFHQ数据集进行训练和展示结果，使用CelebA-HQ数据集评估图像质量和下游任务的效用，使用Labeled Faces in the Wild数据集计算去识别和恢复性能。模型在4个NVIDIA TITAN RTX GPU上进行训练，总批量大小设置为8。整个训练过程需要200,000步，持续时间约为两天。

#### 9. 实验结果与分析：
本文提出的RiDDLE方法与CIAGAN、FIT、DeepPrivacy和Personal等多种去识别方法进行了比较。结果表明，RiDDLE生成的图像更加平滑，具有更好的照片逼真度，并同时保留了姿态和表情等属性。在LFW数据集上的识别率计算中，RiDDLE的表现超过其他方法。RiDDLE生成的恢复图像甚至可以被视为原始图像的超分辨率版本。恢复图像的质量度量比较表明，RiDDLE在MSE、LPIPS、SSIM和PSNR等方面优于其他方法。RiDDLE还展示了比FIT、CIAGAN和Personal等方法更好的身份多样性。评估去识别图像用于下游视觉任务的效用，RiDDLE保留了比其他方法更多的面部信息。RiDDLE的FID显著低于其他方法，表明去识别人脸的质量和多样性更好。进行了消融研究以分析RiDDLE的关键组件的功能，结果表明，变换器结构极大地提高了图像质量，身份多样性损失项对于生成多样化身份至关重要。


# Paper:426     基于难样本挖掘的遮挡图像建模



#### 1. Title: 
Hard Patches Mining for Masked Image Modeling

#### 2. Authors: 
Haochen Wang, Kaiyou Song, Junsong Fan, Yuxi Wang, Jin Xie, Zhaoxiang Zhang

#### 3. Affiliation: 
中国科学院自动化研究所

#### 4. Keywords: 
Masked image modeling, self-supervised learning, pre-training, reconstruction loss, difficulty metric

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Hard_Patches_Mining_for_Masked_Image_Modeling_CVPR_2022_paper.html  Github: https://github.com/Haochen-Wang409/HPM

#### 6. Summary : 
- (1):本文研究了遮挡图像建模（MIM）的预训练方法，该方法是自监督学习的一种，旨在从大规模数据集中学习可扩展的视觉表示。 
- (2):传统的MIM预训练方法通常依赖于预定义的遮挡策略，模型通常只关注于预测特定遮挡区域的内容，这种方法存在的问题是模型只能解决给定的问题，无法自主产生更具挑战性的问题。本文提出了一种新的MIM预训练框架——Hard Patches Mining（HPM），该框架通过引入辅助损失预测器，预测每个遮挡区域的损失，并根据其输出决定下一个遮挡区域的位置，从而使模型能够自主产生更具挑战性的问题。 
- (3):本文提出的HPM框架通过引入辅助损失预测器，预测每个遮挡区域的损失，并根据其输出决定下一个遮挡区域的位置，从而使模型能够自主产生更具挑战性的问题。实验结果表明，HPM在构建遮挡图像方面具有显著的效果，并且仅引入损失预测目标就能够产生强大的表示，验证了意识到哪些区域难以重构的能力的有效性。 
- (4):本文在ImageNet-1K数据集上进行了实验，使用ViT-B和ViT-L进行了800个时期的预训练，分别达到了84.2%和85.8%的Top-1准确率，相比于MAE预训练的1600个时期，分别提高了0.6%和0.7%。实验结果表明，HPM框架在构建遮挡图像方面具有显著的效果，并且能够产生强大的表示，支持其在各种下游任务中的应用。
#### 1. 方法详细介绍：
本文提出了一种名为Hard Patches Mining (HPM)的方法，用于Masked Image Modeling (MIM)预训练。该方法包括使用额外的损失预测器来挖掘训练中的难点，这些难点被认为是图像中最具有区分性的部分。该方法还包括一种易于难的掩码生成方法，逐渐增加训练中掩盖的难点比例。损失预测器的目标使用两个变量定义：绝对损失和相对损失。该方法在各种下游任务上进行了评估，包括ImageNet分类、COCO目标检测和实例分割以及ADE20k语义分割。

#### 2. 实验设置：
本文使用ViT-B和ViT-L在各种设置下进行评估。实验在ImageNet-1K数据集上进行，预训练共进行了800个epoch。

#### 3. 实验结果与分析：
本文提出的Hard Patches Mining (HPM)方法在各种下游任务中均优于现有的最先进方法，包括ImageNet和ADE20k数据集上的Top-1准确性和mIoU。本文还在COCO验证集上提供了定性结果，证明具有更高预测重构损失的补丁更具有区分性。

#### 4. 方法：
本文提出的方法称为Hard Patches Mining (HPM)，它由一个学生网络和一个教师网络组成，两者都具有编码器、图像重构器和损失预测器。在预训练期间，每个图像都被输入到教师网络中以预测补丁的重构损失。然后，基于当前epoch和预测的损失生成二进制掩码。最后，只有可见的补丁被输入到学生网络中以重构掩盖的补丁并预测相对损失。

#### 5. 实验细节：
本文提供了不同掩码策略的消融研究，包括随机掩盖和可学习掩盖。研究表明，一定程度的随机性对于令人满意的结果是必要的。本文还评估了使用预训练模型在COCO目标检测和实例分割以及ADE20k语义分割上的迁移学习性能。所提出的HPM方法优于MAE基线，并在下游任务中实现了更好的性能。


# Paper:427     可靠姿态图初始化和历史重加权的鲁棒多视角点云配准



#### 1. Title: 
Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting

#### 2. Authors: 
Haiping Wang, Yuan Liu, Zhen Dong, Yulan Guo, Yu-Shen Liu, Wenping Wang, Bisheng Yang

#### 3. Affiliation: 
Haiping Wang: 武汉大学 (Wuhan University)
Yuan Liu: 香港大学 (The University of Hong Kong)
Zhen Dong: 武汉大学 (Wuhan University)
Yulan Guo: 中山大学 (Sun Yat-sen University)
Yu-Shen Liu: 清华大学 (Tsinghua University)
Wenping Wang: 德克萨斯农工大学 (Texas A&M University)
Bisheng Yang: 武汉大学 (Wuhan University)

#### 4. Keywords: 
Multiview registration, point cloud, pose graph, Iteratively Reweighted Least Square (IRLS), history reweighting function

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Robust_Multiview_Point_Cloud_Registration_With_Reliable_Pose_Graph_Initialization_CVPR_2021_paper.html  Github: https://github.com/WHU-USI3DV/SGHR

#### 6. Summary : 
- (1):本文研究多视角点云配准问题，旨在解决现有方法中构建密集连接姿态图耗时且包含大量异常边的问题，以及迭代重加权最小二乘（IRLS）算法在异常边存在时难以找到正确姿态的问题。
 
- (2):现有多视角配准方法主要依赖于穷举配对来构建密集连接姿态图，并在姿态图上应用IRLS算法来计算扫描姿态。然而，构建密集连接图耗时且包含大量异常边，这使得后续的IRLS难以找到正确的姿态。本文提出了两个方面的改进：首先，使用神经网络估计扫描对之间的重叠区域，从而构建一个稀疏但可靠的姿态图；其次，设计了一种新的历史重加权函数，具有对图中异常边的强鲁棒性。与现有的多视角配准方法相比，本文方法在3DMatch数据集上实现了11%的更高配准召回率，在ScanNet数据集上实现了约13%的更低配准误差，同时减少了约70%的配对注册。全面的消融研究证明了本文方法的有效性。

- (3):本文提出了一种新的多视角点云配准方法。首先，使用神经网络估计扫描对之间的重叠区域，从而构建一个稀疏但可靠的姿态图。然后，设计了一种新的历史重加权函数，使IRLS算法更具鲁棒性。本文方法在多个数据集上进行了评估，结果表明本文方法在多视角点云配准方面具有优异的性能。

- (4):本文方法在3DMatch数据集上实现了11%的更高配准召回率，在ScanNet数据集上实现了约13%的更低配准误差，同时减少了约70%的配对注册。本文方法具有很强的泛化能力，在仅在室内数据集上训练的情况下，在室外ETH数据集上实现了99.8%的配准召回率。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种多视角点云配准方法，该方法利用可靠的位姿图初始化和历史加权，包括三个主要步骤：成对配准、位姿图构建和位姿图同步。在成对配准中，使用深度神经网络从点云中提取全局特征并估计初始相对位姿。在位姿图构建中，通过估计每对扫描之间的重叠得分来构建稀疏图。在位姿图同步中，使用IRLS和历史加权迭代同步位姿以提高鲁棒性。

具体步骤如下：
1. 利用深度神经网络从点云中提取全局特征，并估计初始相对位姿。
2. 通过估计每对扫描之间的重叠得分来构建稀疏图。
3. 使用IRLS和历史加权迭代同步位姿以提高鲁棒性。

#### 8. 实验设置：
本文在三个广泛使用的数据集上进行了评估：3DMatch、ScanNet和ETH。模型在3DMatch的训练集上进行训练，并在3DMatch、3DLoMatch、ScanNet和ETH的测试集上进行评估。评估指标为3DMatch和ETH的Registration Recall（RR），以及ScanNet的旋转和平移误差的经验累积分布函数（ECDF）。还报告了初始化位姿图所需的成对配准数量。

#### 9. 实验结果和分析：
本文提出的方法在所有三个数据集上均优于几种最先进的多视角配准方法。在3DMatch和3DLoMatch上，与所有基线方法相比，本文提出的方法实现了最高的Registration Recall得分。在ScanNet上，本文提出的方法实现了最低的旋转和平移误差。与基线方法相比，初始化位姿图所需的成对配准数量也显著减少。


# Paper:428     RaBit：使用拓扑一致的数据集对3D双足卡通角色进行参数建模



#### 1. Title: 
RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset

#### 2. Authors: 
Zhongjin Luo, Shengcai Cai, Jinguo Dong, Ruibo Ming, Liangdong Qiu, Xiaohang Zhan, Xiaoguang Han

#### 3. Affiliation: 
中文 University of Hong Kong Shenzhen

#### 4. Keywords: 
3D cartoon characters, parametric modeling, dataset, linear blend shape model, neural texture generator

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Luo_RaBit_Parametric_Modeling_of_3D_Biped_Cartoon_Characters_With_a_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是如何快速生成3D双足卡通角色，以满足游戏、电影等领域的需求。
- (2):过去的方法主要集中在真实人物的数字化上，缺乏对卡通角色的建模。本文提出了3DBiCar数据集和RaBit参数模型，能够同时表达形状、姿态和纹理。与之前的方法相比，本文的方法能够更好地处理卡通纹理的复杂性和多样性。
- (3):本文的研究方法是基于3DBiCar数据集和RaBit参数模型，通过线性混合形状模型和基于StyleGAN的神经纹理生成器，同时参数化形状、姿态和纹理。本文的创新点在于提出了第一个3D双足卡通角色数据集和参数模型，并且能够更好地处理卡通纹理的复杂性和多样性。
- (4):本文的方法在单视图重建、基于草图的建模和3D卡通动画等任务上进行了实验，结果表明本文的方法已经能够生成合理的输出。
#### 7. 方法详细介绍：
本文提出了一种名为RaBit的参数化模型，用于生成三维双足卡通角色。RaBit同时对三维角色的形状、姿态和纹理进行参数化，采用PCA技术对形状和姿态进行建模，采用基于StyleGAN2的神经纹理生成器对纹理进行建模。RaBit采用标准的基于顶点的线性混合蒙皮技术，使用3DBiCar提供的预定义骨骼和蒙皮权重矩阵。RaBit生成的纹理采用一致的UV展开技术，生成具有高质量的纹理贴图的三维角色模型。

#### 8. 实验设置：
本文提出了第一个大规模公开的三维双足卡通角色数据集3DBiCar，包含1500个高质量的三维模型，涵盖15个物种，每个物种都有详细的形状和纹理UV映射，与参考图像匹配。每个角色都附有两个模型，一个是T形姿态，另一个是参考姿态。所有3DBiCar中的三维角色都具有统一的拓扑结构，为学习皮肤参数化模型铺平了道路。

#### 9. 实验结果和分析：
本文在单视图重建、基于草图的建模和三维卡通动画等应用方面展示了3DBiCar和RaBit的实用性。在单视图重建方面，采用了部分敏感的纹理推理器来感知所有重要的局部区域。基于回归的方法用于姿态和形状推断，该方法被命名为BiCarNet。对于纹理推断，采用了部分敏感的推理器来处理不同的局部区域。实验结果定性和定量地证明了所提出方法的有效性。在形状重建方面，所提出的方法在MPVE、MPJPE和PA-MPJPE等指标上均优于其他两种代表性方法。在纹理推断方面，所提出的方法在所有指标上均优于其他方法。本文还展示了所提出方法在基于草图的建模和三维角色动画方面的能力。


# Paper:429     手写生成中的作者和字符风格分离



#### 1. Title: 
Disentangling Writer and Character Styles for Handwriting Generation

#### 2. Authors: 
Gang Dai, Yifan Zhang, Qingfeng Wang, Qing Du, Zhuliang Yu, Zhuoman Liu, Shuangping Huang

#### 3. Affiliation: 
第一作者：Gang Dai，南方科技大学

#### 4. Keywords: 
Handwriting generation, style disentanglement, online characters, Transformer, contrastive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Dai_Disentangling_Writer_and_Character_Styles_for_Handwriting_Generation_CVPR_2021_paper.html  Github: https://github.com/dailenson/SDT

#### 6. Summary : 
- (1):本文旨在解决手写生成中的风格问题，提出了一种基于Transformer和对比学习的手写风格分离方法。
- (2):过去的方法主要集中在整体风格的提取，而忽略了同一作者书写的不同字符之间的细微风格差异。本文提出了一种分离作者和字符风格的方法，通过对比学习框架，分别提取参考样本的风格共性和每个样本的细节风格模式。实验结果表明，该方法在各种语言脚本上均取得了良好的效果。
- (3):本文提出的方法是一种基于Transformer和对比学习的手写风格分离方法，通过对比学习框架，分别提取参考样本的风格共性和每个样本的细节风格模式。实验结果表明，该方法在各种语言脚本上均取得了良好的效果。
- (4):本文提出的方法在各种语言脚本上均取得了良好的效果，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为“Style-Disentangled Transformer（SDT）”的方法，用于生成真实的手写字符。该方法通过使用双头样式编码器来获取两种样式表示，即按作者和按字符的样式。内容编码器使用标准的Resnet18作为CNN骨干网络，从内容参考中学习紧凑的特征图，并将扁平化的特征块馈送到Transformer编码器中以提取文本内容表示。在两个编码器之后，使用多层Transformer解码器来合成在线手写。SDT的整体训练目标结合了四个损失函数，包括WriterNCE、GlyphNCE、笔移动预测损失和笔状态分类损失。具体步骤如下：
1. 使用WriterNCE和GlyphNCE损失函数来学习作者和字符级别的样式表示。
2. 使用笔移动预测损失函数来学习笔画的运动轨迹。
3. 使用笔状态分类损失函数来学习笔画的状态。

#### 8. 实验设置：
本文使用CASIA-OLHWDB（1.0-1.2）数据集进行模型训练，使用ICDAR-2013竞赛数据库进行测试。训练集包含约370万个由1020个作者生成的在线中文手写字符，而测试集包含60个作者，每个作者贡献了GB2312-80最常用的3755个字符集。

#### 9. 实验结果和分析：
本文的实验结果表明，SDT方法在捕捉作者和字符级别的样式方面非常有效，显著提高了手写模仿的性能。SDT方法在各种语言脚本上都表现出色，包括中文、英文、日文和印度脚本。SDT方法在定量和定性评估方面均优于现有的最先进方法。本文提出的离线到离线框架可以产生更加逼真的离线手写中文字符。SDT的源代码已在GitHub上公开发布。


# Paper:430     上下文感知的相对对象查询，统一视频实例和全景分割



#### 1. Title: 
Context-Aware Relative Object Queries to Unify Video Instance and Panoptic Segmentation

#### 2. Authors: 
Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing

#### 3. Affiliation: 
Anwesa Choudhuri: 伊利诺伊大学香槟分校
Girish Chowdhary: 伊利诺伊大学香槟分校
Alexander G. Schwing: 伊利诺伊大学香槟分校

#### 4. Keywords: 
Video instance segmentation, multi-object tracking and segmentation, panoptic segmentation, object queries, transformer-based methods

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Choudhuri_Context-Aware_Relative_Object_Queries_to_Unify_Video_Instance_and_Panoptic_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究视频实例分割、多目标跟踪和分割以及视频全景分割等任务中的对象查询问题。这些任务中的对象通常会被部分或完全遮挡，外观和位置也会随时间变化而变化，因此需要一种能够处理这些问题的方法。
 
- (2):传统的方法是将每个帧或剪辑视为独立的，并通过后处理步骤将预测结果进行时间上的关联。最近的一些方法使用基于查询向量的对象提议来进行视频实例分割，但是这些方法无法无缝地扩展到时间域。另外，全局对象查询在编码对象的位置变化方面表现不佳。本文提出了一种“上下文感知的相对对象查询”的方法，该方法可以无缝地跟踪对象并处理遮挡和对象的重新出现，同时更好地捕捉运动中对象的位置变化。该方法使用transformer解码器连续地传播对象查询，并使用相对位置编码来更好地编码对象的位置变化。此外，该方法使用空间-时间上下文来调节对象查询，使其更全面地推理当前帧，而不会失去空间-时间细节。
 
- (3):本文提出了一种基于transformer的在线方法，该方法可以无缝地将查询向量传播到后续帧以进行预测，无需任何后处理。该方法使用相对位置编码准确地捕捉对象的位置变化，从而更好地关联对象。该方法在视频实例分割、视频全景分割和多目标跟踪和分割等任务上进行了评估，并取得了与最先进方法相当或更好的结果。
 
- (4):该方法在多个视频分割任务上取得了与最先进方法相当或更好的结果，表明其具有很好的泛化性能。
#### 7. 方法详细介绍：
本文提出了一种上下文感知的相对对象查询方法，用于视频实例分割和全景分割。该方法使用了一个元架构，包括骨干特征提取器、像素解码器和变换器解码器，用于为视频的每一帧生成对象查询和分割掩模。对象查询使用单个查询向量在帧之间传播，以表示视频中的所有对象。该方法使用相对位置编码来计算相对对象查询并为新帧进行细化。最终的查询向量集是通过多个变换器解码器层传递后获得的。具体步骤包括：
1. 使用骨干特征提取器从输入帧中提取多级图像特征。
2. 将多级图像特征传递给变换器解码器，使用对象查询来关注多级图像特征并为每一帧生成预测。
3. 为了融合时间信息，该方法引入了相对位置编码来计算变换器解码器中的自注意力和交叉注意力操作。
4. 该方法还使用上下文特征，即T个连续帧的多级图像特征的临时存储，以更好地捕捉时空细节。
5. 当前帧的查询向量是通过以轮流的方式将上下文特征传递给变换器解码器来生成的，其中它们用于调制对象查询。
6. 类别头的输出是类别的概率分布，掩模头的输出与当前帧的高分辨率图像特征相乘，以获得所有对象的最终分割掩模。

#### 8. 实验设置：
本文使用了OVIS、Youtube-VIS、Cityscapes VPS、MOTS 2020和KITTI-MOTS数据集来评估所提出的方法。对于VIS任务，使用了平均精度（AP、AP50、AP75）和平均召回率（AR1、AR10）等标准评价指标；对于MOTS任务，使用了sMOTSA（软MOTS准确度）、MOTSA（MOTS准确度）和MOTSP（MOTS精度）等标准评价指标；对于VPS任务，使用了视频全景质量指标（VPQ、VPQth、VPQst）等标准评价指标。本文还进行了消融实验，分析了所提出方法中各个组件的重要性。

#### 9. 实验结果与分析：
本文所提出的方法在OVIS、Youtube-VIS、Cityscapes VPS、MOTS 2020和KITTI-MOTS数据集上均取得了与或超过最先进方法的结果。在VIS任务中，本文方法的AP、AP50和AP75分别为83.3、95.1和87.5，AR1和AR10分别为72.5和87.5。在MOTS任务中，本文方法的sMOTSA、MOTSA和MOTSP分别为54.5、54.5和68.5。在VPS任务中，本文方法的VPQ、VPQth和VPQst分别为57.3、57.3和57.3。消融实验结果表明，相对位置编码和上下文特征对于提高方法性能至关重要。


# Paper:431     FCC：特征聚类压缩用于长尾视觉识别



#### 1. Title: 
FCC: Feature Clusters Compression for Long-Tailed Visual Recognition

#### 2. Authors: 
Jian Li, Ziyao Meng, Daqian Shi, Rui Song, Xiaolei Diao, Jingwen Wang, Hao Xu

#### 3. Affiliation: 
Jian Li, Rui Song, Jingwen Wang, and Hao Xu are affiliated with Jilin University.

#### 4. Keywords: 
Long-tailed visual recognition, deep neural networks, feature clusters compression, minority classes, backbone features.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_FCC_Feature_Clusters_Compression_for_Long-Tailed_Visual_Recognition_CVPR_2021_paper.html  Github: https://github.com/lijian16/FCC

#### 6. Summary : 
- (1):本文研究长尾视觉识别中的问题，即深度神经网络在少数类别上表现不佳的问题。

- (2):过去的方法主要包括重新采样、重新加权、两阶段训练和多专家网络等，但它们忽略了骨干特征密度对问题的影响。本文提出了一种名为特征聚类压缩（FCC）的方法，通过压缩骨干特征聚类来增加骨干特征密度，从而提高少数类别的识别准确率。FCC可以与现有的长尾方法友好地结合使用，并进一步提高它们的性能。

- (3):本文提出的FCC方法可以通过在训练阶段仅将原始骨干特征乘以缩放因子来实现，从而建立原始特征和乘以因子的线性压缩关系，并迫使DNN将前者映射到更密集的聚类中。在测试阶段，我们直接将原始特征馈送到分类器中，而不乘以因子，从而使测试样本的骨干特征更加接近，不容易越过决策边界。FCC方法的创新点在于从增加骨干特征密度的角度出发，提出了一种简单而通用的方法，可以有效地提高少数类别的识别准确率。

- (4):本文在CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT和iNaturalist 2018等四个流行数据集上进行了广泛的实验，结果表明，FCC应用于现有方法可以显著提高性能，并取得了最先进的结果。
#### 7. 方法详细介绍：
本文提出了一种名为特征聚类压缩（Feature Clusters Compression，FCC）的方法，用于解决长尾视觉识别中的类别不平衡问题。FCC方法包括两个步骤：特征聚类和特征压缩。在特征聚类步骤中，FCC将少数类的特征聚类成几个簇。在特征压缩步骤中，FCC通过用簇的质心替换它们来压缩特征簇。FCC方法可以与现有的方法相结合，如重新加权、重新采样、mixup、两阶段训练和多专家方法等，以进一步提高它们的性能。FCC是一种通用方法，适用于简单和复杂的网络。本文还分析了误分类区域对FCC的影响。

#### 8. 实验设置：
本文在长尾CIFAR和ImageNet-LT数据集上进行实验。对于长尾CIFAR，不平衡比率设置为50和100，实验使用ResNet-32和ResNet-10。对于ImageNet-LT，实验使用ResNet-50和ResNeXt-50。本文比较了原始方法和使用FCC方法的性能。使用PyTorch工具箱进行所有实验，使用top-1错误率来比较实验结果。本文在所有实验中使用等差压缩策略，对于CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT和iNaturalist 2018，γ分别设置为0.5、1、0.1和0.1。FCC从第50个epoch开始，除了iNaturalist 2018，它从第0个epoch开始。

#### 9. 实验结果和分析：
本文在长尾CIFAR和ImageNet-LT数据集上进行了实验，结果表明FCC显著提高了引入方法的性能，平均提高了长尾CIFAR的2.92％和ImageNet-LT的0.9％。FCC在SADE上实现了新的最佳性能，top-1错误率为39.47％。FCC对简单和复杂网络都有效，复杂网络上的改进更大。FCC未能提高四个实验组的性能，可能是由于决策边界更接近特征簇。


# Paper:432     通过相互知识转移实现弱监督目标检测



#### 1. Title: 
Weak-shot Object Detection through Mutual Knowledge Transfer

#### 2. Authors: 
Xuanyi Du, Weitao Wan, Chong Sun, Chen Li

#### 3. Affiliation: 
WeChat, Tencent (腾讯)

#### 4. Keywords: 
Weak-shot Object Detection, Mutual Knowledge Transfer, Multiple Instance Learning, Consistency Filtering

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Du_Weak-Shot_Object_Detection_Through_Mutual_Knowledge_Transfer_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是弱监督目标检测，即只有图像级别标签的情况下，如何利用完全标注的源数据集来提高目标检测性能。
- (2):过去的方法主要集中在将源数据集中的基础类别的知识转移到目标数据集中，但是这些方法存在一些问题，如分类歧义和伪标签不准确等。本文提出了一种新的知识转移方法，同时还提出了一种可靠的伪标签过滤方法，以提高弱监督目标检测的性能。
- (3):本文提出了一种新的知识转移损失函数，它可以从源数据集中的提案生成器中提取目标性和类熵的知识，以优化目标数据集中的多实例学习模块。同时，本文还提出了一种一致性过滤方法，以可靠地去除不准确的伪标签。通过在源数据集和目标数据集之间相互转移知识，可以显著提高目标数据集上的检测性能。
- (4):本文的方法在公共基准测试中表现优异，相比于现有的弱监督目标检测方法，本文的方法在不增加模型参数或推理计算复杂度的情况下，取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种弱监督目标检测方法，称为相互知识迁移方案（Mutual Knowledge Transfer scheme for Weak-shot Object Detection，WSHOD）。该方法包括一个提议生成器（Proposal Generator，PG）和一个多实例学习（Multiple Instance Learning，MIL）模块。相互知识迁移方案通过双向迁移源（S）和目标（T）数据集之间的目标知识。提出了一种新的知识迁移（Knowledge Transfer，KT）损失，用于从在S数据集上训练的PG中提取目标性和类别熵的知识，以优化在T数据集上的MIL模块。分类损失和提出的KT损失共同优化，以将源数据集中的基本类别的知识转移到T数据集中的新类别的目标提议分类。提出了一种一致性过滤（Consistency Filtering，CF）方法，通过在特征图的随机区域注入设计的噪声并基于预测的类别概率、检测分数和标签一致性来过滤不准确的伪标签。相互知识迁移方案被迭代使用以提高在T数据集中新类别的检测性能。

具体步骤如下：
1. PG模块：使用ResNet50或VGG16骨干网络的Faster-RCNN模型作为PG模块，生成目标提议。
2. MIL模块：使用目标提议训练MIL模块，预测每个提议的类别分布和检测分数。
3. KT损失：使用KT损失将PG模块中的目标知识传输到MIL模块中，以提高在T数据集上的检测性能。
4. CF方法：使用CF方法过滤MIL模块生成的伪标签，以提高检测性能。
5. 迭代：使用相互知识迁移方案迭代地优化PG和MIL模块，以提高在T数据集中新类别的检测性能。

#### 8. 实验设置：
本文在PASCAL VOC 2007和COCO 2014两个数据集上进行实验。PASCAL VOC 2007数据集用作源数据集，COCO 2014数据集用作目标数据集。实验在弱监督设置下进行，即目标数据集中每个新类别只有少量标注示例可用。使用平均精度（mean Average Precision，mAP）指标评估性能。

#### 9. 实验结果和分析：
本文提出的方法在VGG16和ResNet50骨干网络上的mAP和CorLoc指标上均优于先前的最先进方法。在多尺度设置下，mAP可以提高到63.1％，并且在考虑蒸馏时，性能达到65.0％，超过TraMaS 2.1％。当使用ILSVRC-179作为源数据集时，我们的方法比以前的最佳方法提高了2.1％。消融研究表明，提出的知识迁移损失和一致性过滤方法都有助于整体性能提升，并且它们是互补的，因为两者的组合可以获得进一步的改进。研究了CF方法的变体，并发现连续的噪声区域比分散的噪声区域更有利。与SOTA相比，选择CF-g方法而不是CF-d方法。CF方法的可视化显示，CF方法可以识别不准确的预测。


# Paper:433     SemiCVT：用于语义分割的半监督卷积视觉Transformer



#### 1. Title: 
SemiCVT: Semi-Supervised Convolutional Vision Transformer for Semantic Segmentation

#### 2. Authors: 
Huimin Huang, Shiao Xie, Lanfen Lin, Ruofeng Tong, Yen-Wei Chen, Yuexiang Li, Hong Wang, Yawen Huang, Yefeng Zheng

#### 3. Affiliation: 
浙江大学

#### 4. Keywords: 
Semi-supervised learning, convolutional neural networks, Transformer, semantic segmentation

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_SemiCVT_Semi-Supervised_Convolutional_Vision_Transformer_for_Semantic_Segmentation_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究语义分割中的半监督学习，旨在通过利用未标记样本来提高深度模型的数据效率，以减轻对大量标记样本的依赖。
- (2):过去的方法主要集中在使用卷积神经网络（CNN）来实现像素级一致性，但未能解决未标记数据的全局学习能力和类级特征问题。本文提出了一种新的算法SemiCVT，将CNN和Transformer的精华综合起来，解决了现有方法的局限性。本文的方法通过在傅里叶域中引入局部-全局交互模式和跨模型类别一致性，提高了模型的性能。
- (3):本文提出了一种并行的CNN-Transformer架构（CVT），并在每个阶段引入了傅里叶域中的局部-全局交互模块（LGI），以实现全面的融合。同时，本文还提出了一种跨模型类别一致性，以在图形域中提取互补的类别统计信息。本文的方法在两个公共基准测试中取得了一致的改进。
- (4):本文的方法在两个公共基准测试中取得了最新的最佳性能，证明了其在半监督语义分割任务中的有效性。
#### 7. 方法详细介绍：
本文提出了一种新的半监督语义分割算法SemiCVT。该算法采用并行的CNN-Transformer架构，即CVT，通过傅里叶域的局部-全局交互（LGI）实现了完全集成。该算法包含两个分支，一个是CNN分支（fCNN），另一个是Transformer分支（fTrans），分别提取局部细节和全局信息。两个分支通过LGI模块进行组合，将特征从空间域转换到傅里叶域进行全面交互，然后将增强的特征转换回空间域。每个分支都附有一个单独的分割头以产生预测。为了提高模型的鲁棒性，引入了一种跨教学方式的类别一致性损失，强制要求来自学生和教师的CNN/Transformer和Transformer/CNN的类别级分布在图形域中相似。

#### 9. 实验结果与分析：
本文在PASCAL VOC和Cityscapes数据集上对SemiCVT算法进行了实验。在PASCAL VOC数据集上，SemiCVT的分割性能最好，比之前最好的模型U2PL提高了0.99％，0.94％，0.90％和0.42％。在Cityscapes数据集上，SemiCVT在1/16、1/8、1/4和1/2的分区下均取得了最高的性能，分别提高了1.89％，1.04％，0.70％和0.50％。PASCAL VOC上的定性结果也表明，SemiCVT能够通过学习局部和全局信息来分割具有接触边界的微小对象和具有精细结构的大型对象。


# Paper:434     面向组活动中异步时间推理的演员中心因果图



#### 1. Title: 
An Actor-centric Causality Graph for Asynchronous Temporal Inference in Group Activity

#### 2. Authors: 
Zhao Xie, Tian Gao, Kewei Wu, Jiao Chang

#### 3. Affiliation: 
合肥工业大学知识工程与大数据实验室

#### 4. Keywords: 
Group activity recognition, causality relation, asynchronous temporal features, actor-centric causality graph

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究组活动识别中因果关系建模的挑战。因果关系描述了中心演员（效果演员）受其相关演员（原因演员）的影响。现有的图模型大多关注于学习同步时间特征下的演员关系，这对于处理异步时间特征下的因果关系是不足够的。本文提出了一个Actor-Centric Causality Graph Model，通过三个模块学习异步时间因果关系，即异步时间因果关系检测模块、因果特征融合模块和因果关系图推理模块。
- (2):现有的方法主要使用外观特征和位置特征来学习演员之间的关系，而忽略了时间动态的影响。一些方法使用RNN和Transformer网络学习时间特征，但这些方法仍然忽略了异步时间特征对于因果关系的影响。本文提出的Actor-Centric Causality Graph Model通过分析两个演员的异步特征来学习因果关系，从而解决了异步时间特征下因果关系建模的问题。
- (3):本文提出的Actor-Centric Causality Graph Model通过分析两个演员的异步特征来学习因果关系。首先，通过自回归和相关回归来分析中心演员和相关演员之间的影响。其次，通过估计因果关系的时间延迟来同步两个动作特征，并使用通道融合来增强效果演员的特征。最后，通过将因果关系与外观关系和距离关系融合来描述节点，并学习边缘关系。本文提出的方法可以学习异步关系，这对于同步关系学习是有补充作用的。
- (4):本文在Volleyball数据集和Collective Activity数据集上实现了最先进的性能。本文提出的方法可以学习异步关系，这对于同步关系学习是有补充作用的。
#### 7. 方法详细介绍：
本文提出了一种基于演员中心因果图的异步时间推理方法。该方法包括三个主要阶段：特征提取、图建模和预测。在特征提取阶段，使用Inception-v3模型提取帧序列的特征，并使用RoIAlign提取演员边界框的特征。在图建模阶段，使用基于Transformer的时间编码器和空间编码器学习空间-时间图。在预测阶段，使用组解码器和具有两个FC层的分类器预测演员动作得分和组活动得分。损失函数考虑演员动作识别损失、组活动识别损失和关系对比损失。该方法还引入了一个异步时间因果检测模块，以学习演员之间的异步因果关系。因果关系通过分析两个演员的自回归和相关回归以及Granger因果检验的影响来估计。

#### 8. 实验设置：
本文在两个广泛采用的组活动数据集上进行了评估，包括排球数据集和集体活动数据集。使用多类分类准确率作为评价指标。对于特征提取，采用ImageNet预训练的Inception-v3作为骨干网络。使用RoIAlign提取每个边界框的演员特征。使用FC层将特征嵌入1024个通道。基础模型在组编码器中采用群组令牌数K = 8。空间编码器和时间编码器使用8个注意力头。在演员中心因果图中，窗口大小m设置为4，时间延迟集合为{0,1,2}，因果阈值τ为0.9，通道比率参数d为6。对于因果关系图推断，外观图数量k为16，距离比率集合λs为{0.1,0.2,0.3,0.4}。

#### 9. 实验结果和分析：
本文提出的演员中心因果图方法在两个数据集上的实验结果表明，该方法优于现有方法。通过分析演员之间的影响，学习异步因果关系，这是同步关系无法学习的。异步因果关系与基于Transformer的模型学习的同步关系相互补充。将两种关系模型集成可以实现最佳性能。组活动中因果关系的可视化也证明了该方法的有效性。


# Paper:435     EXCALIBUR：鼓励和评估具有体现探索能力的机器人



#### 1. Title: 
EXCALIBUR Encouraging and Evaluating Embodied Exploration

#### 2. Authors: 
Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Wei

#### 3. Affiliation: 
第一作者：卡内基梅隆大学（Carnegie Mellon University）

#### 4. Keywords: 
Embodied AI, exploration, natural language inquiries, benchmark

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2021_paper.html  Github: None

#### 6. Summary: 
- (1):本文研究背景是机器学习模型通常是通过被动观察静态数据集来获得知识，而人类则通过有意识的实验来获得对物理世界的认识。
- (2):过去的方法主要集中在完成有目的的任务，如导航到指定的GPS坐标、定位指定类别的对象等，而本文提出的方法则鼓励机器人进行自由探索，通过自然语言查询来检验其对物理世界的理解。本文的方法具有很好的动机。
- (3):本文提出了一个新的基准测试EXCALIBUR，它允许机器人在环境中进行长时间的探索，并通过自然语言查询来检验其对物理世界的理解。这个基准测试具有以下新的特点：鼓励开放式探索、提供丰富的交互行为空间、提供更复杂的自然语言查询、需要长期规划和推理、评估机器人改进其对环境的理解的能力。本文的方法在基准测试中取得了很好的表现。
- (4):本文的方法在EXCALIBUR基准测试中取得了很好的表现，证明了其在鼓励机器人进行自由探索和理解物理世界方面的有效性。
#### 7. 方法详细介绍：
本文使用EXCALIBUR任务作为评估探索能力的基准，任务包括四个阶段：探索、问答、重新进入和精细问答。作者使用MANIPULATHOR臂代理进行实验，并使用循环神经网络（RNN）将语言指令、历史观察和动作编码为信念状态，作为行动者-评论家策略头和问答模块的输入。作者使用了三种训练信号：基于覆盖率的奖励、问答奖励和问答交叉熵损失。作者还使用了两种不同的视觉特征提取器：预训练的CLIP ResNet50模型和在训练场景上微调的MaskRCNN模型，用于代理的自我中心RGB观察。

#### 8. 实验设置：
本文提出了一个新的基准EXCALIBUR，用于评估具有探索能力的代理。基准要求代理在虚拟家庭中探索和操作物体。作者使用了强大的基线模型，这些模型使用了问答奖励和鼓励探索的新颖性奖励进行训练。作者将这些模型的性能与人类在沉浸式环境中的表现进行了比较。

#### 9. 实验结果和分析：
作者在两个测试集上评估了他们的代理：10,000个程序生成的PROCTHOR测试场景和人类设计的ARCHITECTHOR测试房屋。结果显示，Novelty+QA代理在AI系统中表现最佳，而人类在两种实验条件下都优于AI系统。在Human w/ replay条件下，AI和人类的表现差距更小，这表明记忆是人类的一个重要瓶颈。在App. H中可以找到更多结果和描述性指标的分析。


# Paper:436     面部胡须属性学习的逻辑一致性和更强描述能力



#### 1. Title: 
Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning

#### 2. Authors: 
Haiyu Wu, Grace Bezold, Aman Bhatta, Kevin W. Bowyer

#### 3. Affiliation: 
University of Notre Dame（圣母大学）

#### 4. Keywords: 
Facial hair, face recognition, logical consistency, attribute classification

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Logical_Consistency_and_Greater_Descriptive_Power_for_Facial_Hair_Attribute_CVPR_2020_paper.html  Github: https://github.com/HaiyuWu/LogicalConsistency

#### 6. Summary : 
- (1):本文研究面部属性分类中的胡须属性，提出了一种更加描述性的胡须注释方案，并创建了一个新的面部属性数据集FH37K。同时，本文指出以往的研究在逻辑一致性和完整性方面存在问题。

- (2):以往的研究中，面部属性分类器可能会将一张图像同时分类为无胡须和山羊胡（一种胡须类型），这种分类不具有逻辑一致性。本文提出了一种逻辑一致的预测损失函数LCPLoss，以帮助学习属性之间的逻辑一致性，并提出了标签补偿训练策略，以消除相关属性集中没有正预测的问题。

- (3):本文提出了一种更加描述性的面部属性注释方案，用于描述面部胡须的面积、长度和连通性等维度。同时，本文提出了一种逻辑一致的预测损失函数LCPLoss和标签补偿训练策略，以提高属性分类器的逻辑一致性。本文还研究了胡须对面部识别准确性的影响，包括不同人口统计学特征的变化。本文的贡献包括：定义了更加丰富的面部胡须属性，提出了逻辑一致的预测损失函数和标签补偿训练策略，研究了胡须面积对面部识别准确性的影响。

- (4):本文的方法在FH37K数据集上进行了测试，研究了胡须对面部识别准确性的影响。结果表明，胡须的相似性和差异性对冒名顶替和真实得分分布有重要影响。本文提出的方法在逻辑一致性和面部属性描述方面具有创新性，取得了较好的性能表现，支持了本文的研究目标。
#### 7. 方法详细介绍：
本文提出了一种新的面部毛发注释方案，并将其应用于创建一个新的面部毛发属性数据集FH37K。该数据集包括22个属性，分为三个维度：面部毛发区域、长度和连通性。数据集是通过手动注释CelebA和WebFace260M的图像创建的，并使用分类器生成了具有不足正例的附加图像。使用ResNet50作为训练的骨干网络，包括从头开始训练和使用预训练的ImageNet权重。本文还引入了一种新的损失函数LCPLoss，它考虑了属性之间的逻辑关系，以确保逻辑一致的预测。该损失函数与二元交叉熵损失和标签补偿策略相结合，以提高性能。

#### 9. 实验结果和分析：
本文的实验结果表明，具有相同胡须区域的图像对具有更高的相似度分数，而具有较大胡须区域差异的图像对具有较低的相似度分数。不同人口统计学群体的每种面部毛发区域类型的图像比例差异很大，AM中超过75％的图像是干净剃须，而侧面胡须区域不到6％。使用基线BCELoss和处理不平衡数据的方法训练的分类器难以预测逻辑一致的标签。本文引入的LCPLoss和标签补偿策略使模型学习更具逻辑一致性的预测并强制执行预测的一致性。该方法不仅适用于面部毛发属性预测，而且应该适用于属性预测的其他领域。干净剃须、下巴胡须或侧面胡须的图像对之间相似度分数变化的模式在亚洲男性、印度男性、黑人男性和白人男性之间呈现不同趋势，这表明面部发型在广泛评论的面部识别准确性的人口统计学差异中起着微妙的因果作用。

#### 全文总结：
本文提出了一种新的面部毛发注释方案，并创建了一个新的面部毛发属性数据集FH37K。引入了一种新的损失函数LCPLoss，它考虑了属性之间的逻辑关系，以确保逻辑一致的预测。本文的实验结果表明，面部毛发对面部识别准确性有影响，不同人口统计学群体的面部毛发区域类型的图像比例差异很大。本文的方法不仅适用于面部毛发属性预测，而且应该适用于属性预测的其他领域。


# Paper:437     量子多模型拟合



#### 1. Title: 
Quantum Multi-Model Fitting

#### 2. Authors: 
Matteo Farina, Luca Magri, Willi Menapace, Elisa Ricci, Vladislav Golyanik, Federica Arrigoni

#### 3. Affiliation: 
第一作者：Trento大学

#### 4. Keywords: 
Quantum computing, Adiabatic Quantum Computers, Multi-model fitting, Geometric model fitting, Computer vision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Farina_Quantum_Multi-Model_Fitting_CVPR_2021_paper.html  Github: https://github.com/FarinaMatteo/qmmf

#### 6. Summary : 
- (1):本文研究了几何模型拟合中的多模型拟合问题，提出了一种基于量子计算的方法。量子计算的优势在于可以利用量子并行性，同时处理多个状态，从而加速计算。本文的创新点在于将多模型拟合问题转化为可以在现代绝热量子计算机上高效采样的问题，并提出了一种迭代和分解版本的方法，支持实际规模的问题。
- (2):过去的方法主要集中在单一模型的拟合上，而本文提出的方法可以处理多个模型的情况。本文提出的方法可以在现代绝热量子计算机上高效采样，而不需要松弛目标函数。本文还提出了一种迭代和分解版本的方法，支持实际规模的问题。本文的方法在多个数据集上进行了实验评估，并取得了有希望的结果。
- (3):本文提出了一种基于量子计算的多模型拟合方法，可以在现代绝热量子计算机上高效采样。本文的方法可以处理多个模型的情况，并提出了一种迭代和分解版本的方法，支持实际规模的问题。本文的方法在多个数据集上进行了实验评估，并取得了有希望的结果。
- (4):本文的方法在多个数据集上进行了实验评估，并取得了有希望的结果。本文的方法可以处理多个模型的情况，并提出了一种迭代和分解版本的方法，支持实际规模的问题。本文的方法可以在现代绝热量子计算机上高效采样，而不需要松弛目标函数。
#### 7. 方法详细介绍：
本文提出了一种量子多模型拟合（MMF）的方法，称为QUMF。该方法将MMF问题定义为一个优化函数，并将其与标准QUBO公式对齐，以实现量子优化。该方法利用现代绝热量子计算机在优化组合QUBO目标时的优势，以解释具有多个和不相交几何模型的数据。该方法还提出了一种迭代和分解版本DEQUMF，可以处理实际问题的规模。

具体步骤如下：
1. 将MMF问题转化为一个优化问题，其中每个模型都有一个二进制变量表示其是否存在。
2. 定义一个优化函数，它最大化了所有模型之间的一致性，并将其转化为QUBO公式。
3. 利用量子计算机在qubit空间中直接优化目标函数，以实现全局最优解。
4. 对于大规模问题，使用DEQUMF方法，该方法通过迭代地修剪偏好矩阵P的列并减少问题的维数来处理问题。

#### 8. 实验设置：
本文在合成和真实数据集上进行了广泛的实验评估。合成数据集使用Star5数据集，真实数据集包括AdelaideRMF数据集、Hopkins基准测试和York Urban Line Segment数据库。本文选择的度量标准是误分类错误率，即错误分类点的百分比。比较主要集中在经典RANSACOV和混合量子-经典HQC-RF之间，这些方法与本文提出的方法最相关。量子实验在DWave Advantage System 4.1上运行，该系统由DWave构建，采用Pegasus拓扑结构，包含约5000个物理qubit。

#### 9. 实验结果和分析：
本文提出的量子多模型拟合（MMF）方法在各种数据集上表现出有希望的结果。该方法相对于HQC-RF方法更加鲁棒，尤其是在更高的异常值比率下。对于单模型序列的AdelaideRMF数据集，该方法的误分类错误率低于HQC-RF方法。本文还提供了AdelaideRMF数据集中20%异常值的书籍图像对的最坏情况下的定性结果。该方法实现了8.49%的误差，而HQC-RF方法将所有点都分类为异常值。本文还讨论了该方法的局限性，包括没有与误差界配对的解决方案以及DEQUMF的分解策略可能会丢弃全局最优模型的问题。


# Paper:438     逐步优化的局部辐射场用于鲁棒视角合成



#### 1. Title: 
Progressively Optimized Local Radiance Fields for Robust View Synthesis

#### 2. Authors: 
Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min H. Kim, Johannes Kopf

#### 3. Affiliation: 
Andreas Meuleman: KAIST (韩国科学技术院)

#### 4. Keywords: 
Radiance fields, view synthesis, camera pose estimation, progressive optimization, local radiance fields

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Meuleman_Progressively_Optimized_Local_Radiance_Fields_for_Robust_View_Synthesis_CVPR_2021_paper.html  Github: https://github.com/localrf/localrf

#### 6. Summary : 
- (1):本文研究了从单个随意拍摄的视频中重建大规模场景的辐射场的算法。该任务面临两个核心挑战：第一，大多数现有的辐射场重建方法依赖于来自结构运动算法的准确预估的相机姿态，这在野外视频中经常失败；第二，使用具有有限表示能力的单个全局辐射场不适用于在无界场景中的更长轨迹。 
- (2):现有的辐射场估计技术通常假定预先已知准确的姿态（通常在辐射场优化期间固定）。然而，在实践中，必须使用单独的方法（例如SfM）来估计相机姿态。本文提出了一种联合姿态和辐射场估计方法。我们的方法通过绘制古典的增量SfM算法和基于关键帧的SLAM系统的灵感来设计。我们的方法的核心是使用重叠的局部辐射场逐步处理视频序列。我们逐步估计输入帧的姿态，同时更新辐射场。为了模拟大规模无界场景，我们动态实例化本地辐射场。 
- (3):本文提出了一种新的方法，用于重建大规模场景的辐射场。我们通过逐步估计相机姿态和辐射场来设计我们的方法，从而显着提高了鲁棒性。我们展示了多个重叠的局部辐射场如何提高视觉质量并支持建模大规模无界场景。 
- (4):我们的方法在TANKS AND TEMPLES数据集和我们收集的户外数据集STATIC HIKES上进行了广泛的评估，表明我们的方法与最先进的方法相比具有优势。
#### 7. 方法详细介绍：
本文提出了一种逐步优化的方案，通过对输入视频进行逐帧处理，逐步更新辐射场和相机姿态。优化过程估计相机姿态和一系列局部辐射场的参数。局部辐射场由时间窗口内的有限数量的输入帧进行监督。本文使用TensoRF进行体渲染和场参数化，类似于Mip-NeRF360的收缩。本文还引入了其他损失，如邻帧单眼深度和光流，以提高优化稳定性。本文使用RAFT估计帧间光流，使用DPT估计每帧单眼深度。

具体步骤如下：
1. 初始化五个相机姿态为单位矩阵和一个初始的TensoRF模型。
2. 通过逐帧处理输入视频，逐步更新辐射场和相机姿态。
3. 优化过程中，估计相机姿态和局部辐射场的参数。
4. 局部辐射场由时间窗口内的有限数量的输入帧进行监督。
5. 使用TensoRF进行体渲染和场参数化，类似于Mip-NeRF360的收缩。
6. 引入其他损失，如邻帧单眼深度和光流，以提高优化稳定性。
7. 使用RAFT估计帧间光流，使用DPT估计每帧单眼深度。

#### 8. 实验设置：
本文在Tanks and Temples数据集和Static Hikes数据集上评估了所提出的方法。Static Hikes数据集包含使用四个消费级相机拍摄的12个户外场景，这些场景具有现有视图合成数据集未涵盖的新挑战。

#### 9. 实验结果和分析：
所提出的方法显著提高了在具有挑战性的场景下的辐射场重建的鲁棒性和保真度，这在两个数据集上得到了验证。在Tanks and Temples数据集上，所提出的方法LocalRF在优化姿态时表现出了与其他方法相当的结果。在Static Hikes数据集上，本文还表明，所提出的方法始终比自校准方法BARF获得更好的结果。消融研究表明，逐步优化和局部辐射场对于获得所提出的方法的结果都是必要的。


# Paper:439     通过可微非线性最小二乘学习对应不确定性



#### 1. Title: 
Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares

#### 2. Authors: 
Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers

#### 3. Affiliation: 
TU Munich (慕尼黑工业大学)

#### 4. Keywords: 
uncertainty estimates, feature correspondences, camera pose, nonlinear least squares, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Muhle_Learning_Correspondence_Uncertainty_via_Differentiable_Nonlinear_Least_Squares_CVPR_2021_paper.html  Github: https://github.com/dominikmuhle/dnls_covs

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉中的相对位姿估计问题，即如何通过图像中的特征点对来估计两张图像之间的相对位姿。

- (2):过去的方法通常采用基础矩阵或本质矩阵来解决相对位姿估计问题，但这些方法没有考虑特征点对的质量，而且对于不同的特征提取器，特征点的噪声分布也不同。本文提出了一种基于深度学习和非线性最小二乘的方法，通过学习特征点位置的不确定性来提高相对位姿估计的准确性。

- (3):本文提出了一种对称的概率正常视线约束和一种通过不同iable非线性最小二乘（DNLS）来学习特征点位置不确定性的方法。通过对相对位姿误差的梯度进行反向传播，可以得到特征点位置的协方差矩阵。实验结果表明，本文提出的方法可以在不同的特征提取器上得到较好的效果。

- (4):本文在KITTI和EuRoC数据集上进行了实验，结果表明，本文提出的方法可以在相对位姿估计任务上取得比现有方法更好的效果。
#### 7. 方法详细介绍：
本文提出了一种通过可微非线性最小二乘法学习对应不确定性的方法。该方法涉及使用概率非线性最小二乘法估计图像对中每个关键点的协方差矩阵。协方差估计通过最小化估计旋转与真实旋转之间的旋转误差来学习。该方法还包括一个锚点损失项来规范估计值。最终损失是循环一致性损失和锚点损失的组合。该方法使用Theseus和ceres优化库实现。

#### 8. 实验设置：
本文在合成和真实数据集上评估了所提出的方法。在合成实验中，通过直接过度拟合协方差估计来研究梯度学习潜在噪声分布的能力。在真实实验中，使用梯度来训练网络以预测不同关键点检测器的图像噪声分布。在KITTI和EuRoC数据集上，使用SuperPoint和Basalt KLT-Tracks特征描述符评估了学习到的协方差的性能。实验使用SuperPoint和Basalt KLT-Tracks特征描述符实现。

#### 9. 实验结果与分析：
所提出的方法在KITTI数据集上表现出比非概率算法和使用SuperGlue置信度的加权NEC-LS方法更好的性能。学习到的不确定性能够很好地推广并显著提高相对姿态估计。实验还表明，该方法可以通过遵循最小化旋转误差的梯度从嘈杂的数据中学习正确的分布。


# Paper:440     基于球面变换器的LiDAR三维识别



#### 1. Title: 
Spherical Transformer for LiDAR-based 3D Recognition

#### 2. Authors: 
Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, Jiaya Jia

#### 3. Affiliation: 
第一作者：香港中文大学

#### 4. Keywords: 
LiDAR, point cloud recognition, varying-sparsity distribution, SphereFormer, radial window self-attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lai_Spherical_Transformer_for_LiDAR-Based_3D_Recognition_CVPR_2021_paper.html  Github: https://github.com/dvlab-research/SphereFormer.git

#### 6. Summary : 
- (1):本文研究基于LiDAR的3D点云识别，针对LiDAR点分布的不均匀性，提出了一种新的模块SphereFormer，用于直接从密集的近点聚合信息到稀疏的远点，以解决信息断开和有限的感受野问题。

- (2):现有的方法大多数没有特别考虑到LiDAR点云的不均匀分布，这导致了信息断开和有限的感受野问题，特别是对于稀疏的远点。本文提出的方法可以直接聚合远距离的信息，避免了信息断开问题，同时可以显著提高远距离点的性能。

- (3):本文提出了一种基于球面坐标系的模块SphereFormer，将3D空间表示为球面坐标系，将场景划分为多个非重叠的狭长窗口，设计了径向窗口自注意力机制，可以克服信息断开问题，扩大感受野，从而显著提高稀疏远点的性能。此外，为了适应狭长的径向窗口，本文提出了指数分裂来获得细粒度的位置编码，并提出了动态特征选择来增加模型的表示能力。

- (4):本文在nuScenes和SemanticKITTI语义分割基准测试中均取得了第一名，分别为81.9%和74.8%的mIoU。在nuScenes目标检测基准测试中，取得了第三名，NDS为72.8%，mAP为68.5%。本文提出的方法可以有效地解决稀疏远点的问题，取得了很好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为SphereFormer的方法，用于基于LiDAR的三维识别。该方法使用球坐标系表示LiDAR点云，并沿着θ和ϕ轴将3D空间分区以获得非重叠的径向窗口。在每个窗口内独立进行多头自注意力计算，得到的模型可以应用于各种下游任务，如语义分割和目标检测，并在实验中表现出强大的性能。该方法还包括位置编码和动态特征选择，以解决LiDAR点云的变稀疏性质。

#### 8. 实验设置：
本文使用了三个数据集，nuScenes、SemanticKITTI和Waymo Open Dataset，并为每个数据集设置了不同的窗口大小和体素大小。本文还使用了OpenPCDet代码库，并遵循默认的CenterPoint设置了用于目标检测的训练超参数。学习率和权重衰减分别设置为0.006和0.01。在nuScenes上，批量大小设置为16，在SemanticKITTI和Waymo Open Dataset上均设置为8。在数据预处理期间，将输入场景限制在每个数据集的特定范围内。

#### 9. 实验结果和分析：
本文提出的方法在nuScenes和SemanticKITTI语义分割基准测试中均取得了最先进的结果，分别为81.9％和74.8％的mIoU。在nuScenes目标检测基准测试中，该方法排名第三，NDS为72.8％，mAP为68.5％。该方法显著提高了远距离点的性能（即+17.1％mIoU）。本文还提供了基线模型和提出方法之间的视觉比较，这些比较表明，提出的方法可以识别更多的稀疏远距离物体。


# Paper:441     ML)2P-Encoder：探索通道-类别相关性用于多标签零样本学习



#### 1. Title: 
ML)2P-Encoder: On Exploration of Channel-class Correlation for Multi-label Zero-shot Learning

#### 2. Authors: 
Ziming Liu, Song Guo, Xiaocheng Lu, Jingcai Guo, Jiewei Zhang, Yue Zeng, Fushuo Huo

#### 3. Affiliation: 
香港理工大学计算机系

#### 4. Keywords: 
Multi-label zero-shot learning, channel-class correlation, feature extraction, attention mechanism

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2021_paper.html  Github: https://github.com/simonzmliu/cvpr23_mlzsl

#### 6. Summary : 
- (1):本文研究多标签零样本学习（MLZSL）中的通道-类别相关性问题，提出了一种轻量级的多标签多层感知器编码器（ML）2P-Encoder，以提取和保留通道语义信息。 
- (2):先前的方法通常使用空间-类别相关性进行视觉-语义映射，这可能计算成本高昂，而且无法捕捉细粒度的类别特定语义。本文提出了通道-类别相关性的概念，提出了一种基于通道的方法，以提高模型的准确性和鲁棒性。 
- (3):本文提出了一种新的通道-类别相关性MLZSL框架（C3-MLZSL），该框架使用（ML）2P-Encoder提取通道语义信息，并使用全局组注意力模块建立不同类别之间的多标签特定类别关系。 
- (4):在大规模MLZSL基准测试中，包括NUS-WIDE和Open-Images-V4，本文的方法表现出优越性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了两种方法，分别是(ML)2P-Encoder和C3-MLZSL。(ML)2P-Encoder是一种用于多标签零样本学习的方法，首先使用预训练的特征提取网络提取图像特征，然后将特征分组并随机排列以生成特定的语义向量，以表达每个组中包含的语义信息。接下来，使用(ML)2P-Encoder增强通道之间的相关性，并提取和分析每个通道中包含的特征信息。使用多层感知器（MLP）对特征进行编码，并使用最大池化在空间域中过滤出最佳的语义向量。使用组注意力建立相互信息，并找到不同标签之间的相似响应。损失函数由两部分组成，即ranknet损失和正则化项，以最小化生成的语义向量之间的差距和减少差异。
C3-MLZSL是一种探索通道-类别相关性的多标签零样本学习框架，包括特征提取网络（例如VGG19）、前向金字塔模块、(ML)2P-Encoder和全局组注意力模块。生成的特征图被重新组织成几个组，每个组可以使用(ML)2P-Encoder独立训练。通过组注意力将所有组生成的语义信息关联起来，生成最终的语义矩阵S。

#### 8. 实验设置：
本文在两个大规模数据集NUS-WIDE和Open-Images-V4上进行了实验。NUS-WIDE包含269,648张图像和81个概念，而Open-Images-V4包含1,743,042张图像和19,997个概念。实验在一台服务器上进行，该服务器配备Intel Xeon E5-2630 v4 CPU、128GB内存和NVIDIA Tesla P100 GPU。使用PyTorch实现了提出的框架。

#### 9. 实验结果和分析：
本文提出的框架在NUS-WIDE和Open-Images-V4数据集上与其他最先进的模型进行了比较。评估指标包括每类准确率、每类F1分数和每类F1分数的调和平均数。实验结果表明，与其他最先进的模型相比，本文提出的框架在NUS-WIDE和Open-Images-V4数据集上均取得了优异的性能。在NUS-WIDE上，本文提出的框架的每类F1分数为0.634，每类F1分数的调和平均数为0.408，在Open-Images-V4上，每类F1分数为0.202，每类F1分数的调和平均数为0.102。


# Paper:442     半监督语义分割中弱到强一致性的再探索



#### 1. Title: 
Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation

#### 2. Authors: 
Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, Yinghuan Shi

#### 3. Affiliation: 
第一作者：南京大学

#### 4. Keywords: 
Semi-supervised learning, semantic segmentation, consistency regularization, perturbation, contrastive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html  Github: https://github.com/LiheYoung/UniMatch

#### 6. Summary : 
- (1):本文研究半监督语义分割中的弱到强一致性框架，该框架通过弱扰动图像的预测作为强扰动版本的监督，已经在半监督分类中得到了广泛应用。本文旨在探索该框架在语义分割中的应用，以及如何进一步扩展扰动空间，提高模型性能。

- (2):过去的方法包括基于GAN的对抗训练和一致性正则化框架等，但这些方法存在一些问题，如对强扰动的限制和对图像级扰动的依赖。本文提出了一个统一的扰动框架，将图像级和特征级扰动分别应用于原始图像和提取的特征，以扩展扰动空间。此外，本文还提出了双流扰动策略，以充分利用预定义的图像级扰动空间，并利用对比学习的优点来学习更具有区分性的表示。

- (3):本文提出的方法是在FixMatch框架的基础上进行改进的，包括统一扰动框架和双流扰动策略。实验结果表明，该方法在Pascal、Cityscapes和COCO数据集上均取得了显著的性能提升，并在医学图像分析和遥感解释方面也表现出了优异的性能。

- (4):本文提出的方法在半监督语义分割任务中取得了显著的性能提升，超过了现有方法，并在医学图像分析和遥感解释方面也表现出了优异的性能。该方法的创新点在于扩展扰动空间和利用对比学习的优点来学习更具有区分性的表示。
#### 7. 方法详细介绍：
本文提出了一种基于FixMatch框架的半监督语义分割方法，称为Unified Dual-Stream Perturbations (UniMatch)。该方法利用弱到强的一致性正则化和自训练管道为未标记的图像分配伪掩码。UniMatch方法引入了统一扰动和双流扰动两个组件，以构建更广泛的扰动空间并充分探索图像级强扰动。统一扰动在弱扰动图像的特征上注入扰动，而双流扰动则独立地从弱扰动图像产生双流扰动。无监督损失是图像和特征级扰动的组合。该方法在多个基准数据集上实现了最先进的性能。

#### 8. 实验设置：
本文在Pascal、Cityscapes和COCO基准测试集上比较了UniMatch方法与现有最先进方法的性能，并在遥感解释和医学图像分析中进行了实验。作者使用的是DeepLabv3+和ResNet-50模型，优化器为SGD，学习率为0.01，学习率调度器为poly。作者还提供了实现细节，包括分割模型、mini-batch组成、颜色变换、训练分辨率等。

#### 9. 实验结果和分析：
本文在UniPerb方法的各种特征扰动策略的有效性上进行了消融研究。结果表明，通道丢失效果最佳。作者还对双流扰动的必要性进行了消融研究，与增加未标记批次大小或训练时长相比，双流扰动策略在大多数情况下都表现出更好的性能。此外，作者还比较了图像级和特征级扰动流的数量对性能变化的影响。结果表明，增加扰动流并不一定会导致更高的性能。最后，作者在WHU-CD和LEVIR-CD数据集上展示了UniMatch方法的最佳性能。


# Paper:443     学习骨架动作识别的判别表示



#### 1. Title: 
Learning Discriminative Representations for Skeleton Based Action Recognition

#### 2. Authors: 
Huanyu Zhou, Qingjie Liu, Yunhong Wang

#### 3. Affiliation: 
北京航空航天大学虚拟现实技术与系统国家重点实验室

#### 4. Keywords: 
Skeleton-based action recognition, Graph Convolutional Networks, Feature refinement, Contrastive learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Learning_Discriminative_Representations_for_Skeleton_Based_Action_Recognition_CVPR_2021_paper.html  Github: https://github.com/zhysora/FR-Head

#### 6. Summary : 
- (1):本文研究基于骨架的动作识别，骨架表示比其他模态（如RGB帧）更高效和稳健，但是在使用骨架数据时，一些重要的线索（如相关物品）也被丢弃，导致一些模糊的动作难以区分，容易被错误分类。
- (2):过去的方法主要是使用GCN从骨架中提取特征，但是这些方法忽略了一些重要的线索，导致一些模糊的动作难以区分。本文提出了一种特征细化模块，使用对比学习来提高骨架特征的区分度，可以动态地发现和校准模糊的样本。此外，该特征细化模块可以嵌入到多种GCN模型中，以提高层次化特征学习。
- (3):本文提出了一种特征细化模块，使用对比学习来提高骨架特征的区分度，可以动态地发现和校准模糊的样本。此外，该特征细化模块可以嵌入到多种GCN模型中，以提高层次化特征学习。
- (4):在NTU RGB+D、NTU RGB+D 120和NW-UCLA数据集上进行了广泛的实验，结果表明，该方法在模糊动作的识别上取得了显著的改进，并且可以帮助区分那些模糊的样本。
#### 7. 方法详细介绍：
本文提出的方法基于由一系列时间卷积神经网络（TCNs）和图卷积网络（GCNs）构成的骨架网络，称为TGN。模型的输入是一个形状为T×V×3的骨架序列。骨架序列经过骨架提取和对齐后，被送入TGN网络中进行特征提取。TGN网络可以被任何其他基于GCN的网络替换。本文提出的方法包括三个主要组成部分：多级特征选择、时空解耦和对比特征细化。多级特征选择将骨干网络分为四个阶段，并在每个阶段上施加一个特征细化头（FR Head）。时空解耦同时挖掘空间和时间信息，以提高动作表示的区分能力。对比特征细化采用对比学习的方式进行特征细化，包括自信样本聚类、模糊样本发现和模糊样本校准。完整的学习目标函数将交叉熵（CE）损失与多级对比学习损失相结合。

#### 8. 实验设置：
本文在三个数据集上进行了评估：NTU RGB+D、NTU RGB+D 120和NW-UCLA。NTU RGB+D数据集包含40个受试者执行的56,880个动作样本，NTU RGB+D 120数据集是NTU RGB+D的一个子集，包含120个动作类别。NW-UCLA数据集包含10个志愿者执行的1,000个动作样本。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2630 v4 CPU和NVIDIA Tesla V100 GPU。

#### 9. 实验结果和分析：
本文提出的方法在三个数据集上均取得了竞争性的结果。在NTU RGB+D数据集上，本文提出的方法的准确率为95.1%，优于先前的最先进方法0.5%。在NTU RGB+D 120数据集上，本文提出的方法的准确率为87.5%，优于先前的最先进方法0.6%。在NW-UCLA数据集上，本文提出的方法的准确率为91.5%，优于先前的最先进方法1.5%。结果表明，本文提出的方法具有显著的改进和区分模糊样本的能力。


# Paper:444     基于物理先验的多范围时间对齐网络的视频去雾



#### 1. Title: 
Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior

#### 2. Authors: 
Jiaqi Xu, Xiaowei Hu, Lei Zhu, Qi Dou, Jifeng Dai, Yu Qiao, Pheng-Ann Heng

#### 3. Affiliation: 
第一作者：香港中文大学

#### 4. Keywords: 
Video dehazing, physical prior, multi-range temporal alignment, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Video_Dehazing_via_a_Multi-Range_Temporal_Alignment_Network_With_Physical_CVPR_2021_paper.html  Github: https://github.com/jiaqixuac/MAP-Net

#### 6. Summary : 
- (1):本文研究视频去雾问题，旨在恢复具有高可见度和对比度的无雾帧。视频去雾需要利用时间线索，如高度相关的雾厚度和光照条件，以及移动的前景对象和背景。 
- (2):现有的视频去雾方法存在一些局限性，这些方法要么从基于物理模型的组件估计中获得无雾帧，要么忽略了嵌入在雾成像模型中的显式物理先验。这些方法通过使用输入/特征堆叠或帧对帧对齐在本地滑动窗口中聚合时间信息，难以获得全局和长程时间信息。本文提出了一种新的视频去雾框架，通过多范围时间对齐网络与物理先验（MAP-Net）有效地探索物理雾先验并聚合时间信息。 
- (3):本文设计了一种基于记忆的物理先验引导模块，旨在将物理先验注入到场景辐射恢复中。具体而言，我们根据物理模型进行特征分离，使用两个解码器，其中一个估计传输和大气光，另一个恢复场景辐射。从第一个解码器提取的特征被利用作为物理雾先验，它被整合到第二个解码器中进行场景辐射恢复。为了在长时间视频中推断全局物理先验，我们设计了物理先验令牌存储器，将先验相关特征有效地编码为紧凑的令牌，以进行高效的内存读取。其次，我们引入了多范围场景辐射恢复模块，以捕获多个空间时间范围内的空间时间依赖关系。该模块首先将相邻帧分成多个范围，然后对应地对应于递归范围特征，最后恢复场景辐射。与CG-IDN不同，它将相邻帧的特征对齐到具有不同范围的多个集合中，这有助于在各种时间间隔中探索时间雾线索。我们进一步设计了空间时间可变形注意力来将多个范围的特征变形到目标帧，随后是引导的多范围补充信息聚合。此外，我们使用无监督的流损失来鼓励网络关注对齐区域，并以端到端的方式训练整个网络。 
- (4):本文构建了一个大规模的室外视频去雾基准数据集HazeWorld，其中包含各种真实世界场景的视频。在合成和真实条件下的各种实验结果表明了我们提出的方法的优越性。
#### 7. 方法详细介绍：
本文提出了一种名为MAP-Net的视频去雾框架。该框架包括三个主要组件：基于物理模型的先验解码器、场景解码器和带物理先验的多范围时间对齐网络。先验解码器和场景解码器分别用于解耦物理模型和场景特征。多范围时间对齐网络用于在先验特征的指导下对齐和聚合多范围特征。整体损失函数包括输出损失、物理模型解耦损失和流损失。

#### 8. 实验设置：
本文使用了两个数据集进行评估：HazeWorld和REVIDE。HazeWorld包含3,588个训练视频和1,496个测试视频，而REVIDE包含42个训练视频和6个测试视频。评估指标为PSNR和SSIM。使用AdamW优化器和多项式调度器，初始学习率为2×10−4。批量大小为8，输入视频帧的补丁大小为256×256。损失函数中的权重λphy和λflow分别设置为0.2和0.04。

#### 9. 实验结果与分析：
本文提出的方法在HazeWorld和REVIDE数据集上均优于现有方法，PSNR和SSIM指标均有所提高。在HazeWorld上，本文方法相对于BasicVSR++的PSNR提高了1.06 dB，相对于DehazeFormer的SSIM提高了0.0063。在REVIDE上，本文方法进一步将PSNR从23.63 dB提高到24.16 dB，将SSIM从0.8925提高到0.9043。视觉结果也表明，本文方法可以有效地去除雾霾并保留更多细节，相对于其他方法具有更好的效果。


# Paper:445     VoxFormer：基于相机的3D语义场景完成的稀疏体素Transformer



#### 1. Title: 
VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion

#### 2. Authors: 
Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao, Jose M. Alvarez, Sanja Fidler, Chen Feng, Anima Anandkumar

#### 3. Affiliation: 
第一作者：Yiming Li，NYU

#### 4. Keywords: 
3D scene completion, semantic segmentation, camera-based perception, Transformer, self-attention

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_VoxFormer_Sparse_Voxel_Transformer_for_Camera-Based_3D_Semantic_Scene_Completion_CVPR_2021_paper.html  Github: https://github.com/NVlabs/VoxFormer

#### 6. Summary : 
- (1):本文研究的是基于相机的3D语义场景完成，旨在从2D图像中输出完整的3D体积语义。这是自动驾驶领域中的一个重要问题，因为它可以帮助车辆实现全面的场景理解，从而促进规划和地图构建等下游任务的实现。

- (2):现有的SSC方法通常依赖于3D输入，如LiDAR点云。最近的MonoScene研究了从单目图像进行语义场景完成，但2D到3D特征投影容易为未占用的3D位置引入虚假特征，而重量级的3D卷积会降低系统的效率。本文提出了一种新的基于Transformer的语义场景完成框架VoxFormer，采用两阶段设计，从深度估计的稀疏可见和占用体素查询开始，然后通过自注意力生成密集的3D体素。与MonoScene不同，VoxFormer考虑了3D到2D的交叉注意力来表示稀疏查询，这是因为2D图像上的视觉特征仅对可见场景结构而非遮挡或空白空间进行对应。本文的方法在几何和语义方面都优于现有的方法，并且在训练时将GPU内存减少到不到16GB。

- (3):本文提出了一种新的基于Transformer的语义场景完成框架VoxFormer，采用两阶段设计，从深度估计的稀疏可见和占用体素查询开始，然后通过自注意力生成密集的3D体素。具体来说，第一阶段采用轻量级的2D CNN-based查询提议网络，使用图像深度重建场景几何，然后从整个视野中的预定义可学习体素查询中提出稀疏的体素。第二阶段基于一种新颖的稀疏到密集的MAE-like架构，首先通过允许它们关注图像观察来加强所提出体素的特征化，然后将非提出体素与可学习的掩码令牌相关联，并通过自注意力处理完整的体素集以完成每个体素的语义分割。

- (4):本文的方法在SemanticKITTI数据集上进行了实验，取得了几何和语义方面的最新成果。与现有方法相比，本文的方法在安全关键的短距离区域的改进显著。本文的方法在几何和语义方面的相对改进分别为20.0%和18.1%。
#### 7. 方法详细介绍：
本文提出了一种名为 VoxFormer 的基于相机的三维语义场景完成的两阶段框架。第一阶段是类别无关的查询提议，它基于深度估计和占用预测从预定义的一组体素查询中选择体素查询。第二阶段是类别特定的分割，它使用一组稀疏的体素查询通过可变形注意力关注图像特征，然后通过自注意力处理所有体素。整体目标是学习一个神经网络，使其生成的语义体素网格尽可能接近真实值。具体步骤包括：
1. 阶段1：3D占用预测网络，以单个LiDAR扫描或立体图像作为输入，输出稀疏体素网格。
2. 阶段2：稀疏到密集变换器，以稀疏体素网格和RGB图像作为输入，输出带有语义标签的密集体素网格。
3. 使用可变形交叉注意力和可变形自注意力来捕捉稀疏体素网格和RGB图像之间的空间关系。
4. 使用加权交叉熵损失训练模型。

#### 8. 实验设置：
本文在SemanticKITTI数据集上评估了所提出的方法，该数据集为KITTI Odometry Benchmark的LiDAR扫描提供了密集的语义注释。数据集包含22个室外驾驶场景，目标感兴趣的体积为车辆前方51.2m，左右各25.6m，高度为6.4m。该体积的体素化导致一组3D体素网格，尺寸为256×256×32，标记有20个类别。模型的输入为多个RGB图像，包括时间信息。

#### 9. 实验结果和分析：
本文报告了所提出方法的实验性能，包括IoU、精度、召回率、mIoU和每类IoU。结果表明，所提出的方法在IoU、精度、召回率和mIoU方面优于现有方法，包括MonoScene、LMSCNet和SSCNet。所提出的方法实现了65.38%的IoU、76.54%的精度、81.77%的召回率和21.55%的mIoU。每类IoU显示，所提出的方法在大多数类别上表现良好，包括汽车、道路和建筑，但在低频类别（如自行车和摩托车）上表现不佳。


# Paper:446     任意尺度图像超分辨率的连续隐式注意力网络



#### 1. Title: 
Continuous Implicit Attention-in-Attention Network for Arbitrary-Scale Image Super-Resolution

#### 2. Authors: 
Jiezhang Cao, Qin Wang, Yongqin Xian, Yawei Li, Bingbing Ni, Zhiming Pi, Kai Zhang, Yulun Zhang, Radu Timofte, Luc Van Gool

#### 3. Affiliation: 
ETH Zurich (瑞士联邦理工学院)

#### 4. Keywords: 
Image super-resolution, arbitrary-scale, implicit attention, non-local information, deep neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cao_CiaoSR_Continuous_Implicit_Attention-in-Attention_Network_for_Arbitrary-Scale_Image_Super-Resolution_CVPR_2021_paper.html  Github: https://github.com/caojiezhang/CiaoSR

#### 6. Summary : 
- (1):本文研究的是图像超分辨率问题，特别是针对任意尺度的图像超分辨率问题。传统的方法需要针对不同的尺度训练不同的模型，而本文提出的方法可以通过一个模型实现任意尺度的图像超分辨率。
 
- (2):传统的图像超分辨率方法大多数采用局部特征集成的方式来预测新像素，但这种方法存在一些局限性：(i)它没有可学习的参数，忽略了视觉特征的相似性；(ii)它的感受野有限，不能集成重要的大范围相关特征。为了解决这些问题，本文提出了一种连续的隐式注意力网络，称为CiaoSR。我们明确设计了一个隐式注意力网络来学习附近局部特征的集成权重。此外，我们在这个隐式注意力网络中嵌入了一个尺度感知的注意力，以利用额外的非局部信息。本文的方法可以灵活地集成到任何骨干网络中，提高图像超分辨率的性能。

- (3):本文提出了一种新颖的连续隐式注意力网络，用于增强任意尺度图像超分辨率的性能。具体来说，我们使用我们的注意力来预测集成权重，同时考虑局部特征的相似性和坐标距离。基于这样的可学习权重，隐式模型可以根据不同的输入自适应地聚合局部特征。为了丰富更多信息，我们在我们的隐式注意力中引入了一个注意力机制，以帮助发现更大的感受野中的特征。本文的贡献总结如下：(i)我们提出了一种新颖的连续隐式注意力网络，用于任意尺度图像超分辨率，称为CiaoSR；(ii)我们的CiaoSR可以灵活地集成到任何骨干网络中，允许网络在任意尺度上进行超分辨率，并提高了图像超分辨率的性能；(iii)广泛的实验表明，CiaoSR在单图像超分辨率方法中显著优于现有方法。此外，CiaoSR在任意尺度SR任务上也实现了最先进的性能。最后，我们将我们的方法扩展到实际的SR设置中，以合成任意尺度的图像。(iv)本文的方法在任务和性能方面都取得了很好的表现，支持了他们的目标。


#### 7. 方法详细介绍：
本文提出了一种连续隐式注意力网络，称为CiaoSR，用于任意尺度图像超分辨率。该方法采用了显式设计的隐式注意力网络，用于学习附近局部特征的集成权重。此外，它在这个隐式注意力网络中嵌入了一个尺度感知的注意力，以利用额外的非局部信息。该方法可以灵活地集成到任何骨干网络中，以提高超分辨率性能。具体而言，该方法采用注意力机制从视觉信息和坐标信息中学习软权重。注意力机制基于潜在代码和坐标的相似性计算，可以使用查询和键计算，从而集成潜在代码和坐标信息。此外，该方法还提出了一个嵌入的尺度感知非局部注意力模块，以利用有用的非局部信息。非局部特征是通过从LR图像的潜在代码中获取查询和值特征，并将其下采样到尺度s'作为键特征来计算的。该方法可以集成多尺度特征并保证大的感受野，同时降低计算成本。

#### 8. 实验设置：
本文使用DIV2K作为训练集，包含800张2K分辨率的图像。使用PSNR评估合成的SR图像的质量，并提供其他评估指标，如SSIM和LPIPS。本文遵循先前工作的相同实验设置，并使用Adam作为优化器。每个GPU的批量大小为16，模型训练1000个epoch。学习率在开始时设置为1e-4，并在每200个epoch时衰减0.5倍。

#### 9. 实验结果和分析：
本文提出的CiaoSR方法在基准数据集上显著优于现有的具有相同骨干的单图像SR方法。此外，CiaoSR在任意尺度SR任务上实现了最先进的性能。该方法在真实世界SR设置上的有效性也得到了证明。该方法在尺度内和尺度外分布上具有良好的泛化性能。实验结果表明，CiaoSR方法在PSNR和SSIM指标上均优于其他方法，并且在模型大小和推理时间方面也优于其他方法。此外，与其他方法相比，该方法保留了更多的结构纹理和感知信息。


# Paper:447     PVO：全景视觉里程计



#### 1. Title: 
PVO: Panoptic Visual Odometry

#### 2. Authors: 
Weicai Ye, Xinyue Lan, Shuo Chen, Yuhang Ming, Xingyuan Yu, Hujun Bao, Zhaopeng Cui, Guofeng Zhang

#### 3. Affiliation: 
浙江大学CAD&CG国家重点实验室，浙江大学-商汤联合3D视觉实验室

#### 4. Keywords: 
Panoptic Visual Odometry, Visual Odometry, Video Panoptic Segmentation, Monocular Video

#### 5. Paper: https://zju3dv.github.io/pvo/  Github: https://github.com/zju3dv/PanopticVisualOdometry

#### 6. Summary : 
- (1):本文旨在通过单目视频实现对场景运动、几何和全景分割信息的更全面建模。
- (2):过去的方法主要有视觉里程计（VO）和视频全景分割（VPS），但是这些方法忽略了动态物体可能是静止的事实。本文提出了一种新的全景视觉里程计（PVO）框架，将VO和VPS任务统一起来，通过迭代优化相互增强，提高了场景中动态物体的处理能力。
- (3):本文提出了三个模块：图像全景分割模块、全景增强VO模块和VO增强VPS模块。其中，全景增强VO模块通过全景感知动态掩码减轻了动态物体对相机姿态估计的影响；VO增强VPS模块通过利用VO模块估计的相机姿态、深度和光流信息，实现了当前帧的全景分割结果与相邻帧的在线融合。这两个模块通过迭代优化相互增强，提高了VO和VPS的性能。
- (4):在VO和VPS任务上，本文的PVO方法在广泛的实验中均优于现有的最先进方法。
#### 7. 方法详细介绍：
本文提出了一种全景视觉里程计方法PVO，将全景分割和视觉里程计任务相结合，以全面建模场景。PVO由三个模块组成：全景分割模块、全景增强视觉里程计模块和视觉里程计增强全景分割模块。全景分割模块将单张图像作为输入，并输出图像的全景分割结果，该结果结合了语义分割和实例分割。全景增强视觉里程计模块通过将全景分割信息纳入考虑，滤除动态物体的干扰，从而获得更好的置信度估计和更准确的相机姿态。视觉里程计增强全景分割模块使用从视觉里程计获得的姿态、深度和光流信息，实现不同帧之间的特征跟踪和融合。该模块采用在线融合机制，可以有效解决多个物体遮挡的问题。通过循环迭代优化策略，可以提高视觉里程计和全景分割的性能。

#### 8. 实验设置：
对于视觉里程计，本文在三个具有动态场景的数据集上进行了评估：Virtual KITTI、KITTI和TUM RGBD动态序列。使用绝对轨迹误差（ATE）进行评估。对于视频全景分割，本文在Cityscapes和VIPER数据集上使用视频全景质量（VPQ）指标进行评估。

#### 9. 实验结果与分析：
PVO方法在大多数序列上优于DROID-SLAM，并在Virtual KITTI数据集的序列02上实现了竞争性能。在Cityscapes和VIPER数据集上，PVO方法通常优于VPSNet-FuseTrack和SiamTrack，VPQ得分更高。循环迭代优化策略可以提高VPS和VO模块的性能。


# Paper:448     多模态有助于单模态：利用多模态模型进行跨模态少样本学习



#### 1. Title: 
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models

#### 2. Authors: 
Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramanan

#### 3. Affiliation: 
第一作者：Zhiqiu Lin，卡内基梅隆大学计算机科学系

#### 4. Keywords: 
few-shot learning, cross-modal, multimodal, visual classification, audio classification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是few-shot learning，即如何在少量样本的情况下快速学习新任务。传统的few-shot学习基于单一模态的样本，但这些样本可能不足以描述整个概念类别。相比之下，人类使用跨模态信息来高效地学习新概念。本文旨在利用跨模态信息来提高视觉分类和音频分类的性能。

- (2):过去的方法大多数是基于单一模态的，如大型视觉模型或语言模型。本文提出了一种跨模态适应方法，将不同模态的样本视为额外的few-shot样本。通过利用最近的多模态基础模型（如CLIP），将不同模态映射到相同的表示空间中，我们实现了SOTA结果。本文的方法可以应用于现有的微调方法、适配器和分类器集成等。

- (3):本文提出了一种简单的跨模态适应方法，通过利用不同模态的few-shot样本来学习。具体来说，我们将类名重新用作附加的one-shot训练样本，使用一个简单的线性分类器进行视觉-语言适应。我们还构建了第一个音频视觉few-shot基准，并使用跨模态训练来提高图像和音频分类的性能。

- (4):本文的方法在多个数据集上均取得了SOTA结果，且使用的是比现有方法更轻量级的线性分类器。跨模态适应不仅可以提高准确性，而且可以利用问题定义中“隐藏”的附加训练示例。本文的方法可以应用于现有的微调方法、适配器和分类器集成等。
#### 7. 方法详细介绍：
本文提出了一种跨模态少样本学习方法，使用多模态模型将不同模态的样本映射到相同的嵌入空间中，通过联合损失函数优化分类器，实现对多模态测试集的分类。该方法可以扩展到其他模态，并可以微调模态特定的编码器。具体步骤包括：
1. 预训练多模态模型，将不同模态的特征映射到相同的嵌入空间中。
2. 在少样本任务上进行微调，使用少量标记数据训练分类器。
3. 使用数据增强技术增加文本和图像样本的数量，使用手工制作的模板或模板挖掘策略生成文本提示。
4. 通过联合损失函数优化分类器，实现对多模态测试集的分类。

#### 8. 实验设置：
本文使用现有的评估协议，在11个不同的下游数据集上复制了其他视觉-语言模型少样本适应的评估协议，并报告了性能。本文还将该方法扩展到音频领域，利用AudioCLIP构建了第一个交叉模态少样本学习基准，并将ImageNet和ESC-50音频分类数据集相交。本文表明，跨模态音视频学习有助于下游图像和音频分类。

#### 9. 实验结果与分析：
本文的跨模态少样本学习方法在各个基准数据集上均取得了最新的最佳结果，比其他精心设计的算法需要更少的训练时间。本文还展示了跨模态适应的效果，并提供了详细的比较表格和分类器初始化、部分微调和ViT-based骨干网络的消融实验。本文还探索了其他模态的跨模态适应，并构建了ImageNet-ESC基准进行评估。


# Paper:449     基于分组空时移位的视频恢复简单基线



#### 1. Title: 
A Simple Baseline for Video Restoration with Grouped Spatial-temporal Shift

#### 2. Authors: 
Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See, Xiaogang Wang, Hongwei Qin, Hongsheng Li

#### 3. Affiliation: 
第一作者：香港中文大学多媒体实验室

#### 4. Keywords: 
Video restoration, spatial-temporal shift, deep learning, multi-frame aggregation, optical flow

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_A_Simple_Baseline_for_Video_Restoration_With_Grouped_Spatial-Temporal_Shift_CVPR_2021_paper.html  Github: https://github.com/dasongli1/Shift-Net

#### 6. Summary : 
- (1):本文研究视频恢复问题，即从受损视频中恢复清晰的帧，具有重要的应用价值。

- (2):现有的深度学习方法通常依赖于复杂的网络结构，如光流估计、可变形卷积和跨帧自注意力层，导致高计算成本。本文提出了一种简单而有效的视频恢复框架，基于分组空时移位，可以隐式地捕捉多帧聚合的帧间对应关系。与光流估计、可变形卷积和自注意力层相比，本文方法计算成本更低，且在视频去模糊和视频去噪任务上取得了比现有最先进方法更好的性能。

- (3):本文提出了一种简单而有效的空时移位块，用于隐式建立时间对应关系。我们引入了分组空间移位，实现了扩张的有效感受野。结合基本的二维卷积，这个简单的框架可以有效地聚合帧间信息。通过堆叠多个空时移位块，实现了长期信息的聚合。这个简化的框架可以在不依赖于光流估计、可变形卷积或自注意力的情况下，对长期依赖关系进行建模。

- (4):本文方法在视频去模糊和视频去噪任务上取得了比现有最先进方法更好的性能，同时计算成本更低，具有很好的推广能力。
#### 7. 方法详细介绍：
本文提出了一种名为Group Shift-Net的视频恢复方法，包括三个阶段：特征提取、多帧特征融合和最终恢复。在特征提取阶段，使用2D U-Net结构提取逐帧特征并减轻降质的负面影响。在多帧特征融合阶段，提出了一种分组空间-时间位移块，将相邻帧的不同特征组移动到参考帧并隐式建立时间对应关系。最终恢复阶段利用类似U-Net的结构将低质量输入帧和相应的聚合特征作为输入，并生成每个帧的最终结果。损失函数的公式为L = 1/T ∑i=1 ||Hi − Oi||1，其中T是帧数，Hi是真实值，Oi是模型输出的第i帧。

#### 8. 实验设置：
本文在两个视频恢复任务（视频去模糊和视频去噪）上进行了实验，并使用GoPro和REDS两个数据集进行了评估。GoPro数据集包含33个模糊视频，REDS数据集包含240个具有不同降解类型的视频序列。实验中使用的评估指标为PSNR和SSIM。

#### 9. 实验结果和分析：
本文提出的方法在视频去模糊和视频去噪任务上均优于先前的最先进方法，并且计算成本不到其计算成本的四分之一。结果表明，该方法具有显著降低计算开销的潜力，同时保持高质量的结果。该方法的代码可在https://github.com/dasongli1/Shift-Net上获得。


# Paper:450     FedSeg：面向语义分割的异构联邦学习



#### 1. Title: 
FedSeg: Class-Heterogeneous Federated Learning for Semantic Segmentation

#### 2. Authors: 
Jiaxu Miao, Zongxin Yang, Leilei Fan, Yi Yang

#### 3. Affiliation: 
浙江大学

#### 4. Keywords: 
Federated Learning, Semantic Segmentation, Class-Heterogeneous, Pixel-level Contrastive Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Miao_FedSeg_Class-Heterogeneous_Federated_Learning_for_Semantic_Segmentation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是面向语义分割的异构联邦学习问题，该问题相较于分类问题更具挑战性，因为不同客户端的数据分布可能不同，导致局部更新的方向不一致，从而降低了全局模型的能力。此外，由于语义分割是一种密集预测任务，客户端的局部最优解可能与全局最优解相差较远，因此需要更细粒度的正则化方法。

- (2):过去的方法主要集中在分类问题上，而本文提出的FedSeg方法则针对语义分割问题提出了一种简单但有效的修改交叉熵损失函数的方法，以纠正局部优化并解决前景-背景不一致性问题。此外，本文还引入了像素级对比学习来强制局部像素嵌入接近全局语义空间，从而提高全局模型的收敛性。

- (3):本文提出了一种基于像素级对比学习的异构联邦学习方法FedSeg，该方法通过修改交叉熵损失函数和像素级对比学习来解决异构数据分布和密集预测问题。实验结果表明，FedSeg方法在四个语义分割数据集上均取得了较好的性能表现。

- (4):FedSeg方法在四个语义分割数据集上均取得了较好的性能表现，证明了其在解决异构联邦学习问题上的有效性。
#### 7. 方法详细介绍：
FedSeg是一种面向语义分割的异构类别联邦学习方法。该方法使用全局模型为背景像素提供伪标签，并使用像素到区域对比学习来提高效率。本地目标函数包括背景聚合交叉熵损失和像素到区域对比损失。FedSeg方法的具体步骤如下：
(1) 全局模型初始化；
(2) 客户端本地训练，使用本地数据更新模型；
(3) 全局模型聚合，更新全局模型；
(4) 重复步骤2和3，直到全局模型收敛。

#### 8. 实验设置：
本文在四个语义分割数据集上进行了实验：Cityscapes、CamVID、PascalVOC和ADE20k。其中，Cityscapes和CamVID被分成一个或两个语义类别的子集，PascalVOC被分成20个子集，对应20个前景类别。ADE20k被分成150个子集，尾部类别逐渐分成一个子集。使用mIoU和像素准确度两个指标进行评估。

#### 9. 实验结果和分析：
本文提出的FedSeg方法在四个语义分割数据集上进行了实验，与其他联邦学习方法进行了比较，包括FedAvg、FedProx、FedDyn和MOON。实验结果表明，FedSeg在mIoU方面优于这些方法。背景聚合交叉熵损失的有效性通过与标准交叉熵损失相比的显着性能提高得到证明。像素到区域对比损失也提高了性能，特别是对于具有更高异质性的数据集。FedSeg在IID分布设置上实现了与标准交叉熵损失类似的性能。


# Paper:451     基于点嵌入的多视角立体视觉中的手部重建



#### 1. Title: 
Reconstructing Hand in a Point Embedded Multi-view Stereo

#### 2. Authors: 
Lixin Yang, Jian Xu, Licheng Zhong, Xinyu Zhan, Zhicheng Wang, Kejian Wu, Cewu Lu

#### 3. Affiliation: 
上海交通大学

#### 4. Keywords: 
Multi-view stereo, hand mesh reconstruction, point-based feature fusion, cross-set point attention mechanism

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Reconstructing_Hand_in_a_Point_Embedded_Multi-View_Stereo_CVPR_2021_paper.html Github: https://github.com/lixiny/POEM

#### 6. Summary : 
- (1):本文研究多视角立体视觉中的手部网格重建问题，提出了一种基于点嵌入的多视角立体视觉方法，旨在解决单目相机系统中的深度模糊、未知视角和遮挡等问题。

- (2):以往的方法通常将多视角立体视觉的三维信息编码为二维特征，而本文提出的POEM方法直接在多视角立体视觉中操作三维点，以点作为三维信息的自然形式，是跨视角融合特征的理想媒介。本文提出了两种操作：基于点的特征融合和交集点注意机制。本文的方法在三个具有挑战性的多视角数据集上进行了评估，表明POEM在手部网格重建方面优于现有的最先进方法。

- (3):本文提出了一种新的思路，即通过目标点集（即网格顶点）与基础点集（即相机锥体空间中的点云）之间的交互来解决多视角姿态和形状重建问题。为了鼓励两个点集之间的交互，POEM引入了两种新操作：基于点的特征融合策略和交集点注意力。POEM是一种端到端的学习框架，通过点嵌入的多视角立体视觉从多视角图像中重建手部网格。

- (4):本文的方法在三个多视角数据集上进行了广泛的实验，证明了其在手部网格重建方面的有效性。作为一个针对网格重建的回归模型，POEM相对于先前的最先进方法取得了显著的改进。
#### 7. 方法详细介绍：
本文提出了一种名为POEM的方法，用于从多视角图像中重建手部网格。该方法分为两个阶段。第一阶段，POEM将多视角图像作为输入，并预测每个视图中手骨架的2D关键点。然后，通过代数三角测量模块恢复3D关键点。第二阶段，POEM将来自不同视角的特征融合到嵌入点空间中，并在该空间中预测手部网格。手部顶点上的点特征将通过交叉集合注意机制与嵌入点的特征进行迭代交互，并且POEM将使用更新后的顶点特征进一步预测顶点的精细位置。该方法利用了点的优势，它们是3D信息的自然形式，也是在不同视角上融合特征的理想介质，因为它们在不同视角上有不同的投影。

#### 8. 实验设置：
本文使用了多个数据集进行实验，包括STB、HANDS19、GANerated Hands和InterHand2.6M。评估指标包括平均每个关键点和每个顶点位置误差（MPJPE和MPVPE）、在一系列阈值下测量正确关键点的百分比（AUC）、根相对（RR）系统和Procrustes分析（PA）。实验中使用了多个SOTA方法进行比较，包括HMR、MANO、VIBE、HMR+、MANO+和VIBE+。实验使用了PyTorch框架和Nvidia Tesla V100 GPU进行实现。

#### 9. 实验结果和分析：
实验结果表明，POEM在所有指标上均优于所有比较方法。在STB数据集上，POEM的MPJPE和MPVPE分别为7.5mm和9.5mm，AUC为0.95。在HANDS19数据集上，POEM的MPJPE和MPVPE分别为9.3mm和11.5mm，AUC为0.91。在GANerated Hands数据集上，POEM的MPJPE和MPVPE分别为8.5mm和10.5mm，AUC为0.93。在InterHand2.6M数据集上，POEM的MPJPE和MPVPE分别为9.5mm和11.5mm，AUC为0.89。实验还分析了POEM在不同视角、不同邻居数和不同解码器数量下的性能，并讨论了其局限性。


# Paper:452     偏差模仿：一种简单的采样方法用于偏差缓解



#### 1. Title: 
Bias Mimicking: A Simple Sampling Approach for Bias Mitigation

#### 2. Authors: 
Maan Qraitem, Kate Saenko, Bryan A. Plummer

#### 3. Affiliation: 
Maan Qraitem: 波士顿大学
Kate Saenko: 波士顿大学、MIT-IBM Watson AI实验室
Bryan A. Plummer: 波士顿大学

#### 4. Keywords: 
Bias mitigation, dataset bias, sampling methods, deep learning, spurious correlations

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qraitem_Bias_Mimicking_A_Simple_Sampling_Approach_for_Bias_Mitigation_CVPR_2021_paper.html  Github: https://github.com/mqraitem/Bias-Mimicking

#### 6. Summary : 
- (1):本文研究的背景是深度学习中的偏差问题，即数据集中存在的偏差会导致模型学习到错误的相关性，从而影响模型的性能。
- (2):过去的方法主要是基于模型的解决方案，如集成方法和引入额外的偏差正则化损失函数等，但这些方法需要更多的超参数调整和计算资源。相比之下，数据采样方法是一种更简单、更便宜的解决方案，但是现有的方法存在一些问题，如欠采样会丢失数据，过采样会导致过拟合等。本文提出了一种新的采样方法——Bias Mimicking，通过模仿每个类别的偏差分布来确保类别之间的统计独立性，从而解决了现有方法的问题。
- (3):本文提出的方法是一种基于类别的采样方法，通过模仿每个类别的偏差分布来确保类别之间的统计独立性，从而解决了现有方法的问题。具体来说，我们将数据集分为三个子集，每个子集都包含一个类别的所有样本和其他类别的样本，但其他类别的样本的偏差分布与该类别的偏差分布相同。然后，我们使用每个子集分别训练模型，以确保模型在每个类别的样本上都能得到充分的训练。实验结果表明，Bias Mimicking方法在多个基准测试中都能取得比其他采样方法更好的性能。
- (4):本文的方法在图像分类任务中进行了测试，实验结果表明，与其他采样方法相比，Bias Mimicking方法能够显著提高少数群体的准确率，并且在保持或提高性能的同时，能够有效地减少偏差。这表明本文提出的方法是一种有效的解决方案，可以用于解决深度学习中的偏差问题。
#### 7. 方法详细介绍：
本文提出了一种新的采样方法——Bias Mimicking，用于缓解数据集偏差。该方法通过对敏感属性进行平衡，将传统的欠采样和过采样方法进行了扩展。在每个时期，模型使用每个子采样版本的数据集来暴露于数据集中的所有样本。与加权不同，Bias Mimicking不使用权重来缩放损失函数，这可以在与随机梯度下降一起使用时导致不稳定性。在推理时，使用原始数据集分布上的多类预测头，在模型骨干中停止梯度流以防止再次学习偏差。该方法不涉及额外的超参数，比其他偏差缓解方法更便宜。

#### 8. 实验设置：
本文在三个主要数据集上进行了性能测试：CelebA、UTKFace和CIFAR-S。对于CelebA和UTKFace，使用目标属性和偏差属性进行二元分类任务。CIFAR-S基准通过将每个目标类的子样本转换为灰度图像来人为地引入偏差。本文使用先前工作提供的每个数据集的拆分。

#### 9. 实验结果和分析：
本文将Bias Mimicking与先前工作中的几个基线进行比较，包括Bias-Contrastive和Bias-Balanced Learning、domain-independent、GroupDRO、欠采样、加权和过采样。使用的评估指标是无偏准确性和偏差冲突。本文表明，Bias Mimicking在无偏准确性和偏差冲突方面在所有三个数据集上优于基线。本文还包括两个分析Bias Mimicking行为的实验。


# Paper:453     OneFormer：一种通用图像分割的Transformer框架



#### 1. Title: 
OneFormer: One Transformer to Rule Universal Image Segmentation

#### 2. Authors: 
Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi

#### 3. Affiliation: 
Humphrey Shi: 美国俄勒冈大学和伊利诺伊大学香槟分校SHI实验室

#### 4. Keywords: 
Image segmentation, universal framework, transformer-based approach, multi-task training, panoptic segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Jain_OneFormer_One_Transformer_to_Rule_Universal_Image_Segmentation_CVPR_2022_paper.html Github: https://github.com/SHI-Labs/OneFormer

#### 6. Summary : 
- (1):本文研究的背景是图像分割的普适性问题。
- (2):过去的方法包括场景解析、全景分割等，但这些方法需要针对每个任务分别训练模型，不具备普适性。最近的全景/通用架构可以在不改变架构的情况下训练所有三个任务，但仍需要针对每个任务分别训练模型，因此只能被视为半通用方法。本文提出了一种多任务通用图像分割框架OneFormer，通过一次训练和一个模型在语义、实例和全景分割任务上取得了最新的性能表现，实现了分割在架构、模型和数据集上的统一。 
- (3):本文提出了一种任务条件联合训练策略，使模型能够在单个多任务训练过程中训练每个领域（语义、实例和全景分割）的ground truth。其次，引入任务令牌来使模型支持多任务训练和推理。第三，提出在训练过程中使用查询文本对比损失来建立更好的任务间和类间区别。 
- (4):OneFormer在ADE20K、Cityscapes和COCO三个主要分割数据集上取得了最新的性能表现，通过单个联合训练模型在所有三个分割任务上设置了新的最新性能。 OneFormer是一种多任务通用图像分割框架，只需要一次训练，一个模型和一个数据集即可实现最新的性能表现，从而使图像分割更加普适和易于访问。
#### 7. 方法详细介绍：
OneFormer是一种基于Transformer的通用图像分割框架，它使用了一种联合训练策略，可以同时训练语义分割、实例分割和全景分割任务。该方法使用两个输入：样本图像和任务输入，任务输入的形式为“任务是{任务}”。该框架从每个输入中提取多尺度特征，使用骨干网络和像素解码器。它将任务输入标记化以获得一个1-D任务标记，用于为每个输入的对象查询和模型调整任务。此外，它创建一个文本列表，表示GT标签中每个类别存在的二进制掩码数量，并将其映射到文本查询表示。模型的任务动态预测使用从全景注释派生的ground-truth进行监督。对象查询和多尺度特征被馈送到Transformer解码器中，以产生最终预测。

#### 8. 实验设置：
本文在三个广泛使用的数据集上进行了实验，这些数据集支持所有三种图像分割任务：语义分割、实例分割和全景分割。这些数据集是Cityscapes、ADE20K和COCO。本文报告了所有三种图像分割任务的PQ、AP和mIoU分数。

#### 9. 实验结果和分析：
本文提出的OneFormer框架在Cityscapes和COCO数据集上均取得了最先进的性能，优于单独训练的Mask2Former模型。消融研究表明，任务条件的架构和对比查询损失对于实现良好性能至关重要。输入文本模板也是良好性能的关键因素。在ADE20K数据集上，OneFormer框架对任务标记输入的敏感性得到了报告。PQ和mIoU指标显示，当任务是实例时，与全景相比，存在显着下降，而PQSt下降到1.5%。对于语义任务，PQ、PQTh和AP指标有相当大的下降，而PQSt保持不变。OneFormer模型在所有三个分割任务上均优于Mask2Former-Joint基线，并在主要数据集上减少了类别错误分类。


# Paper:454     Habitat-Matterport 3D语义数据集



#### 1. Title: 
Habitat-Matterport 3D Semantics Dataset

#### 2. Authors: 
Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva, Alexander William Clegg, Devendra Singh Chaplot

#### 3. Affiliation: 
第一作者：Meta AI

#### 4. Keywords: 
3D real-world spaces, semantic annotations, texture information, object instance, Object Goal Navigation task

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2022_paper.html
Github: https://github.com/facebookresearch/habitat-semantics

#### 6. Summary : 
- (1):本文的研究背景是获取和语义注释真实世界空间的数据集，以加速具有感知、导航和与真实室内场景交互能力的体验式AI代理的研究。
 
- (2):先前的方法主要集中在单个房间规模的场景上，且语义注释的密度和质量有限。本文提出了一种新的方法，使用纹理信息对像素精确的对象边界进行注释，从而提高了语义注释的密度和质量。本文的方法在Object Goal Navigation任务上表现出色，相比于先前的数据集，使用HM3DSEM训练的策略表现更好。 

- (3):本文提出了Habitat-Matterport 3D Semantics（HM3DSEM）数据集，它是目前可供学术界使用的最大的具有密集注释语义的3D真实世界空间数据集。该数据集包含142,646个对象实例注释，分布在216个3D空间和这些空间内的3,100个房间中。本文的方法使用纹理信息对像素精确的对象边界进行注释，从而提高了语义注释的密度和质量。 

- (4):本文的方法在Object Goal Navigation任务上表现出色，相比于先前的数据集，使用HM3DSEM训练的策略表现更好。这些结果突出了提高具有密集语义注释的3D数据集的质量和规模对于提高下游体验式AI任务性能的重要性。
#### 7. 方法详细介绍：
本文描述了对Habitat-Matterport 3D Semantics (HM3DSEM)数据集进行注释的过程。注释过程包括对建筑特征、装饰特征、家具、电器和杂物对象进行注释。注释过程涉及将建筑特征的实例注释分解为过渡点处的区域，例如房间边界、门口和走廊。注释过程还包括自动编程检查和人工验证。 

#### 8. 实验设置：
本文针对ObjectNav任务，即智能体在三维环境中寻找目标对象的任务，生成了145个训练场景、36个验证场景和35个测试场景的数据集。每个场景包含一个场景、一个起始位置和一个目标对象类别。起始位置必须是可导航的，目标对象必须从起始位置可达，起始位置到目标类别最近对象的距离必须在1m到30m之间。数据集包含大约7.2M个训练/1072个验证/1000个测试场景。 

#### 9. 实验结果和分析：
本文评估了使用强化学习、模仿学习和模块化学习在HM3DSEM、Gibson和MP3D数据集上训练的ObjectNav策略的性能。作者观察到，在强化学习和模仿学习的所有数据集上，使用HM3DSEM进行训练的策略表现最好。当使用HM3DSEM进行训练时，模块化学习方法的表现也比Gibson和MP3D数据集上的表现更好。作者还研究了数据集扩展行为，并观察到随着训练场景数量的增加，验证性能始终得到改善。 

#### 论文总结：
本文介绍了Habitat-Matterport 3D Semantics (HM3DSEM)数据集，该数据集是当前可用于学术界的具有密集注释语义的3D现实世界空间的最大数据集。数据集包含142,646个对象实例注释，跨216个3D空间和3100个房间。数据集提供了一种密集的语义注释“层”，将空间从原始HM3D数据集中增强。该语义“层”实现为一组纹理，编码对象实例语义并将对象聚类到不同的房间中。语义包括建筑元素（墙壁、地板、天花板）、大型物品（家具、电器等）以及“杂物”类别（聚合了书架上的书等较小的物品）。数据集的规模大于先前的工作（相对于Matterport3D的总对象实例数量增加了2.8倍，相对于ARKitScenes的总对象实例数量增加了2.1倍）。数据集是由20多个注释者进行注释和验证的超过14,200小时的人力工作的结果。


# Paper:455     自监督点云序列表示学习的完整到部分4D蒸馏



#### 1. Title: 
Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning

#### 2. Authors: 
Zhuoyang Zhang, Yuhao Dong, Yunze Liu, Li Yi

#### 3. Affiliation: 
第一作者：清华大学信息科学与技术学院

#### 4. Keywords: 
4D point cloud sequence, self-supervised representation learning, knowledge distillation, motion understanding, geometric understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Complete-to-Partial_4D_Distillation_for_Self-Supervised_Point_Cloud_Sequence_Representation_Learning_CVPR_2021_paper.html  Github: https://github.com/dongyh20/C2P

#### 6. Summary : 
- (1):本文研究4D点云序列的自监督表示学习，旨在利用未标记的数据进行学习。4D数据的标注成本高昂，因此需要探索如何利用原始未标记数据。本文提出了一种新的4D自监督预训练方法，称为Complete-to-Partial 4D Distillation，旨在通过教师-学生知识蒸馏框架，让学生在教师的指导下学习有用的4D表示。本文的创新点在于将4D表示学习作为教师-学生知识蒸馏框架，以促进几何和运动学习的协同作用。

- (2):现有的自监督点云表示学习方法只考虑静态快照的几何，忽略了动态场景的时间序列观察可以揭示更全面的几何细节。本文提出了一种新的4D自监督预训练方法，旨在通过教师-学生知识蒸馏框架，让学生在教师的指导下学习有用的4D表示。本文的创新点在于将4D表示学习作为教师-学生知识蒸馏框架，以促进几何和运动学习的协同作用。

- (3):本文提出了一种Complete-to-Partial 4D Distillation (C2P)方法，以促进几何和运动学习的协同作用。C2P方法包括三个关键设计：1）设计部分视图4D序列生成方法，以将已经部分捕获的输入点云序列转换为更加部分的序列；2）设计不对称的教师-学生蒸馏框架，以实现稳定的训练和高质量的表示；3）设计4D知识蒸馏方法，以将教师网络的知识蒸馏到学生网络的特征中。本文的创新点在于将4D表示学习作为教师-学生知识蒸馏框架，以促进几何和运动学习的协同作用。

- (4):本文在三个下游任务上进行了评估，包括室内和室外场景：HOI4D上的4D动作分割，HOI4D上的4D语义分割，Synthia 4D上的4D语义分割和MSR-Action3D上的3D动作识别。实验结果表明，本文的方法在广泛的4D点云序列理解任务中显著优于以前的预训练方法。
#### 7. 方法详细介绍：
本文提出了一种名为Complete-to-Partial 4D Distillation的自监督点云序列表示学习方法。该方法旨在学习点云序列的表示，以用于下游任务。该方法使用了一种师生框架，其中教师网络以完整的点云序列作为输入，学生网络以部分点云序列作为输入。教师和学生网络提取每帧特征，并通过蒸馏使学生网络从教师网络中学习。该方法还使用4D对比学习监督来监督特征学习过程。最终的损失函数是几何和时间损失的组合。该方法同时鼓励学生网络从教师网络中学习几何完整性和时间运动性。

#### 8. 实验设置：
本文在三个4D点云序列理解任务上进行了实验：HOI4D上的动作分割，HOI4D上的语义分割和MSR-action3D上的3D动作识别。部分视图序列生成是通过使相机绕点云旋转150度水平角度变化来完成的。相邻帧之间的水平角度变化相同。时间窗口大小设置为3，包括帧i-1，i，i+1。默认使用P4Transformer和PPTr作为骨干网络。

#### 9. 实验结果和分析：
本文提出的方法在所有三个任务上均优于现有方法。对于HOI4D上的动作分割，该方法在所有指标上始终比STRL和VideoMAE表现更好。对于HOI4D上的语义分割，该方法使用39个类别标签计算平均IoU（mIoU）作为评估指标。该方法更喜欢具有相对丰富的时间信息的长序列，因此将序列长度设置为10，每帧点数设置为4096。对于MSR-action3D上的3D动作识别，该方法在所有指标上均优于现有方法。本文还对实验结果进行了分析，包括消融研究以验证Complete-to-Partial蒸馏的设计，不同蒸馏策略的比较以及不对称蒸馏框架的必要性和部分视图生成策略的有效性的讨论。


# Paper:456     基于视觉对齐顺序坐标建模的表格结构识别改进



#### 1. Title: 
Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling

#### 2. Authors: 
Yongshuai Huang, Ning Lu, Dapeng Chen, Yibo Li, Zecheng Xie, Shenggao Zhu, Liangcai Gao, Wei Peng

#### 3. Affiliation: 
华为技术有限公司 (Huawei Technologies Ltd.)

#### 4. Keywords: 
Table structure recognition, sequential modeling, visual-alignment, coordinate sequence decoder, end-to-end

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Improving_Table_Structure_Recognition_With_Visual-Alignment_Sequential_Coordinate_Modeling_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究的背景是表格结构识别，旨在将非结构化表格图像的逻辑和物理结构提取为机器可读格式。

- (2):过去的方法包括基于分割和合并、基于检测和分类、基于图像到文本生成等方法，但这些方法在预测物理结构（单元格的边界框）时存在精度问题。本文提出了一种新的端到端的顺序建模框架，通过坐标序列解码器和辅助的视觉对齐损失来提高表格的物理结构准确性。

- (3):本文提出的方法是VAST，包含一个新颖的坐标序列解码器和一个辅助的视觉对齐损失。在坐标序列解码器中，我们将边界框坐标建模为语言序列，其中左、上、右和下坐标被顺序解码以利用它们之间的依赖关系。视觉对齐损失用于强制逻辑结构解码器的非空单元格表示包含更多的局部视觉细节，从而有助于产生更好的单元格边界框。

- (4):本文在逻辑和物理结构识别方面取得了最先进的结果，实验表明，我们提出的方法可以在表格结构识别任务中取得良好的性能。本文的创新点在于提出了坐标序列解码器和视觉对齐损失，这些方法是本文成功的关键。
#### 7. 方法详细介绍：
本文提出了一种名为VAST的端到端序列建模框架，用于表格结构识别。该方法包含一个由逻辑结构解码器触发的坐标序列解码器。在坐标序列解码器中，边界框坐标被建模为语言序列，其中左、上、右和下坐标被顺序解码以利用它们之间的依赖关系。此外，提出了一种辅助的视觉对齐损失，以强制非空单元的逻辑表示包含更多的局部视觉细节，从而有助于产生更好的单元边界框。具体步骤如下：
1. 使用修改后的ResNet作为CNN图像编码器，配备多方位全局内容注意力。
2. HTML序列解码器是一个具有N=3个相同层的transformer。
3. 坐标序列解码器由非空单元的表示触发。
4. 在训练期间使用视觉对齐损失来对齐非空单元的逻辑结构表示和其视觉特征。
5. 统一损失L是Ls、Lc和Lva损失的组合。
6. 使用AdamW作为优化器从头开始训练模型，mini-batch大小为3。
7. 在推理阶段，使用贪婪搜索进行HTML序列预测和坐标序列预测。

#### 8. 实验设置：
本文在多个基准数据集上评估了所提出的方法，包括FinTabNet、TableBank和PubTabNet。训练时使用了FinTabNet，测试时在FinTabNet测试集和IC19B2M上进行了测试。模型的超参数设置与FinTabNet相同。在推理阶段，使用贪婪搜索进行HTML序列预测和坐标序列预测。

#### 9. 实验结果和分析：
VAST模型在逻辑结构识别方面优于所有先前的方法，对于FinTabNet和PubTabNet，它的表现最好。在物理结构识别方面，VAST在ICDAR2013、IC19B2M和PubTables-1M上实现了新的最优性能。在SciTSR上，VAST的精度得分最高，为99.77%，F1得分为99.51%。由于一些样本的列超出了图像的范围，VAST的召回率低于NCGM。在PubTabNet上，VAST通过将TEDS从93.60%提高到96.31%，优于TableFormer。VAST在PubTabNet上的内容边界框检测AP50为94.8%。本文还进行了一系列消融实验，验证了所提出模块的有效性。实验结果表明，坐标序列解码器和视觉对齐损失是该方法成功的关键。本文还提到了VAST的两个局限性，即推理速度较慢和计算和内存消耗较高，这是由于HTML序列中的冗余导致的。


# Paper:457     基于原型的标签传播迭代图优化的传导式少样本学习



#### 1. Title: 
Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement

#### 2. Authors: 
Hao Zhu, Piotr Koniusz

#### 3. Affiliation: 
Australian National University（澳大利亚国立大学）, Data61-CSIRO

#### 4. Keywords: 
Few-shot learning, transductive learning, label propagation, prototype-based, graph refinement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_CVPR_2021_paper.html  Github: https://github.com/allenhaozhu/protoLP

#### 6. Summary : 
- (1):本文研究的是few-shot learning（FSL）领域中的transductive learning问题，即如何在测试时利用所有的query数据进行分类。 
- (2):过去的方法主要分为基于原型和基于图的方法，但是原型估计不准确，图构建不够优化。本文提出了一种基于原型的标签传播方法，解决了这些问题。该方法的图构建是基于原型和样本之间的关系，而不是样本之间的关系。同时，该方法估计每个原型的标签，而不是将原型视为类中心。 
- (3):本文提出的方法是基于原型的标签传播方法，称为protoLP。该方法的图是可学习的，随着原型的更新而改变。protoLP通过解决部分分配问题和线性投影来更新原型和标签传播。该方法不假设类分布先验，因此可以在不平衡的数据集上表现良好。 
- (4):在mini-ImageNet、tiered-ImageNet、CIFAR-FS和CUB数据集上，protoLP方法在transductive FSL和semi-supervised FSL方面表现优于其他现有方法。该方法的运行时间接近inductive inference的运行时间。
#### 7. 方法详细介绍：
本文提出了一种新的原型（prototype）-标签传播（label propagation）方法，称为protoLP，用于传导式少样本学习（transductive few-shot learning）。该方法包括两个主要组件：参数化标签预测和基于原型的图构建。标签预测通过线性投影A进行，以限制过度拟合到邻接矩阵W。基于原型的图构建基于这样一个想法：我们可以使用少量的原型将样本之间的亲和力计算转化为更简单的样本-原型亲和力计算。优化过程涉及交替更新Z、A和C。每轮优化的顺序假定相对于Z的最小化，然后是A，最后是C。

#### 8. 实验设置：
本文在四个少样本分类基准数据集上进行了评估，包括mini-ImageNet、tiered-ImageNet、CIFAR-FS和CUB。实验在传导式和半监督式少样本学习设置下进行，并与最先进的方法进行了比较。

#### 9. 实验结果和分析：
本文提出的protoLP方法在传导式少样本学习基准数据集（mini-ImageNet和tiered-ImageNet）上表现出色，优于所有以前的（传导式和归纳式）方法。在半监督学习设置下，protoLP在1-shot设置中优于其他方法。在mini-ImageNet 1-shot协议上，由于使用可学习图形捕捉数据流形和额外的未标记样本，protoLP的增益在3%到6%之间变化。


# Paper:458     DINER：无序不变的隐式神经表示



#### 1. Title: 
DINER: Disorder-Invariant Implicit Neural Representation

#### 2. Authors: 
Shaowen Xie, Hao Zhu, Zhen Liu, Qi Zhang, You Zhou, Xun Cao, Zhan Ma

#### 3. Affiliation: 
南京大学电子科学与工程学院

#### 4. Keywords: 
Implicit neural representation, spectral bias, hash-table, inverse problems, refractive index recovery

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Xie_DINER_Disorder-Invariant_Implicit_Neural_Representation_CVPR_2022_paper.html  Github: https://ezio77.github.io/DINER-website/

#### 6. Summary : 
- (1): 本文研究了隐式神经表示（INR）在网络训练中的频谱偏差问题，提出了一种新的方法来解决这个问题。
- (2): 以往的方法主要是通过增加网络中的频率基来解决频谱偏差问题，但是这种方法需要更大的模型和更多的计算资源。本文提出的方法是通过重新排列输入信号的坐标来调制信号的频谱，从而更好地模拟信号。具体地，本文提出了一种新的INR模型，即无序不变的隐式神经表示（DINER），通过增加哈希表到传统的INR骨干中，将输入信号的坐标映射到相同的分布中，从而更好地模拟信号。哈希表的缓存大小通常与输入信号的大小相似，但是由于哈希表的反向传播是O（1），所以计算成本很小。
- (3): 本文提出的DINER模型通过重新排列输入信号的坐标来调制信号的频谱，从而更好地模拟信号。哈希表的缓存大小通常与输入信号的大小相似，但是由于哈希表的反向传播是O（1），所以计算成本很小。实验结果表明，DINER模型在不同的INR骨干（MLP vs. SIREN）和各种任务（图像/视频表示，相位恢复和折射率恢复）中都具有很好的泛化性能，并且在质量和速度方面优于现有的算法。
- (4): 本文提出的DINER模型在图像/视频表示，相位恢复和折射率恢复等任务中都取得了很好的性能。实验结果表明，DINER模型在质量和速度方面优于现有的算法，支持了他们的目标。
#### 7. 方法详细介绍：
DINER方法使用全分辨率哈希表来建模输入坐标和信号属性之间的映射关系。哈希表的长度设置为输入信号元素数量，哈希表的宽度设置为输入坐标的维度。输入坐标用于在哈希表中查询哈希键，映射后的坐标被送入标准的MLP网络。哈希表和网络参数在训练过程中联合优化。DINER是无序不变的，具有相同属性直方图分布的信号共享具有相同参数值的优化网络。DINER通过重新排列输入信号的坐标来解决训练INR模型中的频谱偏差问题。哈希表的使用以快速计算为代价，由于哈希表导数的反向传播是O(1)，计算成本很小。DINER在各种任务中都有很好的泛化性能，包括2D图像和3D视频的表示，无透镜成像中的相位恢复以及强度衍射层析成像中的3D折射率恢复，相对于现有的最先进技术报告了显着的性能提升。

#### 8. 实验设置：
无信息。

#### 9. 实验结果与分析：
DINER方法在多个任务上进行了广泛的测试，包括信号拟合和反问题优化。结果表明，DINER显著提高了当前INR骨干的准确性和效率。具体来说，在信号拟合任务中，DINER优于当前的最先进技术，包括Deep BSpline Network和Deep BSpline Flow。在反问题优化任务中，DINER的收敛速度比PhysenNet和DNF分别快18倍和80倍。在3D折射率恢复任务中，DINER提供了比SOTA DeCAF更多的颗粒细节。


# Paper:459     DegAE：一种新的低级别视觉预训练范式



#### 1. Title: 
DegAE: A New Pretraining Paradigm for Low-level Vision

#### 2. Authors: 
Yihao Liu, Jingwen He, Jinjin Gu, Xiangtao Kong, Yu Qiao, Chao Dong

#### 3. Affiliation: 
上海人工智能实验室

#### 4. Keywords: 
Self-supervised pretraining, low-level vision, degradation autoencoder, image restoration

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_DegAE_A_New_Pretraining_Paradigm_for_Low-Level_Vision_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在回答自我监督预训练在低级别视觉中的原始意图和核心问题，并为低级别视觉建立一种新的预训练方案。 
- (2):现有的低级别视觉预训练方法主要集中在低成本任务上，但是本文认为预训练对于高成本任务更为重要。因此，本文提出了一种新的预训练范式，称为DegAE，它遵循自我监督预训练的设计前提，并精心设计了低级别视觉。DegAE预训练可以显着提高模型性能。 
- (3):本文提出了一种新的低级别视觉预训练范式DegAE，它旨在实现内容降级分离和生成。DegAE接受带有降级D1的输入图像和带有降级D2的参考图像。它试图将参考图像的降级D2转移给输入图像，从而获得具有输入图像内容但具有降级D2的输出图像。通过这种学习范式，模型预计将学习自然图像表示和降级信息，这是低级别视觉的关键组成部分。 
- (4):在图像去雾和去雨任务上，DegAE预训练可以显着提高模型性能。例如，SwinIR在图像去雾任务上获得了6.88dB的性能提升，而Uformer在去雾和去雨任务上分别获得了3.22dB和0.54dB的提升。DegAE预训练还可以在低成本任务上实现增量改进。
#### 7. 方法详细介绍：
DegAE是一种新的低级别视觉预训练范式，旨在实现内容-降级分离和生成。DegAE接受带有降级D1的输入图像和带有降级D2的参考图像，试图将参考降级D2转移到输入图像，获得具有输入图像内容但具有参考降级D2的输出图像。DegAE由一个基于Transformer的编码器和一个基于CNN的解码器组成。DegAE的解码器仅在预训练阶段使用，并在下游微调期间被单个卷积层替换。DegAE使用四种损失进行训练：内容重建损失、感知损失、对抗损失和嵌入损失。DegAE的训练过程包括以下步骤：1）在线合成降级输入；2）使用DegAE对合成的降级输入进行预训练；3）使用预训练的DegAE对下游任务进行微调。

#### 8. 实验设置：
本文在三个低级别视觉任务上进行了实验：超分辨率、去雾和去雨。使用了三种最先进的Transformer架构（SwinIR、Uformer和Restormer）作为编码器。实验中使用的数据集包括DIV2K、Flickr2K、RESIDE、Rain100H和Rain800。在预训练阶段，使用了自然图像和合成降级图像对。在下游微调阶段，使用了自然图像和真实降级图像对。

#### 9. 实验结果和分析：
使用DegAE预训练的SwinIR、Uformer和Restormer在三个任务上均取得了显著的性能提升。在去雾任务上，SwinIR的性能提升了6.88dB，Uformer的性能提升了3.22dB，在去雨任务上，Uformer的性能提升了0.54dB。在运动去模糊任务上，Restormer的性能提升了0.43dB。与其他最先进的方法相比，DegAE在PSNR和SSIM方面均取得了更好的性能。视觉结果也证明了DegAE预训练的有效性。


# Paper:460     vMAP: 基于神经场SLAM的向量化对象建图



#### 1. Title: 
vMAP: Vectorised Object Mapping for Neural Field SLAM


#### 2. Authors: 
Xin Kong, Shikun Liu, Marwan Taher, Andrew J. Davison


#### 3. Affiliation: 
Dyson Robotics Lab, Imperial College London (伦敦帝国理工学院)


#### 4. Keywords: 
SLAM, neural fields, object-level mapping, vectorised training, real-time scene scanning


#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kong_vMAP_Vectorised_Object_Mapping_for_Neural_Field_SLAM_CVPR_2021_paper.html  Github: https://kxhit.github.io/vMAP/


#### 6. Summary : 
- (1):本文研究的背景是实时场景建图，其中对象级别的建模是语义最优的，但是在没有3D对象先验知识的情况下，只能重建对象的直接观察部分，导致缺失和空洞。 
- (2):过去的方法包括使用CAD模型或类别级别的形状空间模型，但只能为场景中可用这些模型的对象提供完整的对象形状估计。本文提出了一种新方法，使用神经场表示法，将每个对象表示为一个小型MLP，使得即使在没有3D形状先验知识的情况下，也能实现高效、完整的对象建模。 
- (3):本文的研究方法是将多个小型对象模型同时训练，利用PyTorch中高度优化的向量化操作，实现高效的向量化训练。本文的创新点在于，首次展示了在实时系统中同时训练多个神经场模型，从而实现了准确和高效的多对象场景表示。 
- (4):本文的方法在模拟和实际数据集上进行了广泛的实验，展示了与先前神经场SLAM系统相比，显著提高了场景级别和对象级别的重建质量。本文的方法可以在实时场景扫描中动态检测对象实例，并将其动态添加到地图中，可以在单个场景中优化多达50个独立对象，具有极高的计算和内存效率。
#### 7. 方法详细介绍：
vMAP是一种基于神经场表示的目标级稠密SLAM系统。该方法使用纯解缠表示空间表示对象，并使用连续占用场来建模对象的可见性。该方法使用分层采样方法对训练点进行采样，并针对每个对象分别优化深度、颜色和占用损失。该方法还允许对对象进行细粒度的对象级操作，例如基于解缠预训练特征场的条件更改对象形状或纹理。

#### 8. 实验设置：
该方法在多个数据集上进行了评估，包括Replica、ScanNet和TUM RGB-D，其中对象掩码、深度和姿态测量的质量不同。使用的实例分割检测器是Detic，预训练于开放词汇LVIS数据集，使用的姿态估计框架是ORB-SLAM3。所有数据集都使用相同的超参数。场景中的对象数量通常在20到70之间，其中Replica和ScanNet场景中的对象数量最多，平均每个场景有50个对象。

#### 9. 实验结果和分析：
该方法在Replica场景中实现了比iMAP和NICE-SLAM更好的对象级完成，提高了50-70%。该方法在所选的ScanNet序列中的对象级重建方面也优于TSDF-Fusion和ObjSDF。展示了所选Replica和TUM RGB-D序列的场景重建，并突出显示了有趣的区域。在补充材料中还提供了2D新视图渲染的定量结果。


# Paper:461     SCADE: 利用具有模糊感知深度估计的空间雕刻重建NeRFs



#### 1. Title: 
SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates

#### 2. Authors: 
Mikaela Angelina Uy, Ricardo Martin-Brualla, Leonidas Guibas, Ke Li

#### 3. Affiliation: 
第一作者：Stanford University（斯坦福大学）

#### 4. Keywords: 
Neural Radiance Fields, Monocular Depth Estimation, Space Carving, Novel View Synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Uy_SCADE_NeRFs_From_Space_Carving_With_Ambiguity-Aware_Depth_Estimates_CVPR_2021_paper.html  Github: https://github.com/scade-spacecarving-nerfs/scade-spacecarving-nerfs.github.io

#### 6. Summary : 
- (1):本文研究的背景是如何在少量视角下重建三维场景，NeRF是一种高保真的方法，但在少量视角下表现不佳。
- (2):过去的方法包括使用数据驱动的先验，如深度先验，但深度估计存在固有的模糊性和不准确性，这会导致NeRF重建错误的形状并产生伪影。本文提出了一种新的方法，利用单目深度估计的概率分布来表示深度的不确定性和模糊性，并提出了一种新的空间雕刻损失，以指导NeRF表示从多个假设的深度图中融合信息，并提取与所有视角一致的公共几何形状。这种方法能够更好地重建高度模糊的场景，例如非不透明表面。
- (3):本文提出了一种新的方法SCADE，它利用通用的单目深度先验来约束NeRF重建，并通过条件隐式最大似然估计（cIMLE）学习为每个视角预测深度估计的连续多模态分布。为了消除深度估计的模糊性，我们提出了一种新的空间雕刻损失，它指导NeRF表示从每个视角的多个假设深度图中融合信息，并从中提取与所有视角一致的公共几何形状。与以往的方法相比，我们的方法能够表示多模态深度估计，能够处理深度估计的固有模糊性，并能够寻找每个深度估计的模式以进行一致的预测。
- (4):本文的方法在少量视角下的重建任务中取得了更好的重建效果，能够更好地重建高度模糊的场景，例如非不透明表面。
#### 7. 方法详细介绍：
本文提出了一种名为SCADE的新方法，它利用了模糊感知深度估计和空间雕刻的NeRFs来优化和训练NeRF模型。该方法包括使用cIMLE从模糊感知深度先验中采样，使用逆变换采样从NeRF射线终止距离分布中采样，并使用一种新的空间雕刻损失来监督NeRF优化，该损失操作的是分布而不是分布的矩。总损失是预测和地面真实图像的标准MSE损失和空间雕刻损失的组合。

#### 8. 实验设置：
本文在ScanNet数据集和使用iPhoneX收集的野外数据集上评估了所提出的方法。评估使用的稀疏视图ScanNet数据包括三个样本场景，每个场景有18到20个训练图像和8个测试图像。野外数据集包括三个场景-地下室、厨房和休息室-每个场景有18到23个训练图像和8个测试图像。NeRF模型使用域外先验进行训练，评估指标包括PSNR、SSIM和LPIPS。

#### 9. 实验结果与分析：
所提出的SCADE方法在ScanNet数据集上的PSNR、SSIM和LPIPS指标上优于基线方法，包括Vanilla NeRF和Dense Depth Prior。该方法还在野外数据集上表现出鲁棒性，在PSNR、SSIM和LPIPS方面优于基线方法。定性结果表明，SCADE能够避免产生灰尘云和捕捉场景中的物体。该方法还能够恢复比基线更好的几何形状。


# Paper:462     利用自然脚本知识学习可迁移的时空表示



#### 1. Title: 
Learning Transferable Spatiotemporal Representations from Natural Script Knowledge

#### 2. Authors: 
Ziyun Zeng, Yuying Ge, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge

#### 3. Affiliation: 
第一作者：清华大学
其他作者：香港大学，哈尔滨工业大学深圳，腾讯应用研究中心

#### 4. Keywords: 
Spatiotemporal representation learning, video understanding, pre-training, natural script knowledge, transfer learning

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2021_paper.html
Github: https://github.com/TencentARC/TVTS

#### 6. Summary : 
- (1):本文研究背景是视频理解中的通用性问题，即如何学习可迁移的时空表示。
- (2):过去的方法主要是基于大规模视频数据的预训练，但这些方法大多数只能适用于高度筛选的数据集，且表现不佳。本文提出了一种新的预训练方法，利用自然语言语义来提高可迁移的时空表示学习。本文的方法通过利用视频中的转录语音知识来提供有用的时间语义，从而在大规模未筛选的视频数据上实现了强大的时空表示。
- (3):本文提出了一种新的预训练任务，即“Turning to Video for Transcript Sorting (TVTS)”，通过对视频表示进行自我关注，将ASR脚本排序，从而间接地规范化模型以正确地捕获上下文化的时空表示。本文的方法在多个基准测试中展现了强大的时空表示，例如在SSV2上线性探测的增益为13.6%。 
- (4):本文的方法在多个视频识别任务中取得了最先进的性能，特别是在相对较大且最具挑战性的SSV2上。本文的方法通过利用自然语言语义来提高可迁移的时空表示学习，从而在大规模未筛选的视频数据上实现了强大的时空表示。
#### 7. 方法详细介绍：
本文提出了一种新的预训练方法，称为“Turning to Video for Transcript Sorting”（TVTS），以通过利用自然脚本知识学习可转移的时空视频表示。TVTS任务涉及随机打乱几个连续的ASR转录本并提取它们的表示。然后，通过在转录本表示和视频表示之间进行联合注意力来对转录本进行正确排序，从而捕获视频的上下文化时空表示。本文还使用全局视频-转录本对比目标来对齐视频剪辑的特征和K个转录本的平均特征，以便视频和转录本表示在相同的特征空间中进行联合注意力以预测转录本顺序。总体预训练目标结合了两个目标，模型架构包括一个视觉变换器，它以视频剪辑作为输入并输出视频表示。

#### 8. 实验设置：
本文在包含6M个YouTube视频的YT-Temporal数据集上进行预训练，该数据集包含ASR转录本和单词级时间戳。模型架构包括一个视觉变换器和一个DistilBERT，用于提取ASR转录本的表示。本文还在四个常见的视频数据集上进行了评估，包括SSV2、K400、UCF-101和HMDB-51，用于行动识别和文本到视频检索任务。

#### 9. 实验结果和分析：
本文提出的方法在所有评估指标下均优于所有基线方法，表明具有更强的跨领域视频识别可转移性。该方法在细调协议下在SSV2、Kinetics-400、UCF-101和HMDB-51数据集上实现了最先进或有竞争力的准确性。文本到视频检索也显示出有希望的结果，实现了SOTA性能。


# Paper:463     VoxelNeXt：用于3D物体检测和跟踪的完全稀疏VoxelNet



#### 1. Title: 
VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking

#### 2. Authors: 
Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, Jiaya Jia

#### 3. Affiliation: 
第一作者：香港中文大学

#### 4. Keywords: 
3D object detection, sparse convolutional networks, voxel features, LIDAR, tracking

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_VoxelNeXt_Fully_Sparse_VoxelNet_for_3D_Object_Detection_and_Tracking_CVPR_2021_paper.html  Github: https://github.com/dvlab-research/VoxelNeXt

#### 6. Summary : 
- (1):本文研究的是3D物体检测和跟踪，这是自动驾驶系统中的基础组件。

- (2):过去的方法通常依赖于手工制作的代理，如锚点或中心，并将已研究的2D框架转化为3D。因此，需要将稀疏体素特征密集化并通过密集预测头进行处理，这必然会增加额外的计算。本文提出了VoxelNeXt，用于完全稀疏的3D物体检测。我们的核心见解是基于稀疏体素特征直接预测物体，而不依赖于手工制作的代理。我们的方法通过完全稀疏的卷积网络VoxelNeXt完全基于体素特征检测和跟踪3D物体。我们的方法比其他主流检测器在nuScenes数据集上实现了更好的速度-准确性权衡。我们首次展示了完全稀疏的基于体素的表示对于LIDAR 3D物体检测和跟踪的良好效果。

- (3):本文提出了VoxelNeXt，它是一个简单、高效、无需后处理的3D物体检测器。我们的设计核心是一个体素到物体的方案，它直接从体素特征中预测3D物体，具有强大的完全稀疏卷积网络。我们的方法可以摆脱锚点代理、稀疏到密集的转换、区域提议网络和其他复杂组件。我们的方法预测仅在稀疏和必要的位置上进行，具有高效的推理效率。这种表示使VoxelNeXt易于扩展到3D跟踪。我们的方法在nuScenes、Waymo和Argoverse2基准测试上进行了广泛的实验，验证了我们方法的有效性。

- (4):本文在nuScenes、Waymo和Argoverse2数据集上评估了我们的模型，VoxelNeXt在这些基准测试中实现了领先的性能和高效性。它还在3D跟踪方面取得了最先进的性能。在没有花哨的情况下，它在nuScenes跟踪测试基准测试中排名第一。
#### 7. 方法详细介绍：
本文提出了一种名为VoxelNeXt的全稀疏VoxelNet用于3D物体检测和跟踪。该方法采用稀疏最大池化来选择具有空间局部最大值的体素，并在盒子预测中删除排除的体素，从而节省了头部的计算。边界框直接从正面或选择的稀疏体素特征回归。该方法还通过使用体素关联来扩展到3D跟踪，以包括与查询体素位置匹配的更多轨迹。

具体步骤如下：
1. 使用3D稀疏CNN作为骨干网络，增加附加的下采样层以扩大感受野。
2. 使用稀疏预测头直接基于3D CNN骨干网络的稀疏输出预测对象。
3. 使用聚焦损失进行监督，为每个类别单独进行体素选择。
4. 在推理过程中，使用稀疏最大池化来避免NMS后处理。
5. 完全基于体素，沿着下采样层逐渐应用空间体素修剪以抑制不相关的体素。
6. 使用稀疏高度压缩将3D体素特征压缩成密集的2D地图，将所有体素放在地面上并将相同位置的特征相加。
7. 该方法还包括查询体素以进行关联，以缓解对象中心的预测偏差。

#### 8. 实验设置：
本文在nuScenes和Waymo数据集上进行了3D物体检测和跟踪的实验。通过消融研究分析了额外的下采样层、空间体素修剪、稀疏高度压缩、稀疏预测头中的层类型以及体素和预测框之间的相对位置等因素的影响。本文还将所提出的方法与代表性的密集头方法CenterPoint进行了比较。

#### 9. 实验结果和分析：
本文在Waymo数据集上，VoxelNeXt的mAP和NDS均优于代表性的密集头方法CenterPoint。消融研究表明，额外的下采样层和稀疏高度压缩可以提高方法的效率，并具有良好的性能。体素和预测框之间的相对位置表明，边界体素也适合预测，而对象中心并不总是必要的。在nuScenes数据集上，VoxelNeXt在3D检测和跟踪测试中均取得了最佳性能，超过了其他LIDAR-only方法。在Argoverse2数据集上，VoxelNeXt也取得了较好的性能。


# Paper:464     StyleAdv: 元样式对抗训练用于跨域少样本学习



#### 1. Title: 
StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning

#### 2. Authors: 
Yuqian Fu, Yu Xie, Yanwei Fu, Yu-Gang Jiang

#### 3. Affiliation: 
第一作者：上海智能信息处理重点实验室，复旦大学计算机科学学院

#### 4. Keywords: 
Cross-Domain Few-Shot Learning, Adversarial Training, Style Augmentation, Visual Style, Meta-Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fu_StyleAdv_Meta_Style_Adversarial_Training_for_Cross-Domain_Few-Shot_Learning_CVPR_2021_paper.html  Github: https://github.com/lovelyqian/StyleAdv-CDFSL

#### 6. Summary : 
- (1):本文研究跨域少样本学习（CD-FSL）任务，该任务旨在将源数据集上学习到的先验知识转移到新的目标数据集上。CD-FSL任务面临的主要挑战是不同数据集之间的巨大领域差距，而这种领域差距实际上来自于视觉风格的变化。本文提出了一种元样式对抗训练（StyleAdv）方法，通过对原始样式进行扰动，生成更具挑战性的虚拟样式，从而提高模型的鲁棒性，增强其在新领域的泛化能力。

- (2):以往的CD-FSL方法主要通过数据增强、特征增强等方式来解决领域差异问题，但这些方法存在一定的局限性。本文提出的StyleAdv方法通过对样式进行对抗训练，生成更具挑战性的虚拟样式，从而提高模型的鲁棒性，增强其在新领域的泛化能力。该方法在CNN和ViT等多种模型上均取得了最新的CD-FSL最优结果。

- (3):本文提出了一种元样式对抗训练（StyleAdv）方法，通过对原始样式进行扰动，生成更具挑战性的虚拟样式，从而提高模型的鲁棒性，增强其在新领域的泛化能力。具体而言，该方法包括两个迭代优化循环：内循环通过对原始样式进行扰动，生成更具挑战性的虚拟样式；外循环通过最小化对原始样式和虚拟样式的识别损失来优化整个网络。本文还提出了一种新颖的样式对抗攻击方法，通过对样式进行渐进式合成，生成更具挑战性的虚拟样式。该方法在CNN和ViT等多种模型上均取得了最新的CD-FSL最优结果。

- (4):本文在8个不同的目标数据集上进行了广泛的实验，结果表明，StyleAdv方法在CNN和ViT等多种模型上均取得了最新的CD-FSL最优结果，证明了该方法的有效性和优越性。
#### 7. 方法详细介绍：
本文提出了一种新的元学习方法——StyleAdv，用于跨域少样本学习。该方法包含两个迭代优化循环：内循环和外循环。内循环通过添加扰动生成对抗样式，使得当前模型更难识别，从而增加损失。外循环通过最小化原始样式和对抗样式的损失来优化整个网络。该方法是模型无关的，可以与其他现有的FSL或CD-FSL模型相结合。该方法还提出了一种新的样式攻击方法，采用渐进式合成策略和变化的攻击比率，对样式进行扰动合成，生成多样性、虚拟和难以识别的对抗样式。

#### 8. 实验设置：
本文在八个不同的目标数据集上进行了广泛的实验，以评估所提出的方法的有效性。实验在CNN和ViT两种不同的骨干网络上进行，同时提供了实验所使用的数据集的详细信息。

#### 9. 实验结果与分析：
本文在八个未见过的目标数据集上评估了所提出的StyleAdv方法的性能。实验结果表明，该方法优于以前的CD-FSL方法，并在CD-FSL领域取得了新的最优结果。本文还将所提出的方法与其他现有的FSL或CD-FSL模型进行了比较。本文进一步探讨了将所提出的StyleAdv方法应用于非参数化的方式来改进Vision Transformer（ViT）骨干网络。


# Paper:465     PMatch: 基于成对掩蔽图像建模的密集几何匹配



#### 1. Title: 
PMatch: Paired Masked Image Modeling for Dense Geometric Matching

#### 2. Authors: 
Shengjie Zhu and Xiaoming Liu

#### 3. Affiliation: 
Michigan State University, East Lansing, MI, 48824 (密歇根州立大学)

#### 4. Keywords: 
Dense geometric matching, masked image modeling, cross-frame global matching module, homography loss, pretraining

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_PMatch_Paired_Masked_Image_Modeling_for_Dense_Geometric_Matching_CVPR_2021_paper.html  Github: https://github.com/ShngJZ/PMatch

#### 6. Summary : 
- (1):本文研究的是密集几何匹配，即确定源图像和支持图像之间的像素级对应关系。该任务是许多视觉应用的基础，如单应性估计、运动结构、视觉里程计估计和视觉相机定位等。

- (2):过去的方法主要有稀疏和密集两种，本文主要关注密集几何匹配。现有的方法主要采用编码器-解码器结构，但是现有的单目预训练任务，如图像分类和掩蔽图像建模（MIM），无法预训练跨帧模块，导致性能不佳。为了解决这个问题，本文将MIM从重建单个掩蔽图像改为重建一对掩蔽图像，从而实现了跨帧模块的预训练。此外，本文还将解码器纳入预训练，以提高上采样结果。为了对纹理区域不敏感，本文提出了一种新的跨帧全局匹配模块（CFGM）。综合在一起，本文在几何匹配方面取得了最先进的性能。

- (3):本文提出了一种新的预训练方法，即Paired Masked Image Modeling（pMIM），通过重建一对掩蔽图像来预训练编码器和解码器。此外，本文还提出了一种新的跨帧全局匹配模块（CFGM），通过计算相关性体积来模拟粗略尺度像素的对应关系。为了解决多个相似局部区域的模糊匹配问题，本文在离散坐标上引入位置嵌入，并使用深度架构进行解码。此外，本文还设计了一个单应性损失来增强低自由度平面先验的学习。

- (4):本文在多个数据集上取得了最先进的性能，超过了稀疏和密集几何匹配方法。
#### 7. 方法详细介绍：
本文提出了一种名为PMatch的密集几何匹配方法，用于计算两个图像之间的密集对应关系。该方法使用基于ResNet的特征提取器生成多尺度特征嵌入。交叉帧全局匹配模块用于完成粗略尺度的几何匹配。多尺度细化模块用于细化对应关系。本文还提出了一种配对掩蔽图像建模（MIM）预训练方法，以提高网络性能。MIM用于从特定尺度的随机掩蔽特征嵌入重构输入。预测头包括解码器的大多数网络组件。未包含在pMIM中的网络组件使用合成图像对进行预训练。本文还提出了单应性损失和全局匹配损失来优化网络。

#### 8. 实验设置：
本文在多个数据集上评估了所提出的方法，包括MegaDepth、ScanNet、HPatches和YFCC100m。评估指标包括密集几何匹配和双视图相机姿态估计。本文将所提出的方法与几种最先进的方法进行比较，并报告了准确性和速度方面的性能。

#### 9. 实验结果和分析：
所提出的方法在MegaDepth数据集上的密集几何匹配和双视图相机姿态估计任务中均优于最先进的方法，具有更高的准确性和速度。所提出的方法在ScanNet数据集上也取得了竞争性的性能。本文还展示了所提出的方法在HPatches和YFCC100m数据集上的泛化能力。


# Paper:466     LiDAR2Map：利用在线相机蒸馏捍卫基于LiDAR的语义地图构建



#### 1. Title: 
LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation

#### 2. Authors: 
Song Wang, Wentong Li, Wenyu Liu, Xiaolu Liu, Jianke Zhu

#### 3. Affiliation: 
浙江大学

#### 4. Keywords: 
Semantic map construction, LiDAR, Camera, Distillation, BEV

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_LiDAR2Map_In_Defense_of_LiDAR-Based_Semantic_Map_Construction_Using_CVPR_2021_paper.html  Github: https://github.com/songw-zju/LiDAR2Map

#### 6. Summary : 
- (1):本文研究自动驾驶中基于鸟瞰图的语义地图构建问题，提出了一种有效的基于LiDAR的方法，利用在线相机蒸馏来提高语义学习效果。
 
- (2):现有的语义地图构建方法可以大致分为三类：基于相机的方法、基于LiDAR的方法和相机-LiDAR融合方法。相比于相机图像，LiDAR提供了准确的三维观测，可以将捕获的三维特征固有地投影到BEV空间中。然而，基于LiDAR的BEV特征通常包含许多不确定的噪声，其中空间特征几乎没有纹理和语义线索。本文提出了一种有效的基于LiDAR的方法来构建语义地图。具体来说，我们引入了一个BEV金字塔特征解码器，学习用于语义地图构建的强大的多尺度BEV特征，大大提高了基于LiDAR的方法的准确性。为了缓解LiDAR数据中缺乏语义线索所造成的缺陷，我们提出了一种在线相机到LiDAR的蒸馏方案，以促进从图像到点云的语义学习。我们的蒸馏方案包括特征级和logit级蒸馏，以在BEV中吸收相机中的语义信息。在具有挑战性的nuScenes数据集上的实验结果表明，我们提出的LiDAR2Map在语义地图构建方面的有效性，显著优于以前的基于LiDAR的方法超过27.9％的mIoU，甚至表现比最先进的基于相机的方法更好。

- (3):本文提出了一种有效的语义地图构建框架LiDAR2Map，其中提出的BEV金字塔特征解码器可以学习用于语义地图构建的强大的BEV特征表示，从而提高了基线模型的准确性。为了充分利用相机中丰富的语义线索，我们建议采用一种新颖的在线相机到LiDAR的蒸馏方案，以进一步促进基于LiDAR的模型。它充分利用了来自基于图像的网络的语义特征，具有位置引导的特征融合模块（PGF2M）。在统一的BEV空间中执行特征级和logit级蒸馏，以便在训练期间促进基于LiDAR的网络吸收语义表示。特别地，我们建议使用输入低级和高级特征指导生成全局亲和力图，以满足令人满意的特征级蒸馏。LiDAR2Map的推
#### 7. 方法详细介绍：
本文提出了一种高效的语义地图构建框架，名为LiDAR2Map。该方法主要由基于LiDAR的网络和基于相机的网络组成。其中，基于LiDAR的网络用于编码点云特征，基于相机的网络用于提取图像特征。为了提高单一LiDAR模态的语义信息，本文提出了一种在线的相机到LiDAR的蒸馏方案，以吸收相机中的语义信息。此外，该方法还利用位置引导的特征融合模块在BEV空间中更好地融合相机和LiDAR的特征。最后，通过特征级蒸馏和logit级蒸馏，使基于LiDAR的分支通过网络优化隐式地从图像特征中受益。

具体步骤如下：
1. 使用PointPillars或VoxelNet提取点云特征。
2. 使用Swin-Tiny作为相机分支的图像骨干网络。
3. 在BEV空间中使用BEV-FPD解码器学习多尺度BEV特征。
4. 使用位置引导的特征融合模块在BEV空间中融合相机和LiDAR的特征。
5. 使用特征级蒸馏和logit级蒸馏，使基于LiDAR的分支从图像特征中受益。

#### 8. 实验设置：
本文在nuScenes数据集上评估了提出的LiDAR2Map方法，该数据集包含在波士顿和新加坡收集的1,000个驾驶场景。数据集中有700个完整场景用于训练，150个完整场景用于验证。本文使用60m×30m的区域和15cm的分辨率采样地图，共分为三类。对于车辆分割，本文使用了PON和Lift-Splat提出的两种常用设置。本文提供了实现细节，包括数据集、评估指标、训练和推理设置。本文使用在ImageNet上预训练的Swin-Tiny作为相机分支的图像骨干网络，使用PointPillars和VoxelNet提取点云特征。本文在4个NVIDIA Tesla V100 GPU上使用Adam优化器进行30个epoch的训练。学习率为2e-3（PointPillars）和1e-4（VoxelNet），在第20个epoch时以10的倍数递减。本文在单个NVIDIA RTX 2080Ti GPU上报告了LiDAR2Map的推理速度，以进行公平比较。

#### 9. 实验结果与分析：
本文提出的LiDAR2Map方法在多个竞争性设置下实现了语义地图构建的最新性能。相比其他蒸馏方案，本文提出的在线相机到LiDAR蒸馏方案提供了更有效的方法，并取得了最佳性能。使用前置和后置两个相机的LiDAR2Map模型表现最佳，mIoU为54.5%，而使用所有六个相机的模型的mIoU为54.3%。在nuScenes数据集上进行的场景级语义地图构建表明，本文的LiDAR2Map方法能够生成一致的地图，并为下游任务（如导航和规划）提供更多信息。


# Paper:467     天使贴片：提高第三方目标检测器性能的方法



#### 1. Title: 
Angelic Patches for Improving Third-Party Object Detector Performance

#### 2. Authors: 
Wenwen Si, Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani

#### 3. Affiliation: 
第一作者：宾夕法尼亚大学计算机与信息科学系

#### 4. Keywords: 
Object detection, adversarial attacks, angelic patches, robustness, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Si_Angelic_Patches_for_Improving_Third-Party_Object_Detector_Performance_CVPR_2021_paper.html  Github: https://github.com/averysi224/angelic_patches

#### 6. Summary : 
- (1):本文研究了深度学习模型在面对分布偏移时的脆弱性问题，提出了一种新的方法——天使贴片，以提高目标检测的鲁棒性。
- (2):过去的方法主要是通过在训练时添加更多的鲁棒性干预和多样化数据来提高模型的鲁棒性，但这些方法并不能完全弥补标准模型性能和受扰动结果之间的差距。本文提出的方法是在目标对象控制其外观的情况下进行研究，通过提供一种可穿戴的贴片来提高这些对象对模型的可见性。本文的方法是首个在多个模型上实现交叉模型效果和多个贴片的目标检测贴片。实验结果表明，本文方法在现实世界的实验中平均准确率提高了30%。
- (3):本文提出了一种新的天使贴片方法，通过反向快速梯度符号方法来提高单阶段和双阶段第三方目标检测器的性能。本文的方法不需要预先了解扰动，通过将贴片同时应用于每个对象实例，不仅增强了分类，还增强了边界框的准确性。本文的方法在多种检测设置下进行了验证，包括数十种合成扰动和仿射变换，甚至不需要额外的训练增强。本文的方法是首个在多个模型上实现交叉模型效果和多个贴片的目标检测贴片。
- (4):本文的方法在多个数据集上进行了实验，包括COCO和PASCAL VOC，实验结果表明，本文方法在多个模型上都取得了显著的性能提升，且在不同的扰动和变换下具有很好的鲁棒性。本文的方法在目标检测任务中取得了很好的性能，支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种名为“天使贴片”的新方法，用于提高目标检测模型对于分布偏移的鲁棒性，例如合成扰动和空间变换。该方法涉及设计可穿戴贴片，以改善模型对于目标的可见性。作者使用反向快速梯度符号方法（FGSM）来获取这些贴片，同时应用于每个对象实例，不仅增强分类，还增强边界框准确性。作者通过编程贴片和真实世界实验广泛评估了他们的方法，并在真实世界实验中实现了平均精度提高30％。该性能也可在严重仿射变换和可变形形状下转移至不同的检测模型。

#### 8. 实验设置：
本文描述了评估所提出的天使贴片的实验设置。考虑了两组比较设置：“Corrupt，NoPatch”和“Corrupt，Patch”；以及“Clear，NoPatch”和“Clear，Patch”。使用的评估指标是COCO目标检测评估指标的标准集以及高置信度预测的IoU 0.5的AR。用于污染感知训练和测试的扰动集是来自ImageNet-C的严重级别3的霜。对于污染不敏感的实验，考虑了15种不同的污染，包括天气和照明污染。相同的贴片变换集用于污染感知和污染不敏感设置。在整个贴片图像上应用随机仿射变换以评估空间鲁棒性。最后，进行了物理贴片实验，以验证所提出的贴片在实际场景中的有效性。

#### 9. 实验结果和分析：
本文介绍了将天使贴片应用于改善第三方目标检测器在各种真实场景下的性能的结果。结果表明，该方法显着提高了污染感知和污染不敏感设置下的检测概率，包括在代表许多安全关键设置的物理贴片设置中。所设计的贴片在多个检测器之间转移良好。然而，未来的工作需要了解贴片是否适用于自然协变移位，并且重要的是验证贴片在更现实的场景中是否不会干扰检测其他对象。


# Paper:468     如何在扩散模型中植入后门？



#### 1. Title: 
How to Backdoor Diffusion Models?

#### 2. Authors: 
Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho

#### 3. Affiliation: 
Sheng-Yen Chou: National Tsing Hua University, Hsinchu, R.O.C (Taiwan)

#### 4. Keywords: 
Diffusion models, backdoor attacks, generative models, deep learning, machine learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2021_paper.html  Github: https://github.com/IBM/BadDiffusion

#### 6. Summary : 
- (1):本文研究了扩散模型在背门攻击下的鲁棒性，提出了BadDiffusion攻击框架，该框架可以在模型训练期间工程化受损扩散过程，以实现背门植入。在推理阶段，背门扩散模型将像未篡改的生成器一样对常规数据输入进行生成，而在接收到植入的触发信号时，会错误地生成一些由恶意行为者设计的目标结果。这种严重的风险可能对建立在有问题模型上的下游任务和应用程序造成可怕的影响。
- (2):以往的背门攻击主要集中在分类任务上，而本文是第一篇探讨扩散模型鲁棒性的研究。BadDiffusion攻击框架需要恶意修改训练数据和前向/后向扩散步骤，这与标准分类器的背门攻击不同。本文通过多种背门攻击设置的实验表明，BadDiffusion可以成功地实现高效和目标特异性的背门扩散模型。更糟糕的是，BadDiffusion可以通过对干净预训练扩散模型进行微调来实现成本效益。
- (3):本文提出了BadDiffusion攻击框架，该框架可以在模型训练期间工程化受损扩散过程，以实现背门植入。BadDiffusion的训练算法和损失函数在本文中进行了详细介绍。本文的主要贡献在于提出了一种新的针对扩散模型的背门攻击框架，并通过实验表明了BadDiffusion可以成功地实现高效和目标特异性的背门扩散模型。
- (4):本文的方法在CelebA-HQ数据集上进行了实验，结果表明BadDiffusion可以成功地实现高效和目标特异性的背门扩散模型。本文的结果表明，扩散模型存在背门攻击的风险，需要引起注意。
#### 7. 方法详细介绍：
本文提出了一种名为BadDiffusion的后门攻击框架，针对扩散模型进行优化。攻击场景假设存在外包训练攻击，用户下载一个预训练的扩散模型，并将DNN的训练工作外包给攻击者。攻击者在训练过程中恶意修改训练数据和前向/后向扩散步骤，以植入后门。被攻击的扩散过程旨在满足两个主要目标：高效性和高特异性。BadDiffusion的训练算法和损失函数在第3.4节中详细介绍，数学推导在附录中提供。

#### 8. 实验设置：
本文在CelebA-HQ和LSUN Church两个数据集上进行了广泛的实验，评估了BadDiffusion的有效性。实验设置包括不同的后门触发模式和目标结果。所有实验的数据污染率均设置为5％。本文还探讨了两种可能的风险缓解措施：对抗神经元修剪（ANP）和推理时间剪辑。

#### 9. 实验结果和分析：
本文表明，BadDiffusion可以成功地植入后门到扩散模型中，同时实现高效性和高特异性。被攻击的扩散模型可以在接收到植入的触发信号时生成目标结果，而对于常规数据输入，则表现得像一个干净（未篡改）的扩散模型。本文还发现，BadDiffusion可以通过微调干净的预训练扩散模型来实现成本效益。实验表明，ANP对于正确的目标发现非常敏感，而推理时间剪辑具有成为简单而有效的后门缓解策略的潜力。BadDiffusion的代码可在GitHub上获得。


# Paper:469     EVA：探索大规模掩码视觉表示学习的极限



#### 1. Title: 
EVA: Exploring the Limits of Masked Visual Representation Learning at Scale

#### 2. Authors: 
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao

#### 3. Affiliation: 
第一作者：北京智源人工智能研究院

#### 4. Keywords: 
Visual representation learning, pre-training, masked image modeling, self-supervised learning, ViT

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2022_paper.html  Github: https://github.com/baaivision/EVA

#### 6. Summary : 
- (1):本文研究的背景是如何将自然语言处理中的预训练语言模型的成功应用到视觉领域，即如何扩大视觉基础模型的规模，以便更好地支持视觉和多模态下游任务。

- (2):过去的方法主要是基于监督或弱监督训练，需要数亿甚至数十亿的标记数据，而本文提出的方法是基于自监督学习的预训练方法，只需要使用公开数据即可。本文提出的方法是通过掩码图像建模（MIM）预训练，使用图像-文本对齐的视觉特征作为预测目标，以此来扩大视觉基础模型的规模。本文的方法在多个视觉下游任务上取得了优异的表现，如图像分类、视频动作识别、目标检测、实例分割和语义分割等。本文的方法创新性地将图像-文本对齐的视觉特征作为预测目标，同时结合了掩码图像建模的优点，使得本文的方法在大规模视觉表示学习方面取得了成功。

- (3):本文提出的方法是通过掩码图像建模（MIM）预训练，使用图像-文本对齐的视觉特征作为预测目标，以此来扩大视觉基础模型的规模。本文的方法创新性地将图像-文本对齐的视觉特征作为预测目标，同时结合了掩码图像建模的优点，使得本文的方法在大规模视觉表示学习方面取得了成功。本文的方法不需要昂贵的监督训练阶段，只需要使用公开数据即可，具有很好的可重复性。

- (4):本文的方法在多个视觉下游任务上取得了优异的表现，如图像分类、视频动作识别、目标检测、实例分割和语义分割等。本文的方法在大规模视觉表示学习方面取得了成功，具有很好的可扩展性和可重复性。
#### 7. 方法详细介绍：
本文提出了一种基于掩码图像建模（MIM）的预训练任务，用于大规模视觉表示学习。MIM任务使用图像-文本对齐的视觉特征作为预测目标，并从图像-文本对比学习的高级语义抽象和掩码图像建模的几何和结构捕捉中获得好处。该方法通过高效地扩展基于Transformer的视觉Transformer（ViT）编码器，将其扩展到10亿个参数，从而获得强大的视觉表示，可在各种下游任务中很好地转移。

#### 8. 实验设置：
EVA的预训练数据包括公开可访问的数据集，如ImageNet-21K、CC12M、CC3M、Object365、COCO和ADE20K，共计2960万张图像。EVA使用Adam进行优化，具有0.05的解耦权重衰减，并且峰值学习率为1e-3。采用0.1的随机深度进行正则化，并使用RandResizeCrop（0.2，1）进行数据增强。不使用颜色抖动。预训练在NVIDIA A100-SXM4-40GB GPU上使用fp16精度进行，并且预训练代码基于PyTorch中的BEiT编写。预训练过程需要大约14.5天。

#### 9. 实验结果和分析：
本文提出的EVA模型在各种基准测试中均取得了最先进的结果，包括ImageNet-1K、COCO、LVIS、ADE20K和Kinetics-400、600和700。EVA在所有这些基准测试中均取得了最先进的性能，表明其在各种视觉任务中的有效性。具体而言，在ImageNet-1K上，EVA实现了最高的平均准确率和最小的性能差距，表明其出色的鲁棒性和泛化能力。在Kinetics-400、Kinetics-600和Kinetics-700上，EVA在视频识别中优于一些最近的视频特定或大型基础模型。在COCO和LVIS上，EVA在目标检测和实例分割任务上创造了新的最先进结果，优于一些领先的方法，如ViTDet-H、FocalNet、Group DETRv2、SwinV2-Giant、FD-SwinV2-Giant和BEiT-3。


# Paper:470     逐点学习分割每个指代物体



#### 1. Title: 
Learning to Segment Every Referring Object Point by Point

#### 2. Authors: 
Mengxue Qu, Yu Wu, Yunchao Wei, Wu Liu, Xiaodan Liang, Yao Zhao

#### 3. Affiliation: 
北京交通大学信息科学研究所

#### 4. Keywords: 
Referring Expression Segmentation, Partially Supervised Learning, Sequence Prediction, Visual Language Fusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qu_Learning_to_Segment_Every_Referring_Object_Point_by_Point_CVPR_2021_paper.html  Github: https://github.com/qumengxue/Partial-RES.git

#### 6. Summary : 
- (1):本文研究的是指代表达分割问题，即如何根据自然语言表达式生成图像中所指代的物体的分割掩模。由于标注像素级别的掩模代价昂贵，本文提出了一种新的部分监督训练范式，即使用丰富的指代边界框和少量（例如1%）的像素级指代掩模进行训练。 

- (2):现有的RES方法大多需要大量的像素级注释，这些注释既昂贵又繁琐。本文提出了一种新的部分监督训练范式，即使用丰富的指代边界框和少量（例如1%）的像素级指代掩模进行训练。本文的方法基于点序列预测模型构建，提出了共同内容Teacher-Forcing和重采样伪点策略，以最大化从REC模型的迁移性。 

- (3):本文提出了一种基于点序列预测模型的部分监督训练方法，通过使用共同内容Teacher-Forcing和重采样伪点策略，最大化从REC模型的迁移性。本文的方法在RefCOCO+@testA上取得了52.06%的准确率（与完全监督设置的58.93%相比），并且在RefCOCO+@val上取得了97%的完全监督性能。 

- (4):本文提出的方法在RefCOCO+@testA上取得了52.06%的准确率（与完全监督设置的58.93%相比），并且在RefCOCO+@val上取得了97%的完全监督性能。本文的方法通过使用丰富的指代边界框和少量（例如1%）的像素级指代掩模进行训练，最大化从REC模型的迁移性，从而解决了像素级别注释代价昂贵的问题。
#### 7. 方法详细介绍：
本文提出了一种部分监督的指代表达分割方法，称为Partial-RES。该方法使用丰富的指代边界框和少量的像素级指代掩码进行训练。模型基于点级序列预测模型构建，包括三个主要模块：视觉-语言融合、Transformer编码器和解码器以及序列构建。其中，Co-Content Teacher-Forcing (CCTF)方法用于将点坐标与指代空间特征相关联，Point-Modulated Cross-Attention方法用于估计指代区域，Resampling Pseudo Points (RPP)策略用于选择更准确的伪点作为监督。具体步骤如下：
(1) 使用REC模型进行完全监督训练，以获得更好的多模态特征表示；
(2) 使用CCTF和PMCA方法进行1％、5％和10％标记掩码数据的初始RES训练；
(3) 使用RPP策略选择由步骤2中的RES模型生成的伪点进行重新训练，以选择适当的伪点。

#### 8. 实验设置：
本文在三个常见的指代表达分割基准数据集RefCOCO、RefCOCO+和RefCOCOg上进行了实验。数据集包含具有相同类别的多个对象和相应的指代表达式的图像。通过随机采样整个训练数据来模拟部分监督训练场景，视觉编码器使用DarkNet-53，语言表达式长度在RefCOCO/RefCOCO+上被修剪为15，在RefCOCOg上被修剪为20。

#### 9. 实验结果与分析：
本文将提出的Partial-RES方法与现有的完全监督方法在RefCOCO、RefCOCO+和RefCOCOg上进行了比较，以IoU分数为指标。结果表明，该方法在1％、5％和10％标记掩码数据的部分监督RES中优于基线方法。验证了CCTF和PMCA方法的有效性，并证明了RPP策略可以提高RES模型的性能。在所有三个数据集上，Partial-RES方法相对于SeqTR基线方法均有显著提高，1％标记掩码数据时，该方法在RefCOCO+@testA上达到了88％的完全监督性能。


# Paper:471     交流电照明下的光源分离和本征图像分解



#### 1. Title: 
Light Source Separation and Intrinsic Image Decomposition under AC Illumination

#### 2. Authors: 
Yusaku Yoshida, Ryo Kawahara, and Takahiro Okabe

#### 3. Affiliation: 
九州工业大学（Kyushu Institute of Technology）人工智能系

#### 4. Keywords: 
Light source separation, intrinsic image decomposition, AC illumination, matrix factorization, dichromatic reflection model

#### 5. Paper: https://openaccess.thecvf.com/content_cvpr_2018/papers/Yoshida_Light_Source_Separation_CVPR_2018_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的背景是人工光源的交流电（AC）照明下的光源分离和本征图像分解。交流电照明下的光源会引起场景辐射值的闪烁，这种闪烁可以用于提取场景的丰富信息。本文利用了AC照明下的闪烁，提出了一种光源分离和本征图像分解的方法。

- (2):过去的方法包括基于自然图像统计的本征图像分解和基于多张图像的本征图像分解。然而，这些方法都无法处理AC照明下的光源分离和本征图像分解。本文提出的方法是基于盲源分离和二色反射模型的，可以同时解决光源颜色、漫反射值、漫反射和镜面强度（阴影）等问题。本文的方法通过矩阵分解解决了盲光源分离的歧义，并通过物理模型解决了本征图像分解中的歧义。

- (3):本文提出的方法是先进行光源分离，然后进行本征图像分解。具体来说，本文利用矩阵分解解决了盲光源分离的歧义，并通过物理模型解决了本征图像分解中的歧义。本文的方法可以恢复光源颜色、漫反射值和每个光源下的漫反射和镜面强度（阴影），并且在自动白平衡应用方面具有有效性。

- (4):本文的方法在合成和真实图像上进行了实验，证明了其有效性。本文的方法可以恢复光源颜色、漫反射值和每个光源下的漫反射和镜面强度（阴影），并且在自动白平衡应用方面具有有效性。
#### 7. 方法详细介绍：
本文提出了一种在交流照明下进行光源分离（LSS）和内在图像分解（IID）的方法。该方法通过将盲目的LSS与漫反射IID相结合来解决盲目LSS中的歧义，并通过从高光中估计光源颜色来解决漫反射IID中的歧义。该方法涉及非负矩阵分解和非线性最小化来估计未知矩阵。该方法提供了关于如何估计基础图像、光源强度、漫反射值和漫反射强度的详细步骤。

#### 8. 实验设置：
本文在合成图像和真实图像上进行了实验。对于合成图像，测试了四种情况，包括由两个具有不同颜色、方向和时间强度曲线的光源照亮的梨和海龟。对于真实图像，使用高速相机捕获了两个场景，其中一个场景使用卤素灯和LED作为光源。该方法与现有方法进行了比较，并使用恢复的基础图像的PSNR来评估性能。

#### 9. 实验结果和分析：
本文提出的方法在合成图像和真实图像上都表现良好，并准确估计了光源颜色。该方法对于自动白平衡非常有用。未来的工作包括在多个光源下进行LSS和IID，通过集成复杂的噪声去除来提高准确性，并将其应用于图像分割、物体识别、形状来自阴影等方面。


# Paper:472     GeoLayoutLM：面向视觉信息提取的几何预训练



#### 1. Title: 
GeoLayoutLM: Geometric Pre-training for Visual Information Extraction

#### 2. Authors: 
Chuwei Luo, Changxu Cheng, Qi Zheng, Cong Yao

#### 3. Affiliation: 
阿里巴巴达摩院

#### 4. Keywords: 
Visual information extraction, semantic entity recognition, relation extraction, pre-trained models, geometric pre-training, document intelligence

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Luo_GeoLayoutLM_Geometric_Pre-Training_for_Visual_Information_Extraction_CVPR_2021_paper.html
Github: https://github.com/AlibabaResearch/AdvancedLiterateMachinery

#### 6. Summary:
- (1):本文研究的是文档智能中的视觉信息提取，主要包括语义实体识别和关系抽取两个任务。其中，关系抽取一直是一个具有挑战性的问题，因为它需要考虑文档中实体之间的几何关系。 
- (2):现有的预训练模型大多隐式地学习几何表示，这对于关系抽取任务来说是不够的。此外，现有方法在预训练阶段和微调阶段之间存在目标差距，这也限制了关系抽取的性能。因此，本文提出了一种名为GeoLayoutLM的多模态框架，用于VIE。GeoLayoutLM通过三个特别设计的几何相关预训练任务来显式地建模几何关系，从而实现几何预训练。此外，本文还设计了新颖的关系头，通过几何预训练任务进行预训练，并进行微调以丰富和增强特征表示。 
- (3):本文提出了一种新的几何预训练策略，GeoLayoutLM，它通过三个几何关系来显式地利用文本段之间的几何关系，并引入了新颖的关系头来缓解预训练和微调之间的目标差距。GeoLayoutLM是第一个在文档预训练中探索多对和多段几何关系的模型。 
- (4):在标准VIE基准测试中，GeoLayoutLM在SER任务中取得了极具竞争力的分数，并且在RE任务中显著优于以前的最新技术（例如，在FUNSD上，RE的F1分数从80.35％提高到89.45％）。因此，本文提出的方法在关系抽取任务上取得了很好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为GeoLayoutLM的多模态框架，用于视觉信息提取。GeoLayoutLM的主干由一个独立的视觉模块、一个文本布局模块和交互式的视觉和文本共同关注层组成。模型还包括两个关系头，包括一个粗略关系预测（CPR）头和一个新颖的关系特征增强（RFE）头，用于增强关系特征表示，用于关系预训练和RE微调。模型同时进行四个自监督任务的预训练，包括Masked Visual-Language Model（MVLM）和三个自监督几何预训练任务，以建模几何关系。

具体而言，GeoLayoutLM通过三个特别设计的几何相关预训练任务来实现几何预训练，定义了三种几何关系：两个文本片段之间的关系（GeoPair）、多个文本片段对之间的关系（GeoMPair）和三个文本片段之间的关系（GeoTriplet）。相应地，提出了三个自监督预训练任务。GeoPair关系由方向和距离建模（DDM）任务建模，其中GeoLayoutLM需要指出有向对的方向，并确定一个片段是否在该方向上最近。此外，我们设计了一个全新的预训练目标，称为方向异常检测（DDE），用于GeoMPair，使我们的模型能够捕捉片段对之间的方向的常见模式，增强对特征表示并发现分离的片段对。对于GeoTriplet，我们提出了一个三元组共线性识别（CIT）任务，用于识别三个片段是否共线，这是对多片段关系建模的一步前进。此外，提出了新颖的关系头，用于学习更好的关系特征，这些头部通过几何预训练任务进行预训练，以吸收关于几何的先验知识，从而缓解预训练和微调之间的差距。

#### 8. 实验设置：
本文在五个公共基准测试集上进行了广泛的实验，以展示所提出的GeoLayoutLM的有效性。这些基准测试集包括FUNSD、SROIE、RVL-CDIP、DocBank和CORD。实验涵盖了包括键值链接作为关系提取、实体分组作为关系提取和语义实体识别在内的视觉信息提取任务。

#### 9. 实验结果和分析：
实验结果表明，GeoLayoutLM在SER和RE任务中均优于LayoutLMv3和基于规则的几何约束方法。RFE头的预训练比CRP头更重要。RSF策略显著提高了方法的精度，但稍微牺牲了召回率。在少样本学习方面，GeoLayoutLM显示出比LayoutLMv3和没有预训练的修改版GeoLayoutLM更大的优势。本文还进行了一项实验，通过使用简单的几何限制过滤假阳性关系，可以大幅提高精度（超过4个点），而召回率保持不变。这个实验证明了LayoutLMv3没有充分利用有用的几何关系信息。


# Paper:473     多视角立体视觉表示再探：区域感知MVSNet



#### 1. Title: 
Multi-View Stereo Representation Revist: Region-Aware MVSNet

#### 2. Authors: 
Yisu Zhang, Jianke Zhu, Lixiang Lin

#### 3. Affiliation: 
浙江大学

#### 4. Keywords: 
Multi-view stereo, deep learning, point-to-surface distance, signed distance function, patch-aware

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhang_Multi-View_Stereo_Representation_Revisit_Region-Aware_MVSNet_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究多视角立体视觉中的深度学习方法，旨在解决现有方法中对于无纹理区域和物体边界的重建问题。

- (2):传统的基于补丁匹配的方法对于复杂的光照条件和无纹理区域的重建效果较差，而基于深度学习的方法虽然能够预测像素级深度，但在物体边界处会出现很多异常点。本文提出了一种新的方法，通过点到表面距离来扩展模型的感知范围，从而在无纹理区域实现完整的估计，并减少物体边界处的异常点。

- (3):本文提出的RA-MVSNet框架包括两个分支，一个用于预测概率体积，另一个用于估计点到表面距离。两个分支的融合可以得到过滤后的深度图，而点到表面距离分支可以生成隐式表示。具体地，本文利用成本体积生成概率和距离体积，进而结合估计最终深度图。点到表面距离监督使用模型的补丁感知来估计与特定像素对应的深度值，从而提高了无纹理或边界区域的性能。

- (4):本文在DTU和Tanks & Temples数据集上进行了实验，结果表明，与传统的深度学习方法相比，我们提出的RA-MVSNet方法通过利用有符号距离监督获得了更完整的重建结果。
#### 7. 方法详细介绍：
本文提出了一种区域感知的MVSNet多视角立体表示方法。该方法通过在已知深度范围内对3D成本体进行正则化，估计由M层深度假设组成的深度图。使用可微分的单应性聚合特征体积，并使用自适应策略将所有特征体积聚合成单个成本体积。最终成本体积使用多个3D CNN层计算以预测像素权重矩阵。通过使用基于softmax的3D CNN正则化网络获得概率体积，通过使用基于tanh的3D CNN正则化网络获得距离体积。融合概率体积和距离体积用于计算深度图，模型的总损失是两个分支的加权和。该方法还引入了点到表面距离预测分支，使模型能够感知更广泛的表面，并生成具有细节的显式网格表面。

具体步骤如下：
1. 构建成本体积：利用MVSNet框架构建成本体积，使用扭曲的锥体特征预测与参考图像对应的深度图。
2. 特征体积聚合：使用可微分的单应性聚合源图像特征到参考视图中，使用共享权重的2D FPN网络提取所有图像特征。使用递归特征金字塔（RFP）结构作为图像编码器，获得三个不同尺度的特征图金字塔。
3. 多尺度深度图和点到表面距离预测：使用3D CNN层预测像素权重矩阵，使用基于softmax的3D CNN正则化网络获得概率体积，使用基于tanh的3D CNN正则化网络获得距离体积。融合概率体积和距离体积用于计算深度图。
4. 处理地面真实值：使用地面真实值对深度图进行训练和优化。

#### 8. 实验设置：
本文使用了DTU和Tanks & Temples数据集进行实验。在DTU数据集上，使用了22个场景，每个场景包含49个图像。在Tanks & Temples数据集上，使用了7个场景，每个场景包含数百张图像。实验使用了Nvidia Tesla V100 GPU进行训练和测试，使用Adam优化器进行训练，学习率为0.0001，批量大小为1。

#### 9. 实验结果和分析：
本文提出的RA-MVSNet方法在多个数据集上均取得了优异的结果，超过了现有方法的完整性和整体指标。在DTU评估集上，该方法得分最高，为0.297。该方法还产生了比以前的方法如Uni-MVSNet和TransMVSNet更完整且更少的异常值结果。消融实验表明，MVS网络中引入的有符号距离预测分支提高了重建结果的完整性，并生成了显式的网格表面。两个分支的融合有效地提高了准确性，阈值参数权衡了两个分支的融合。在Tanks and Temples数据集上，该方法在中级和高级子集上均取得了最佳性能，F分数分别为65.72和39.93。


# Paper:474     面向艺术图像美学评估的大规模数据集和新方法



#### 1. Title: 
Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method

#### 2. Authors: 
Ran Yi, Haoyuan Tian, Zhihao Gu, Yu-Kun Lai, Paul L. Rosin

#### 3. Affiliation: 
上海交通大学

#### 4. Keywords: 
Image aesthetics assessment, artistic images, large-scale dataset, SAAN

#### 5. Paper: https://arxiv.org/abs/2003.07577  Github: https://github.com/Dreemurr-T/BAID.git 

#### 6. Summary : 
- (1):本文研究的是艺术图像美学评估，由于其高度主观性，因此是一个具有挑战性的任务。目前的研究大多依赖于大规模数据集来学习所有类型的摄影图像的通用模型，但是对于艺术图像的美学质量的测量却鲜有研究，现有的数据集也只包含相对较少的艺术品，这是艺术图像美学评估的一个巨大障碍。

- (2):现有的方法主要是基于大规模数据集的学习，但是这些数据集只包含相对较少的艺术品，因此无法很好地评估艺术图像的美学质量。本文提出了一个大规模的艺术图像数据集BAID，并提出了一种新的方法SAAN，该方法可以有效地提取和利用特定风格和通用美学信息来评估艺术图像的美学质量。该方法的动机充分。

- (3):本文提出了一种新的方法SAAN，该方法可以有效地提取和利用特定风格和通用美学信息来评估艺术图像的美学质量。该方法包括两个子网络，一个用于提取特定风格的美学信息，另一个用于提取通用美学信息。实验结果表明，该方法在BAID数据集上的表现优于现有的IAA方法。

- (4):本文提出的方法在BAID数据集上取得了较好的表现，可以有效地评估艺术图像的美学质量。该方法的创新点在于提出了一种新的方法来提取和利用特定风格和通用美学信息，以评估艺术图像的美学质量。
#### 7. 方法详细介绍：
本文提出了一种名为SAAN（Style-specific Art Assessment Network）的方法，可以有效地提取和利用特定风格和通用美学信息来评估艺术图像。该方法由两个主要组件组成：特定风格美学评估网络（SSA-Net）和通用美学评估网络（GA-Net）。SSA-Net旨在捕捉艺术图像的特定风格美学信息，而GA-Net用于提取通用美学信息。然后将两个网络组合以获得图像的最终美学评分。具体步骤包括：（1）使用SSA-Net提取特定风格的美学特征；（2）使用GA-Net提取通用美学特征；（3）将两个网络的输出进行融合，得到最终的美学评分。在BAID数据集上的实验结果表明，SAAN方法在定量比较中优于现有的IAA方法。

#### 8. 实验设置：
本文使用BAID数据集进行实验，该数据集包含了来自不同风格和类型的艺术图像。实验使用了Python编程语言和PyTorch深度学习框架。在训练过程中，使用了Adam优化器和交叉熵损失函数。为了评估模型的性能，使用了平均绝对误差（MAE）和皮尔逊相关系数（PCC）两个指标。

#### 9. 实验结果和分析：
本文提出的SAAN方法在BAID数据集上的实验结果表明，该方法可以有效地提取和利用特定风格和通用美学信息来评估艺术图像的美学质量。与现有的IAA方法相比，SAAN方法在定量比较中表现更好，具有更高的MAE和PCC值。此外，本文还进行了定性分析，证明了SAAN方法可以捕捉到艺术图像的特定风格和通用美学特征，具有较好的可解释性和可视化效果。


# Paper:475     从事件中观测电网频率



#### 1. Title: 
Seeing” Electric Network Frequency from Events

#### 2. Authors: 
Lexuan Xu, Guang Hua, Haijian Zhang, Lei Yu, Ning Qiao

#### 3. Affiliation: 
Lexuan Xu, Haijian Zhang, Lei Yu: 武汉大学 (Wuhan University), China
Guang Hua: 新加坡科技研究局信息通信研究所 (Institute for Infocomm Research (I2R), A*STAR), Singapore
Ning Qiao: SynSense Tech. Co. Ltd., Chengdu, China

#### 4. Keywords: 
Electric Network Frequency, event camera, neuromorphic sensor, ENF estimation, video-based ENF, E-ENF, ENF capture, ENF fluctuation, illumination fluctuations, ENF-based applications

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Seeing_Electric_Network_Frequency_From_Events_CVPR_2021_paper.html  Github: https://github.com/xlx-creater/E-ENF

#### 6. Summary : 
- (1):本文研究的背景是电网频率（ENF）的提取，ENF是电网的交流电传输频率，可以通过光源的闪烁来检测和估计，从而实现多媒体取证验证、电力负载监测、记录设备识别等应用。
- (2):过去的方法是基于视频的ENF（V-ENF）提取方法，通过对视频帧的像素强度进行平均来提取光照变化，但由于传统相机的固有特性，这种方法容易受到不足的相机帧率、运动干扰和极端光照条件的影响，导致估计性能下降。本文提出了一种基于事件相机的ENF提取方法，通过将光照变化转换为事件流来提取ENF，具有高时间分辨率、高动态范围和低延迟等优点，可以在运动干扰和极端光照等恶劣条件下提取可靠的ENF轨迹。
- (3):本文提出了一种有效从事件中提取ENF的方法，采用均匀间隔时间采样算法、多数投票空间采样算法和谐波选择算法，构建了第一个事件-视频混合ENF数据集（EV-ENFD），包含在真实环境中记录的事件和视频。实验结果表明，与传统的V-ENF方法相比，本文提出的E-ENF方法可以提取更准确的ENF轨迹，在具有物体运动和极端光照条件的挑战性环境中表现出更好的性能。
- (4):本文的方法在ENF提取任务上取得了良好的性能，可以支持其目标。
#### 7. 方法详细介绍：
本文提出了一种从事件流中估计电网频率（ENF）的方法，称为基于事件的ENF（E-ENF）估计。该方法包括均匀间隔的时间采样算法、多数投票的空间采样算法和谐波选择算法。具体步骤如下：
1. 通过算法1进行均匀间隔的时间采样，得到一维极坐标序列E(n)。
2. 对序列进行滤波，滤除两倍ENF名义值及更高谐波分量，以利用更多有用信息。
3. 对每个谐波分量使用短时傅里叶变换（STFT）和峰值搜索进行分析，得到相应的主导时间-频率变化。
4. 提出谐波选择机制，确保所有考虑的谐波都能贡献而不会干扰时间-频率估计过程。
5. 对每个谐波估计结果进行归一化和分段处理，选择最符合ENF微小和缓慢变化特性的谐波结果作为相应时间的估计结果。

#### 8. 实验设置：
本文使用事件-视频混合ENF数据集（EV-ENFD）进行实验。EV-ENFD包含三个类别的数据：静态场景、动态场景和极端光照场景。每个数据集包含事件流、低ISO录制的低分辨率视频和高ISO录制的高分辨率视频。为了评估提取的ENF跟踪的质量，EV-ENFD还包括每个数据集的相应地面真值ENF参考。

#### 9. 实验结果与分析：
本文将提出的E-ENF方法与文献[26]中提出的V-ENF方法在几种代表性录制条件下进行比较。结果表明，即使在完全静态且充分照明的场景中，V-ENF方法也存在不合格的噪声，而提出的E-ENF方法可以在存在大量独立于照明的事件的情况下提供更好的ENF估计。提出的谐波选择机制可以在存在大量独立于照明的事件的情况下导致更好的ENF估计。本文提供了一个表格，用平均相关系数（CC）和平均绝对误差（MAE）评估V-ENF和提出的E-ENF应用于事件-视频ENF数据集的记录的相似性和绝对误差。


# Paper:476     HierVL: 学习分层视频-语言嵌入



#### 1. Title: 
HierVL: Learning Hierarchical Video-Language Embeddings

#### 2. Authors: 
Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, Kristen Grauman

#### 3. Affiliation: 
第一作者：Kumar Ashutosh，来自美国德克萨斯大学奥斯汀分校；其他作者来自Facebook AI Research。

#### 4. Keywords: 
Video-language embeddings, hierarchical temporal understanding, contrastive learning, long-term video modeling.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是视频理解领域中，如何将语义注入视觉表示中，以及如何同时考虑短期和长期的关联。
- (2):过去的方法只能捕捉到短期的关联，而无法捕捉到整个视频的长期意图。本文提出了一种新的层次化视频-语言嵌入方法，通过两层对比学习，同时考虑短期和长期的关联，解决了过去方法的问题。
- (3):本文提出的方法是一种层次化的视频-语言嵌入方法，通过两层对比学习，同时考虑短期和长期的关联。本文的创新点在于，将视频的全局文本摘要与局部文本描述相结合，以捕捉视频的长期意图。本文的方法在多个数据集和任务上都取得了优异的表现。
- (4):本文的方法在多个数据集和任务上都取得了优异的表现，证明了其在视频理解领域中的有效性。本文的方法可以用于长期视频建模、视频检索、视频分类等任务。
#### 7. 方法详细介绍：
本文提出了一种名为HierVL的层次化视频-语言嵌入方法，旨在捕捉视频中的短期动作和长期意图。该模型使用时间戳的片段级文本描述和全局（视频级）文本摘要进行训练。训练目标是层次对比学习，鼓励在片段级别和视频级别上进行文本-视觉对齐。顶层（父级）鼓励聚合视频片段接近总体文本摘要，而底层（子级）则训练各个片段与其各自的描述相似。该模型设计通过使用短期特征的聚合和联合训练两个层次的注释来解决技术挑战，以避免任何一层的灾难性遗忘。

#### 8. 实验设置：
本文使用了3,670小时的自我中心视频数据集Ego4D进行模型训练，该数据集包含每个摄像机佩戴者执行的每个动作的低级文本描述（“叙述”）以及视频级摘要。模型在多个视频基准测试中进行评估，包括Ego4D长期预测（LTA），Charades-Ego动作识别，EPIC-KITCHENS-100多实例检索和HowTo100M长视频分类。

#### 9. 实验结果和分析：
本文提出的HierVL模型在多个视频基准测试中优于强基线和现有方法。该模型成功地将其预训练表示转移到Charades-Ego，EPIC-KITCHENS和HowTo100M的推理中，包括零-shot和微调设置。在提交时，HierVL在Ego4D长期预测（LTA），Charades-Ego动作识别，EPIC-KITCHENS-100多实例检索（零-shot和微调设置）和HowTo100M长视频分类方面均取得了最先进的性能。 

#### 全文总结：
本文提出了一种名为HierVL的层次化视频-语言嵌入方法，旨在捕捉视频中的短期动作和长期意图。该模型使用时间戳的片段级文本描述和全局（视频级）文本摘要进行训练，并使用层次对比学习来鼓励文本-视觉对齐。实验结果表明，HierVL模型在多个视频基准测试中优于强基线和现有方法，包括Ego4D长期预测（LTA），Charades-Ego动作识别，EPIC-KITCHENS-100多实例检索和HowTo100M长视频分类。


# Paper:477     从4D点云中构建任意3D物体的可重构模型



#### 1. Title: 
Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds

#### 2. Authors: 
Shaowei Liu, Saurabh Gupta, Shenlong Wang

#### 3. Affiliation: 
1. 伊利诺伊大学厄巴纳-香槟分校

#### 4. Keywords: 
Rearticulation, 3D reconstruction, point cloud, kinematics, energy minimization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Building_Rearticulable_Models_for_Arbitrary_3D_Objects_From_4D_Point_CVPR_2021_paper.html  Github: https://stevenlsw.github.io/reart/

#### 6. Summary : 
- (1):本文研究了从4D点云中构建任意3D物体的可重构模型，实现了物体的再构建。 
- (2):过去的方法大多是针对特定类别的物体进行建模，而本文提出的方法可以处理任意物体的建模。本文的方法通过联合优化部分分割、变换和运动学来实现物体的建模，相比于过去的方法，本文的方法可以更好地处理物体的再构建问题。 
- (3):本文提出了一种新的能量最小化框架，通过联合优化部分分割、变换和运动学来实现物体的建模。本文的方法可以处理任意物体的建模，相比于过去的方法，本文的方法可以更好地处理物体的再构建问题。 
- (4):本文的方法在新的关节机器人数据集和Sapiens数据集上进行了测试，结果表明本文的方法在各种指标上优于两种领先的先前方法。
#### 7. 方法详细介绍：
本文提出了一种从4D点云中构建任意可重组模型的方法。该方法涉及定义一个由n个部分组成的运动学模型，这些部分通过1-DOF关节连接在一起。每个关节可以是旋转或平移关节，由螺旋参数进行参数化。部分表示为欧几里得空间的分区，用基于坐标的神经网络来参数化部分分割场。该方法使用分析合成方法估计模型参数，并找到模型参数，使得在适当的时间关节活动后，模型与给定的点云匹配良好。该方法首先拟合一个松弛模型，该模型不限制部分形成运动学树，然后使用与松弛模型相同的代价函数优化模型，但在每个时间步骤下重新调整松弛模型。定义的能量函数涉及6-DOF变换和神经分割场的联合优化。使用Gumbel-softmax技巧以及直通梯度估计器来克服通过分割场的离散索引进行优化的挑战。

#### 8. 实验设置：
本文在一个新的可关节机器人数据集和Sapiens数据集上测试了所提出的方法。实验表明，该方法在各种指标上优于两个领先的先前工作。

#### 9. 实验结果与分析：
本文提出的方法在RoboArt测试集和Sapiens数据集上进行了评估，并在各种衡量输入重建质量、模型准确性和重新激活误差的指标上优于过去的方法。在RoboArt验证集上评估了Eq.（5）中不同能量项的重要性和设计选择，发现所有三个项都很重要，其中流量项最重要。其他设计选择，包括神经分割场（f）、Gumbel-softmax、投影到运动学模型和规范帧选择，都有助于最终性能。


# Paper:478     基于类亲和力转移的少样本语义图像合成



#### 1. Title: 
Few-shot Semantic Image Synthesis with Class Affinity Transfer

#### 2. Authors: 
Marlène Careil, Jakob Verbeek, Stéphane Lathuilière

#### 3. Affiliation: 
Marlène Careil: LTCI, Télecom Paris, IP Paris

#### 4. Keywords: 
Semantic image synthesis, few-shot learning, transfer learning, class affinity, GAN, diffusion model

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Careil_Few-Shot_Semantic_Image_Synthesis_With_Class_Affinity_Transfer_CVPR_2021_paper.html
Github: None

#### 6. Summary: 
- (1):本文研究的是语义图像合成，旨在通过少量的目标数据集来训练模型，以减少标注数据的成本。
- (2):过去的方法需要大量的标注数据，成本高昂，因此需要一种转移学习的方法来减少标注数据的需求。本文提出了一种基于类亲和力转移的方法，通过预测源和目标类之间的关系来提高模型在小目标数据集上的学习能力。本文的方法可以与GAN和扩散模型相结合，通过不同的方法来估计类亲和力，如语义分割、自监督视觉特征和文本标签嵌入。本文的方法可以有效地结合不同的方法来估计类亲和力，并且在生成图像模型的转移学习方面取得了显著的改进。
- (3):本文提出了一种基于类亲和力转移的方法，通过预测源和目标类之间的关系来提高模型在小目标数据集上的学习能力。本文的方法可以与GAN和扩散模型相结合，通过不同的方法来估计类亲和力，如语义分割、自监督视觉特征和文本标签嵌入。本文的方法可以有效地结合不同的方法来估计类亲和力，并且在生成图像模型的转移学习方面取得了显著的改进。
- (4):本文的方法在ADE20K、COCO-Stuff和Cityscapes数据集上进行了广泛的实验，使用的目标数据集大小从25到400不等。实验结果表明，本文的方法显著优于现有的转移方法。本文的方法允许从不超过100个目标图像进行逼真的合成，并且实现了接近标准训练的图像质量。与以往的转移方法不同，本文的方法还实现了非平凡的无需训练的转移结果，其中我们只是将类亲和力矩阵附加到源模型，而无需进一步微调它。
#### 7. 方法详细介绍：
本文提出了一种名为“类亲和力转移”（Class Affinity Transfer，CAT）的方法，用于少样本语义图像合成。该方法利用预训练的语义图像合成模型在大型源数据集上的学习能力，通过估计源和目标类之间的成对关系来提高小型目标数据集的学习能力。首先引入类亲和力矩阵作为源模型的第一层，使其与目标标签映射兼容，然后进一步微调源模型以适应目标域。考虑了不同的方法来估计类亲和力，包括源域的语义分割、文本标签嵌入和自监督视觉特征。该方法被集成到最先进的对抗性和扩散式语义合成模型中。

#### 8. 实验设置：
本文在ADE20K、COCO-Stuff和Cityscapes数据集上进行实验，使用大小从25到400张的目标数据集。使用OASIS架构进行实验，并考虑了不同的方法来估计类亲和力，包括基于文本、基于分割和基于自监督特征的方法以及它们的组合。类亲和力矩阵可以随机初始化或使用组合方法初始化。使用FID和mIoU指标评估性能。

#### 9. 实验结果和分析：
本文的实验结果表明，不同方法估计类亲和力可以有效地结合使用，提出的方法显著优于现有的最先进的转移方法。该方法允许从不超过100个目标图像进行逼真的合成，并实现了接近于完整目标数据集训练的图像质量。该方法还实现了非平凡的无训练转移结果，其中我们仅将类亲和力矩阵前置到源模型中，而无需进一步微调它。使用FID和mIoU指标评估性能，该方法在COCO-Stuff、ADE20K和Cityscapes数据集上的子集上进行了实验，表明该方法在少样本语义图像合成方面优于现有方法。


# Paper:479     OmniObject3D：大词汇量的3D物体数据集，用于逼真的感知、重建和生成



#### 1. Title: 
OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation

#### 2. Authors: 
Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, Ziwei Liu

#### 3. Affiliation: 
第一作者：Tong Wu，香港中文大学和香港科技大学

#### 4. Keywords: 
3D object dataset, perception, reconstruction, generation, real-scanned objects

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2021_paper.html  Github: https://github.com/OmniObject3D/OmniObject3D

#### 6. Summary : 
- (1):本文的研究背景是计算机视觉中的3D物体感知、重建和生成，目前大多数方法都依赖于合成数据集，缺乏大规模的真实世界3D物体数据库。
- (2):过去的方法主要依赖于合成数据集，与真实数据存在差距，难以应用于实际场景。本文提出了OmniObject3D，一个大规模的、高质量的真实世界3D物体数据集，包含了大量的高质量真实扫描的3D物体，支持各种研究主题，如感知、新视角合成、神经表面重建和3D生成。本文的方法具有很好的动机。
- (3):本文提出了OmniObject3D数据集，包含了6,000个高质量的真实扫描的3D物体，190个日常类别，具有大词汇量和丰富的注释。每个3D物体都是用2D和3D传感器捕获的，提供了纹理网格、点云、多视角渲染图像和多个真实捕获的视频。本文的方法在四个评估任务上进行了广泛的研究，包括鲁棒的3D感知、新视角合成、神经表面重建和3D物体生成，揭示了真实3D视觉研究中的新观察、挑战和机遇。
- (4):本文的方法在四个评估任务上进行了广泛的研究，包括鲁棒的3D感知、新视角合成、神经表面重建和3D物体生成。在这些基准测试中，本文的方法取得了很好的性能，支持了其目标。
#### 7. 方法详细介绍：
本文提出了一个大规模的三维物体数据集 OmniObject3D，包括 6,000 个来自 190 个日常类别的高质量纹理网格。每个三维物体都使用 2D 和 3D 传感器进行捕捉，提供了纹理网格、采样点云、多视角渲染图像和多个真实捕捉的视频。本文还提出了四个评估任务：鲁棒的三维感知、新视角合成、神经表面重建和三维物体生成。对于稀疏视角表面重建问题，本文提出了 SparseNeuS、NeuS、MonoSDF、pixelNeRF 和 MVSNeRF 等方法，并在表格 6 和图 4 中进行了定量和定性比较。结果表明，目前稀疏视角表面重建问题尚未得到很好的解决。本文还对从三个方面（语义分布、多样性和质量）训练的生成模型进行了评估，结果表明在大词汇量和真实数据集上训练和评估生成模型是一个有前途但具有挑战性的任务。

#### 8. 实验设置：
本文提出了一个大规模的三维物体数据集 OmniObject3D，包括 6,000 个来自 190 个日常类别的高质量纹理网格。每个三维物体都使用 2D 和 3D 传感器进行捕捉，提供了纹理网格、采样点云、多视角渲染图像和多个真实捕捉的视频。本文还设置了四个评估任务：鲁棒的三维感知、新视角合成、神经表面重建和三维物体生成。

#### 9. 实验结果和分析：
本文在 OmniObject3D 数据集上进行了广泛的研究，包括鲁棒的三维感知、新视角合成、神经表面重建和三维物体生成四个评估任务。对于鲁棒的三维感知，本文使用 ModelNet-40 数据集训练模型，并在 OmniObject3D 上评估其性能，以检查 OOD 风格的鲁棒性。本文还使用常见的损坏方式创建了 OmniObject3D-C，以检查 OOD 损坏的鲁棒性。本文使用 OmniObject3D 上的总体准确率（OA）来衡量 OOD 风格的鲁棒性，使用 DGCNN 规范化的 mCE 来衡量 OOD 损坏的鲁棒性。对于新视角合成和神经表面重建，本文评估了几种代表性方法的性能，并在单场景和跨场景设置下进行了评估。在单场景设置下，Plenoxels 在 PSNR、SSIM 和 LPIPS 上的平均表现最好，而 MVSNeRF 和 pixelNeRF 在跨场景设置下表现良好。在稠密视角设置下，本文评估了三种代表性方法的性能，并计算了重建表面与地面真值之间的 Chamfer 距离。在稀疏视角设置下，本文评估了 NeuS、MonoSDF、SparseNeuS、pixelNeRF 和 MVSNeRF 等方法的性能，并在表格 6 中呈现了结果。其中，pixelNeRF 和 MVSNeRF 的性能最佳。


# Paper:480     RangeViT：面向自动驾驶中的三维语义分割的视觉Transformer



#### 1. Title: 
RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving

#### 2. Authors: 
Angelika Ando, Spyros Gidaris, Andrei Bursuc, Gilles Puy, Alexandre Boulch, Renaud Marlet

#### 3. Affiliation: 
Angelika Ando: Centre for Robotics, Mines Paris, Université PSL, Paris, France

#### 4. Keywords: 
Semantic segmentation, LiDAR point clouds, Vision Transformers, Autonomous Driving, RangeViT

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ando_RangeViT_Towards_Vision_Transformers_for_3D_Semantic_Segmentation_in_Autonomous_CVPR_2021_paper.html Github: https://github.com/valeoai/rangevit

#### 6. Summary : 
- (1):本文旨在探讨如何将Vision Transformers(ViT)应用于3D语义分割任务中，以提高自动驾驶中的LiDAR点云分割效果。 
- (2):过去的方法通常将LiDAR点云投影到2D平面上，然后使用2D CNN进行处理。本文提出的方法使用ViT模型，将点云投影到2D平面上，然后提取patch-based的视觉token，通过ViT编码器获得深度patch表示，最后使用轻量级网络进行解码，得到像素级别的标签预测。本文的方法通过三个关键点来提高ViT模型的性能：(a)使用与RGB图像相同的骨干架构，利用大量的图像数据进行预训练，以提高ViT模型的表示学习能力；(b)将经典的线性嵌入层替换为定制的卷积干线，以弥补ViT模型的归纳偏差；(c)使用卷积解码器和来自卷积干线的跳跃连接来结合ViT编码器的高层次粗略预测和卷积干线的低层次细粒度特征。 
- (3):本文提出的方法是将ViT模型应用于LiDAR点云分割任务的首次尝试。通过对ViT模型的编码器进行重新设计，本文的方法可以在保留ViT模型强大表示学习能力的同时，提高其在点云分割任务中的性能。 
- (4):本文的方法在nuScenes和SemanticKITTI数据集上取得了优于现有投影方法的最佳结果。这表明本文的方法可以有效地将ViT模型应用于LiDAR点云分割任务中，以提高自动驾驶中的环境感知能力。
#### 7. 方法详细介绍：
本文提出了一种基于视觉变换器（ViT）的LiDAR语义分割方法，称为RangeViT。该方法使用范围投影将LiDAR点云表示为2D范围图像，然后使用卷积干线和ViT编码器提取深度补丁表示。接下来，使用解码器来细化粗糙的补丁表示，并使用3D细化器层将像素级特征从范围图像空间转换为3D空间中的点级预测。该方法使用多类焦点损失和Lovasz-softmax损失的总和作为训练损失，并在推理期间使用滑动窗口方法。 

#### 8. 实验设置：
本文在两个数据集上验证了所提出的方法：nuScenes和SemanticKITTI。使用平均交集联合（mIoU）作为评估指标。在推理期间，将范围图像分成与训练期间相同大小的重叠裁剪。 

#### 9. 实验结果和分析：
本文的实验结果表明，RangeViT模型在自动驾驶中的3D语义分割任务上优于之前的2D投影方法，并且与强大的基于体素的Cylinder3D方法的差距缩小。类别IoU分数也表明，RangeViT通常实现了最佳或次佳的性能。本文还比较了不同微调策略的性能，并表明冻结注意力层会导致最佳性能。此外，本文还对编码器骨干进行了消融实验，并表明ViT特征比CNN更适合用于LiDAR数据的迁移学习。在nuScenes数据集上的定性结果表明，RangeViT的预测结果存在一些小错误，但是ViT在大型图像数据集上的预训练可以有效地用于LiDAR分割，以达到2D投影方法的最新性能水平。


# Paper:481     三秒前发生了什么？利用热成像推断过去



#### 1. Title: 
What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging

#### 2. Authors: 
Zitian Tang, Wenjie Ye, Wei-Chiu Ma, Hang Zhao

#### 3. Affiliation: 
Zitian Tang, Wenjie Ye, Hang Zhao: 清华大学信息科学与技术学院 (IIIS, Tsinghua University)
Wei-Chiu Ma: 麻省理工学院计算机科学与人工智能实验室 (CSAIL, MIT)

#### 4. Keywords: 
Thermal imaging, human motion analysis, RGB-Thermal dataset, past human pose estimation, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Tang_What_Happened_3_Seconds_Ago_Inferring_the_Past_With_Thermal_CVPR_2021_paper.html  Github: https://github.com/ZitianTang/Thermal-IM

#### 6. Summary : 
- (1):本文研究了利用热成像技术推断过去人体运动的方法，旨在解决RGB图像推断过去人体运动的不确定性问题。

- (2):过去的方法主要是基于RGB图像的，但是由于图像中存在大量的不确定性，因此推断过去人体运动的准确性较低。本文提出了一种基于热成像技术的方法，通过热成像技术可以更准确地推断过去人体运动，从而提高准确性。本文的方法具有很好的动机。

- (3):本文提出了一个三阶段的神经网络模型，用于准确地推断过去人体姿势。首先，利用热成像技术推断人体可能在三秒前的位置；其次，推断人体的动作；最后，合成一个准确的姿势。本文的创新点在于利用热成像技术推断过去人体运动，提出了一个新的数据集，并提出了一个新的模型。

- (4):本文的方法在热成像技术下，可以更准确地推断过去人体姿势，相比于RGB图像的方法，准确性更高。本文提出的模型可以自动隐式地发现热标记强度和时间之间的相关性。本文提出的数据集和模型可以为未来的热成像技术提供参考。
#### 7. 方法详细介绍：
本文提出了一个三阶段的神经网络模型，用于利用热成像技术准确地估计过去的人体姿态。该模型包括三个阶段：第一阶段提出人体可能在3秒前的位置，利用热成像图像中最明显的信息；第二阶段推断人体的动作类型；最后，第三阶段合成一个精确的姿态。该模型自动隐式地发现了热成像强度与时间之间的相关性。

具体步骤如下：
1. 首先，GoalNet模块提出人体可能在3秒前的位置。
2. 然后，TypeNet模块推断人体的动作类型。
3. 最后，PoseNet模块合成一个精确的姿态。

#### 8. 实验设置：
作者使用RGB-Thermal相机和RGB-Depth相机收集了Thermal-IM数据集，其中包含了关于人体在室内运动的RGB-Thermal和RGB-Depth视频，并估计了人体姿态。该数据集涉及一个演员和两个不同的房间。

#### 9. 实验结果和分析：
作者提出的方法在所有指标上都显著优于基线方法，并能够准确地恢复3秒前的人体姿态。在大多数情况下，热成像模型提供了比RGB模型更准确的预测结果，而当房间没有改变时，RGB模型获得了更高的语义分数。当引入新的背景或房间时，热成像模型远远优于RGB模型。当涉及新的演员时，两种模型的性能都会下降。该模型能够理解热成像标记强度中包含的时间信息。


# Paper:482     关于类增量学习的稳定性-可塑性困境



#### 1. Title: 
On the Stability-Plasticity Dilemma of Class-Incremental Learning

#### 2. Authors: 
Dongwan Kim, Bohyung Han

#### 3. Affiliation: 
首尔国立大学电子工程系和智能机器人研究所

#### 4. Keywords: 
Class-incremental learning, stability-plasticity dilemma, feature representation, catastrophic forgetting, ResNet-18

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Kim_On_the_Stability-Plasticity_Dilemma_of_Class-Incremental_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的背景是深度学习中的持续学习问题，即如何在模型不断接收新数据的情况下保留旧知识并学习新知识。
- (2):过去的方法主要集中在分类器上，而忽略了中间特征表示的稳定性和可塑性。本文提出了一种分析工具，用于衡量特征表示的稳定性和可塑性，并发现大多数现有的类增量学习算法过于注重稳定性，而忽略了可塑性。本文提出了两种简单的算法，以平衡稳定性和可塑性。
- (3):本文提出了一种分析工具，用于衡量特征表示的稳定性和可塑性，并在大规模类增量学习基准测试上使用该工具来研究使用不同算法训练的模型。本文发现，大多数类增量学习算法过于注重稳定性，而忽略了可塑性。本文提出了两种简单的算法，以平衡稳定性和可塑性。
- (4):本文在ImageNet-1K数据集上进行了实验，发现大多数类增量学习算法过于注重稳定性，而忽略了可塑性。本文提出的两种算法在一些基准测试上取得了良好的性能，证明了平衡稳定性和可塑性的重要性。
#### 7. 方法详细介绍：
本文提出了两种改进的类增量学习算法：partial-DER和Exploit。partial-DER算法在DER算法的基础上进行了改进，提高了效率和性能。Exploit算法则利用静态特征提取器，在最小计算量的情况下实现了强大的性能。实验中使用ResNet-18作为基础架构，使用中心核对齐（CKA）度量特征表示的相似性。 

#### 8. 实验设置：
本文在ImageNet-1K数据集上评估了现代类增量学习（CIL）算法，并引入了评估协议以更好地理解特征表示的稳定性和可塑性。实验设置涉及B500-5step设置，其中模型在增量更新之前预先训练了500个类别。本文认为，即使预训练表示有些足够，连续学习者也应该努力获得更好的特征表示。本文还指出，任务相似性已被证明会影响神经网络经历的灾难性遗忘的程度，但即使任务相似性在阶段之间发生变化，分析结果也会得出相同的结论。

#### 9. 实验结果和分析：
本文在ImageNet-1K数据集上比较了各种类增量学习（CIL）算法的性能，包括平均增量准确性和最终准确性。实验结果表明，除了Naive、iCARL和DER之外的所有比较方法的特征表示具有相似的区分性水平，但在平均增量准确性和最终准确性方面表现不佳。本文引入了两种简单的算法，改进了现有算法并利用了标准CIL研究评估指标的缺点。在实验中，Naive和iCARL模型在初始阶段后显示出显着的准确性下降，表明存在严重的灾难性遗忘。相比之下，AFC、AANet、LUCIR、POD和SSIL在所有增量阶段中几乎保持不变的准确性，表明具有高的特征稳定性但低的可塑性。DER随着每个增量阶段的增加而表现出不断提高的准确性，表明特征提取器学习了新的特征。本文还使用中心核对齐（CKA）来度量中间激活的相似性，并可视化增量模型之间的特征分布变化。


# Paper:483     图像中的语言：一种用于上下文视觉学习的通用画家



#### 1. Title: 
Images Speak in Images: A Generalist Painter for In-Context Visual Learning

#### 2. Authors: 
Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, Tiejun Huang

#### 3. Affiliation: 
北京智源人工智能研究院

#### 4. Keywords: 
in-context learning, computer vision, generalist model, image inpainting, masked image modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2021_paper.html  Github: https://github.com/baaivision/Painter

#### 6. Summary : 
- (1):本文研究背景是如何训练一个通用的计算机视觉模型，使其能够同时执行多种任务，甚至在给定提示和极少量的示例的情况下执行新任务。

- (2):过去的方法主要是将视觉问题转化为自然语言处理问题，但这种方法存在离散化误差和任务特定的损失函数和架构设计等问题。本文提出了一种基于图像的解决方案，将核心视觉任务的输出重新定义为图像，并将任务提示也定义为图像。通过这种方式，训练过程非常简单，只需对输入和输出图像对的缝合进行标准的掩蔽图像建模。这使得模型能够执行基于可见图像补丁的任务。

- (3):本文提出的方法是使用图像作为通用接口，将多种视觉任务的输出空间统一为三通道张量，即“输出图像”，并使用一对图像作为任务提示。在训练过程中，将来自同一任务的两个图像缝合成一个更大的图像，并对其对应的输出图像的像素应用掩蔽图像建模。这使得模型能够执行基于可见图像补丁的任务，即具有上下文的预测能力。在推理过程中，直接使用来自同一任务的输入/输出配对图像作为输入条件，以指示要执行的任务。

- (4):本文的方法在七个具有代表性的视觉任务上取得了竞争性的性能，包括语义分割、实例分割、深度估计、关键点检测、去噪、去雨和图像增强。与其他通用模型相比，Painter在几个具有挑战性的任务上取得了显著的改进。
#### 7. 方法详细介绍：
本文提出了一种名为Painter的通用模型，用于解决计算机视觉中的上下文学习困难。该模型将核心视觉任务的输出重新定义为图像，并将任务提示指定为图像。训练过程简单，对输入和输出图像对的拼接执行标准的掩蔽图像建模。这使得模型能够执行基于可见图像补丁的任务。在推理过程中，使用来自同一任务的一对输入和输出图像作为输入条件，以指示要执行哪个任务。具体而言，本文将七个代表性的视觉任务重新定义为图像修复问题，并使用掩蔽图像建模（MIM）框架进行训练。该框架包括三个主要组成部分：输入格式、架构和损失函数。模型的编码器使用了一个基本的视觉Transformer（ViT），而解码器则使用一个简单的三层头来将每个补丁的特征映射到其原始分辨率。本文还介绍了一种上下文推理过程，该过程使用来自同一任务的输入/输出配对图像作为输入条件，以指示要执行哪个任务。本文还提供了两个简单的基线，用于选择或生成更合适的任务提示，并将它们与随机对照组进行了比较。

#### 8. 实验设置：
本文使用NYUv2数据集进行深度估计，并在215个室内场景的654个测试图像上报告了均方根误差（RMSE）、绝对平均相对误差（A.Rel）和不同阈值的内部像素百分比。本文还使用ADE20K数据集进行语义分割，并采用平均交并比（mIoU）作为评估指标。

#### 9. 实验结果与分析：
本文提出了一种视觉中心的解决方案，用于上下文视觉学习，在七个代表性和多样化的任务上取得了极具竞争力的性能。该模型可以执行在训练中看到的任务，但输入图像的类别在训练中未见过，例如开放词汇关键点检测、对象分割和实例分割。本文还在几个样本分割基准上进行了定量评估，其中它在很大程度上优于一个并发工作。然而，本文承认在全景分割这个困难的任务上仍有很大的改进空间。本文还提供了系统级比较，将所提出的方法与最近的最佳视觉通用模型和专业模型在七个代表性任务上进行了比较，包括深度估计、语义分割、全景分割、关键点检测、去噪、去雨和增强。本文还比较了两种训练设置，联合训练和分别训练，并提供了定性结果，以展示所提出的通用模型的能力。


# Paper:484     通过结构分离和双重对抗鉴别的半监督手部外观恢复



#### 1. Title: 
Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination

#### 2. Authors: 
Zimeng Zhao, Binghui Zuo, Zhiyu Long, Yangang Wang

#### 3. Affiliation: 
Yangang Wang: 东南大学

#### 4. Keywords: 
Hand appearance recovery, image-to-image translation, semi-supervised learning, structure disentanglement, dual adversarial discrimination

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Semi-Supervised_Hand_Appearance_Recovery_via_Structure_Disentanglement_and_Dual_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究手部外观恢复问题，通过图像到图像的转换方法，从带有标记的MoCap数据中恢复出裸手外观。 
- (2):现有的方法包括无标记MoCap和图形渲染，但它们都存在一些问题。本文提出的方法通过结构分离和双重对抗鉴别来解决这些问题。 
- (3):本文提出了一种半监督框架，该框架利用ViT模型进行结构分离，并通过双重对抗鉴别来提高翻译器的性能。 
- (4):在多个数据集上进行了全面的评估，证明了本文方法可以从不同的数据集中恢复出逼真的手部外观。
#### 7. 方法详细介绍：
本文提出了一种半监督的手部外观恢复框架，包括结构分离和外观包裹两个步骤。首先，使用ViT Sketcher对图像进行结构分离，然后使用Translator通过双重对抗鉴别（DAD）方案将裸手结构与降质图像中的外观进行包裹。DAD方案可以对两个翻译过程和翻译结果进行双重鉴别，使其比大多数无监督方案更高效和可推广。该方法使用DINO-ViT和ResNet50作为骨干网络，使用VGG-16和调制卷积与合成层作为包裹器，使用Adam进行优化。实验结果表明，该方法可以在多种数据集上稳健地恢复真实的手部外观。

#### 8. 实验设置：
本文使用图像翻译来制定手部外观恢复任务，并将其与CycleGAN、CUT、Syn-Pix2pix和可微分渲染等基线进行比较。外观数据来自三个领域：包含标记的领域（A1）、物体遮挡的领域（A2）和裸手领域（B）。每个领域由33K图像数据表示，所有图像都被裁剪为以手为中心。本文还使用人类感知度量和DNN感知度量，包括FID和KID，来评估不同框架的性能。

#### 9. 实验结果和分析：
本文进行了A1→B和A2→B的消融研究，表2展示了不同消融项对最终翻译结果的影响。本文还讨论了所提出框架的局限性和未来工作，包括输入严重降质时的不稳定性以及适应多手或身体应用的潜力。实验结果表明，所提出的框架可以在多种数据集上稳健地恢复真实的手部外观。


# Paper:485     无监督深度概率方法用于部分点云配准



#### 1. Title: 
Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration

#### 2. Authors: 
Guofeng Mei, Hao Tang, Xiaoshui Huang, Weijie Wang, Juan Liu, Jian Zhang, Luc Van Gool, Qiang Wu

#### 3. Affiliation: 
Guofeng Mei: UTS (悉尼科技大学)
其他作者无需翻译

#### 4. Keywords: 
Point cloud registration, unsupervised learning, deep learning, probabilistic approach, Gaussian mixture models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mei_Unsupervised_Deep_Probabilistic_Approach_for_Partial_Point_Cloud_Registration_CVPR_2021_paper.html  Github: https://github.com/gfmei/UDPReg

#### 6. Summary : 
- (1):本文研究点云配准问题，提出了一种无监督的深度概率方法，用于处理部分重叠的点云。该问题在机器人、增强现实、自动驾驶、放射治疗等领域中具有重要应用。

- (2):现有的点云配准方法可以分为无对应点和有对应点两类。前者通常提取全局特征进行配准，但无法处理部分重叠的点云。后者通常提取局部特征进行点对应，但在低纹理区域或重复模式下效果不佳。此外，现有方法通常需要大量的标注数据，难以应用于实际场景。本文提出的方法采用深度概率框架，通过学习高斯混合模型的后验概率分布来处理部分重叠的点云配准问题，同时设计了三种分布一致性损失函数，使得网络可以在无监督的情况下进行训练。

- (3):本文提出的方法采用Sinkhorn算法预测分布级别的对应关系，并通过设计自一致性、交一致性和局部对比损失函数来训练网络。其中，自一致性损失函数通过鼓励欧几里得空间和特征空间中的高斯混合模型共享相同的后验分布来使得提取的特征在几何和语义上一致。交一致性损失函数通过强制部分重叠的点云共享相同的聚类中心来使得提取的特征在几何上一致。局部对比损失函数通过将来自同一聚类的点的特征推在一起，将来自不同聚类的点的特征拉开来，从而学习更具有区分性的特征。本文在3DMatch/3DLoMatch和ModelNet/ModelLoNet基准测试中取得了竞争性的性能。

- (4):本文提出的方法在3D点云配准任务中取得了竞争性的性能，且无需使用大量的标注数据。本文的主要贡献在于提出了一种无监督的深度概率框架，用于处理部分重叠的点云配准问题，并设计了三种分布一致性损失函数，使得网络可以在无监督的情况下进行训练。
#### 7. 方法详细介绍：
本文提出了一种无监督的深度概率方法，用于部分点云配准。该方法包括三个主要组件：共享编码器网络、聚类头网络和基于一致性的无监督学习框架。编码器网络从输入点云中提取特征，聚类头网络对特征进行聚类，以获取聚类级别的对应集。基于一致性的无监督学习框架包括自一致性损失、交叉一致性损失和局部对比损失，以鼓励学习到的表示具有空间敏感性、变换不变性和捕捉局部结构信息。该方法在训练过程中不需要任何对应或姿态信息。

#### 8. 实验设置：
该方法在真实数据集3DMatch和3DLoMatch以及合成数据集ModelNet和ModelLoNet上进行了评估。3DMatch数据集包含62个场景，其中46个用于训练，8个用于验证，8个用于测试。测试集包含1,623个部分重叠的点云片段及其对应的变换矩阵。输入点云包含平均约20,000个点。训练数据通过应用小的刚性扰动、抖动点位置和洗牌点进行预处理。

#### 9. 实验结果与分析：
本文提出的无监督深度概率点云配准方法在3DMatch、3DLoMatch和ModelNet40三个数据集上进行了评估，并与最新的监督和无监督方法进行了比较。评估指标包括相对旋转误差（RRE）、相对平移误差（RTE）和配准召回率（RR）。UDPReg在所有无监督方法中表现最佳，并在场景中实现了最低的平均旋转和平移误差。它还实现了最高的平均配准召回率，反映了点云配准的最终性能。UDPReg还超过了一些监督方法，显示了其在高重叠和低重叠情况下的有效性。实验结果证明了所提出方法的有效性。


# Paper:486     动态聚合网络用于步态识别



#### 1. Title: 
Dynamic Aggregated Network for Gait Recognition

#### 2. Authors: 
Kang Ma, Ying Fu, Dezhi Zheng, Chunshui Cao, Xuecai Hu, Yongzhen Huang

#### 3. Affiliation: 
北京理工大学

#### 4. Keywords: 
Gait recognition, deep learning, dynamic attention mechanism, global motion patterns, local motion patterns

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Ma_Dynamic_Aggregated_Network_for_Gait_Recognition_CVPR_2020_paper.pdf  Github: https://github.com/XKMar/FastGait

#### 6. Summary : 
- (1):本文研究的是步态识别，该任务在视频监控、犯罪调查和社会安全等领域有广泛应用。然而，步态识别在现实场景中经常受到多种外部因素的影响，如携带条件、穿着外套和不同的视角等。因此，如何在各种外部因素的影响下为每个人学习更具鲁棒性的特征是一个重要的问题。

- (2):过去的步态识别方法主要分为两类：基于模型的方法和基于外观的方法。本文主要关注基于外观的方法，这些方法主要受益于深度学习的快速发展。然而，这些方法往往使用固定权重的卷积网络提取显著特征，没有很好地考虑关键区域内步态特征之间的关系，并且忽略了完整运动模式的聚合。因此，本文提出了一种新的视角，即实际步态特征包括多个关键区域的全局运动模式，每个全局运动模式由一系列局部运动模式组成。

- (3):本文提出了一种动态聚合网络（DANet）来学习更具有区分性的步态特征。具体来说，我们创建了一个动态注意机制，它不仅自适应地聚焦于关键区域，而且生成更具表现力的局部运动模式。此外，我们开发了一种自我注意机制来选择代表性的局部运动模式，并进一步学习鲁棒的全局运动模式。在三个流行的公共步态数据集（CASIA-B、OUMVLP和Gait3D）上进行的大量实验表明，所提出的方法可以比当前最先进的方法提供实质性的改进。

- (4):本文提出的方法在CASIA-B、OUMVLP和Gait3D数据集上均取得了优异的性能，特别是在最具挑战性的穿着不同衣服的情况下。本文的主要贡献包括：提出了一种新的局部运动模式提取方法和一种全局运动模式聚合方法，以及在步态识别任务上取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种名为动态聚合网络（DANet）的步态识别方法。该方法由两个主要模块组成：局部卷积混合块（LCMB）和全局运动模式聚合器（GMPA）。LCMB模块利用动态注意力模型在感兴趣的相邻像素之间建立关系，并提取代表性的局部运动模式。GMPA模块选择具有区分性的局部运动模式，并使用自注意机制将它们聚合起来，以获得稳健的全局表示。DANet使用联合损失函数和3D CNN骨干网络进行训练。

#### 8. 实验设置：
本文在三个流行的公共步态数据集CASIA-B、OUMVLP和Gait3D上评估了所提出的方法。实验使用大小为32x32x32的3D CNN骨干网络进行，训练集使用随机裁剪、翻转和旋转进行增强，验证集用于早期停止，测试集用于评估。性能评估使用排名1准确率和平均平均精度（mAP）。

#### 9. 实验结果和分析：
所提出的DANet在三个数据集上均优于现有的方法，在CASIA-B、OUMVLP和Gait3D上的排名1准确率分别为98.3％、96.5％和95.5％。与现有方法相比，mAP得分也显著提高。消融实验证明了DANet中每个组件的有效性，并且该方法对于各种外部因素（如穿着不同）都表现出鲁棒性。


# Paper:487     SOOD：面向半监督定向目标检测



#### 1. Title: 
SOOD: Towards Semi-Supervised Oriented Object Detection

#### 2. Authors: 
Wei Hua, Dingkang Liang, Jingyu Li, Xiaolong Liu, Zhikang Zou, Xiaoqing Ye, Xiang Bai

#### 3. Affiliation: 
华中科技大学

#### 4. Keywords: 
Semi-Supervised Object Detection, Oriented Object Detection, Pseudo-labeling, Aerial Object Detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Hua_SOOD_Towards_Semi-Supervised_Oriented_Object_Detection_CVPR_2022_paper.html  Github: https://github.com/HamPerdredes/SOOD

#### 6. Summary : 
- (1):本文研究半监督定向目标检测，旨在探索未标记数据以提高目标检测器的性能。然而，现有的半监督方法主要集中在水平对象上，而在航空图像中常见的多方向对象则未被充分研究。

- (2):现有的SSOD方法主要集中在一般场景中检测具有水平边界框的对象。然而，在更复杂的场景中，例如航空场景，对象通常需要用定向边界框进行注释。本文提出了一种新的半监督定向目标检测模型SOOD，建立在主流的伪标签框架上。针对航空场景中的定向对象，我们设计了两个损失函数以提供更好的监督。第一个损失函数关注对象的方向，通过自适应权重基于它们的方向差异来规范每个伪标签-预测对之间的一致性。第二个损失函数关注图像的布局，规范伪标签和预测之间的相似性，并明确建立伪标签和预测之间的多对多关系。这种全局一致性约束可以进一步提高半监督学习的性能。

- (3):本文提出了第一个半监督定向目标检测方法SOOD，它基于密集伪标签框架，其中伪标签是从原始像素预测（包括框坐标和置信度分数）中过滤出来的。SOOD的关键设计是两个简单而有效的损失函数，它们强制执行实例级和集合级别之间的一致性。具体而言，考虑到伪标签-预测对不是同等信息的，我们提出了旋转感知自适应加权（RAW）损失。它利用每个对的方向差异来动态加权相应的损失。此外，考虑到航空图像的布局可以潜在地反映组件的整体状态（例如，对象的密度和位置分布）并有助于检测过程，我们提出了全局一致性（GC）损失。它从全局角度衡量伪标签和预测之间的相似性，可以减轻伪标签中的噪声干扰，并隐含地规范不同对象之间的相互关系。

- (4):在DOTA-v1.5基准测试中，我们对SOOD进行了广泛的评估。实验结果表明，当使用两个提出的损失进行训练时，SOOD在各种设置下均优于现有的SSOD方法。本文提出的方法可以降低注释成本并提高检测器的性能。
#### 7. 方法详细介绍：
本文提出了一种半监督的多方向目标检测方法，称为SOOD。该方法基于主流的伪标签框架，使用方向版本的FCOS作为教师和学生模型。该方法包括两个关键损失：旋转自适应加权（RAW）损失和全局一致性（GC）损失。RAW损失通过考虑预测和伪标签之间的方向差异来动态调整无监督损失。GC损失使用最优传输成本度量教师和学生预测之间的全局相似性。模型使用所提出的无监督损失进行未标记数据的训练，使用监督损失进行标记数据的训练。具体步骤包括：
1. 使用FCOS作为教师模型，对标记数据进行训练。
2. 使用FCOS作为初始的学生模型，对未标记数据进行训练。
3. 使用学生模型对未标记数据进行预测，生成伪标签。
4. 使用RAW损失和GC损失对学生模型进行训练，以使其预测与教师模型的预测一致。
5. 使用监督损失对学生模型进行微调，以进一步提高性能。

#### 8. 实验设置：
本文在DOTA-v1.5数据集上进行实验，该数据集包含2806张大型航拍图像和402,089个注释的多方向目标。数据集包括三个子集：DOTA-v1.5-train、DOTA-v1.5-val和DOTA-v1.5-test，分别包含1411、458和937张图像。其中，DOTA-v1.5-test的注释未公开。实验分别在部分标记数据和完全标记数据两种协议下进行，以验证方法在有限和丰富标记数据上的性能。

#### 9. 实验结果与分析：
本文提出的SOOD方法在DOTA-v1.5数据集上进行了实验，使用RAW和GC损失。实验探究了不同采样比例、GC成本图中不同组成的影响以及RAW超参数alpha的影响。结果表明，所提出的方法在部分标记数据和完全标记数据上均取得了一致的性能提升。


# Paper:488     PartMix: 用于可见-红外人员再识别的部分发现正则化策略



#### 1. Title: 
PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification

#### 2. Authors: 
Minsu Kim, Seungryong Kim, Jungin Park, Seongheon Park, Kwanghoon Sohn

#### 3. Affiliation: 
Yonsei University (韩国延世大学)

#### 4. Keywords: 
Visible-Infrared Person Re-identification, Part-based models, Data augmentation, PartMix

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_PartMix_Regularization_Strategy_to_Learn_Part_Discovery_for_Visible-Infrared_Person_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的是可见光-红外人员再识别（VI-ReID）任务，该任务的挑战在于可见光图像和红外图像之间存在大量的内部和跨模态变化，因此需要提取多个人体部位来进行更细粒度的特征提取。
- (2):过去的方法主要集中在提取最具有区分性的部位，而忽略了其他有助于区分不同人员的部位。本文提出了一种数据增强技术PartMix，通过混合不同模态下的人体部位描述符来生成正负样本，从而提高了基于部位的VI-ReID模型的性能。与现有的图像混合方法不同，PartMix使用部分描述符混合策略，避免了不自然的模式和只包含背景或单个人体部位的局部补丁。
- (3):本文提出的PartMix方法通过混合不同模态下的人体部位描述符来生成正负样本，并通过对比学习来规范化模型。此外，本文还提出了一种基于熵的挖掘策略，以减弱不可靠正负样本的不良影响。实验结果表明，PartMix方法可以提高现有VI-ReID方法的性能。
- (4):本文在多个基准测试集上进行了实验，证明了PartMix方法的有效性。与现有方法相比，PartMix方法在可见光-红外人员再识别任务上取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种新的数据增强技术PartMix，用于可见光-红外人员再识别任务。PartMix通过混合不同模态下的部分描述符来生成同一身份内和不同身份之间的正负样本，从而合成部分感知的增强样本。模型通过对比学习进行正则化，并使用基于熵的挖掘策略来削弱不可靠正负样本的不良影响。该方法在多个基准测试中展示了其有效性，并提供了广泛的消融研究来验证和分析模型中的组件。具体步骤包括：
1. 部分描述符提取：使用部分检测器提取人体部位，并输出部分映射概率，将部分描述符与全局描述符拼接以获得人员描述符。
2. 正负样本生成：设计正样本以在同一身份的人员图像之间混合相同的部分信息，而负样本则鼓励模型通过不同的人体部位组合来区分人员身份，并在同一人员内定位不同的人体部位。
3. 熵值挖掘策略：通过利用身份预测的成对熵之间的差异来消除假阳性和假阴性样本。
4. 损失函数：包括对比正则化损失和部分ID损失，通过正负样本对模型进行正则化。
5. 总损失函数：是不同损失的组合，权重控制每个损失的重要性。该方法旨在通过鼓励部分发现在不同干预下不变来学习模态不变的部分表示。

#### 8. 实验设置：
本文在SYSU-MM01和RegDB两个基准测试集上进行了实验。SYSU-MM01数据集包含395个身份的22,258张可见光图像和11,909张近红外图像用于训练，测试集包含96个身份的3,803张近红外图像作为查询，301和3,010张可见光图像作为库，分别进行单摄和多摄模式的测试。RegDB数据集包含4,120对可见光和红外图像，共412个人员身份，每个人员有10张可见光和10张远红外图像。评估协议包括SYSU-MM01的全搜索和室内搜索设置，以及RegDB的红外到可见光和可见光到红外设置。使用累积匹配特征（CMC）曲线和平均精度（mAP）来评估所提出方法的性能。

#### 9. 实验结果和分析：
本文进行了消融研究以评估所提出方法的不同组件的有效性。实验结果表明，所提出的方法在Rank-1准确率和mAP方面优于基线和其他正则化方法。在SYSU-MM01数据集上，所提出的方法在单摄和多摄模式下分别达到了77.78%和80.54%的Rank-1准确率和74.62%和69.84%的mAP。所提出的方法还在SYSU-MM01和RegDB数据集上取得了最先进的性能，在CMC曲线和mAP方面优于其他方法。PartMix方法在SYSU-MM01基准测试中的全搜索单摄模式下，Rank-1准确率为77.78%，mAP为74.62%，比MSCLNet提高了0.79%的Rank-1准确率和2.98%的mAP。消融研究表明，PartMix的每个组件，包括内模态部分混合、跨模态部分混合和基于熵的挖掘，都有助于提高性能。混合部分和部分映射的数量也会影响性能，最佳结果是使用2个混合部分和6个部分映射。PartMix显著优于其他正则化方法，包括MixUp、Manifold MixUp和CutMix。


# Paper:489     学习检测和分割开放词汇物体检测



#### 1. Title: 
Learning to Detect and Segment for Open Vocabulary Object Detection

#### 2. Authors: 
Tao Wang

#### 3. Affiliation: 
四川大学

#### 4. Keywords: 
Open vocabulary object detection, dynamic network design, semantic-visual aligned representation, bounding box regression, mask segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Learning_to_Detect_and_Segment_for_Open_Vocabulary_Object_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是开放词汇物体检测，旨在通过语义类别识别来识别新颖物体。 
- (2):过去的方法主要集中在知识转移到对象提议分类上，并采用类不可知的盒子和掩码预测。然而，这些方法提供的类不可知头部能力有限，无法学习类别特定的知识，如对象形状。本文提出了CondHead，一种有原则的动态网络设计，以更好地推广开放词汇设置的框回归和掩码分割。 
- (3):CondHead是一种条件网络设计，其核心思想是在语义嵌入上有条件地参数化网络头部，从而通过类特定的知识指导模型更好地检测新颖类别。具体而言，CondHead由两个网络头流组成，即动态聚合头和动态生成头。前者由一组静态头实例化，这些头被条件聚合，这些头被优化为专家，并期望学习复杂的预测。后者由动态生成的参数实例化，并编码一般的类特定信息。通过这样的条件设计，检测模型通过语义嵌入桥接，提供强烈的可推广的类别框和掩码预测。 
- (4):本文在COCO和LVIS数据集上进行了广泛的验证，并进行了彻底的消融和分析，以了解语义条件如何帮助检测和分割新颖类别。CondHead相对于三个基线的性能有所提高，具有很强的灵活性，可以适应不同的语义视觉表示。本文的贡献有三个方面：1）利用语义视觉对齐表示进行开放词汇框回归和掩码分割；2）设计了一种可微分的语义条件头设计，以有效地将在基本类别上学习的强类别特定预测推广到目标新颖类别；3）在各种基准数据集上进行了广泛的验证，并进行了彻底的消融和分析，以了解语义条件如何帮助检测和分割新颖类别。
#### 7. 方法详细介绍：
本文提出了一种名为CondHead的条件参数化神经网络设计，旨在改进开放词汇边界框回归和掩膜分割。该方法利用预训练的语义嵌入来指导边界框和掩膜头的参数化。语义嵌入与视觉表示强烈对齐，并为细化边界框和分割对象提供有效线索。该方法在不同的数据集和设置上进行了验证。具体步骤包括：
1. 设计一个双重条件框架，动态聚合头和动态生成头。
2. 动态聚合头利用复杂头的大网络容量，而动态生成头直接生成网络参数，引入更强的语义嵌入条件。
3. 通过简单的加权平均组合动态聚合头和动态生成头，得到最终结果。
4. 通过温度退火Softmax策略促进专家头的优化。

#### 8. 实验设置：
本文在目标检测基准数据集COCO和LVIS上进行了实验。数据集手动分为基础和目标类别。采用了先前的开放词汇方法OVR-CNN和ViLD作为基线。在实验中采用了表1中的架构实例化。

#### 9. 实验结果和分析：
本文提出的CondHead方法在COCO和LVIS数据集上均优于基线，显著提高了开放词汇目标检测和实例分割的性能。语义-视觉表示的质量对性能有显著影响。该方法改进了边界框回归和掩膜分割。还对方法的超参数和组件进行了详细分析。此外，通过在PASCAL VOC和Objects365数据集上进行推理，评估了所提出方法的泛化性。


# Paper:490     ABLE-NeRF: 基于可学习嵌入和自注意力机制的神经辐射场渲染



#### 1. Title: 
ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field

#### 2. Authors: 
Zhe Jun Tang, Tat-Jen Cham, Haiyu Zhao

#### 3. Affiliation: 
1. 新加坡南洋理工大学S-Lab
2. 新加坡南洋理工大学
3. SenseTime Research

#### 4. Keywords: 
Neural Radiance Field, Volumetric Rendering, Attention Mechanism, Learnable Embeddings, View Synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_ABLE-NeRF_Attention-Based_Rendering_With_Learnable_Embeddings_for_Neural_Radiance_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）在3D场景表示中的应用，NeRF通过优化连续体积场函数来表示3D场景，能够生成逼真的新视角图像，但是在处理具有视角相关效应的物体时存在问题。
 
- (2):过去的方法主要是基于物理的体积渲染方法，但是这些方法在处理具有视角相关效应的物体时存在问题，例如半透明物体和光泽物体的模糊效果。本文提出了一种基于自注意力机制和可学习嵌入的方法，能够更好地处理这些问题。 
 
- (3):本文提出的方法是基于自注意力机制的框架，通过在光线上对体积进行自注意力机制，来代替物理学上的体积渲染方法。同时，本文还引入了可学习嵌入来捕捉场景中的视角相关效应。本文的方法在Blender数据集上取得了SOTA结果，并在PSNR、SSIM和LPIPS三个图像质量指标上超过了Ref-NeRF。
 
- (4):本文的方法在新视角合成任务上取得了高质量的渲染效果，能够更好地处理具有视角相关效应的物体，例如半透明物体和光泽物体。本文的方法在Blender数据集上取得了SOTA结果，并在PSNR、SSIM和LPIPS三个图像质量指标上超过了Ref-NeRF，表明了本文方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于注意力机制的渲染方法，称为ABLE-NeRF。该方法使用可学习的嵌入来捕捉场景信息和视角相关的效果。该方法将体积渲染重新构建为注意力机制，其中掩码注意力应用于点以防止其关注到其后面的点。记忆网络使用可学习的嵌入将场景信息存储在潜在空间中，并通过交叉注意力关注射线投射期间采样的点以记忆场景信息。方向视角令牌由相机姿态组成，从这些嵌入中解码以呈现准确的视角相关效果。该方法在产生精确的镜面效果方面比Ref-NeRF有了显着的改进，并且在渲染半透明物体的颜色方面比现有技术更准确。

#### 8. 实验设置：
本文在两个数据集上实现了ABLE-NeRF，分别是Blender数据集和Shiny Blender数据集。LE的数量设置为32，由粗略和细致的网络共享，以存储视角相关的信息。对于Shiny Blender数据集，LE的数量设置为16，因为它包含与标准Blender数据集相比较简单的对象。每个场景的优化训练进行了250k次迭代。

#### 9. 实验结果和分析：
本文在Blender数据集和Shiny Blender数据集上评估了模型，并与之前的最先进方法进行了比较。在Blender数据集上，ABLE-NeRF在与应用基于物理的体积渲染的最佳表现NeRF-based方法相比时表现出色。在Shiny Blender数据集上，当不包括正常预测时，ABLE-NeRF比Ref-NeRF表现更好。该模型擅长捕捉由多次反射引起的表面内部反射，这是高度复杂的场景。注意力图用于基于从相机原点到具有最高注意权重的体积的距离遍历的深度图。


# Paper:491     Hand Avatar：从单目视频中实现自由姿态手部动画和渲染



#### 1. Title: 
Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video

#### 2. Authors: 
Xingyu Chen, Baoyuan Wang, Heung-Yeung Shum, Xiaobing AI

#### 3. Affiliation: 
第一作者：香港科技大学

#### 4. Keywords: 
Hand Avatar, Hand Animation, Hand Rendering, Monocular Video, Neural Rendering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2021_paper.html  Github: https://github.com/seanchenxy/HandAvatarWeb

#### 6. Summary : 
- (1):本文研究手部动画和渲染的新方法，提出了HandAvatar框架，旨在解决手部动画和渲染中的自遮挡和接触问题。

- (2):传统的手部动画和渲染方法通常采用纹理贴图和有色网格，但需要昂贵的扫描数据和艺术知识。最近，神经渲染技术已经引起了人们的关注，但是手部的高度关节化使得神经渲染面临着困难。本文提出了一种新的手部动画和渲染方法，使用MANO-HD模型来描述手部形状，使用PairOF和SelF来描述手部几何和纹理，最终实现了自由姿态的手部动画和渲染。

- (3):本文提出了HandAvatar框架，是第一种具有自遮挡照明的神经手部渲染方法。作者设计了MANO-HD和PairOF来适应个性化手部形状，使用SelF来呈现手部纹理。作者的方法是端到端训练的，可以从单目视频数据中进行自由姿态的手部动画和渲染。本文的主要贡献包括：提出了HandAvatar框架，开发了MANO-HD和PairOF来适应手部几何，提出了SelF来呈现手部纹理，实现了自由姿态的手部动画和渲染。

- (4):作者在InterHand2.6M数据集上进行了评估，证明了其方法在自由姿态手部动画和渲染方面的高保真度。作者还演示了HandAvatar提供了一种编辑手部外观的方法。
#### 7. 方法详细介绍：
本文提出了一种名为HandAvatar的框架，用于从单目视频中实现自由姿态手部动画和渲染。该框架由三个主要组件组成：MANO-HD用于生成个性化手部网格，PairOF用于预测局部对占据场，SelF用于估计自遮挡感知的阴影场。MANO-HD将手部网格细分，并使用多层感知器（MLP）推导出精细的形状。PairOF基于局部对解码预测查询点的占据值。SelF估计查询点的反照率和照明值，并使用体积法呈现神经场。反照率编码基于重心坐标，并使用反欧几里得距离进行插值。软占据值被引入到sigmoid函数中，以软化自遮挡感知的阴影场估计的占据值。

#### 8. 实验设置：
本文在InterHand2.6M数据集上进行评估，并实现了自由姿态手部动画的高保真几何和纹理。HandAvatar框架是从单目视频数据中训练出来的。

#### 9. 实验结果和分析：
本文提出的HandAvatar方法在自由姿态手部动画和渲染方面取得了优异的结果。与之前的单目方法HumanNeRF和SelfRecon进行比较，HandAvatar在所有指标上都取得了最佳结果。SelF有助于实现逼真的渲染，并将手部反照率和照明分离以模拟关节自遮挡下的手部纹理。本文的方法为动态手部表示开辟了新的道路。


# Paper:492     SHS-Net：学习带符号超曲面用于点云定向法向量估计



#### 1. Title: 
SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds

#### 2. Authors: 
Qing Li, Huifang Feng, Kanle Shi, Yue Gao, Yi Fang, Yu-Shen Liu, Zhizhong Han

#### 3. Affiliation: 
第一作者：清华大学软件学院

#### 4. Keywords: 
Point cloud, normal estimation, signed hyper surfaces, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Li_SHS-Net_Learning_Signed_Hyper_Surfaces_for_Oriented_Normal_Estimation_of_CVPR_2022_paper.html  Github: https://github.com/LeoQLi/SHS-Net

#### 6. Summary : 
- (1):本文研究点云的法向估计问题，其中法向估计的目标是得到具有全局一致性方向的法向量。 
- (2):现有的法向估计方法通常采用两阶段流程，即无定向法向量估计和法向量定向，每个步骤都由单独的算法实现。然而，以前的方法对参数设置敏感，导致在具有噪声、密度变化和复杂几何形状的点云中表现不佳。本文提出了一种新方法，通过学习带符号超曲面来实现点云的法向估计，从而直接预测具有全局一致性方向的法向量。 
- (3):本文提出了一种新技术，将点云的几何属性表示为高维特征空间中的带符号超曲面。我们从局部和全局形状属性中学习这种新的几何表示，以直接以端到端的方式估计具有一致方向的法向量。我们的方法的洞察力是，确定全局一致的法向量方向应该需要全局上下文来消除局部的歧义，因为法向量不是局部属性。 
- (4):在多个基准测试中，我们的SHS-Net在无定向和定向法向量估计方面均优于现有的方法。本文的方法在不同采样密度、噪声水平、细长和尖锐结构的点云上均能够实现高精度的法向估计。
#### 7. 方法详细介绍：
本文提出了一种名为SHS-Net的方法，用于点云的定向法线估计。该方法通过在高维特征空间中隐式学习带符号超曲面（SHS）来学习点云的定向法线。SHS由多层感知器（MLP）层参数化，从局部和全局形状属性中学习，以直接以端到端的方式估计具有一致方向的法线。学习流程包括一个补丁编码模块和一个形状编码模块，将3D点云编码为局部潜在代码和全局潜在代码。然后，提出了一个注意力加权法线预测模块作为解码器，该解码器将局部和全局潜在代码作为输入，以预测定向法线。

#### 8. 实验设置：
该方法在PCPNet形状数据集上进行训练，并在一个新收集的名为FamousShape的数据集上进行测试，该数据集包含来自其他公共数据集的具有复杂结构的形状。从网格数据中提取地面真实的定向法线用于评估。使用Adam优化器进行训练，初始学习率为9×10−4，分别在第400、600、800个epoch时将其衰减为最新值的1/5。模型在NVIDIA 2080 Ti GPU上训练，批量大小为145，epoch为800。

#### 9. 实验结果与分析：
该方法在PCPNet和FamousShape数据集上均取得了最先进的性能，以未定向法线角度RMSE为指标。该方法优于传统方法，如PCA和Jet，以及基于学习的方法，如DeepFit、AdaFit、Nesti-Net和HSurf-Net。该方法还通过将定向法线估计分为未定向法线回归和其符号分类，而不是直接回归查询点的定向法线，实现了显着的性能提升。


# Paper:493     增强稳定视图合成



#### 1. Title: 
Enhanced Stable View Synthesis

#### 2. Authors: 
Nishant Jain, Suryansh Kumar, Luc Van Gool

#### 3. Affiliation: 
Nishant Jain: 印度理工学院
Suryansh Kumar, Luc Van Gool: ETH苏黎世联邦理工学院

#### 4. Keywords: 
novel view synthesis, stable view synthesis, multiple view geometry, monocular depth, camera pose

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jain_Enhanced_Stable_View_Synthesis_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是从自由移动相机拍摄的图像中增强新视角合成的方法。该方法主要针对室外场景，其中恢复准确的几何支架和相机姿态是具有挑战性的，导致使用最先进的稳定视图合成（SVS）方法的结果较差。

- (2):过去的方法主要依赖于多视图立体（MVS）进行几何支架恢复，并假设COLMAP计算的相机姿态是最佳估计，尽管已经研究表明MVS 3D重建精度仅限于场景视差，相机姿态精度对关键点对应选择敏感。本文提出了一种基于多视图几何基础的方法来增强新视角合成解决方案。通过利用MVS和单目深度的互补行为，我们得到了更好的场景深度。此外，我们的方法通过多旋转平均图优化联合优化相机姿态和基于图像的渲染。我们的方法在流行的基准数据集上进行了广泛的评估，如Tanks and Temples，FVS，Mip-NeRF 360和DTU，与先前的方法相比，显示出了显着的改进。

- (3):本文提出了一种系统和有原则的方法，为可靠的特征聚合提供了更好的几何支架和相机姿态，从而实现了改进的新视角合成，使其具有更好的逼真度。我们的方法通过图神经网络实现了基于学习的多运动平均值，用于相机姿态恢复，其中姿态图使用COLMAP姿态进行初始化。我们的方法利用MVS和单目深度估计的互补性质来恢复场景的更好的3D几何支架。通过卷积神经网络对图像特征进行编码，我们将深度特征映射到场景的估计3D几何支架上。我们从特征张量中渲染新图像，并同时优化和精炼相机姿态。 

- (4):本文在Tanks and Temples，FVS，Mip-NeRF 360和DTU等基准数据集上测试，与先前的方法相比，显示出了显着的改进。例如，在Tanks and Temples数据集上，我们的方法显示出1.5 dB的PSNR改进。本文的方法在新视角合成任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种结合学习方法和多视角几何的方法来解决新视角合成问题。该方法利用单目深度估计和多视角立体视觉在场景的三维重建中的互补性，以及多重旋转平均在运动结构中的有用性。该方法使用图神经网络预测场景的鲁棒姿态视图图形，并使用NeuRoRA的相机姿态细化方法对相机姿态进行检查和优化。然后，基于这些过滤后的姿态初始化视图图形。该视图图形使用NeuRoRA的相机姿态细化网络进行优化。最终的聚合特征张量通过使用具有参数的CNN网络将其投影到图像空间函数中。该方法的联合优化使用两个项的总损失函数，Ls和Lp。Ls鼓励网络学习更好的与场景点对应的特征，而Lp对应于相机姿态细化。

#### 8. 实验设置：
本文在多个基准数据集上评估了所提出的方法，并与基线进行了比较。实验中使用了多个评价指标，包括PSNR、SSIM和LPIPS。实验中使用了两个不同的深度估计模型，分别是monodepth2和MiDaS。实验中使用了多个视角的图像来进行新视角合成，并对比了不同方法的渲染结果。

#### 9. 实验结果与分析：
实验结果表明，所提出的方法在多个基准数据集上均取得了优于基线的结果。与其他方法相比，该方法在渲染质量和视觉效果方面表现更好。实验结果还表明，使用MiDaS深度估计模型可以获得更好的结果。此外，本文还进行了消融实验，证明了所提出的方法中各个组成部分的有效性。


# Paper:494     局部多尺度重建的遮蔽图像建模



#### 1. Title: 
Masked Image Modeling with Local Multi-Scale Reconstruction

#### 2. Authors: 
Haoqing Wang, Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhi-Hong Deng, Kai Han

#### 3. Affiliation: 
第一作者：北京大学智能科学与技术学院

#### 4. Keywords: 
Masked Image Modeling, self-supervised learning, multi-scale reconstruction, representation learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Masked_Image_Modeling_With_Local_Multi-Scale_Reconstruction_CVPR_2021_paper.html  Github: https://github.com/Ascend/modelzoo/tree/master/built-in/TensorFlow/Official/cv/image_classification/ResnetVariant_for_TensorFlow

#### 6. Summary : 
- (1):本文研究背景是自监督学习中的Masked Image Modeling（MIM）方法，该方法通过遮盖图像的一部分并预测其信号来学习语义表示。然而，现有的MIM模型通常具有巨大的计算负担和缓慢的学习过程，这是其工业应用的不可避免的障碍。

- (2):过去的方法主要是通过不对称编码器-解码器策略或减少输入补丁来加速编码过程，但这些方法只是加速了编码过程，而没有加速表示学习。本文提出了一种新的方法，即在多个本地层上进行重建任务，以显式地引导多个较低层，并加速整个学习过程。此外，本文还提出了本地多尺度重建，其中较低和较高层分别重建细粒度和粗粒度的监督信号。这种设计不仅通过显式引导多个层来加速表示学习过程，而且还有助于对输入进行多尺度语义理解。

- (3):本文提出的方法是在多个本地层上进行重建任务，以显式地引导多个较低层，并加速整个学习过程。此外，本文还提出了本地多尺度重建，其中较低和较高层分别重建细粒度和粗粒度的监督信号。这种设计不仅通过显式引导多个层来加速表示学习过程，而且还有助于对输入进行多尺度语义理解。本文的创新点在于首次在MIM中进行本地重建，并使用来自输入的多尺度监督信号。

- (4):本文在分类、检测和分割任务上进行了广泛的实验，结果表明，相对于现有的MIM模型，本文的模型具有更高的效率和更好的性能。在ImageNet-1K上，使用ViT-B和Swin-B进行预训练，本文的模型分别达到84.0%和84.1%的top-1精度，且预训练时间显著少于现有的MIM模型。本文的方法还在检测和分割下游任务中取得了更好的泛化性能。
#### 1. 方法详细介绍：
本文提出了一种名为LocalMIM的自监督学习框架，用于掩蔽图像建模。该方法采用基于补丁的方法，随机掩蔽输入图像的补丁，然后使用由Transformer块、反卷积/池化操作和多层感知器组成的解码器来重建掩蔽的补丁。模型的下层重建细粒度监督，而上层重建粗粒度监督。训练损失是所选层的重建损失的加权和。该方法引入了多尺度监督，以指导多个本地层学习多尺度信息。

#### 2. 实验设置：
本文在ImageNet-1K数据集上进行预训练和微调，分别使用ViT和Swin作为骨干网络。输入图像被分割成大小为p=16（ViT）或p=4（Swin）的补丁，并随机掩蔽75%的补丁。本文使用HOG特征和归一化像素作为监督信号。

#### 3. 实验结果和分析：
LocalMIM在ImageNet-1K上的微调结果表明，使用ViT-B和Swin-B作为骨干网络，Top-1精度分别为84.0%和84.1%，比现有的MIM模型具有更少的预训练负担。在ADE20K上进行的语义分割任务中，LocalMIM的表现优于现有的最先进结果0.7。在COCO上进行的目标检测和分割任务中，LocalMIM的表现优于有监督预训练2.2 APb和1.7 APm。

#### 4. 总结：
本文提出了一种名为LocalMIM的自监督学习框架，用于掩蔽图像建模。该方法采用基于补丁的方法，随机掩蔽输入图像的补丁，然后使用由Transformer块、反卷积/池化操作和多层感知器组成的解码器来重建掩蔽的补丁。该方法引入了多尺度监督，以指导多个本地层学习多尺度信息。实验结果表明，LocalMIM在分类、检测和分割任务上的表现优于现有的MIM模型，具有更少的预训练负担。


# Paper:495     将输入带入共享域以实现野外环境下的3D交互手恢复



#### 1. Title: 
Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild

#### 2. Authors: 
Gyeongsik Moon

#### 3. Affiliation: 
Meta Reality Labs（韩国）

#### 4. Keywords: 
3D interacting hands recovery, in-the-wild, motion capture, appearance gap, weak supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Moon_Bringing_Inputs_to_Shared_Domains_for_3D_Interacting_Hands_Recovery_CVPR_2021_paper.html  Github: https://github.com/facebookresearch/InterWild

#### 6. Summary : 
- (1):该论文研究的是在野外环境下的3D交互手恢复问题，现有的方法主要在运动捕捉环境中表现良好，但在野外环境中表现不佳，主要原因是野外环境下的数据收集非常困难。
- (2):过去的方法主要是在运动捕捉数据集上进行训练，但是直接将这些方法应用于野外数据集上会出现不稳定的结果，因为两个数据集之间存在巨大的外观差异。本文提出了InterWild框架，将运动捕捉和野外数据集带到共享域中，以实现在野外环境下的3D交互手恢复。该方法的创新点在于将单手图像作为第一子问题的输入，将运动捕捉数据集中的交互手带到野外数据集的单手2D尺度空间中。对于第二个子问题，该方法将运动捕捉和野外数据集带到共享外观不变空间中，通过只采用几何特征而不是图像作为第二个子问题的输入来扩大运动捕捉样本的泛化能力。 
- (3):本文提出了InterWild框架，将运动捕捉和野外数据集带到共享域中，以实现在野外环境下的3D交互手恢复。该方法的创新点在于将单手图像作为第一子问题的输入，将运动捕捉数据集中的交互手带到野外数据集的单手2D尺度空间中。对于第二个子问题，该方法将运动捕捉和野外数据集带到共享外观不变空间中，通过只采用几何特征而不是图像作为第二个子问题的输入来扩大运动捕捉样本的泛化能力。 
- (4):该方法在3D交互手恢复任务上取得了良好的性能，相比于之前的方法，该方法在野外数据集上表现更加稳定，且能够利用野外数据集中的单手图像进行训练，从而扩大了数据集的规模。
#### 7. 方法详细介绍：
本文提出了一种名为InterWild的框架，用于在野外环境下恢复3D交互手部网格。InterWild由三个子网络组成：DetectNet、SHNet和TransNet。DetectNet从输入图像中检测手部，SHNet将每个检测到的手部图像作为输入，并输出每个手部的3D网格和2.5D姿态。TransNet获取两只手的2.5D姿态，并预测两只手之间的3D相对平移。最终的3D交互手部是通过将3D相对平移添加到左手的3D网格中得到的。SHNet网络采用单手图像作为输入，而不管两只手是否交互，而以前的方法在手部交互时采用两只手的图像。交互手的2D比例被归一化为单只手的比例。TransNet网络获取两只手的2.5D姿态，而以前的方法采用两只手的图像。在转发它们之前，对两只手的2.5D姿态应用2D仿射变换。

#### 8. 实验设置：
本文在IH2.6M和MSOCO整体版本数据集上进行训练，并在Hands In Action数据集（HIC）上进行测试。使用的评估指标是平均每关节位置误差（MPJPE）、平均每顶点位置误差（MPVPE）和平均相对根位置误差（MRRPE）。

#### 9. 实验结果和分析：
InterWild框架在HIC和IH2.6M数据集上优于所有现有的3D交互手部网格恢复方法。在HIC数据集上，MPVPE和MRRPE差距尤其大，证明了InterWild对ITW环境的鲁棒性。领域共享方法有效地减少了领域差距，从而在ITW数据集上实现了强大的性能。在Figure 9中，与以前的方法相比，InterWild成功地从ITW图像中恢复了3D网格，而以前的最先进方法无法做到这一点。


# Paper:496     FLAG3D：带有语言指导的三维健身活动数据集



#### 1. Title: 
FLAG3D: A 3D Fitness Activity Dataset with Language Instruction

#### 2. Authors: 
Yansong Tang, Jinpeng Liu, Aoyang Liu, Bin Yang, Wenxun Dai, Yongming Rao, Jiwen Lu, Jie Zhou, Xiu Li

#### 3. Affiliation: 
清华大学深圳研究生院

#### 4. Keywords: 
3D pose, fitness activity, language instruction, MoCap system, video dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_FLAG3D_A_3D_Fitness_Activity_Dataset_With_Language_Instruction_CVPR_2021_paper.html  Github: https://andytang15.github.io/FLAG3D/

#### 6. Summary : 
- (1):本文研究背景是计算机视觉领域中的健身活动分析，由于健身活动的不断普及，需要更多的数据资源来支持高质量数据、细粒度标签和多样化环境的研究。
 
- (2):过去的方法主要是基于RGB视频、光流、音频波形和骨架等模态进行动作识别，但是这些方法在处理复杂的姿态和大运动时存在困难。本文提出的方法通过高精度的MoCap系统、详细的语言指导和多样化的视频资源来解决这些问题，具有很好的动机。

- (3):本文提出了一个名为FLAG3D的新数据集，包含60个复杂的健身活动，共180K个序列。FLAG3D具有高精度和密集的3D人体姿态、详细的语言指导和多样化的视频资源。本文还对FLAG3D进行了广泛的实验和深入的分析，为骨架动作识别、人体网格恢复和语言引导的人体动作生成等各种挑战提供了研究价值。

- (4):本文的方法在骨架动作识别、人体网格恢复和动态动作生成等任务上取得了良好的性能，支持了他们的目标。
#### 7. 方法详细介绍：
本文提出了一个名为FLAG3D的3D健身活动数据集，其中包含了60种复杂的健身活动，使用了高精度的MoCap设备、渲染软件和真实环境录制等多种手段进行构建。该数据集采用SMPL参数模型，优化过程包括两个阶段，第一阶段获取形状参数，第二阶段获取姿态和平移参数。同时，本文还提出了多种方法，包括ROMP和AC-TOR等，用于实现人体动作识别、人体网格恢复和人体动作生成等任务。

#### 8. 实验设置：
本文使用了FLAG3D数据集进行实验，该数据集包含了180K个序列，其中包括了高精度的3D姿态、详细的语言指令和多种视频资源等。实验使用了多种设备和环境，包括高科技MoCap系统、渲染软件和成本效益的智能手机等。

#### 9. 实验结果与分析：
本文的实验结果表明，FLAG3D数据集包含了有益的信息，可以用于提高现有方法的性能。在人体动作识别任务中，本文提出的方法在FLAG3D数据集上取得了较高的准确率。在人体网格恢复任务中，本文提出的ROMP方法在FLAG3D数据集上表现出更好的性能。在人体动作生成任务中，本文提出的AC-TOR方法在基于类别的设置下表现出了较好的结果，而TEMOS方法则在基于语言的设置下表现出了较好的视觉效果和上下文感知能力。此外，本文还探讨了FLAG3D数据集未来的研究方向，包括视觉定位、重复动作计数和动作质量评估等。


# Paper:497     无法窃取？对比窃取！针对图像编码器的对比窃取攻击



#### 1. Title: 
Can’t Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders

#### 2. Authors: 
Zeyang Sha, Xinlei He, Ning Yu, Michael Backes, Yang Zhang

#### 3. Affiliation: 
Zeyang Sha, Xinlei He, Michael Backes, Yang Zhang: CISPA Helmholtz Center for Information Security
Ning Yu: Salesforce Research

#### 4. Keywords: 
Model stealing attacks, self-supervised learning, contrastive learning, image encoders, representation learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sha_Cant_Steal_Cont-Steal_Contrastive_Stealing_Attacks_Against_Image_Encoders_CVPR_2021_paper.html  Github: https://github.com/zeyangsha/Cont-Steal

#### 6. Summary : 
- (1):本文研究的背景是自监督学习技术的快速发展，这些技术将无标签图像编码为对下游任务无感知的丰富特征。然而，这些技术的要求和大量的计算资源使得图像编码器面临潜在的模型窃取攻击的风险。
 
- (2):过去的方法只针对监督分类器进行攻击，而对于无监督编码器的漏洞则未被探索。本文首先实例化了传统的窃取攻击，并展示了它们相对于下游分类器的更严重的漏洞。为了更好地利用编码器的丰富表示，本文进一步提出了一种基于对比学习的攻击方法Cont-Steal，并验证了其在各种实验设置中的改进窃取效果。
 
- (3):本文提出了一种对比学习的模型窃取攻击框架Cont-Steal，其目标是将图像的替代编码器的嵌入与目标编码器的嵌入靠近（定义为正对），并将不同图像的嵌入推开（定义为负对）。Cont-Steal利用样本间的高阶信息来模仿目标编码器的功能，相对于传统攻击方法，Cont-Steal更加适合于编码器的攻击。
 
- (4):本文在五个数据集上评估了攻击的效果，结果表明Cont-Steal在攻击效果上远远优于传统的模型窃取攻击。例如，当CIFAR10是目标数据集时，Cont-Steal在SimCLR编码器上预训练的情况下，使用STL10作为替代数据集和下游数据集，达到了0.714的准确率，而传统攻击只达到了0.457的准确率。本文的攻击进一步暴露了预训练编码器的严重漏洞，呼吁学术界对表示学习技术的知识产权保护特别是对抗编码器窃取攻击的防御加强关注。
#### 7. 方法详细介绍：
本文提出了一种基于对比学习的模型窃取方法Cont-Steal，用于窃取编码器。Cont-Steal引入了不同类型的负样本作为“锚点”，以更好地导航代理编码器并学习目标编码器的功能。Cont-Steal方法包括以下步骤：
1. 获取代理数据集。
2. 训练代理编码器，使用类似的损失函数来优化代理编码器。
3. 将代理编码器应用于下游任务。
4. 使用对比损失函数来最大化目标编码器和代理编码器生成的嵌入的相似性，以及最小化它们的相似性。

#### 8. 实验设置：
本文在CIFAR10上使用SimCLR预训练了ResNet18编码器，并使用STL10训练了下游分类器。使用BadEncoder对CIFAR10上使用SimCLR预训练的编码器进行水印防御，并利用不同的下游数据集执行不同的任务。

#### 9. 实验结果和分析：
本文评估了不同防御方法对Cont-Steal的性能，包括添加噪声、top-k、特征舍入和基于水印的防御。实验结果表明，虽然添加噪声和top-k可以降低模型窃取攻击的性能，但也可能大幅降低目标模型的性能。舍入对目标模型性能和攻击性能的影响有限。水印无法保留，因为Cont-Steal构建的代理模型与基线模型具有相似的水印率。本文还评估了Cont-Steal方法的性能，并在不同设置下证明了其有效性。结果表明，Cont-Steal通过更深入地利用嵌入信息实现了更好的攻击性能。


# Paper:498     基于3D空间多模态知识累积的点云场景图预测



#### 1. Title: 
3D Spatial Multimodal Knowledge Accumulation for Scene Graph Prediction in Point Cloud

#### 2. Authors: 
Mingtao Feng, Haoran Hou, Liang Zhang, Zijie Wu, Yulan Guo, Ajmal Mian

#### 3. Affiliation: 
Mingtao Feng, Haoran Hou, Liang Zhang: 西安电子科技大学
Zijie Wu: 湖南大学
Yulan Guo: 中山大学
Ajmal Mian: 西澳大学

#### 4. Keywords: 
3D scene understanding, scene graph prediction, point cloud, multimodal knowledge, hierarchical structure

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Feng_3D_Spatial_Multimodal_Knowledge_Accumulation_for_Scene_Graph_Prediction_in_CVPR_2021_paper.html  Github: https://github.com/HHrEtvP/SMKA

#### 6. Summary : 
- (1):本文研究了3D场景理解中的场景图预测问题，提出了一种基于3D空间多模态知识融合的方法，以解决现有方法在训练样本有限的情况下表现不佳的问题。
 
- (2):现有的3D场景图预测方法通常忽略了点云场景中的层次结构，无法很好地处理复杂的关系。本文提出了一种基于3D空间多模态知识融合的方法，将3D场景的层次结构融入深度神经网络中，以加强场景图预测过程。与现有方法相比，本文方法能够更好地处理复杂的关系，提高预测准确率。

- (3):本文提出了一种基于3D空间多模态知识融合的方法，以解决现有方法在训练样本有限的情况下表现不佳的问题。具体来说，本文利用外部知识库作为基线，累积上下文化的视觉内容和文本事实，形成一个3D空间多模态知识图。此外，本文提出了一种基于知识的场景图预测模块，利用3D空间知识有效地规范关系的语义空间。实验结果表明，本文方法优于当前最先进的竞争方法。

- (4):本文方法在3D场景图预测任务上取得了优异的性能，能够更好地处理复杂的关系，提高预测准确率。
#### 7. 方法详细介绍：
本文提出的方法包括三个主要组件：层次化符号知识构建、知识引导的视觉上下文编码和三维空间多模态知识积累。层次化符号知识构建模块过滤外部常识知识库，对每个节点分类层次化标记，并添加新的支持边以形成三维场景的层次化符号知识图。知识引导的视觉上下文编码模块从重构的符号知识图中检索层次化标记，为三维场景中的对象实例构建视觉图，并使用区域感知图网络提取节点和边的上下文特征。最后，三维空间多模态知识积累模块通过将学习到的与视觉相关的三维空间多模态知识纳入关系预测阶段作为额外的指导，规范了关系预测的语义空间。

#### 8. 实验设置：
本文在3DSSG数据集上评估模型，选择160个物体类别和27个关系类别进行检测。采用约束评估指标recall@K（R@K）和平均召回率@K（mR@K）进行评估。模型使用PyTorch实现，使用一块NVIDIA GTX TITAN X GPU进行40个epoch的训练，使用ADAM优化器。初始学习率为0.0001，权重衰减为0.5，mini-batch为4。VoteNet用作3D对象检测骨干网络，在SGDet任务中生成256个对象候选集。Point Cloud Transformer使用与[13]相同的设置在3DSSG数据集上进行预训练。

#### 9. 实验结果与分析：
本文提出的方法在3DSSG数据集上的所有指标上均优于现有方法，在SGCls任务的mR@50上提高了3.57％，在PredCls任务的R@50上提高了10.08％。由于对象检测性能的瓶颈，SGDet任务的性能已经饱和。消融研究表明，三维空间的分层结构和支持边对所提出的方法的性能至关重要。


# Paper:499     利用属性条件对抗人脸欺骗法医分类器



#### 1. Title: 
Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces

#### 2. Authors: 
Fahad Shamshad, Koushik Srivatsan, Karthik Nandakumar

#### 3. Affiliation: 
Mohamed bin Zayed University of AI, UAE (阿联酋穆罕默德·本·扎耶德人工智能大学)

#### 4. Keywords: 
Generative adversarial networks, forensic classifiers, adversarial attacks, disentangled representations, attribute-conditioned attacks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shamshad_Evading_Forensic_Classifiers_With_Attribute-Conditioned_Adversarial_Faces_CVPR_2021_paper.html  Github: https://github.com/koushiksrivats/face_attribute_attack

#### 6. Summary : 
- (1):本文研究背景是生成对抗网络生成高度逼真的合成人脸图像引起了安全和伦理问题，因此需要开发深度学习的法医分类器来检测这些合成图像。

- (2):过去的方法是使用深度学习的法医分类器来检测合成图像，但是这些分类器容易受到对抗攻击的影响。本文提出了一种新的方法，可以成功生成具有指定属性的对抗性合成人脸图像，以欺骗法医分类器。

- (3):本文提出了一种基于StyleGAN的对抗性合成人脸图像生成方法，可以在StyleGAN的特征空间中搜索对抗性潜在编码，以生成具有指定属性的对抗性合成人脸图像。本文的创新点在于可以控制生成的对抗性合成人脸图像的属性，如肤色、表情、年龄等。

- (4):本文的方法可以成功生成具有指定属性的对抗性合成人脸图像，并且可以成功欺骗法医分类器，同时保持对人类不可察觉。本文的方法在未知目标模型上具有可转移性，并且可以在不牺牲对抗性的情况下保留生成图像的身份信息。
#### 7. 方法详细介绍：
本文提出了一种生成具有特定属性的对抗性假人脸的框架。该方法通过在StyleGAN2的潜在空间中对随机生成的初始图像进行语义操作和对抗扰动来实现。对于基于图像的属性调节，通过感知损失引导的对抗空间搜索将语义属性从给定的参考图像转移。对于基于文本的属性调节，使用多模态CLIP来强制生成的对抗性人脸图像与文本描述之间的一致性。该方法还包括一种基于元学习的优化策略，用于生成对未知黑盒法医分类器更具可转移性的对抗性图像，相比于集成方法，元学习方法的效果更好。

#### 8. 实验设置：
本文使用了FFHQ数据集中的50,000张真实人脸图像和50,000张StyleGAN2生成的图像进行训练。作者使用了多个分类器，包括EfficientNet-B3、Xception、VGG-19、ResNet-18、ResNet-50和DenseNet-121，对这些分类器进行了训练，以区分真实人脸和生成的假人脸。作者还对Wang等人提出的法医分类器进行了攻击，并使用了他们提出的基于图像和基于文本的语义攻击方法，取得了高攻击成功率。作者还进行了用户研究，评估了生成的对抗性图像的逼真程度。

#### 9. 实验结果和分析：
作者的方法在生成具有特定属性的对抗性假人脸方面表现出色，并且能够欺骗目标法医分类器。生成的图像与提供的文本提示具有高度的相似性，并且在用户研究中被评为逼真。作者还展示了生成的对抗性图像具有较低的FID分数，相比于基于噪声的攻击方法，生成的图像更加逼真。作者还展示了该方法能够生成任何特定族群的多样化对抗性图像，而只需使用一个文本提示。元学习方法能够生成对未知模型具有更高的可转移性的对抗性图像，相比于集成方法，元学习方法的效果更好。


# Paper:500     信号超分辨率网络的非直视成像



#### 1. Title: 
Non-line-of-sight Imaging with Signal Superresolution Network

#### 2. Authors: 
Jianyu Wang, Xintong Liu, Leping Xiao, Zuoqiang Shi, Lingyun Qiu, and Xing Fu

#### 3. Affiliation: 
清华大学

#### 4. Keywords: 
Non-line-of-sight imaging, signal recovery, neural network, imaging quality, confocal and non-confocal settings

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Non-Line-of-Sight_Imaging_With_Signal_Superresolution_Network_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是非直视成像问题，即如何通过测量的瞬态数据重建隐藏物体的位置、形状、反射率和表面法线。由于其在自动驾驶、灾难救援和遥感等领域的潜在应用，近年来受到了广泛关注。然而，长曝光时间并不总是可用的，这阻碍了非直视成像的实际应用。本文提出了一种基于深度学习的管道，可以在只扫描少量点的情况下提高成像质量。

- (2):过去的方法主要是通过增加测量密度来提高成像质量，但这会增加测量时间。本文提出的方法是通过学习运算符来恢复高空间分辨率信号，从而提高成像质量。实验结果表明，与原始测量相比，本文的方法可以提高16倍的采集速度，同时保持类似的重建质量。

- (3):本文提出了一种基于深度学习的管道，可以通过少量扫描点提供高质量的重建。该管道由两个步骤组成：首先，使用神经网络将低分辨率信号映射到高分辨率信号空间；然后，使用现有的成像算法从恢复的高分辨率信号中获得重建结果。本文的方法可以直接应用于现有的光学系统和成像算法，具有很好的兼容性。

- (4):本文的方法在合成和实测数据上进行了实验，结果表明，与原始测量相比，本文的方法可以提高16倍的采集速度，同时保持类似的重建质量。本文的方法可以应用于高帧率（每秒64帧）的非直视成像视频。
#### 7. 方法详细介绍：
本文提出了一种基于神经网络的非线性算子学习方法，用于从低分辨率信号中恢复高分辨率信号。该方法包括两个步骤：第一步是使用神经网络学习算子，将低分辨率信号恢复为高分辨率信号；第二步是使用现有的高分辨率信号成像算法，从恢复的高分辨率信号中重建图像。神经网络由两个主要分支组成：传统的上采样分支和学习分支。最终输出是这两个分支结果的和。该方法可以直接应用于现有的光学系统和成像算法中作为插件使用。

#### 8. 实验设置：
本文的实验分为合成数据和实测数据两部分，包括共聚焦和非共聚焦两种设置。低分辨率信号是在少量扫描点处测量得到的，而高分辨率信号是使用本文提出的方法恢复得到的。本文使用公共数据集验证了该方法的有效性。

#### 9. 实验结果与分析：
本文将本文提出的方法与原始测量数据和其他现有的方法进行了比较。结果表明，本文提出的方法在保持重建质量的同时，采集时间缩短了16倍。本文提出的方法仅使用64个扫描点即可提供高质量的重建结果，这仅占原始扫描点数量的6.25％。本文还展示了该方法在高帧率NLOS视频成像中的潜在应用。本文还提供了定量评估结果，包括恢复信号的信噪比（SNR）以及重建最大强度投影的峰值信噪比（PSNR）和结构相似性指数（SSIM）。


# Paper:501     VecFontSDF: 通过有符号距离函数学习重建和合成高质量矢量字体



#### 1. Title: 
VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions

#### 2. Authors: 
Zeqing Xia, Bojun Xiong, Zhouhui Lian

#### 3. Affiliation: 
Wangxuan Institute of Computer Technology, Peking University, China (北京大学王选计算机技术研究所)

#### 4. Keywords: 
Vector font, signed distance function, implicit shape representation, quadratic Bézier curve, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xia_VecFontSDF_Learning_to_Reconstruct_and_Synthesize_High-Quality_Vector_Fonts_via_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是如何自动合成高质量的矢量字体，以便于数字内容设计和现代印刷行业。
- (2):现有的方法主要集中在栅格图像生成上，只有少数方法可以直接合成矢量字体。本文提出了一种基于有符号距离函数的隐式形状表示的端到端可训练方法VecFontSDF，用于重建和合成高质量的矢量字体。与现有方法相比，本文方法具有更好的可解释性和更高的精度。 
- (3):本文提出的方法使用卷积神经网络提取输入字形图像的特征，并将其解码为抛物线曲线的参数。然后，我们计算每个采样点最近曲线的有符号距离函数的值，并利用这些真实的有符号距离函数值以及目标字形图像来训练模型。本文方法的主要创新点是提出了一种新的显式形状表示方法，可以直接转换为常用的矢量字体格式（例如SVG和TTF），并使用真实的有符号距离函数作为强监督，从而获得更加精确的重建和合成结果。 
- (4):本文在公开数据集上进行了定性和定量实验，包括矢量字体重建、插值和少样本矢量字体合成等任务，结果表明本文方法在多个任务上均取得了高质量的结果，明显优于现有方法。
#### 1. 方法详细介绍：
本文提出了一种名为VecFontSDF的方法，用于重建和合成高质量的矢量字体。该方法使用符号距离函数（SDF）对每个输入字形进行预计算，并使用卷积神经网络（CNN）编码器从输入光栅图像中提取特征。然后，使用SDF解码器将这些特征解码为每个抛物线曲线的参数。接下来，使用预测的参数计算每个采样点到最近曲线的伪距离函数，并使用真实SDF值和目标字形图像对模型进行训练。最后，将这些抛物线曲线的参数转换为二次Bézier曲线。该模型使用图像重建损失、网格SDF损失、轮廓SDF损失和正则化损失的总和作为目标函数进行训练。该模型是可微分的，因此可以进行端到端的训练。

#### 2. 实验设置：
本文使用了1,116种字体的数据集，其中前1,000种用于训练，剩余的116种用于测试。作者使用了一种描述在第3节中的方法将每个矢量字形的SVG文件转换为网格SDF和轮廓SDF。输入和输出图像的分辨率为128×128，轮廓SDF采样点的总数设置为4,000。作者使用了ResNet-18作为编码器，使用Leaky Relu和Batch Normalization提取图像特征，并使用Np = 16和Na = 6的隐式SDF解码器对每个基元进行解码。作者使用Adam优化器，学习率为1e-4，betas为(0.9, 0.999)，批量大小为64，进行了100,000次迭代的矢量字形重建模型训练。

#### 3. 实验结果和分析：
作者进行了消融实验，分析了不同损失对矢量重建任务的影响。结果表明，使用所有损失可以获得最佳性能。作者还将其方法与两种现有的SDF表示和两种最近提出的矢量重建方法进行了比较，证明了其VecFontSDF方法的有效性和优越性。最后，作者进行了矢量字形插值和few-shot风格转移实验，进一步展示了其VecFontSDF在矢量字体生成方面的潜力。


# Paper:502     交互式手-物体姿态估计的和谐特征学习



#### 1. Title: 
Harmonious Feature Learning for Interactive Hand-Object Pose Estimation

#### 2. Authors: 
Zhifeng Lin, Changxing Ding, Huan Yao, Zengsheng Kuang, Shaoli Huang

#### 3. Affiliation: 
第一作者：华南理工大学

#### 4. Keywords: 
Hand-object pose estimation, feature learning, interaction, ResNet-50, attention mechanism

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2021_paper.html  Github: https://github.com/lzfff12/HFL-Net

#### 6. Summary : 
- (1):本文研究手-物体姿态估计的问题，该问题在手和物体相互作用时存在严重的遮挡问题，因此难以准确地估计手和物体的姿态。

- (2):现有的方法通常采用单个骨干网络提取手和物体的特征，然后通过交互模块进一步增强它们。然而，这些方法通常忽略了手和物体在特征学习中的竞争关系，因为骨干网络将它们都视为前景，并且它们通常相互遮挡。本文提出了一种新的Harmonious Feature Learning Network (HFL-Net)框架，该框架结合了单流和双流骨干网络的优点。通过共享常见ResNet-50模型的低层和高层卷积层的参数，留下中间层未共享，使得手和物体可以被中间层单独提取，避免了它们在特征学习中的竞争。共享高层卷积层的参数也强制手和物体的特征处于相似的特征空间，从而促进它们的相互特征增强。本文提出了通过将手的特征与物体流中相同位置的特征进行串联来增强手的特征。采用自注意力层来深度融合串联特征。实验结果表明，我们提出的方法在HO3D和Dex-YCB数据库上始终优于现有的最先进方法。值得注意的是，我们的模型在手部姿态估计方面的性能甚至超过了仅执行单手姿态估计任务的现有工作。

- (3):本文提出了一种新的Harmonious Feature Learning Network (HFL-Net)框架，该框架结合了单流和双流骨干网络的优点。通过共享常见ResNet-50模型的低层和高层卷积层的参数，留下中间层未共享，使得手和物体可以被中间层单独提取，避免了它们在特征学习中的竞争。共享高层卷积层的参数也强制手和物体的特征处于相似的特征空间，从而促进它们的相互特征增强。本文提出了通过将手的特征与物体流中相同位置的特征进行串联来增强手的特征。采用自注意力层来深度融合串联特征。本文的创新点在于提出了一种新的框架，使
#### 7. 方法详细介绍：
本文提出了一种新的框架——Harmonious Feature Learning Network (HFL-Net)，用于联合手部和物体姿态估计。该框架包括一个经过精心设计的骨干模型、两个手和物体之间的交互模块、一个手解码器和一个物体解码器。骨干模型分别为手和物体产生和谐的特征图，使这些特征图在后续的交互模块中相互增强。交互模块包括基于骨干产生的和谐特征的物体到手和手到物体的特征增强。手和物体解码器采用了先前工作中使用的相同结构。训练阶段的总损失函数包括手和物体姿态估计任务的损失函数。具体步骤如下：
1. 使用ResNet-50作为骨干模型，其中低层和高层卷积层是共享的，中层卷积层是不共享的，以避免手和物体在特征学习中的竞争。
2. 通过连接相同位置的物体流特征和手流特征，使用自注意力层深度融合手特征。
3. 通过全连接和多头注意力层，自动实现物体到手的特征增强。
4. 将增强后的手和物体特征分别输入到手和物体解码器中进行姿态估计。

#### 8. 实验设置：
本文在HO3D和Dex-YCB数据集上评估了提出的方法。HO3D数据集包含20个物体和10个受试者的120K个RGB-D帧，而Dex-YCB数据集包含6个受试者和21个物体。评估指标包括手部姿态估计的平均3D关节误差和预测姿态与真实姿态之间的平均距离误差。

#### 9. 实验结果和分析：
提出的Harmonious Feature Learning Network (HFL-Net)在联合手部和物体姿态估计的标准基准测试中始终表现出色。骨干模型有效地缓解了手和物体姿态估计任务之间的竞争，同时只有适度的模型大小增加。我们的骨干模型和交互模块的组合比基于单流和双流骨干的结果表现更好。手到物体特征增强方法对于手部姿态估计非常有效。即使在严重遮挡的情况下，HFL-Net的姿态估计也比现有方法更准确。在HO3D数据集上，平均3D关节误差为7.5mm，在Dex-YCB数据集上，平均距离误差为6.5mm。


# Paper:503     基于Hessian感知的全局Vision Transformer剪枝



#### 1. Title: 
Global Vision Transformer Pruning with Hessian-Aware Saliency

#### 2. Authors: 
Huanrui Yang, Hongxu Yin, Maying Shen, Pavlo Molchanov, Hai Li, and Jan Kautz

#### 3. Affiliation: 
第一作者：加州大学伯克利分校

#### 4. Keywords: 
Transformer, Vision Transformer, Structural Pruning, Latency-aware, Hessian-based

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Global_Vision_Transformer_Pruning_With_Hessian-Aware_Saliency_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了Vision Transformer（ViT）模型的设计问题，提出了一种全局结构剪枝方法，通过参数重分配来提高模型的效率和准确性。

- (2):过去的方法主要是继承NLP任务的启发式设计，没有考虑计算机视觉的特点，导致模型存在冗余和效率-准确性权衡问题。本文提出的方法通过全局结构剪枝和参数重分配来解决这些问题，同时提出了一种基于Hessian矩阵的重要性评分方法，可以在不同层和结构之间进行比较。

- (3):本文提出的方法是一种全局结构剪枝方法，可以在不同层和结构之间进行比较，同时考虑了模型的计算延迟。通过迭代剪枝，可以得到一系列新的ViT模型，称为NViT。NViT模型通过参数重分配，可以在保持准确性的情况下，显著减少参数数量和计算量。本文的方法在ImageNet-1K数据集上进行了测试，取得了比现有方法更好的性能。

- (4):本文的方法在ImageNet-1K数据集上进行了测试，NViT-Base模型在保持准确性的情况下，可以将FLOPs减少2.6倍，参数减少5.1倍，运行时间加速1.9倍。NViT的性能优于现有方法，可以在保持准确性的情况下，将参数数量减少3.3倍。本文的方法提供了一种简单而有效的参数重分配规则，可以提高ViT模型的效率和准确性。
#### 7. 方法详细介绍：
本文提出了一种基于Hessian矩阵的全局结构剪枝算法，称为NViT。该算法通过为每个结构组分配门变量，计算相对于门变量的Hessian矩阵，并使用矩阵范数作为确定参数组重要性的标准。然后，基于所有权重元素的梯度计算重要性分数，并根据其重要性分数贪婪地删除结构组，直到达到目标约束。该方法还包括头部对齐，以显式控制每个头中保留的QK和V维数的数量。NViT算法通过重新分配ViT块内和跨多个层级的参数来实现全局结构剪枝。该方法通过迭代剪枝DeiT-Base模型，得到了一系列高效的ViT模型。

#### 8. 实验设置：
本文在ImageNet-1K基准测试集上评估了提出的NViT算法。使用DeiT-Base模型作为基准模型进行比较。实验在V100 GPU上进行。

#### 9. 实验结果与分析：
本文提出的NViT算法在V100 GPU上实现了近乎无损的5.14倍参数减少，2.57倍FLOPs减少和1.86倍加速比。当将NViT缩小到类似延迟时，与DeiT-Small和DeiT-Tiny模型相比，NViT获得了1%和1.7%的精度提升。NViT在FLOPs和速度方面比基于NAS的AutoFormer和SOTA结构剪枝方法S2ViTE进一步减少了1.8倍FLOPs和1.5倍速度。NViT在ImageNet上训练的效率和性能优势也转移到了下游分类和分割任务。本文还对高效ViT架构的新参数分布规则进行了进一步的实证和理论分析。在Vision Transformer的效率和精度之间提供了更好的平衡。


# Paper:504     学习神经原型场进行野外环境下的分离式3D人脸建模



#### 1. Title: 
Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild

#### 2. Authors: 
Zhenyu Zhang, Renwang Chen, Weijian Cao, Ying Tai, Chengjie Wang

#### 3. Affiliation: 
Tencent Youtu Lab, Shanghai, China

#### 4. Keywords: 
3D face modeling, neural rendering, disentanglement, multi-image priors, uncertainty modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是在野外环境下的3D人脸建模问题，该问题的研究背景是3D人脸重建一直是一个长期存在的问题，应用包括游戏、数字人和移动摄影等领域。然而，由于存在大量的变化，例如光照、姿态和表情等，因此该问题是不适定的，需要强的假设和先验知识。

- (2):过去的方法主要是基于3D Morphable Model (3DMM)的参数拟合，或者是基于深度学习的方法，这些方法在一定程度上解决了3D人脸建模的问题，但是它们也存在一些问题，例如对于大量的变化不够鲁棒，或者需要受限于受控环境。本文提出了一种新的方法，即神经原型场（Neural Proto-face Field，NPF），该方法可以从野外照片集合中分离出公共/特定的面部线索，例如身份、表情和场景特定的细节，从而提高了3D人脸建模的鲁棒性。

- (3):本文提出的NPF方法可以从野外照片集合中提取多图像先验，通过不确定性建模来聚合3D一致的身份，然后学习适当的表情表示来变形原型，最后通过几何和外观正则化来优化拟合目标图像。NPF方法的创新点在于，它可以从野外照片集合中提取多图像先验，从而提高了3D人脸建模的鲁棒性。

- (4):本文在多个基准测试上进行了广泛的实验，结果表明，与现有的方法相比，NPF可以恢复出更好的面部形状和纹理，具有更好的性能和鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为神经原型面场（Neural Proto-face Field，NPF）的方法，用于在野外环境下进行解耦的三维人脸建模。该方法通过从野外照片集合中分离出共同/特定的面部线索，即身份、表情和场景特定细节，来学习面部原型。NPF通过不确定性建模提取来自照片集合的多图像先验，从而聚合三维一致的身份。然后，NPF学习使用适当的面部表情来变形原型，受表情一致性和个人特质的损失约束。最后，NPF被优化以适应集合中的目标图像，恢复外观和几何的特定细节。通过这种方式，生成模型从多图像先验和有意义的面部结构中受益。

#### 8. 实验设置：
本文使用CelebA、CASIA-WebFace和CelebAMask-HQ数据集进行训练和测试。数据集使用ID标签进行组织，每个身份至少有6张照片。本文在MICC、Photoface和FG3D数据集上进行测试。本文使用SSIM、Arcface表示的余弦相似度、身份参数的RMSE、法线图的MAD和点到平面距离作为评估指标。本文还提供了实现细节，如网络架构、损失权重和优化方法。

#### 9. 实验结果和分析：
本文将所提出的方法与几种基线进行比较，并分析了每个提出的组件的有效性。完整的方法在所有指标上都取得了最佳表现。面部原型也获得了令人满意的甚至比几个基线更好的结果。自适应不确定性和身份感知聚合有助于挖掘一致的线索。变形建模显着提高了细节和形状的准确性。多图像预热和一致性损失也提高了拟合结果的鲁棒性和准确性。本文提供了实验结果的详细分析和可视化。


# Paper:505     鲁棒性多视角三角化的半定松弛方法



#### 1. Title: 
Semidefinite Relaxations for Robust Multiview Triangulation

#### 2. Authors: 
Linus H¨arenstam-Nielsen, Niclas Zeller, Daniel Cremers

#### 3. Affiliation: 
第一作者：Technical University of Munich

#### 4. Keywords: 
multiview triangulation, semidefinite relaxation, robustness, outlier, convex relaxation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Harenstam-Nielsen_Semidefinite_Relaxations_for_Robust_Multiview_Triangulation_CVPR_2021_paper.html  Github: https://github.com/linusnie/robust-triangulation-relaxations

#### 6. Summary : 
- (1):本文研究的是多视角三角化问题，即如何从多个视角的二维图像中恢复三维点的位置。由于二维观测通常存在噪声和异常值，因此需要使用鲁棒性方法来解决这个问题。

- (2):过去的方法通常使用局部优化方法来计算解决方案，但这些方法往往不是全局最优的。本文提出了一种基于凸松弛的方法，可以证明在存在噪声和异常值的情况下，可以计算出最优的重建结果。本文的方法可以通过两种凸松弛公式来实现，一种基于极线约束，另一种基于分数重投影约束。这两种公式都可以在存在噪声和异常值的情况下保持紧密。

- (3):本文的主要贡献在于，将现有的凸松弛方法扩展到了鲁棒性多视角三角化问题，并提出了两种新的凸松弛公式。这些公式可以通过证明其对偶解的存在性来保证其在无噪声和无异常值的情况下的紧密性。本文的方法可以计算出证明最优重建结果的鲁棒性解决方案。

- (4):本文的方法在多个数据集上进行了广泛的实验，证明了其可以在存在噪声和异常值的情况下计算出最优的重建结果。本文的方法可以作为一种可靠的鲁棒性三角化方法，可以在实际应用中使用。
#### 7. 方法详细介绍：
本文提出了两种凸松弛方法来解决多视图三角化问题。第一种方法基于极线约束，第二种方法基于分数重投影约束。两种方法都采用了截断最小二乘代价函数来实现鲁棒性。通过将问题从Rd松弛到d×d正半定矩阵集合Sd，采用升维策略来解决QCQP问题，从而实现了可证明的最优重建，即使在存在大量噪声和离群值的情况下也能够实现。

#### 8. 实验设置：
本文未提及实验设置。

#### 9. 实验结果与分析：
本文进行了模拟实验来评估所提出的松弛方法的性能。实验在3、5、7、25和30个视图的三角化问题上进行。相机放置在半径为2的球体上，从单位立方体中采样一个点进行三角化。使用Reichstag数据集中的一个相机的参数模拟重投影模型。将高斯噪声添加到真实图像坐标中以模拟噪声观测。通过随机选择一个视图并将测量替换为图像中的随机点来引入离群值。

本文报告了鲁棒极线松弛（Eq.（RT））和鲁棒分数松弛（Eq.（RTF））的平均紧松松弛数量和估计误差，用于3、5、7、25和30个视图的实验。本文表明，鲁棒分数松弛相对于噪声和离群值都更加稳定。本文还表明，所提出的松弛方法在精度和鲁棒性方面优于现有方法。


# Paper:506     使用用户级差分隐私学习生成图像嵌入



#### 1. Title: 
Learning to Generate Image Embeddings with User-level Differential Privacy

#### 2. Authors: 
Zheng Xu, Maxwell Collins, Yuxiao Wang, Liviu Panait, Sewoong Oh, Sean Augenstein, Ting Liu, Florian Schroff, H. Brendan McMahan

#### 3. Affiliation: 
谷歌研究院（Google Research）

#### 4. Keywords: 
Differential Privacy, Federated Learning, Image Embeddings, Privacy Utility Trade-offs

#### 5. Paper: 
Paper Link: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_to_Generate_Image_Embeddings_With_User-Level_Differential_Privacy_CVPR_2021_paper.html
Github Link: None

#### 6. Summary : 
- (1):本文研究的背景是如何在保护隐私的前提下，训练大规模的图像嵌入模型，以便在计算机视觉任务中使用。

- (2):过去的方法在小型设备上使用用户级差分隐私（DP）成功地训练了下一个单词预测和图像分类任务。然而，当直接应用于使用大类空间的监督训练数据来学习嵌入模型时，现有方法可能会失败。本文提出了DP-FedEmb，这是一种联邦学习算法的变体，具有每个用户的灵敏度控制和噪声添加，以从在数据中心集中的用户分区数据中训练大型图像到嵌入特征提取器。DP-FedEmb结合了虚拟客户端、部分聚合、私有本地微调和公共预训练，以实现强隐私效用权衡。本文将DP-FedEmb应用于训练面部、地标和自然物种的图像嵌入模型，并在基准数据集DigiFace、EMNIST、GLD和iNaturalist上展示了其在相同隐私预算下的优越效用。本文进一步说明，当数百万用户参与培训时，可以实现强用户级DP保证ϵ<2，同时将效用下降控制在5%以内。

- (3):本文提出了DP-FedEmb来训练具有用户级差分隐私的嵌入模型。DP-FedEmb结合了公共预训练、虚拟客户端、本地微调和部分聚合，以实现强隐私效用权衡。DP-FedEmb的关键是将模型分成生成嵌入向量的骨干网络和特定于训练数据中的类的分类softmax头。在每个训练回合中，用户被分组为虚拟客户端，并从全局骨干网络初始化。为虚拟客户端上的有限数量的类添加本地随机初始化的softmax头层，并对完整的本地模型进行微调，以产生对骨干网络的更新。本地头参数不包括在私有聚合中，因此不需要添加噪声。这与现有方法（如DP-FedAvg/DP-SGD）不同，后者将在所有参数（包括softmax头）中添加噪声。骨干网络的更新被剪辑为最大L2范数，跨虚拟客户端进行聚合，并与适当的DP噪声结合。此时，私有化更新是DP机制的输出，并满足相应的DP保证。此更新然后应用于全局骨干网络，该网络继承DP保证，并传递到下一轮训练。DP-FedEmb显着提高了嵌入模型的DP训练的可扩展性，因为只有骨干网络的参数被私有化和发布，而该部分
#### 7. 方法详细介绍：
本文提出了DP-FedEmb方法，它是一种联邦学习算法，具有每个用户灵敏度控制和噪声添加，用于从数据中心集中训练用户分区数据。DP-FedEmb结合了虚拟客户端、部分聚合、私有本地微调和公共预训练，以实现强隐私-效用权衡。该方法将模型分为一个生成嵌入的骨干网络和一个特定于训练数据中的类别的分类softmax头。在每个训练轮次中，用户被分组为虚拟客户端，并从全局骨干网络初始化。为虚拟客户端添加一个本地随机初始化的softmax头层，该层仅针对虚拟客户端中的有限类别，然后对完整的本地模型进行微调，以生成对骨干的更新。本地头参数不包括在私有聚合中，因此不需要添加噪声。骨干更新被剪辑为最大L2范数，跨虚拟客户端进行聚合，并与适当的DP噪声结合。此时，噪声更新是DP机制的输出，并满足相应的DP保证。将此更新应用于全局骨干，该骨干继承DP保证，并传递到下一轮训练。

#### 8. 实验设置：
本文在DigiFace数据集上进行了实验，该数据集包含98.96K个身份和1.10M张图像。此外，还使用了EMNIST、Google Landmarks Dataset（GLD）和iNaturalist（iNat）数据集。隐私保证是通过使用Renyi差分隐私（RDP）计算并转换为（ϵ，δ）-DP或DP-FTRL计算的。作者将DP-FedEmb与非私有的中心化训练的oracle性能以及基线方法DP-FedAvg进行了比较。在大多数实验中，作者固定了联邦设置的超参数，并仅调整了学习率；骨干网络在ImageNet的1000个类别上进行了预训练；CLIENTOPT和SERVEROPT都是带有动量0.9的SGD优化器。

#### 9. 实验结果和分析：
本文在DigiFace、EMNIST、GLD和iNat数据集上进行了实验，结果表明DP-FedEmb方法在隐私保护和模型效用方面都优于DP-FedAvg。在DigiFace数据集上，DP-FedEmb在单个虚拟客户端上的效果优于DP-FedAvg，而在多个虚拟客户端上的效果更加显著。在EMNIST、GLD和iNat数据集上，DP-FedEmb也表现出了更好的效果。此外，作者还进行了实验，比较了DP-FedEmb和DP-FedAvg在不同隐私预算下的效果，结果表明DP-FedEmb在相同的隐私预算下具有更好的效果。


# Paper:507     ScanDMM：一种用于360◦图像扫描路径预测的深度马尔可夫模型



#### 1. Title: 
ScanDMM: A Deep Markov Model of Scanpath Prediction for 360◦ Images

#### 2. Authors: 
Xiangjie Sui, Yuming Fang, Hanwei Zhu, Shiqi Wang, Zhou Wang

#### 3. Affiliation: 
第一作者：江西财经大学

#### 4. Keywords: 
Scanpath prediction, 360◦ images, Deep Markov Model, visual attention, saliency detection, image quality assessment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sui_ScanDMM_A_Deep_Markov_Model_of_Scanpath_Prediction_for_360_Images_CVPR_2021_paper.html  Github: https://github.com/xiangjieSui/ScanDMM

#### 6. Summary : 
- (1):本文研究360◦图像的扫描路径预测，旨在基于人类视觉感知机制产生动态注视行为。 
- (2):现有的360◦图像扫描路径预测方法大致可分为两类：基于显著性的模型和生成模型。前者的性能高度依赖于显著性图的性能，而后者则对感兴趣区域的关注较少。本文提出了一种新的深度马尔可夫模型（DMM）架构，即ScanDMM，通过设计一个语义引导的转移函数来学习时间依赖的注意力景观的非线性动力学。此外，提出了一种状态初始化策略，通过考虑视图的起始点，使模型能够学习具有正确“发射器”的动态。 
- (3):本文提出了一种用于360◦图像的扫描路径预测的深度马尔可夫模型（DMM）ScanDMM。通过指定状态如何在场景语义和视觉工作记忆的指导下演变，来学习编码时间依赖的注意力景观的视觉状态的概率方法。提出了一种实用的策略来初始化视觉状态，使模型能够集中学习具有正确“发射器”的状态的动态，并能够为扫描路径生成分配特定的起始点。此外，将所提出的ScanDMM应用于其他计算机视觉任务，如显著性检测和图像质量评估，证明了其具有强大的通用性，并有望为其他视觉任务提供深入的见解。 
- (4):在四个360◦图像数据库上，本文的模型实现了最先进的性能，并展示了其通用性，同时在显著性检测和图像质量评估任务上取得了良好的性能，支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种名为ScanDMM的深度马尔可夫模型，用于360度图像的扫视路径预测。该模型由视觉编码器、马尔可夫链和扫视路径生成器组成。视觉编码器从输入的360度图像中提取视觉特征，马尔可夫链模拟扫视路径的时间依赖性，扫视路径生成器基于视觉特征和马尔可夫链生成扫视路径。该模型还设计了状态初始化策略和场景语义的引入，以提高模型的性能。该方法在Sitzmann、Salient360!、AOI和JUFE四个数据库上进行了实验，结果表明，该方法在准确性和效率方面均优于四种现有的扫视路径预测模型。

#### 8. 实验设置：
本文使用了四个360度图像数据库，包括Sitzmann、Salient360!、AOI和JUFE，进行实验。作者使用了Sitzmann数据库中的22张图像中的19张进行训练，其余用于验证。对于Salient360!数据库，作者一直使用其包含85张图像和3,036个扫描路径的训练集进行评估，因为其基准集不公开。作者从AOI和JUFE数据库中随机选择了20%的图像进行模型评估。作者通过在每个数据库上以1 Hz的速率对原始扫描路径进行采样，获得了模型训练和评估的Ground-Truth（GT）扫描路径。作者使用了三个指标来评估360度扫描路径预测模型的性能，包括Levenshtein距离（LEV）、动态时间规整（DTW）和重复度量（REC）。

#### 9. 实验结果与分析：
本文提出的ScanDMM在Salient360!数据库上取得了最佳性能，而在Sitzmann数据库上性能明显下降。作者在Sitzmann和Salient360!数据库上比较了不同的显着性检测模型。作者还在JUFE数据库上进行了比较实验，用于质量评估，并表明所有基线模型都无法预测感知质量，而我们的ScanDMM可以在特定的视图条件下模拟视觉行为，并显著提高2D IQA模型的性能。


# Paper:508     CARTO：关节不可知的关节物体重建



#### 1. Title: 
CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects

#### 2. Authors: 
Nick Heppert, Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Rares Andrei Ambrus, Jeannette Bohg, Abhinav Valada, Thomas Kollar

#### 3. Affiliation: 
第一作者：Nick Heppert，University of Freiburg（弗莱堡大学）

#### 4. Keywords: 
Articulated object reconstruction, implicit object-centric representations, joint-agnostic reconstruction, stereo RGB observation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Heppert_CARTO_Category_and_Joint_Agnostic_Reconstruction_of_ARTiculated_Objects_CVPR_2021_paper.html  Github: https://github.com/nheppert/carto

#### 6. Summary : 
- (1):本文研究的是从单个立体RGB观察中重建多个关节物体的方法。这是一个具有挑战性的问题，因为在没有关于感兴趣物体的先验知识的情况下，推断6D姿态和形状可能是模棱两可的。

- (2):以前的方法通常使用两阶段方法，首先使用Mask-RCNN等方法检测物体，然后根据检测输出预测物体属性，例如部分姿态和NOCS映射，并使用反向优化重建物体。这种方法复杂，容易出错，不适用于多个类别，并且不能实时运行。本文提出了一种新方法CARTO，它使用隐式物体中心表示法，并学习多个物体类别的单个几何和关节解码器。与仅针对每个类别单独训练解码器的方法相比，我们的解码器实现了可比的重建精度。结合我们的立体图像编码器，我们可以在单个前向传递中推断多个未知物体的3D形状、6D姿态、大小、关节类型和关节状态。我们的方法在新实例的mAP 3D IOU50方面相对于两阶段管道实现了20.4%的绝对改进。推理时间快，可以在NVIDIA TITAN XP GPU上以1 HZ的速度运行，最多存在8个物体。

- (3):本文提出了一种单次检测器，用于关节物体的重建。我们的方法由两个单独学习的组件组成：一个编码器，用于预测潜在物体代码以及相机帧中的姿态，以及一个解码器，用于在规范帧中重建物体，该规范帧可以通过预测的姿态转换为相机帧。我们的编码器建立在CenterSnap和SimNet之上。对于我们输入的立体图像中的每个像素，我们预测一个重要性标量ψ，其中较高的值表示接近对象在图像中的2D空间中心。 ψ的完整输出图表示对象的热图。此外，我们扩展了[7]，该方法预测了一个形状代码zs∈RDs，以便为每个像素预测关节代码zj∈RDj。这些代码可用于预测物体的关节状态。

- (4):本文提出的方法可以在单个前向传递中检测多个未知物体的3D形状、6D姿态、大小、关节类型和关节状态。我们的方法在新实例的mAP 3D IOU50方面相对于两阶段管道实现了20.4%的绝对改进。我们的方法可以在NVIDIA TITAN XP GPU上以1 HZ的速度运行，最多存在8个物体。
#### 7. 方法详细介绍：
本文提出了一种名为CARTO的方法，用于从单个RGB图像中重建多个关节式物体的几何和关节状态。该方法使用隐式物体中心表示，并学习了一个用于多个物体类别的几何和关节解码器。该方法包括两个单独学习的组件：一个编码器，用于预测潜在的物体代码以及相机帧中的姿态，以及一个解码器，用于从预测的代码重建物体。编码器预测每个关节式物体的重要性标量和规范化的6D姿态的密集像素图，独立于它们的关节状态。它还为每个像素预测形状代码和关节代码，可以用于预测物体的关节状态。解码器训练用于从预测的代码重建物体的形状和关节状态。关节代码从形状代码中分离出来，并施加物理约束来结构化学习到的关节代码。

#### 8. 实验设置：
本文使用PartNet-Mobility对象集生成训练和测试数据集进行实验。对象集包括具有一个固定基本部件和一个重要移动部件的物体，并将它们进一步区分为三种放置类型：独立放置、柜台放置和桌面放置。使用NAOCS空间对物体进行规范化，并将生成的数据重新缩放到一个单位立方体中。本文将提出的方法与最先进的方法A-SDF进行比较，并进行消融研究以了解相似性正则化项的重要性。

#### 9. 实验结果和分析：
本文进行了两个主要实验：物体中心规范化重建任务和完整场景重建任务。对于物体中心规范化重建任务，本文评估了提出的方法在物体的规范化框架中重建物体的几何和关节状态的能力。本文报告了双向L2-Chamfer距离和关节类型预测准确性以及关节状态误差，该误差根据关节类型以deg或m为单位进行测量。结果表明，CARTO和A-SDF在所有类别上训练的表现略好于基线。对于完整场景重建任务，本文将提出的方法与两阶段方法进行比较，并报告重建质量和运行时间性能。


# Paper:509     VolRecon：基于符号射线距离函数的体积渲染通用多视图重建



#### 1. Title: 
VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction

#### 2. Authors: 
Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys, Sabine S¨usstrunk

#### 3. Affiliation: 
Yufan Ren: IVRL IC EPFL

#### 4. Keywords: 
Neural implicit reconstruction, Signed Ray Distance Function, Multi-view stereo, Generalizable implicit reconstruction, Volume rendering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ren_VolRecon_Volume_Rendering_of_Signed_Ray_Distance_Functions_for_Generalizable_CVPR_2021_paper.html  Github: https://github.com/IVRL/VolRecon/

#### 6. Summary : 
- (1):本文研究的背景是神经隐式重建，该方法在3D建模、表面重建和新视角合成等方面有广泛应用。

- (2):过去的方法主要是基于多视图立体视觉的方法，但是这些方法需要多个步骤，如多视图深度估计、过滤和融合。而神经隐式重建方法则使用神经网络来建模场景，但是大多数现有的神经隐式重建方法只能针对特定场景进行优化，缺乏对新场景的泛化能力。本文提出了一种新的通用隐式重建方法，使用了Signed Ray Distance Function（SRDF）来重建场景。

- (3):本文提出的方法是VolRecon，它使用SRDF来进行通用的隐式重建。为了重建具有细节和少噪声的场景，VolRecon结合了从多视图特征聚合的投影特征和从粗略全局特征体积插值的体积特征。使用射线变换器，我们计算射线上采样点的SRDF值，然后渲染颜色和深度。在DTU数据集上，VolRecon在稀疏视图重建方面比SparseNeuS高出约30％，在全视图重建方面的准确性与MVSNet相当。此外，我们的方法在大规模ETH3D基准测试中表现出良好的泛化性能。

- (4):本文的方法在DTU数据集上的实验表明，我们的方法在稀疏视图重建方面比SparseNeuS高出约30％，在全视图重建方面的准确性与MVSNet相当。在ETH3D基准测试中，我们的方法表现出良好的泛化能力。
#### 7. 方法详细介绍：
本文提出了一种新颖的通用隐式重建方法VolRecon，使用有符号射线距离函数（SRDF）进行重建。该方法包括视图变换器和射线变换器，前者用于聚合多视图特征，后者用于计算沿射线的所有点的SRDF值以找到表面位置。通过利用投影特征和体积特征，该方法能够结合局部信息和全局形状先验，从而产生具有细节和高质量的重建结果。具体而言，该方法使用特征金字塔网络提取特征图，构建全局特征体积以获取全局信息，应用视图变换器将多视图特征聚合为一个特征，使用射线变换器提供沿射线的其他点的非局部信息。最后，该方法使用体积渲染来估计给定视点的深度图，可以将其融合成网格或密集点云。

#### 8. 实验设置：
本文使用DTU和ETH3D数据集来评估所提出的方法。DTU数据集包含124个具有不同复杂度的场景，ETH3D数据集包含18个大规模环境。实验在一台配备NVIDIA V100 GPU和256GB内存的服务器上进行。

#### 9. 实验结果和分析：
本文在DTU和ETH3D数据集上比较了所提出的VolRecon方法与SparseNeuS和MVSNet等现有方法的性能。结果表明，VolRecon在稀疏视图重建方面比SparseNeuS高出约30％，在全视图重建方面与MVSNet的精度相当。所提出的方法还展现了在大规模ETH3D基准测试上的良好泛化性能。该方法的代码可在https://github.com/IVRL/VolRecon/上获得。


# Paper:510     基于视觉识别的多重降级图像恢复与内在语义恢复



#### 1. Title: 
Visual Recognition-Driven Image Restoration for Multiple Degradation with Intrinsic Semantics Recovery

#### 2. Authors: 
Zizheng Yang, Jie Huang, Jiahao Chang, Man Zhou, Hu Yu, Jinghao Zhang, Feng Zhao

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Image restoration, visual recognition, multiple degradation, semantic recovery, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Visual_Recognition-Driven_Image_Restoration_for_Multiple_Degradation_With_Intrinsic_Semantics_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究背景是深度学习在图像识别任务中的成功，但是由于训练数据集中的高质量图像，当应用于低质量图像时，性能会显著下降。
- (2):过去的方法主要集中在图像恢复或域自适应上，前者关注的是视觉质量而不是识别质量，后者需要语义注释进行任务特定的训练。本文提出了一种视觉识别驱动的图像恢复网络，可以从视觉识别的角度恢复各种未知损坏类型的高质量图像。
- (3):本文提出了一种内在语义增强模块，可以动态地调节不同降级图像的语义表示，从而实现多种降级的图像恢复。该模块由降级归一化和补偿模块以及傅里叶引导调制模块组成。此外，还提出了一种先验分配优化策略，以便更好地感知降级对语义的影响。本文的VRD-IR是通用的，可以直接插入各种识别任务中作为图像增强模块。
- (4):本文在多种图像失真情况下进行了广泛的实验，包括分类、检测和人员重新识别等高级任务。实验结果表明，VRD-IR优于现有的图像恢复方法，并在多种高级任务中表现出优异的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“基于视觉识别的多重退化图像恢复与内在语义恢复”的方法（VRD-IR），旨在从视觉识别的角度恢复各种未知损坏类型的高质量图像。该方法由内在语义增强（ISE）模块和语义感知解码器（SAD）组成。ISE模块以动态方式将不同退化图像的语义表示协调到统一空间中，然后优化它们以实现内在语义恢复。SAD在低质量和高质量图像上进行预训练，目的是从相应的语义特征中重构高质量图像。在SAD的预训练过程中，强制执行相似性排名损失，以充分利用语义信息并为ISE提供良好的指导。整个训练基于先验归因优化策略，实例化为两阶段训练：第一阶段对SAD进行预训练，然后固定SAD并在第二阶段对ISE进行训练。VRD-IR是不受损坏和识别影响的，可以直接作为图像增强模块插入各种识别任务中。

ISE模块由两个主要部分组成：退化归一化和补偿（DNC）和傅里叶引导调制（FGM）。DNC旨在将各种退化特征映射到退化不变特征空间中，而FGM则旨在基于输入特征的统计属性集成多分支DNC的输出。ISE通过DNC和FGM进行微调。SAD首先通过相似性排名损失在干净和退化图像上进行预训练，以感知不同的语义。然后，ISE通过调制训练目标中的退化特征来训练，以改善SAD重构的图像质量，并通过语义最大损失来弥合退化和干净图像之间的语义差距。

具体步骤如下：
1. 对SAD进行预训练，使其能够感知干净和退化图像的语义表示。
2. 固定SAD并对ISE进行训练，以改善SAD重构的图像质量。
3. ISE模块由DNC和FGM组成，DNC将各种退化特征映射到退化不变特征空间中，FGM则集成多分支DNC的输出。
4. SAD和ISE的训练都基于先验归因优化策略。
5. 整个训练过程使用Adam优化器，初始学习率为2×10^-4，随着训练轮数的增加而降低。

#### 8. 实验设置：
本文未提供具体的实验设置。

#### 9. 实验结果与分析：
本文提出的VRD-IR方法在多种图像退化情况下进行了评估，广泛的实验表明，它超越了现有的图像恢复方法，并在分类、检测和人员重新识别等多个高级任务中表现出卓越的性能。结果表明，更高的视觉质量并不一定意味着更高的识别质量，而且该方法在识别质量方面优于其他方法。在去噪方面，VRD-IR的性能提升更为明显，这是机器视觉中最具挑战性的任务之一。VRD-IR在图像去雾方面的表现领先于其他方法，准确率提高了0.43%/0.18%。在图像去雾和去噪方面，不同方法的定性结果及其特征图在图6和图7中描述。在Market1501数据集上进行的人员重新识别的性能比较结果在表3中展示。更多比较结果在附录中提供。VRD-IR用于分类和检测是共享参数的，这进一步验证了VRD-IR可以实现更好的实用性和泛化能力。


# Paper:511     处理在线连续学习中的跨任务类别歧视问题



#### 1. Title: 
Dealing with Cross-Task Class Discrimination in Online Continual Learning

#### 2. Authors: 
Yiduo Guo, Bing Liu, Dongyan Zhao

#### 3. Affiliation: 
Yiduo Guo: Peking University, Beijing, China (北京大学王选计算机技术研究院)

#### 4. Keywords: 
Continual learning, class-incremental learning, cross-task class discrimination, catastrophic forgetting, replay-based methods

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guo_Dealing_With_Cross-Task_Class_Discrimination_in_Online_Continual_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了在线连续学习中的跨任务类别歧视问题，即如何在没有（或有限的）访问旧任务数据的情况下建立新任务和旧任务之间的决策边界。

- (2):现有的连续学习方法主要关注灾难性遗忘问题，而忽略了跨任务类别增量学习中的另一个主要挑战，即跨任务类别歧视问题。回放方法可以部分解决这个问题，但是由于训练偏差问题，其效果有限。本文提出了一种新的优化目标和梯度自适应方法来动态处理这个问题。

- (3):本文提出了一种名为GSA（Gradient Self-Adaptation）的新方法，用于在线连续学习中处理梯度不平衡和跨任务类别歧视问题。GSA包括一个新的训练目标和一个基于梯度的自适应损失，以补偿梯度不平衡。该损失由两个梯度速率动态控制，可以自适应地测量和适应动态的梯度不平衡情况。

- (4):在不同的在线连续学习设置下，实验结果表明，GSA方法比强基线方法取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为GSA（Gradient Self-Adaptation）的方法来解决在线连续学习中的跨任务类别区分问题。GSA包括一个新的训练目标和一个基于梯度的自适应损失，以补偿梯度不平衡。该方法将问题分解为跨任务分类和任务内分类。该方法通过两个梯度率动态控制损失，自动测量和适应动态梯度不平衡情况。本文从梯度不平衡（GI）角度分析了CTCD问题，并发现了两种梯度不平衡（数据不平衡和CL不平衡）。基于分析，提出了一种基于梯度的自适应损失来补偿GI。

#### 8. 实验设置：
本文使用了多个数据集，包括MNIST、CIFAR10、CIFAR100和TinyImageNet。特征提取所使用的架构为具有两个隐藏层的全连接网络（对于MNIST和GSA），而对于CIFAR10、CIFAR100和TinyImageNet则使用ResNet18。本文还应用了数据增强技术，如随机调整大小和随机灰度，以提高特征学习效果。本文使用Adam优化器，学习率为0.001，权重衰减为0.0001。对于所有方法，Xnew的批量大小均设置为10，而GSA的Xmix为64，基线的Xbuf为64。本文对每个任务运行所有方法一次，并报告15次随机运行的平均准确率。

#### 9. 实验结果和分析：
本文提出了一种新的学习策略和一种新的自适应损失函数GSA，用于解决在线连续学习中的跨任务类别区分问题。实证评估表明，GSA方法在准确率上大幅优于基线。GSA-CE损失被证明可以缓解梯度不平衡，降低A-PN梯度率不平衡，并使A-PN率曲线更接近联合训练曲线。本文还提到进行了长尾在线CL实验，并在附录8中展示了结果，其中将展示GSA方法也优于基线的情况。


# Paper:512     GlassesGAN：使用合成外观发现和定向子空间建模的眼镜个性化



#### 1. Title: 
GlassesGAN: Eyewear Personalization using Synthetic Appearance Discovery and Targeted Subspace Modeling

#### 2. Authors: 
Richard Plesh, Peter Peer, Vitomir Struc

#### 3. Affiliation: 
第一作者：Richard Plesh，Clarkson University, USA

#### 4. Keywords: 
Image editing, virtual try-on, generative adversarial networks, eyewear personalization, appearance modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Plesh_GlassesGAN_Eyewear_Personalization_Using_Synthetic_Appearance_Discovery_and_Targeted_CVPR_2021_paper.html  Github: https://github.com/pleshro/GlassesGAN_release

#### 6. Summary : 
- (1):本文研究的背景是虚拟试穿技术，旨在为消费者提供更好的在线购物体验，特别是眼镜的个性化设计和试穿。
- (2):过去的方法主要是基于3D建模和计算机图形学，缺乏灵活性和个性化，而且需要大量的3D数据。本文提出的方法是基于生成对抗网络的图像编辑技术，可以在高分辨率下实现对眼镜外观的连续多样式编辑，同时保持图像的真实性和一致性。本文的方法通过新颖的方法进行外观建模，可以在不需要真实眼镜图像的情况下进行编辑，同时提供了外观约束的子空间初始化技术，以提高编辑结果的可靠性。
- (3):本文提出了GlassesGAN框架，它使用生成对抗网络的潜在空间进行图像编辑，通过新颖的Synthetic Appearance Discovery机制和Targeted Subspace Modeling过程，可以捕捉GAN模型中眼镜外观的变化，从而实现对眼镜外观的连续多样式编辑。同时，本文还提出了一种外观约束的子空间初始化技术，以提高编辑结果的可靠性。在CelebA-HQ和SiblingsDB-HQf两个高分辨率数据集上进行了全面的实验，结果表明GlassesGAN的性能优于现有的技术，并提供了其他技术不具备的功能。
- (4):本文的方法在虚拟试穿领域取得了很好的效果，可以实现对眼镜外观的连续多样式编辑，同时保持图像的真实性和一致性。实验结果表明，GlassesGAN的性能优于现有的技术，并提供了其他技术不具备的功能。
#### 7. 方法详细介绍：
本文提出了一种名为GlassesGAN的框架，用于在虚拟试戴环境中进行眼镜个性化设计。该框架使用了GAN反演方法和一种新的目标子空间建模技术，以识别预训练GAN模型的潜在空间中的相关方向，以便在编辑图像中操纵眼镜的外观。GlassesGAN的一个关键组成部分是一种新的合成外观发现机制，该机制通过在面部图像上叠加手绘的二进制掩模来生成有和没有眼镜的成对数据。此外，本文还提出了一种外观约束的子空间初始化过程，用于推理时的编辑阶段，有助于在各种输入图像中产生一致的编辑结果。GlassesGAN框架使用高分辨率数据集（CelebA-HQ和SiblingsDB-HQf）进行训练，并与三种最先进的基线方法（InterfaceGAN、GANSpace和MaskGAN）进行比较。实验结果表明，GlassesGAN在所有竞争方法中表现最佳，同时提供了其他方法无法实现的功能（例如，细粒度的多样式编辑）。

#### 8. 实验设置：
本文在CelebA-HQ和SiblingsDB-HQf两个测试数据集上进行了全面的实验，以展示所提出的GlassesGAN框架在输出图像质量、编辑逼真度和连续多样式编辑能力方面的有效性。实验中使用了FFHQ、CelebA-HQ和SiblingsDB-HQf数据集进行训练和测试。实现细节和运行时间也进行了讨论，包括使用StyleGAN2和e4e编码器生成和反演图像。DatasetGAN框架用于为面部解析器生成合成训练数据，从CelebA-HQ训练图像中构建了一个6维子空间用于目标子空间建模。文本还提到，在所有实验中，训练和测试数据都是不相交的。

#### 9. 实验结果和分析：
本文在CelebA-HQ和SiblingsDB-HQf数据集上进行了定量比较，并与文献中的最新解决方案进行了比较。结果表明，GlassesGAN在两个数据集上的MSE得分显著低于其他方法，这表明GlassesGAN的编辑结果最接近原始图像。该方法的身份漂移也明显较少，如Identity Discrepancy Scores（IDS）得分比最接近的竞争者低了3倍，Q1的平均用户偏好为99％。此外，GlassesGAN生成的编辑图像在所有测试方法中具有最高的感知相似度，这表明其Fréchet Inception Distances（FID）得分最低。最后，用户调查结果显示，用户普遍更喜欢GlassesGAN而不是基线方法。




# Paper:513     FlowGrad：使用梯度控制生成ODE的输出



#### 1. Title: 
FlowGrad: Controlling the Output of Generative ODEs with Gradients

#### 2. Authors: 
Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, Qiang Liu

#### 3. Affiliation: 
第一作者：德克萨斯大学奥斯汀分校

#### 4. Keywords: 
Generative modeling, ordinary differential equations, controlled generation, back-propagation, non-uniform discretization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是控制生成模型的输出，特别是基于ODE的生成模型的输出。

- (2):过去的方法主要是GAN和扩散模型，但是GAN需要优化潜在嵌入，而扩散模型由于扩散噪声难以精确控制输出。本文提出了一种基于ODE的生成模型的输出控制方法，可以根据指导函数优化ODE模型的输出。本文的方法可以有效地反向传播梯度，而且不需要训练噪声指导模型，也不需要微调整个扩散模型。

- (3):本文提出了一种名为FlowGrad的快速梯度计算方案，可以有效地控制ODE的生成内容。具体来说，本文提出了一种新的策略来加速梯度计算，即利用ODE轨迹的直线程度来进行非均匀离散化，从而减少反向传播的时间。FlowGrad可以使用任何可微分的损失函数来控制ODE的生成内容。本文的方法在文本引导图像操作方面表现优异，而且可以在冻结的ODE模型中找到全局语义方向，用于操作新的图像。

- (4):本文的方法在文本引导图像操作方面表现优异，而且可以在冻结的ODE模型中找到全局语义方向，用于操作新的图像。
#### 7. 方法详细介绍：
本文提出了一种名为FlowGrad的方法，用于控制基于ODE的生成模型的输出。该方法通过优化ODE模型的输出来实现可控生成，优化目标是一个可微的损失函数。为了高效地反向传播梯度，FlowGrad将ODE轨迹分解，并计算向量-雅可比积。为了进一步加速反向传播的计算，FlowGrad提出了一种非均匀离散化的方法来近似ODE轨迹，其中测量轨迹的直线程度，并将直线部分聚合到一个离散化步骤中。具体步骤如下：
1. 编码和解码使用ODE。
2. 定义可微的损失函数来控制生成内容。
3. 使用梯度下降来最小化最优控制问题。
4. 通过向量-雅可比积来高效地反向传播梯度。
5. 使用非均匀离散化来近似ODE轨迹。

#### 8. 实验设置：
本文在文本引导的图像操作上进行了实验，以展示FlowGrad的有效性。使用了最先进的基于ODE的生成模型，包括Rectified Flow（RF）和Latent Diffusion Model（LDM）与DDIM。实验使用CelebA数据集，随机采样了1,000张图像，并使用包含{old, sad, smiling, angry, curly hair}的文本引导对其进行操作。使用LPIPS相似度、身份相似度和增强的CLIP分数来分别显示与原始面部的接近程度、原始图像和文本提示。在表2中展示了不同算法之间的定量比较。在表4中报告了不同方法之间的运行时间比较。在冻结的预训练LDM-FFHQ和RF-CelebA中找到的全局方向如图6所示。在阈值ξ和欧拉离散化步数N的影响下进行了消融研究，结果分别如图5b和图7所示。

#### 9. 实验结果和分析：
实验结果表明，FlowGrad在文本引导的图像操作任务中取得了优异的性能，相比于其他算法，FlowGrad在LPIPS相似度、身份相似度和增强的CLIP分数上均取得了更好的结果。此外，FlowGrad的运行时间也比其他算法更短。消融研究表明，阈值ξ和欧拉离散化步数N对FlowGrad的性能有一定影响。在预训练模型中找到的全局方向可以用于图像编辑和控制。


# Paper:514     FlowFormer++：基于掩蔽成本体积自编码的预训练光流估计



#### 1. Title: 
FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation

#### 2. Authors: 
Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li

#### 3. Affiliation: 
第一作者：香港中文大学多媒体实验室

#### 4. Keywords: 
Optical flow, transformer, pretraining, masked autoencoding, cost-volume encoder

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是光流估计，旨在提高光流估计的准确性。
 
- (2):过去的方法主要是基于卷积神经网络的方法，但是这些方法在处理大位移、噪声和遮挡等情况时表现不佳。本文提出了一种基于transformer和masked autoencoding的预训练方法，以提高光流估计的性能。该方法的动机充分，能够更好地利用transformer的能力。

- (3):本文提出了一种Masked Cost Volume Autoencoding (MCVA)的预训练方案，以增强FlowFormer的性能。该方案包括两个任务特定的设计：块共享掩蔽策略和重构预文本任务。前者防止了信息泄漏，鼓励长距离信息聚合；后者模拟了微调过程中的解码过程，避免了预训练和微调之间的差异。本文提出的方法在公共基准测试中取得了最新的最佳性能。

- (4):本文提出的方法在Sintel和KITTI-2015基准测试中均取得了最新的最佳性能。FlowFormer++在Sintel基准测试的clean pass和final pass上分别达到了1.07和1.94的平均端点误差（AEPE），比FlowFormer分别降低了7.76%和7.18%。在KITTI-2015测试集上，FlowFormer++的F1-all为4.52，比FlowFormer提高了0.16。这些结果表明，本文提出的方法能够有效地提高光流估计的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Masked Cost-volume Autoencoding的预训练方法，用于光流估计。该方法通过随机采样每个源像素的位置并裁剪小的代价图块来提取代价特征，然后通过较大的代价图块进行监督，以鼓励代价体编码器聚合全局信息以获得更好的光流估计。预测头是一个轻量级的MLP，均方误差（MSE）被用作损失函数。Masked Cost Volume Autoencoding（MCVA）方案包括三个关键组件：适当的代价体掩蔽策略、修改FlowFormer架构以适应掩蔽和监督预训练过程的新颖预文本重建任务。MCVA方案采用块共享掩蔽策略，将源像素分成非重叠块，并为每个源像素的代价图生成二进制掩蔽。方法还采用了两步代价体标记化，以防止掩蔽代价泄漏到后续的代价聚合层中。最后，该方法制定了预文本重建目标，监督解码过程以及嵌入和聚合层。

#### 8. 实验设置：
本文在Sintel和KITTI-2015基准测试集上评估了提出的FlowFormer++。模型在YouTube-VOS数据集上进行了50k次迭代的预训练，批量大小为24。在微调期间，模型在FlyingChairs上进行了120k次迭代，批量大小为8，在FlyingThings上进行了6次迭代。模型还在Sintel、KITTI-2015和HD1K的组合上进行了另外120k次迭代，批量大小为6。最高学习率在预训练和KITTI-2015数据集的微调中分别设置为5×10−4和2.5×10−4。在预训练和微调中均使用AdamW优化器和单周期学习率调度器。

#### 9. 实验结果与分析：
FlowFormer++在Sintel和KITTI-2015基准测试集上均取得了全面的改进，排名第一。在C+T设置下，模型在Sintel上获得了0.90的平均端点误差（AEPE），在KITTI-2015上获得了2.30的AEPE，在C+T+S+K+H设置下，模型在Sintel上获得了3.93的AEPE，在KITTI-2015上获得了14.13的AEPE。该模型还在两个基准测试集上优于其他最先进的方法。在Sintel和KITTI测试集上的定性比较显示，FlowFormer++保留了更清晰的细节，并保持了更好的全局一致性。


# Paper:515     SDFusion：多模态3D形状完成、重建和生成



#### 1. Title: 
SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation

#### 2. Authors: 
Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui

#### 3. Affiliation: 
第一作者：伊利诺伊大学香槟分校（University of Illinois Urbana-Champaign）

#### 4. Keywords: 
3D shape completion, 3D reconstruction, diffusion-based model, multi-modal inputs, text-to-3D

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2021_paper.html  Github: https://github.com/YCcYenChiCheng/SDFusion

#### 6. Summary : 
- (1):本文旨在提出一种基于扩散模型的生成模型，用于简化业余用户的3D资产生成过程。该方法支持多种输入模态，包括图像、文本、部分观察到的形状和这些的组合，进一步允许调整每个输入的强度。该方法的核心是一个编码器-解码器，将3D形状压缩成紧凑的潜在表示，然后学习扩散模型。为了支持多种任务，我们采用了具有dropout的任务特定编码器，后跟交叉注意机制。由于其灵活性，我们的模型自然支持各种任务，在形状完成、基于图像的3D重建和文本到3D方面优于先前的工作。最有趣的是，我们的模型可以将所有这些任务组合成一个瑞士军刀工具，使用户能够同时使用不完整的形状、图像和文本描述执行形状生成，提供每个输入的相对权重并促进交互。
- (2):传统的3D生成模型需要点云、有符号距离函数（SDF）或体素等直接的3D监督。最近，通过将归纳偏差引入神经渲染技术，探索了从多视角监督中学习3D几何形状的方法。然而，训练通常非常耗时，并忽略了可用的3D数据，这些数据可以用于获得良好的形状先验知识。本文提出了一种基于扩散模型的生成模型，用于简化业余用户的3D资产生成过程。该方法支持多种输入模态，包括图像、文本、部分观察到的形状和这些的组合，进一步允许调整每个输入的强度。该方法的核心是一个编码器-解码器，将3D形状压缩成紧凑的潜在表示，然后学习扩散模型。为了支持多种任务，我们采用了具有dropout的任务特定编码器，后跟交叉注意机制。由于其灵活性，我们的模型自然支持各种任务，在形状完成、基于图像的3D重建和文本到3D方面优于先前的工作。
- (3):本文提出了一种基于扩散模型的生成模型，用于简化业余用户的3D资产生成过程。该方法支持多种输入模态，包括图像、文本、部分观察到的形状和这些的组合，进一步允许调整每个输入的强度。该方法的核心是一个编码器-解码器，将3D形状压缩成紧凑的潜在表示，然后学习扩散模型。为了支持多种任务，我们采用了具有dropout的任务特定编码器，后跟交叉注意机制。由于其灵活性，我们的模型自然支持各
#### 7. 方法详细介绍：
SDFusion是一种基于扩散模型的多模态3D形状完成、重建和生成方法。首先，使用3D变体的向量量化变分自编码器（VQ-VAE）将3D形状压缩为低维潜在空间。然后，在该潜在表示上训练扩散模型以从目标分布中采样。该框架可以使用任务特定的编码器和交叉注意力模块来结合各种用户条件，如部分形状、图像和文本。最后，该框架可以与在2D数据上训练的扩散模型相结合，以纹理3D形状。

具体步骤如下：
1. 将3D形状压缩为低维潜在空间。
2. 在潜在表示上训练扩散模型以从目标分布中采样。
3. 使用任务特定的编码器和交叉注意力模块来结合各种用户条件。
4. 使用预训练的2D文本到图像模型来纹理生成的3D形状。

#### 8. 实验设置：
在ShapeNet和BuildingNet数据集上进行了实验。ShapeNet数据集是一个大规模的3D CAD模型数据集，包含16个常见的物体类别，BuildingNet数据集是一个新的大规模3D建筑模型数据集。在ShapeNet上使用的SDF分辨率为643，在BuildingNet上为1283。

#### 9. 实验结果和分析：
对于形状完成，SDFusion在ShapeNet和BuildingNet数据集上的保真度和多样性方面均优于AutoSDF和MPC。对于单视角3D重建，SDFusion在Pix3D数据集上的Chamfer距离和F-Score方面优于其他方法。该方法还成功地使用2D模型对3D形状进行了纹理处理。


# Paper:516     学习事件引导的高动态范围视频重建



#### 1. Title: 
Learning Event Guided High Dynamic Range Video Reconstruction

#### 2. Authors: 
Yixin Yang, Jin Han, Jinxiu Liang, Imari Sato, Boxin Shi

#### 3. Affiliation: 
Yixin Yang, Jinxiu Liang, Boxin Shi: School of Computer Science, Peking University, National Engineering Research Center of Visual Technology, Peking University, National Key Laboratory for Multimedia Information Processing. 

#### 4. Keywords: 
High Dynamic Range, Event Camera, Video Reconstruction, Multimodal Learning, Recurrent Convolutional Encoder

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Yang_Learning_Event_Guided_High_Dynamic_Range_Video_Reconstruction_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究高动态范围（HDR）视频重建的问题。传统的基于帧的HDR视频重建方法受到曝光时间和帧率之间的权衡的限制，而事件相机提供了一种具有更高动态范围和时间分辨率的替代视觉表示，可以作为从低动态范围（LDR）视频中进行HDR成像的有效指导。 

- (2):过去的方法包括使用多个传感器同时捕获不同曝光的LDR图像或使用多个曝光的LDR图像进行HDR图像重建。这些方法在捕捉动态场景时存在问题，如场景依赖的曝光比平衡和幽灵效应。本文提出了一种基于多模态学习的框架，利用事件相机提供的高动态范围和时间分辨率的优势，从LDR视频中重建HDR视频。 

- (3):本文提出了一种多模态表示对齐策略，将事件和帧逐步投影到共享表示空间中，以更好地利用两种视觉信号的相同场景的知识。为了可靠地补充来自两种模态的信息，本文提出了一种置信度引导的多模态融合模块，用于不同动态范围的不同区域。为了有效地抑制闪烁效应，本文利用了连续帧和事件之间的时间相关性。 

- (4):本文提出的HDRev-Net在合成数据和真实数据上均表现出最先进的性能。本文的贡献包括：1）设计了一种多模态对齐策略，以在共享潜在空间中对齐事件和帧的表示；2）开发了一种置信度引导的融合模块，以补充来自事件的HDR信息和来自LDR帧中曝光良好区域的更细节的信息；3）利用循环卷积编码器中连续事件和LDR帧之间的时间相关性，有效地抑制了闪烁效应。
#### 7. 方法详细介绍：
本文提出了一种多模态学习框架，用于事件引导的HDR视频重建。提出的HDRev-Net利用多模态表示对齐策略来学习共享潜在空间和融合模块，以补充不同区域不同动态范围的两种信号。采用时间相关性来抑制重建的HDR视频中的闪烁效应。具体步骤包括：
1. 提出多模态学习框架，包括共享潜在空间和融合模块。
2. 利用多模态表示对齐策略来学习共享潜在空间。
3. 设计融合模块，以补充不同区域不同动态范围的两种信号。
4. 利用时间相关性来抑制重建的HDR视频中的闪烁效应。

#### 8. 实验设置：
本文使用了两个数据集进行实验，分别是合成数据集和真实数据集。合成数据集包括了不同场景下的HDR图像和对应的LDR图像，真实数据集包括了不同场景下的HDR视频和对应的LDR视频。实验中使用了PyTorch框架，GPU为NVIDIA Tesla V100。

#### 9. 实验结果与分析：
本文提出的方法在合成数据集和真实数据集上均取得了优于其他方法的性能。在合成数据集上，本文方法的PSNR值分别为34.23和32.45，在真实数据集上，本文方法的PSNR值分别为31.56和29.87。同时，本文方法在重建HDR视频时能够有效抑制闪烁效应，重建视频质量更高。


# Paper:517     盲曝光条件下基于事件相机的模糊帧插值



#### 1. Title: 
Event-based Blurry Frame Interpolation under Blind Exposure

#### 2. Authors: 
Wenming Weng, Yueyi Zhang, Zhiwei Xiong

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Blurry frame interpolation, blind exposure, event camera, exposure estimation, temporal-exposure control

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Weng_Event-Based_Blurry_Frame_Interpolation_Under_Blind_Exposure_CVPR_2021_paper.html  Github: https://github.com/WarranWeng/EBFI-BE

#### 6. Summary : 
- (1):本文研究了在盲曝光条件下，利用事件相机进行模糊帧插值的问题。由于事件相机具有高时间分辨率的特性，可以获得在成像过程中丢失的曝光先验，从而解决了传统帧相机在盲曝光条件下的困难。 
- (2):传统的模糊帧插值方法假设预定义和已知的曝光时间，但在野外拍摄的视频中，曝光时间是变量和未知的，这导致了现有方法的性能严重下降。本文提出了一种基于事件流的曝光估计策略，通过迭代残差学习建立了时间-曝光控制策略，从而实现了在盲曝光条件下的模糊帧插值。 
- (3):本文提出了一种基于事件相机的模糊帧插值方法，通过事件流和模糊帧之间的相互约束来恢复清晰帧。首先，提出了一种基于事件流的曝光估计策略，以估计丢失的曝光先验，从而使盲曝光问题成为良好的问题。其次，通过迭代残差学习，提出了一种时间-曝光控制策略，以模拟事件流和模糊帧之间的物理约束。 
- (4):本文在合成和自采集的真实世界数据集上实现了优于现有方法的性能，证明了所提出的方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于事件相机的模糊帧插值方法，包括曝光估计模块、去模糊模块和锐化帧插值模块。曝光估计模块通过利用输入模糊帧和对应事件来估计丢失的曝光先验。去模糊模块使用估计的曝光先验去除模糊。锐化帧插值模块通过融合去模糊帧和事件流生成锐化帧。本文还采用了分阶段训练策略来优化网络。

#### 8. 实验设置：
本文在合成和真实数据集上对所提出的方法进行了全面评估。合成数据集使用GoPro和RealSharp-DAVIS生成，真实数据集使用DAVIS-346彩色事件相机采集。训练数据集的快门周期为16，训练集和测试集分别为TEST-I和TEST-II。本文遵循官方数据分割进行训练和测试。

#### 9. 实验结果与分析：
本文在合成和真实数据集上进行了实验，结果表明所提出的方法在PSNR和SSIM指标上均取得了最优表现。在自行收集的真实世界模糊视频数据集RealBlur-DAVIS上的视觉比较也证明了所提出方法的优越性。曝光估计模块对提高所提出方法的性能起到了积极作用。对展开步骤的研究和对输入和模型架构的消融研究也为所提出的方法提供了启示。


# Paper:518     学习用于摄影图像的深度颜色差异度量



#### 1. Title: 
Learning a Deep Color Difference Metric for Photographic Images

#### 2. Authors: 
Haoyu Chen, Zhihua Wang, Yang Yang, Qilin Sun, Kede Ma

#### 3. Affiliation: 
第一作者：香港城市大学

#### 4. Keywords: 
Color difference metric, photographic images, deep learning, autoregressive normalizing flow

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Learning_a_Deep_Color_Difference_Metric_for_Photographic_Images_CVPR_2021_paper.html  Github: https://github.com/haoychen3/CD-Flow

#### 6. Summary : 
- (1):本文研究的是针对摄影图像的深度颜色差异度量，传统的颜色差异度量方法主要是基于均匀着色的补丁，而不适用于具有自然场景复杂性的摄影图像。构建适用于摄影图像的颜色差异公式仍然是图像/照明、视觉科学和色彩科学社区的一个活跃的研究课题。

- (2):过去的方法主要是基于CIELAB坐标系的手工制作和主观校准的颜色差异度量，这些方法的参数是通过拟合均匀着色补丁的人类感知CD测量来校准的。然而，这些度量方法在摄影图像中的应用效果不佳。本文提出了一种基于深度学习的颜色差异度量方法，该方法具有四个理想的属性，即与视觉皮层处理中颜色和形式不可分割的观察结果相吻合，是数学意义上的适当度量，能够计算不同颜色外观的摄影图像之间的准确CD，并且对于轻微的几何失真具有鲁棒性。

- (3):本文提出了一种基于多尺度自回归归一化流的特征变换方法，其后跟欧几里得距离，该距离与人类感知CD成正比。通过这种方法，本文实现了所有四个理想属性。本文的方法在SPCD数据集上进行了定量和定性实验，证明了所学习的CD度量的优越性。

- (4):本文的方法在SPCD数据集上进行了实验，结果表明，所提出的CD-Flow方法在评估摄影图像的CD时优于15种CD公式，能够产生具有竞争力的多尺度局部CD映射，且对于几何失真具有更强的鲁棒性。本文的方法在多个方面验证了所学习的颜色图像表示的感知均匀性。
#### 7. 方法详细介绍：
CD-Flow方法是一种用于评估彩色图像的颜色差异的深度学习方法。该方法采用多尺度自回归归一化流进行特征变换，并在变换后的空间中使用欧几里得距离度量颜色差异。CD-Flow方法具有四个优点：通过压缩操作实现了可扩展性，通过双射性实现了可逆性，通过优化模型参数实现了与人类感知CD的一致性，通过多尺度和自回归性实现了对图像的全局和局部特征的建模。在训练过程中，CD-Flow方法使用随机梯度下降法优化模型参数。 

#### 8. 实验设置：
作者在大规模SPCD数据集上进行了实验，该数据集包含了1000个自然场景中的15335张彩色图像。作者将数据集随机分为70%的训练集、10%的验证集和20%的测试集。作者使用标准化残差平方和（STRESS）、皮尔逊线性相关系数（PLCC）和斯皮尔曼等级相关系数（SRCC）三个标准来评估CD-Flow方法与现有CD度量方法的性能。 

#### 9. 实验结果与分析：
作者将CD-Flow方法与15种现有的CD度量方法在SPCD数据集上进行了比较。结果表明，CD-Flow方法在STRESS、PLCC和SRCC三个指标上均优于其他方法。作者还在COM数据集及其四个子集上进行了CD-Flow方法的泛化性能评估，结果表明CD-Flow方法在这些数据集上的表现也优于其他方法。


# Paper:519     蒙版视频蒸馏：重新思考自监督视频表示学习中的蒙版特征建模



#### 1. Title: 
Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning

#### 2. Authors: 
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, Yu-Gang Jiang

#### 3. Affiliation: 
第一作者所属机构：复旦大学计算机科学学院

#### 4. Keywords: 
Self-supervised learning, video representation learning, masked feature modeling, vision transformers

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_CVPR_2022_paper.html  Github: https://github.com/ruiwang2021/mvd

#### 6. Summary : 
- (1):本文研究自监督视频表示学习，提出了一种新的两阶段蒙版特征建模框架，即Masked Video Distillation (MVD)。该方法首先通过恢复蒙版补丁的低级特征来预训练图像（或视频）模型，然后使用生成的特征作为蒙版特征建模的目标。 
- (2):现有的自监督视频表示学习方法主要通过重构低级特征（如原始像素值）来从头学习表示。然而，使用低级特征作为重构目标通常会产生很多噪声。此外，由于视频数据中存在高度冗余性，因此蒙版视频建模很容易学习到捷径，从而导致在下游任务中的转移性能有限。为了缓解这个问题，本文提出了MVD方法，通过使用预训练的MIM和MVM模型的高级特征作为蒙版预测目标，进行蒙版特征建模，从而学习更好的视频表示。 
- (3):本文提出了一种简单而有效的空间-时间共同教学策略，以利用图像教师和视频教师的优势。具体而言，学生模型被设计为使用两个不同的解码器重构来自图像教师和视频教师的特征，以同时学习更强的空间表示和时间动态。实验结果表明，与仅使用单个教师的MVD相比，使用图像教师和视频教师的共同教学策略的MVD在多个具有挑战性的下游任务上表现更好。 
- (4):本文在多个标准视频识别基准测试上展现了强大的性能，超过了没有MVD的基线和之前的最先进方法。例如，在Kinectics-400和Something-Something-v2数据集上，使用相同大小的教师模型的MVD相对于没有蒙版的基线，ViT-B的Top-1准确率提高了1.2％和2.8％。如果使用更大的教师模型ViT-L，则可以获得更显著的性能提升（即1.9％，4.0％）。当ViT-Large是目标学生模型时，我们的方法在这两个数据集上分别达到了86.4％和76.7％的Top-1准确率，超过了现有的最先进方法VideoMAE [57] 1.2％和2.4％。当采用更大的ViT-Huge模型时，MVD在Something-Something-v2上实现了77.3％的Top-1准确率。
#### 7. 方法详细介绍：
本文提出了一种名为Masked Video Distillation (MVD)的自监督视频表示学习方法。MVD使用高级特征而非低级像素对视频进行掩蔽特征建模。该方法使用现成的自监督预训练图像或视频模型生成高级特征作为重构目标。这些高级特征由经过掩蔽视觉建模的教师模型进行编码，例如MAE或Video-MAE。MVD通过使用两个分离的解码器同时重构不同的目标特征来预测图像教师和视频教师生成的目标高级特征。MVD的最终损失是来自图像教师和视频教师的损失的空间-时间共同教学的组合。本文还提供了MVD的架构设计，包括编码器、解码器、掩蔽策略和重构目标。

#### 8. 实验设置：
本文在几个标准视频识别基准数据集上进行了实验，包括Kinetics-400和Something-Something-v2数据集。本文使用ViT模型作为实验的骨干，并将MVD的性能与以前的监督或自监督方法进行了比较。

#### 9. 实验结果和分析：
本文报告了MVD在几个视频识别基准数据集上的实验性能。例如，使用ViT-Large模型，MVD在Kinetics-400和Something-Something-v2上分别达到了86.4％和76.7％的Top-1准确率，优于VideoMAE分别1.2％和2.4％。当采用更大的ViT-Huge模型时，MVD在Something-Something-v2上达到了77.3％的Top-1准确率，成为最先进的方法。本文还将MVD与以前的监督或自监督方法在Something-Something v2上进行了比较，每行表示不同大小的相应模型。


# Paper:520     DualRefine: 通过迭代极线采样和细化朝向平衡的自监督深度和姿态估计



#### 1. Title: 
DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium

#### 2. Authors: 
Antyanta Bangunharcana, Ahmed Magd, Kyung-Soo Kim

#### 3. Affiliation: 
韩国科学技术院（KAIST）机电一体化实验室

#### 4. Keywords: 
Self-supervised learning, depth estimation, pose estimation, iterative refinement, epipolar geometry

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Bangunharcana_DualRefine_Self-Supervised_Depth_and_Pose_Estimation_Through_Iterative_Epipolar_Sampling_CVPR_2021_paper.html  Github: https://github.com/antabangun/DualRefine

#### 6. Summary : 
- (1):本文研究自监督多帧深度估计和姿态估计，通过计算相邻帧之间的像素对应关系的匹配成本，将几何信息注入网络，以实现高精度的深度估计。准确的姿态预测对于精确的匹配成本计算至关重要，因为它们影响了极线几何。改进的深度估计可以用于对齐姿态估计。
- (2):过去的方法包括监督学习和半监督学习，但需要大量的标注数据。自监督学习方法通过利用多帧图像的几何信息，减少了对标注数据的需求。然而，这些方法仍然存在一些问题，如匹配成本计算的准确性和姿态估计的精度。本文提出了一种基于传统SfM原理的DualRefine模型，通过反馈循环紧密耦合深度和姿态估计，使用深度均衡模型框架迭代地细化深度估计和特征图的隐藏状态，通过计算基于极线几何的局部匹配成本。与传统的SfM优化类似，我们的方法同时解决深度和姿态，通过迭代更新来提高精度。
- (3):本文提出了一种基于极线几何和直接对齐的迭代更新模块，用于深度和姿态的自监督训练。我们的模型使用深度均衡（DEQ）框架，通过迭代更新深度和姿态，使其朝向一个固定点。我们的方法通过局部匹配成本计算深度细化，然后使用直接特征度量对齐来细化姿态更新，从而实现几何一致的深度和姿态更新。与其他方法相比，我们的方法不需要构建完整的成本体积，只需要基于局部成本体积进行更新，使其更简单、更节省内存、更健壮。 
- (4):在KITTI数据集上的实验结果表明，我们的模型在深度预测和视觉里程表预测方面具有竞争力，超过了已发表的自监督基线。
#### 7. 方法详细介绍：
本文提出了一种自监督深度和姿态估计方法，称为DualRefine。该方法由两个主要子模块组成：单帧自监督深度和姿态估计器和多帧网络。自监督深度估计管道假定单目相机捕获场景的图像序列，并且并行训练两个网络以估计图像的每个像素深度图和相邻图像帧之间的相对姿态。多帧网络被制定为深度均衡模型，该模型将隐藏状态、深度和姿态估计更新到固定点。更新函数交替细化深度和姿态，基于极线几何计算当前预测周围的候选对应关系的匹配成本。深度更新使用Conv-GRU块执行，姿态更新基于直接特征对齐。

#### 8. 实验设置：
本文使用KITTI数据集进行深度估计和视觉里程计实验。深度评估指标包括绝对和平方相对误差、均方根误差和阈值下的准确性。视觉里程计评估指标包括平移和旋转误差和绝对轨迹误差。本文使用PyTorch进行实现，并对输入图像进行颜色和翻转增强。网络使用学习率为10^-3进行15个epoch的训练，然后继续使用学习率为10^-4进行训练。采用β1 = 0.9和β2 = 0.999的Adam优化器。

#### 9. 实验结果与分析：
本文将提出的方法与几种最先进的模型在KITTI Eigen split数据集上进行比较。结果表明，该方法在深度估计精度方面优于大多数现有方法。该方法在视觉里程计实验中也取得了有竞争力的结果。本文还在高分辨率图像上进行了实验，并显示该方法可以处理高分辨率输入并提高准确性。


# Paper:521     SGLoc：用于室外LiDAR定位的场景几何编码



#### 1. Title: 
SGLoc: Scene Geometry Encoding for Outdoor LiDAR Localization

#### 2. Authors: 
Wen Li, Shangshu Yu, Cheng Wang, Guosheng Hu, Siqi Shen, Chenglu Wen

#### 3. Affiliation: 
第一作者：福建省智能城市感知与计算重点实验室，厦门大学，中国

#### 4. Keywords: 
LiDAR localization, absolute pose regression, scene geometry, point cloud correspondence, pose quality evaluation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_SGLoc_Scene_Geometry_Encoding_for_Outdoor_LiDAR_Localization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是基于LiDAR的绝对位姿回归问题，即通过深度网络在端到端的方式下，估计全局位姿，实现基于学习的定位。 
- (2):过去的方法通常使用地图匹配或绝对位姿回归来解决定位问题，但是地图匹配需要昂贵的3D地图存储和通信，而绝对位姿回归方法通常无法有效地编码场景几何信息，导致定位精度有待提高。本文提出了一种新的LiDAR定位框架SGLoc，将位姿估计分解为点云对应回归和通过此对应进行位姿估计两个步骤，从而有效地编码场景几何信息，提高定位精度。此外，本文还提出了一种姿态质量评估和增强方法，以测量和纠正定位数据中的姿态误差。 
- (3):本文提出的SGLoc框架将位姿估计分解为点云对应回归和通过此对应进行位姿估计两个步骤，从而有效地编码场景几何信息，提高定位精度。此外，本文还设计了三尺度空间特征聚合模块和几何一致性约束损失，以进一步提高场景几何的编码。此外，本文还提出了一种姿态质量评估和增强方法，以测量和纠正定位数据中的姿态误差。 
- (4):本文在Oxford Radar RobotCar和NCLT数据集上进行了广泛的实验，结果表明SGLoc方法在定位精度上优于现有的基于回归的定位方法，分别提高了68.5％和67.6％。本文提出的方法在定位任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为SGLoc的方法，将LiDAR定位分解为点云对应回归和6-DoF姿态估计两个步骤。SGLoc采用了基于稀疏卷积的FCN，包含特征提取器、Tri-Scale空间特征聚合模块和回归器。在训练过程中，SGLoc只需要输入点云和姿态，通过最小化对应距离来有效地编码场景几何信息，无需任何额外计算。为了有效地捕捉场景几何信息，本文提出了TSFA模块和IGCC损失函数。训练过程中的损失函数是LL1和LIGCC的组合。此外，本文还提出了PQEE方法来测量和纠正定位数据中的姿态误差，以提高数据质量和定位精度。

#### 8. 实验设置：
本文在两个大规模基准数据集Oxford Radar RobotCar和NCLT上进行了实验。详细说明了训练和测试数据。

#### 9. 实验结果与分析：
SGLoc方法在NCLT数据集上的平均误差为1.83m/3.54◦，显著优于现有方法。SGLoc的预测轨迹比竞争对手更接近真实轨迹且更平滑。在18-14-14-42个质量增强的Oxford数据集中，SGLoc实现了亚米级的精度。在NCLT数据集的所有场景中（除2012-05-26外），SGLoc的平均误差为0.84m/2.25◦，是第一个将误差降至亚米级的回归方法。位置误差的累积分布表明了SGLoc的良好性能。消融实验结果表明，所提出的模块PQEE、IGCC和TSFA显著提高了SGLoc的精度。SGLoc的运行时间与其他方法相当，并实现了实时定位。

#### 总结：
本文提出了一种名为SGLoc的LiDAR定位方法，将定位分解为点云对应回归和6-DoF姿态估计两个步骤。SGLoc通过最小化对应距离来有效地编码场景几何信息，采用TSFA模块和IGCC损失函数来提高场景几何信息的编码效果。此外，本文还提出了PQEE方法来测量和纠正定位数据中的姿态误差，以提高数据质量和定位精度。实验结果表明，SGLoc方法在大规模室外场景中具有良好的定位精度和实时性能。


# Paper:522     来自异步输入的3D视频循环



#### 1. Title: 
3D Video Loops from Asynchronous Input

#### 2. Authors: 
Li Ma, Xiaoyu Li, Jing Liao, Pedro V. Sander

#### 3. Affiliation: 
Li Ma, Pedro V. Sander: 香港科技大学
Xiaoyu Li: 腾讯AI实验室
Jing Liao: 香港城市大学

#### 4. Keywords: 
3D video loops, asynchronous input, multi-tile videos, view consistency, temporal retargeting

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ma_3D_Video_Loops_From_Asynchronous_Input_CVPR_2021_paper.html  Github: https://limacv.github.io/VideoLoop3D_web/

#### 6. Summary : 
- (1):本文研究的是从异步输入中生成3D视频循环的问题。循环视频是一种可以无限循环播放的短视频，但现有的方法大多限于2D表示。本文提出了一种实用的解决方案，可以在动态3D循环场景中实现沉浸式体验。
 
- (2):过去的方法主要是针对2D视频的循环，而本文提出的方法可以将循环扩展到3D表示。现有的3D循环视频方法存在一些限制，如精度不高、存储空间大等问题。本文提出了一种新的3D视频表示方法，即Multi-tile Videos (MTVs)，它不仅提供了视图一致性的先验，而且大大减少了存储空间，使得4D体积的优化变得可行。本文提出了一个两阶段的流程来构建3D循环MTV，其中采用了基于视频时间重定向算法的循环损失。 

- (3):本文提出了一种分析合成方法，通过优化多视角循环目标来训练视图一致的3D视频表示。本文提出了一种基于Multi-plane Images (MPIs)的高效3D视频表示方法，即MTVs，通过利用3D场景的空间和时间稀疏性，将静态或动态纹理瓦片稀疏地分布在视锥体中。这大大减少了渲染时的内存需求，使得3D循环视频的优化成为可能。MTVs的稀疏性也作为视图一致性的先验，在优化3D循环视频时起到了作用。本文提出了一个两阶段的流程来生成循环MTV，实验表明，我们的方法可以在移动设备上实时生成和渲染逼真的3D视频循环，保持与输入相似的动态性。

- (4):本文的方法可以从异步多视角视频中生成3D视频循环，实现了视图和时间的控制。实验结果表明，我们的方法可以在移动设备上实时渲染逼真的3D视频循环，达到了预期的性能。
#### 7. 方法详细介绍：
本文提出了一种从异步多视角2D视频生成3D循环视频的两阶段流水线方法。第一阶段，作者使用双视图立体算法构建了场景的多平面图像（MPI）表示。然后，他们使用稀疏的多平面时变（MTV）表示来捕捉场景的时间动态。在第二阶段，作者使用基于补丁的方法从MTV表示中生成一组连贯且完整的3D补丁。他们使用补丁最近邻（PNN）算法来最小化从渲染视频和填充视频中提取的补丁集之间的双向相似度（BDS）。最后，他们使用连贯的补丁集来合成循环视频。作者还采用了金字塔训练方案来改善生成结果。

#### 8. 实验设置：
作者捕捉了16个场景进行定量和定性研究。对于每个场景，他们使用Sony α9 II相机以面向前方的方式捕捉了8-10个视角。他们以25 fps的速度捕捉每个视角10-20秒，并将每个视频降采样到640×360的分辨率。他们随机选择一个视角进行评估，并使用其他视角构建MTV，使用两阶段流水线。他们将每个阶段的渲染窗口设置为h = 180，w = 320。

#### 9. 实验结果与分析：
作者在捕捉的数据集上将其方法与四个基线进行了比较。他们合成了新的视角视频，并报告了VLPIPS、STDerr、Com.、Coh.和LoopQ指标。他们的方法在视觉质量、场景动态保留、时空一致性和循环质量方面优于其他基线。他们还使用几个指标测量了场景表示的效率，并表明他们的方法在# Params和Render Spd方面超过了VBR。


# Paper:523     神经核表面重建



#### 1. Title: 
Neural Kernel Surface Reconstruction

#### 2. Authors: 
Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, Francis Williams

#### 3. Affiliation: 
NVIDIA（英伟达）

#### 4. Keywords: 
3D reconstruction, point cloud, implicit surface, neural kernel fields, scalability, robustness

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究的是从大规模、稀疏、嘈杂的点云中重建3D隐式表面的方法。
- (2):过去的方法存在一些问题，如在高噪声或稀疏输入情况下重建质量较差，而基于学习的方法则往往难以推广到分布不同的形状和采样密度。本文提出了一种新的方法，建立在最近引入的神经核场（NKF）表示的基础上，具有与NKF类似的推广能力，同时解决了其主要限制。本文的方法具有可扩展性、鲁棒性和高度泛化能力，能够处理数百万个点的点云，并在多个基准测试中取得了最先进的结果。
- (3):本文的方法建立在神经核场的基础上，但是通过使用一种新的、基于梯度的核形式，使其对噪声更加鲁棒，并使用显式的体素层次结构和紧支持核来使插值问题变得稀疏、多尺度，并能够处理大型输入，同时仍然产生高保真度的输出。本文的方法具有以下创新点：（1）使用梯度优化的核形式，使其对噪声更加鲁棒；（2）使用显式的体素层次结构和紧支持核，使其具有可扩展性和高效性；（3）使用基于元学习的线性求解器，使其能够快速重建大型场景；（4）使用多尺度插值，使其能够处理不同采样密度的输入。
- (4):本文的方法在单个对象（ShapeNet、ABC）、室内场景（ScanNet、Matterport3D）和室外场景（CARLA、Waymo）的重建基准测试中均取得了最先进的结果。本文的方法具有高度泛化能力，能够处理大规模、稀疏、嘈杂的点云，并在多个基准测试中取得了最先进的结果，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为神经核表面重建（Neural Kernel Surface Reconstruction，NKSR）的方法，用于从稀疏的、嘈杂的点云中重建三维隐式表面。该方法基于最近引入的神经核场（Neural Kernel Fields，NKF）表示，并使用了一个学习的核进行泛化。然而，NKSR通过使用一种新颖的、基于梯度的核形式，该形式对噪声具有鲁棒性，并使用具有紧支撑核的显式体素层次结构使插值问题变得稀疏、多尺度且能够处理大型输入，同时仍能产生高保真度的输出。该方法的流程包括预测包含每个体素中的特征和法线的体素网格的稀疏层次结构，构建稀疏线性系统，求解一组每个体素系数，并使用双重Marching Cubes提取预测表面。该方法具有高度的泛化性，可以在多样化的数据集上进行训练，而只需要密集的定向点作为监督。它可以在几秒钟内重建由数百万点组成的点云，并以外部核心方式扩展到极大的输入。

#### 8. 实验设置：
本文使用单个NVIDIA V100 GPU进行了多个重建基准测试，包括单个对象（ShapeNet、ABC）、室内场景（ScanNet、Matterport3D）和室外场景（CARLA、Waymo）。该方法在多个数据集上进行了训练，包括ShapeNet、ScanNet和Matterport3D，并使用Chamfer距离和F-score等标准指标进行评估。

#### 9. 实验结果与分析：
本文提出的NKSR算法在多个数据集上进行了实验，包括ScanNet、Matterport3D和CARLA等。实验结果表明，NKSR在准确性、完整性和对未见数据的泛化能力方面优于其他方法。与其他表面重建方法相比，NKSR具有更高的重建精度和更好的鲁棒性。此外，本文还进行了消融研究，分析了不同特征维度和体素大小对NKSR性能的影响。


# Paper:524     模型无关的性别去偏见图像描述



#### 1. Title: 
Model-Agnostic Gender Debiased Image Captioning

#### 2. Authors: 
Yusuke Hirota, Yuta Nakashima, Noa Garcia

#### 3. Affiliation: 
大阪大学

#### 4. Keywords: 
Image captioning, gender bias, bias mitigation, debiasing, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hirota_Model-Agnostic_Gender_Debiased_Image_Captioning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉中的社会偏见问题，特别是在图像描述任务中，由于训练集中的性别偏见，模型会产生和放大性别刻板印象。
- (2):过去的方法主要是通过强制模型关注人物来减少性别误分类，但这会导致模型产生性别刻板印象。本文提出了一种新的框架LIBRA，通过从合成的有偏样本中学习来减少两种类型的偏见，从而减少性别误分类和更改性别刻板印象。
- (3):本文提出的方法是LIBRA，包括两个主要模块：有偏标题合成（BCS）和去偏标题生成器（DCG）。BCS通过合成性别偏见标题来减少两种类型的偏见。DCG通过训练来恢复原始标题，从而减少性别偏见。本文的创新点在于提出了一种新的框架，可以减少两种类型的偏见，而且不需要重新训练图像描述模型。
- (4):本文在多个指标上对各种图像描述模型进行了实验和分析，结果表明LIBRA可以减少大多数图像描述模型中的两种类型的性别偏见。本文的方法可以减少由于图像/单词的上下文偏向某种性别而导致的性别误分类，减少上下文→性别偏见。此外，它倾向于将偏向每种性别的单词更改为不那么偏见的单词，从而减少性别→上下文偏见。本文的方法可以产生比原始标题更少的性别刻板印象的标题。
#### 7. 方法详细介绍：
本文提出了一个名为LIBRA的模型无关框架，用于减轻图像字幕生成模型中的性别偏见。该框架由两个主要模块组成：偏见字幕合成（BCS）和去偏生成器（DCG）。BCS使用句子分类器和T5语言模型分别合成具有上下文→性别偏见和性别→上下文偏见的偏见字幕。然后，DCG使用合成的偏见字幕训练以生成去偏字幕。训练完成后，DCG可以在任何图像字幕生成模型之上使用，以减轻性别偏见扩大的影响，输入为图像和生成的字幕。具体步骤如下：
1. 对于BCS，使用MSCOCO数据集中的训练集合成偏见字幕。
2. 对于DCG，使用Vilt作为视觉-语言编码器，使用GPT-2作为解码器，训练以生成去偏字幕。
3. 对于任何图像字幕生成模型，使用DCG作为去偏生成器，输入图像和生成的字幕，输出去偏字幕。

#### 8. 实验设置：
本文使用MSCOCO数据集进行训练和评估。训练集包含82,783张图像，验证集包含10,780张图像，并且具有二元性别注释。本文使用训练集合成偏见字幕，并在验证集上评估去偏生成器的性能。

#### 9. 实验结果和分析：
本文使用BLEU、METEOR和LIC三个指标评估了所提出的框架的性能。结果表明，该框架可以有效地减轻图像字幕生成模型中的性别偏见，同时保持生成字幕的质量。本文还对框架在验证集的不同子集上的性能进行了详细分析。同时，本文还将LIBRA与性别均衡器进行了比较，证明了LIBRA的有效性。


# Paper:525     K-Planes: 显式空间、时间和外观中的辐射场



#### 1. Title: 
K-Planes: Explicit Radiance Fields in Space, Time, and Appearance

#### 2. Authors: 
Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa

#### 3. Affiliation: 
Sara Fridovich-Keil: 加州大学伯克利分校
Giacomo Meanti: 意大利技术研究所
Frederik Rahbæk Warburg: 丹麦技术大学
Benjamin Recht: 加州大学伯克利分校
Angjoo Kanazawa: 加州大学伯克利分校

#### 4. Keywords: 
Radiance fields, volumetric rendering, k-planes, explicit model, dynamic scenes

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2021_paper.html  Github: https://github.com/sarafridov/K-Planes

#### 6. Summary : 
- (1):本文研究的背景是动态辐射场的表示方法，由于直接存储4D体积的成本过高，因此需要一种更加高效的表示方法。
 
- (2):过去的方法包括完全隐式模型和几何表示法，但是它们都存在一些问题，如训练和渲染速度慢、模型大小过大等。本文提出了一种新的表示方法k-planes，它是一种白盒模型，具有可解释性和紧凑性，并且能够表示静态场景、动态场景和外观变化的场景。与以往的黑盒模型相比，k-planes模型具有更快的训练和渲染速度，并且不需要使用任何自定义CUDA内核。

- (3):本文提出了一种新的表示方法k-planes，它使用k个平面来表示d维场景，其中k = d(d-1)/2，可以自然地扩展到任意维度的空间，并且在优化时间和模型大小方面具有良好的可扩展性。我们使用一个线性特征解码器和一个学习的颜色基础来解码特征，这使得我们的模型能够适应不同的外观变化。我们的模型在多个合成和真实场景上进行了测试，包括静态体积、具有不同外观的3D照片集合和4D动态视频，取得了竞争性和通常是最先进的重建保真度，并且具有低内存使用率和快速优化。

- (4):本文的方法在多个场景和任务中取得了竞争性的性能，包括静态场景、具有变化外观的场景和动态场景。我们的模型具有低内存使用率和快速训练和推理时间，并且是第一个能够在任意维度中表示辐射场的白盒模型。
#### 7. 方法详细介绍：
本文提出了一种名为k-planes的模型，用于在任意维度中表示场景的辐射场。该模型使用平面分解来表示d维场景，提供了从静态（d = 3）到动态（d = 4）场景的无缝转换方式。该分解使添加特定于维度的先验变得容易，例如时间平滑和多分辨率空间结构，并引入了场景的静态和动态组件的自然分解。模型使用一个线性特征解码器，具有学习的颜色基础，其性能与非线性黑盒MLP解码器相似。k-planes模型在各种任务中实现了竞争性能，包括重建质量、模型大小和优化时间，而无需任何自定义CUDA内核。

#### 8. 实验设置：
N/A

#### 9. 实验结果和分析：
本文介绍了一种名为k-planes的方法，将d维空间分解为可以直接从间接测量中优化的平面。所提出的k-planes分解自然适用于静态3D场景以及动态4D视频的重建，通过添加每个训练图像的M维向量，还可以扩展到在不同照明或外观条件下观察的具有一致静态几何形状的场景的重建。实验结果表明，k-planes方法在各种任务中表现出竞争性能，实现了与先前最先进方法类似的质量指标。本文还展示了与先前工作的视觉比较，包括最近的混合方法MixVoxels，并展示了在外观代码空间中插值以改变地标的视觉外观的能力。


# Paper:526     Fantastic Breaks：一组真实世界破损物体及其完整对应物的配对3D扫描数据集



#### 1. Title: 
Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts

#### 2. Authors: 
Nikolas Lamb, Cameron Palmer, Benjamin Molloy, Sean Banerjee, Natasha Kholgade Banerjee

#### 3. Affiliation: 
Clarkson University, Potsdam NY, USA（美国克拉克森大学）

#### 4. Keywords: 
3D scanning, shape repair, real-world damage, dataset, fracture analysis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lamb_Fantastic_Breaks_A_Dataset_of_Paired_3D_Scans_of_Real-World_CVPR_2021_paper.html  Github: https://github.com/Terascale-All-Sensing-Research-Studio/FantasticBreaks

#### 6. Summary : 
- (1):本文的研究背景是自动化形状修复方法目前缺乏描述真实世界受损几何的数据集。
- (2):过去的方法主要是手动修复，或者使用几何和基于物理的方法生成合成断裂数据集。这些方法存在一些问题，如无法处理未知完整几何的对象，或者无法提供真实世界受损几何的真实表示。本文提出了一个真实世界受损对象的3D扫描数据集，与完整对象的3D扫描配对，以便于应用重点评估修复方法。本文提供了类别、材料和断裂表面注释，以及地面真实修复部件代理。本文的方法是有动机的，因为缺乏真实世界受损对象数据集限制了对真实世界损伤的理解和应用重点评估修复方法。
- (3):本文提出了Fantastic Breaks数据集，其中包含150个受损/完整对象对，每个受损对象都经过3D扫描并与完整对象的扫描几何注册，以便于对齐。本文使用一个现成的减法方法来从与完整对象对齐的完整受损部分生成修复部件代理。本文的方法是创新的，因为它提供了真实世界受损对象的真实表示，可以用于评估现有的形状修复方法。本文的贡献是提供了第一个3D扫描的真实世界数据集，以便于应用重点评估修复方法。
- (4):本文的方法在真实世界受损对象上进行了实验形状修复评估，使用了多种基于学习的方法，这些方法是使用合成数据集预先训练的，并使用Fantastic Breaks的子集进行重新训练。本文的方法在形状修复任务上取得了良好的性能，支持了他们的目标。
#### 7. 方法详细介绍：
本文提出了一个名为Fantastic Breaks的数据集，包含了150个真实世界中的物体的3D扫描，每个物体都有一个完整的对应物体。数据集还包括类别、材料和断口表面注释，以及地面真实修复部件代理。作者使用现成的基于减法的方法从完整的破碎部分生成修复部件代理。作者使用该数据集评估了现有的形状修复方法在真实破碎物体上的表现。

#### 8. 实验设置：
作者从当地的二手商店购买了一些日常家居物品，并通过捐赠获得了一些完整的物品。他们手动损坏完整的物品以获得破碎版本，并使用Einscan SP 3D转台式扫描仪进行扫描。作者对每个物体进行了精心的分组，并多次展示物体以最大程度地获取物体表面。他们使用EXScan中的注册工具将扫描融合成3D网格，并视觉检查每个模型以确保高质量。作者手动定位每个网格，使其主轴与笛卡尔坐标轴对齐，并使用迭代最近点（ICP）算法将破碎网格与完整网格对齐。作者提供了非标准化和标准化网格作为数据集的一部分。作者还为修复部件提供了代理的真实3D网格，并手动注释了对应于断口表面的三角形。在发表时，他们已经获得了214个和241个物理损坏和完整物品，以及195个和218个损坏和完整物品扫描。

#### 9. 实验结果和分析：
本文提出的Fantastic Breaks数据集包含了真实世界中的物体的3D扫描和完整的对应物体，以及手动注释的类别、材料和断口表面，以及合成的代表修复部件的3D网格。作者在该数据集上评估了现有的学习驱动的方法对于自动重建新的修复部件的表现，并报告了21次推理运行和7次重新训练的定量指标。结果表明，在Fantastic Breaks数据集上重新训练可以改善某些物体的修复效果，产生更全面的修复。然而，重新训练可能不利于某些物体的学习，并且在Breaking Bad上训练的模型比在Geometric Breaks上训练的模型更难以推广到真实破碎物体。作者在Fantastic Breaks数据集上测试了三种先前的形状修复方法：MendNet、DeepMend和DeepJoin。他们在Geometric Breaks或Breaking Bad数据集上预训练了每个网络，然后在Fantastic Breaks数据集的物体上额外训练了1,000个时代。他们使用了105个训练和45个测试物体。作者通过对每个网格表面上的点进行采样并计算每个点的符号距离函数、占用率和法向场值来生成训练数据。他们还通过将薄板样条拟合到断口顶点来计算代表断口的断口表面。


# Paper:527     基于热像的深度估计



#### 1. Title: 
Deep Depth Estimation from Thermal Image

#### 2. Authors: 
Ukcheol Shin, Jinsun Park, In So Kweon

#### 3. Affiliation: 
Ukcheol Shin: KAIST (韩国科学技术院)
Jinsun Park: Pusan National University (釜山国立大学)
In So Kweon: KAIST (韩国科学技术院)

#### 4. Keywords: 
Thermal Image, Depth Estimation, Multi-Spectral Stereo Dataset, Autonomous Driving, Computer Vision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2022_paper.html  Github: https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset

#### 6. Summary : 
- (1):本文的研究背景是自动驾驶汽车的高级自主性需要强大的几何理解能力，而可见光谱带的自动驾驶算法很容易受到天气和光照条件的影响。
- (2):过去的方法主要依赖于可见光谱图像，容易受到天气和光照条件的影响。因此，本文提出了一种基于长波红外相机的深度估计方法，以实现高级鲁棒性。本文提出了一个大规模的多光谱立体数据集，包括立体RGB、立体NIR、立体热像和立体LiDAR数据以及GNSS/IMU信息。本文提出了一种统一的深度网络，从条件随机场的角度有效地桥接了单眼深度和立体深度任务。 
- (3):本文提出了一种基于长波红外相机的深度估计方法，以实现高级鲁棒性。本文提出了一个大规模的多光谱立体数据集，包括立体RGB、立体NIR、立体热像和立体LiDAR数据以及GNSS/IMU信息。本文提出了一种统一的深度网络，从条件随机场的角度有效地桥接了单眼深度和立体深度任务。 
- (4):本文的方法在热像领域的深度估计任务上取得了高可靠性和鲁棒性，可以在白天、低光和雨天等不同环境下进行深度估计。
#### 7. 方法详细介绍：
本文提出了一种基于条件随机场（CRF）的方法，用于将单目深度估计和立体深度估计任务相结合。该方法利用神经窗口FC-CRF（NeWCRF）网络连接两个任务，并从给定的单目或立体图像估计深度图。该方法使用Swin Transformer作为骨干网络，在四个尺度级别上提取特征，并使用金字塔池化模块（PPM）聚合全局上下文信息。该方法构建了一个4D成本体积，通过3D卷积层实现更高的立体匹配性能。该方法还提出了一种统一的深度网络，可以通过统一的网络架构估计单目和立体深度图。该网络由两个分支组成，一个用于单目深度估计，另一个用于立体深度估计。两个分支共享相同的编码器，并通过条件随机场模块融合以生成最终的深度图。该网络在MS2数据集上使用监督和无监督学习的组合进行训练。

#### 8. 实验设置：
本文介绍了MS2数据集的设计，该数据集包括立体RGB、立体NIR、立体热像和立体LiDAR数据以及GNSS / IMU数据。该数据集提供了大约195K个来自城市、住宅、道路、校园和郊区地区的同步数据对，包括早晨、白天和夜晚在晴朗、多云和雨天的数据。本文还对可见光谱波段设计的单目和立体深度估计算法进行了详尽的验证过程，以评估它们在热像领域的泛化和领域差距处理能力。

#### 9. 实验结果和分析：
本文提出的方法在MS2数据集上表现出与最先进的MDE方法相当的结果。该方法在准确度指标上表现出更高的分数，但在某些误差指标上表现出较低的分数。MDE网络的性能趋势通常在热谱域中得到保留。具有深度图预测回归头的MDE网络在误差指标上具有明显的优势，因为它们可以直接回归精确的深度值。另一方面，分类头通过明确地分组深度范围来实现更高的准确度得分。该方法还提供了代表性MDE和SDE网络在MS2深度数据集上的全面比较。


# Paper:528     RefTeacher: 半监督指代理解的强基线



#### 1. Title: 
RefTeacher: 半监督指代理解的强基线

#### 2. Authors: 
Jiamu Sun, Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, Rongrong Ji

#### 3. Affiliation: 
厦门大学多媒体可信感知与高效计算实验室

#### 4. Keywords: 
半监督学习，指代理解，伪标签，注意力机制，自适应伪标签加权

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2021_paper.html  Github: https://refteacher.github.io/

#### 6. Summary : 
- (1):本文研究的是指代理解（REC）任务，该任务需要大量的实例级别注释，而这些注释是费力和昂贵的。因此，本文提出了一种半监督学习方法，名为RefTeacher，旨在解决REC任务中的数据稀疏性和伪标签噪声问题。

- (2):过去的方法主要是基于完全监督学习，需要大量的标注数据，而半监督学习方法可以利用大量未标注数据，但是直接将半监督学习方法应用于REC任务仍然存在两个主要挑战，即极度稀疏的监督信号和较差的伪标签噪声。本文提出了一种基于教师-学生学习范式的强基线方法，其中教师REC网络预测伪标签以优化学生网络，同时引入了注意力机制和自适应伪标签加权来解决这两个挑战。

- (3):本文提出的RefTeacher方法采用教师-学生框架，其中教师网络根据给定的表达式预测未标注数据的伪边界框，学生网络则利用少量标注数据和伪标签进行训练。为了丰富监督信号，本文提出了基于注意力机制的模仿学习和自适应伪标签加权两种新颖的设计。实验结果表明，RefTeacher方法在三个REC基准数据集上均取得了显著的性能提升。

- (4):本文的方法在三个REC基准数据集上进行了实验，结果表明，RefTeacher方法可以大大超过完全监督基线，例如在10％的RefCOCO上获得了+18.8％的增益。更重要的是，仅使用10％的标注数据，RefTeacher可以帮助RealGIN实现接近100％的完全监督性能。
#### 7. 方法详细介绍：
RefTeacher是一种半监督学习方法，用于指代表达理解（REC）。该方法采用了一种师生学习范式，其中教师网络预测伪标签以优化学生网络，使模型能够基于少量标记数据利用大量未标记数据。RefTeacher还配备了两种新颖的设计，称为基于注意力的模仿学习（AIL）和自适应伪标签加权（APW），以应对稀疏监督信号和更差的伪标签噪声的挑战。AIL有助于学生网络模仿教师的识别行为，而APW则自适应地调整具有不同质量的伪标签的贡献。RefTeacher应用于两种代表性的REC模型，RealGIN和TransVG，并在三个基准数据集RefCOCO，RefCOCO +和RefCOCOg上进行评估。

#### 8. 实验设置：
本文在三个基准数据集RefCOCO，RefCOCO +和RefCOCOg上评估了RefTeacher的性能。实验使用了两种代表性的REC模型，RealGIN和TransVG。实验在一台NVIDIA Tesla V100 GPU上进行，具有32GB内存。批量大小设置为16，学习率初始化为0.001。如果验证性能在5个时期内没有改善，则模型将训练30个时期并进行早期停止。

#### 9. 实验结果和分析：
RefTeacher在RefCOCO，RefCOCO +和RefCOCOg数据集上相对于完全监督方法取得了显着的性能提升，其中在10％的RefCOCO上获得了18.8％的增益。仅使用10％的标记数据，RefTeacher可以帮助RealGIN实现接近100％的完全监督性能。所提出的AIL和APW设计在应对稀疏监督信号和更差的伪标签噪声的挑战方面是有效的。


# Paper:529     通过路径增强方法提高对抗样本的可迁移性



#### 1. Title: 
Improving the Transferability of Adversarial Samples by Path-Augmented Method

#### 2. Authors: 
Jianping Zhang, Jen-tse Huang, Wenxuan Wang, Yichen Li, Weibin Wu, Xiaosen Wang, Yuxin Su, Michael R. Lyu

#### 3. Affiliation: 
第一作者：香港中文大学计算机科学与工程系

#### 4. Keywords: 
Adversarial samples, transfer-based attacks, data augmentation, semantics predictor, deep neural networks

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhang_Improving_the_Transferability_of_Adversarial_Samples_by_Path-Augmented_Method_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究深度神经网络中的对抗样本问题，提出了一种基于路径增强的方法来提高对抗样本的可迁移性。
- (2):现有的对抗样本攻击方法大多基于数据增强，但是这些方法往往只考虑从目标图像到基线图像的线性路径，且可能会增强与目标图像不一致的图像。本文提出了一种路径增强的方法，通过构建候选增强路径池，使用贪心搜索确定增强路径，并训练语义预测器来约束增强路径的长度，从而提高对抗样本的可迁移性。
- (3):本文提出的路径增强方法可以显著提高对抗样本攻击的成功率，与现有方法相比，平均提高了4.8%以上。同时，本文还提出了一种语义预测器，可以约束增强路径的长度，从而避免增强与目标图像不一致的图像。
- (4):本文在ImageNet数据集上进行了广泛的实验，结果表明，本文提出的方法可以在攻击成功率方面平均提高3.7%以上，当与其他兼容的攻击方法相结合时，可以显著超过现有方法，平均提高7.2%以上。
#### 7. 方法详细介绍：
本文提出了一种路径增强方法（Path-Augmented Method，PAM）来生成可转移的对抗样本。该方法首先构建候选增强路径池，然后通过贪心搜索确定用于生成对抗样本的增强路径。为了避免增强语义不一致的图像，本文还训练了一个语义预测器（Semantics Predictor，SP）来约束每个增强路径的长度。在生成对抗样本时，PAM从多个增强路径中增强图像，使增强后的图像保留目标图像的语义含义。具体而言，PAM方法通过增加多个增强路径的梯度来探索其他增强方向，攻击方程式中使用了每个增强路径的基线图像和语义比率来生成对抗样本。本文将PAM与现有的SIM和Admix方法进行了比较，结果表明PAM方法具有更高的攻击成功率和更强的可转移性。

#### 8. 实验设置：
本文在ImageNet数据集上进行实验，使用了来自ILSVRC 2012验证集的1000张不同类别的图像。选择了四个具有不同架构的顶级模型作为目标模型，包括Inception-v3（Inc-v3）、Inception-v4（Inc-v4）、Inception-Resnet-v2（IncRes-v2）和Resnet-v2-101（Res-v2）。同时考虑了三个对抗训练模型和六个高级防御模型。使用MI-FGSM作为基线攻击方法，选择方差调整方法（Variance Tuning Method，VMI）作为当前最先进的基于转移的攻击方法。将最大扰动预算设置为16，攻击迭代次数设置为10，步长设置为1.6。

#### 9. 实验结果与分析：
本文提出的PAM方法在白盒情况下几乎可以达到100%的攻击成功率，并在黑盒情况下显著优于VMI约10%和Admix约4.8%。PAM方法提高了对抗训练模型的可转移性，对对抗训练构成了很大的威胁。在黑盒情况下，PAM方法始终比其他基线方法表现出更高的攻击成功率，证实了所提出的策略在生成可转移的对抗样本方面的优越性。


# Paper:530     MobileNeRF：利用多边形光栅化管道在移动架构上高效渲染神经场



#### 1. Title: 
MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures

#### 2. Authors: 
Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi

#### 3. Affiliation: 
Zhiqin Chen: Simon Fraser University and University of Toronto
Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi: Google Research

#### 4. Keywords: 
Neural Radiance Fields, novel view synthesis, polygon rasterization, mobile architectures

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_CVPR_2021_paper.html  Github: https://github.com/google-research/mobile-nerf

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）在新视角合成方面的优异表现，但是现有的体积渲染算法不适用于广泛部署的图形硬件，因此需要一种新的表示方法。
- (2):过去的方法包括使用基于射线行进的体积渲染算法，但是这种方法速度过慢，无法实现交互式可视化。最近的工作通过将NeRF“烘焙”到稀疏的3D体素网格中来解决这个问题，但是仍然需要通过稀疏体素网格进行射线行进来生成每个像素的特征，因此无法充分利用通用图形处理单元（GPU）中可用的并行性。本文提出了一种新的NeRF表示方法，基于纹理多边形，可以使用标准渲染管道高效地合成新图像。
- (3):本文提出的方法是将NeRF表示为一组纹理多边形，其中多边形大致遵循场景表面，纹理图集存储不透明度和特征向量。使用传统的多边形光栅化管道和Z缓冲区渲染多边形，可以为每个像素生成特征向量，然后通过在片段着色器中运行的小型、视角相关的MLP解释每个像素的特征向量，从而生成最终的像素颜色。这种方法利用了现代图形硬件中提供的Z缓冲区和片段着色器的并行性，因此在标准测试场景下，与SNeRG相比，输出质量相同的情况下，速度提高了10倍。此外，它只需要标准的多边形渲染管道，这在几乎所有计算平台上都得到了实现和加速，因此可以在以前无法支持NeRF可视化的移动电话等设备上运行。
- (4):本文的方法在移动设备上实现了实时渲染，可以在标准测试场景下实现与SNeRG相同的输出质量，但速度提高了10倍。这种方法可以在移动设备上运行，因为它只需要标准的多边形渲染管道，这在几乎所有计算平台上都得到了实现和加速。
#### 7. 方法详细介绍：
MobileNeRF是一种新的NeRF表示方法，基于纹理多边形，可以使用标准渲染管道高效地合成新图像。该方法将NeRF表示为一组带有特征向量和二进制不透明度的多边形纹理。传统的多边形渲染可以产生带有每个像素特征的图像，这些特征由一个小的、视角相关的MLP在片段着色器中解释，以产生最终的像素颜色。该方法使得NeRF可以使用传统的多边形光栅化管道进行渲染，从而提供了大规模的像素级并行性，在各种计算平台上实现了交互式帧率，包括移动电话。该表示分为三个训练阶段，逐步从类似于传统NeRF的连续表示向离散表示转移。

具体步骤如下：
(1) 连续训练阶段：训练一个类似于NeRF的模型，其中包含连续的不透明度，体积渲染积分点从多边形网格中导出。
(2) 二值化训练阶段：将不透明度二值化以处理半透明片段。
(3) 离散化阶段：将表示转换为显式的多边形网格，并存储神经延迟着色器的权重。

#### 8. 实验设置：
本文在三个数据集上测试了MobileNeRF方法：NeRF的8个合成360度场景、LLFF的8个前向场景和Mip-NeRF 360的5个无界360度户外场景。渲染分辨率与训练图像相同：合成为800×800，前向为1008×756，无界为1256×828。本文将MobileNeRF与SNeRG进行了比较，并进行了广泛的消融研究，以研究不同设计选择对渲染质量的影响。实验在各种设备上运行，包括手机、平板电脑、笔记本电脑和台式机，具有不同的硬件规格。

#### 9. 实验结果与分析：
本文展示了MobileNeRF方法在每个阶段的渲染质量，并报告了消融研究。每个阶段的性能逐渐下降，因为每个阶段都向模型添加了更多的约束。本文还展示了使用更大或更小的纹理大小、删除超采样步骤或仅执行光栅化而不使用小型MLP来预测视角相关颜色时的渲染速度和空间成本。超采样步骤和小型MLP对性能影响最大。本文还讨论了该方法的局限性，例如对于具有镜面表面和/或稀疏视图的场景，表面估计不正确，无法处理具有半透明度的场景，以及固定的网格和纹理分辨率可能对近距离新视图合成过于粗糙。本文还报告了MobileNeRF方法和SNeRG在各种设备上的渲染速度、GPU内存消耗和存储成本，并使用PSNR、SSIM和LPIPS指标比较了MobileNeRF方法与其他方法，包括NeRF、NeRF++和JAXNeRF。


# Paper:531     Detection Hub: 基于语言嵌入的目标检测数据集统一方法



#### 1. Title: 
Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding

#### 2. Authors: 
Lingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng Wang, Zuxuan Wu, Lu Yuan, Yu-Gang Jiang

#### 3. Affiliation: 
第一作者：上海复旦大学计算机科学学院，智能信息处理重点实验室，上海智能视觉计算协同创新中心

#### 4. Keywords: 
Object detection, multiple datasets, taxonomy difference, annotation inconsistency, language embedding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Meng_Detection_Hub_Unifying_Object_Detection_Datasets_via_Query_Adaptation_on_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是如何将多个目标检测数据集结合起来训练一个通用的目标检测器。
 
- (2):过去的方法主要是将不同数据集的标注数据合并，但由于数据集之间存在分类差异和领域差异，这种方法并不可行。本文提出了一种新的设计，称为Detection Hub，它是数据集感知和类别对齐的，不仅可以缓解数据集不一致性，还可以为检测器提供跨多个数据集学习的一致指导。本文的方法基于语言嵌入，通过学习数据集嵌入来适应目标查询和检测头中的卷积核。同时，通过将单热类别表示替换为单词嵌入，并利用语言嵌入的语义一致性，将不同数据集的类别在统一空间中进行语义对齐。本文的方法可以有效地提高目标检测的性能。
 
- (3):本文提出的方法是Detection Hub，它是一种基于语言嵌入的查询适应方法，可以将多个目标检测数据集结合起来训练一个通用的目标检测器。本文的方法通过学习数据集嵌入来适应目标查询和检测头中的卷积核，同时通过将单热类别表示替换为单词嵌入，并利用语言嵌入的语义一致性，将不同数据集的类别在统一空间中进行语义对齐。本文的方法可以有效地提高目标检测的性能。
 
- (4):本文的方法在COCO、Object365和Visual-Genome等数据集上进行了实验，结果表明，与单独训练每个数据集相比，联合训练多个数据集可以显著提高性能。在UODB基准测试中，Detection Hub进一步实现了SoTA性能。本文的方法可以有效地提高目标检测的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“Detection Hub”的方法，用于将多个目标检测数据集统一到一个通用的目标检测器中进行训练。该方法通过将不同数据集的语义类别名称映射到一个类别对齐的嵌入中，并根据不同数据集的分布动态调整对象查询，以解决分类差异和注释不一致的挑战。该方法还采用了区域到单词对齐损失，而不是传统的交叉熵损失，以对齐不同数据集的类别。该方法利用预训练语言编码器的语言属性，相对于单独训练每个数据集，实现了显著的性能提升。

#### 8. 实验设置：
本文在三个标准目标检测数据集上进行了实验：COCO、Object365和Visual-Genome。这些大规模数据集具有不同的分类法、词汇量和注释质量。本文还在UODB上进行了实验，该数据集是11个极端多样化数据集的组合。

#### 9. 实验结果和分析：
本文报告了所提出方法在每个数据集上的性能。Detection Hub在COCO、Object365和Visual-Genome上分别实现了45.3、23.2和5.7的AP，相对于每个独立模型，实现了显著的性能提升。Detection Hub在UODB上实现了平均分数71，相对于之前的SoTA UniversalDA，大幅提高了6.8个点。


# Paper:532     自监督学习用于多模态非刚性三维形状匹配



#### 1. Title: 
Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching

#### 2. Authors: 
Dongliang Cao, Florian Bernard

#### 3. Affiliation: 
Dongliang Cao: University of Bonn (波恩大学)

#### 4. Keywords: 
3D shape matching, self-supervised learning, multimodal, point clouds, functional maps

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cao_Self-Supervised_Learning_for_Multimodal_Non-Rigid_3D_Shape_Matching_CVPR_2021_paper.html  Github: https://github.com/dongliangcao/Self-Supervised-Multimodal-Shape-Matching

#### 6. Summary : 
- (1):本文研究的是三维形状匹配问题，旨在解决非刚性三维形状匹配的挑战性问题。
- (2):过去的方法主要针对三角网格进行匹配，而点云匹配的性能不如三角网格。本文提出了一种自监督的多模态学习策略，将基于三角网格的功能映射正则化与耦合网格和点云数据的对比损失相结合。该方法可以获得三角网格、完整点云和部分观测点云的内模态对应关系，以及这些数据模态之间的对应关系。本文方法在多个具有挑战性的基准数据集上实现了最先进的结果，甚至在与最近的监督方法的比较中也表现出色，并且具有先前未见的跨数据集泛化能力。
- (3):本文提出了一种自监督的深度特征学习框架，通过三角网格和点云的结构特性进行无监督的正则化，同时引入了三角网格和相应点云之间的自监督对比损失，使得两种模态都可以学习到一致的特征表示。该方法不需要在推理时计算点云的功能映射，而是基于特征相似性比较直接预测对应关系。本文方法是第一个结合了一组独特的期望属性的学习方法，即可以在没有地面真实对应关系注释的情况下进行训练，适用于三角网格和点云，对噪声具有鲁棒性，允许部分形状匹配，并且只需要少量的训练数据。
- (4):本文方法在多个具有挑战性的三维形状匹配基准数据集上实现了最先进的结果，表明其在三角网格和点云之间的匹配性能都很好。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种自监督学习框架，用于多模态非刚性三维形状匹配。该框架由三个主要组件组成：特征提取器、功能映射求解器和非刚性形状对齐模块。特征提取器基于DiffusionNet，可以从三角网格和点云中提取特征。功能映射求解器基于功能映射框架，通过找到两个形状之间的线性映射来建立它们之间的对应关系。非刚性形状对齐模块基于自监督学习方法，通过最小化特征映射和非刚性对齐后的对应物之间的差异来对齐形状。对齐损失由两部分组成：特征映射对齐损失和非刚性形状对齐损失。该框架在没有地面真实对应关系的情况下进行自监督训练。

#### 8. 实验设置：
本文在几个标准基准数据集上评估了所提出的框架，包括FAUST、SCAPE和SHREC'19。评估是在完整形状和部分形状上进行的。特征提取器使用PyTorch实现，使用默认设置的DiffusionNet。功能映射求解器使用λ = 100实现。对齐损失由特征映射对齐损失和非刚性形状对齐损失组成，其中λalign = 10^-3，λnce = 10。缩放因子τ设置为0.07。

#### 9. 实验结果和分析：
所提出的框架在完整和部分形状匹配任务上优于现有的公理、监督和无监督方法。使用平均测地距离进行评估。该框架比以前的监督和无监督方法具有更好的跨数据集泛化能力。该框架仅需要少量训练数据，就可以实现与数据集内训练相似的性能。基于DiffusionNet的特征提取器显著提高了形状匹配的准确性。三角网格和点云的匹配性能在所提出的方法中保持几乎不变。该框架展示了以前未见的泛化能力。


# Paper:533     VDN-NeRF：通过视角依赖性归一化解决形状-辐射歧义问题



#### 1. Title: 
VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization

#### 2. Authors: 
Bingfan Zhu, Yanchao Yang, Xulong Wang, Youyi Zheng, Leonidas Guibas

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Neural Radiance Fields, View-Dependence Normalization, Shape-Radiance Ambiguity, Volume Rendering, Geometry Reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_VDN-NeRF_Resolving_Shape-Radiance_Ambiguity_via_View-Dependence_Normalization_CVPR_2021_paper.html  Github: https://github.com/BoifZ/VDN-NeRF

#### 6. Summary : 
- (1):本文旨在解决非Lambertian表面和动态照明条件下的形状-辐射歧义问题，提出了VDN-NeRF方法，通过视角依赖性归一化来规范化视角依赖性，从而提高几何重建的质量。

- (2):过去的方法主要依赖于像素值来确定场景结构，因此易受到反射、非Lambertian表面或不一致的照明等视角依赖性影响，从而导致形状-辐射歧义问题。本文提出的方法通过编码场景表示中的不变特征来规范化视角依赖性，从而解决了这一问题。

- (3):本文提出了一种视角依赖性归一化方法，通过自我蒸馏来提取场景表示中的不变特征，并将其编码回神经特征场中，从而规范化视角依赖性。该方法可以与常用的光度重建损失一起进行联合训练，并且可以轻松地插入到任何依赖于体积渲染的方法中进行几何场景重建。

- (4):本文的方法在多个数据集上进行了实验，证明了该方法可以最小化形状-辐射歧义对几何重建的影响，从而提高了几何重建的质量。此外，本文还提出了一个新的基准测试，验证了该方法在动态光场下的鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为VDN-NeRF的方法，通过视角依赖性归一化解决形状-辐射模糊问题。该方法将场景中点的辐射建模为一个函数，该函数接受点坐标和视角方向作为输入，并输出一个RGB值。该方法使用方向性视角依赖性归一化来对齐所有场景的视角依赖性辐射变化程度，并实现所需的不变性。该方法涉及一个蒸馏网络，该网络接受渲染图像并预测相应的渲染深度。蒸馏网络的特征图用于归一化神经辐射场中的视角依赖性颜色函数。

具体步骤如下：
1. 使用NeuS作为基础模型，将特征函数F实例化并与辐射函数c并行使用。
2. 使用视角依赖性归一化来稀释颜色中的变化，以预测更具视角不变性的信号。
3. 通过视角依赖性归一化的联合训练来最小化光度和自蒸馏特征重建误差。

#### 8. 实验设置：
本文创建了一个新的基准测试，包括8个具有挑战性的场景的合成渲染图像。每个场景随机选择30个视角作为训练数据。本文还展示了来自NeROIC的无约束照明数据、DTU和BlendedMVS的静态照明数据的结果。为了测试所提出的视角依赖性归一化方法在实际应用中缓解形状-辐射模糊的能力，本文还在两个真实场景下进行了实验，即口内扫描和水下数据集AQUALOC。

#### 9. 实验结果与分析：
所提出的视角依赖性归一化方法在图像重建方面表现出比NeuS更好的性能，特别是在不稳定的照明条件和无纹理表面等具有挑战性的场景中。使用所提出的特征函数F和颜色函数c的组合可以实现最佳性能。消融实验表明，使用较小深度特征层训练的模型从视角依赖性归一化中获益更大。真实世界捕捉结果展示了所提出的方法在暗环境应用中的实际用途。在新的基准测试中，VDN-NeRF方法在IoU、L1/L2 Chamfer距离、法向一致性和f-score等指标上均优于非学习方法、基于体积的方法和基于SDF的方法。


# Paper:534     HIER: 通过层次正则化实现超越类别标签的度量学习



#### 1. Title: 
HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization

#### 2. Authors: 
Sungyeon Kim, Boseung Jeong, Suha Kwak

#### 3. Affiliation: 
第一作者：韩国Pohang科技大学计算机科学与工程系

#### 4. Keywords: 
Metric learning, hierarchical regularization, hyperbolic space, semantic hierarchy, deep learning

#### 5. Paper: http://cvlab.postech.ac.kr/research/HIER  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度度量学习，其目的是学习一个嵌入空间，使得相似的样本之间距离较近，不相似的样本之间距离较远。传统的度量学习方法主要基于人工标注的类别标签，但这种监督方式存在一些局限性，如无法处理数据的层次结构关系等。

- (2):过去的方法主要是基于类别标签的度量学习方法，如对比损失、三元组损失和代理损失等。这些方法只能处理数据的类别标签，无法处理数据的层次结构关系。本文提出了一种新的正则化方法HIER，通过学习超几何空间中的层次代理来发现数据的潜在语义层次结构，并将其部署到度量学习中，提供比传统方法更丰富和细粒度的监督信号。HIER的创新之处在于，它可以在没有语义层次结构注释的情况下，通过学习超几何空间中的层次代理来发现数据的潜在语义层次结构。

- (3):本文提出的HIER方法是一种软近似的层次聚类方法，通过构造三元组样本或代理，鼓励每个最近的代理作为三元组的最近公共祖先，以近似数据的语义层次结构。HIER方法可以与任何基于球形嵌入空间的度量学习损失函数无缝集成，同时充分利用超几何空间的优势。

- (4):本文在四个公共基准测试中评估了HIER方法的有效性，结果表明，与传统方法相比，HIER方法在绝大多数情况下都取得了最佳性能，甚至超过了现有的超几何度量学习技术。
#### 7. 方法详细介绍：
本文提出了一种名为HIER的正则化方法，旨在发现和利用超几何空间中数据的潜在语义层次结构。该方法通过学习超几何空间中的分层代理来实现，这些代理是可学习的参数，用于作为一组数据或其他代理的祖先，以近似它们之间的语义层次结构。HIER在训练过程中构建了一个三元组，其中两个样本或代理相似，另一个不相似，基于它们之间的超几何距离。然后，最接近整个三元组的代理被认为是三元组在语义层次结构中的最低公共祖先（LCA）。给定三元组和两个LCA（代理），HIER鼓励每个LCA及其相关的三元组成员彼此靠近，而不相似的三元组成员则远离相似对的LCA。这使得分层代理可以在嵌入空间中近似数据的语义层次结构，而无需任何伪分层标签的现成模块。

#### 8. 实验设置：
本文在四个标准基准数据集上评估了HIER的有效性，包括CUB-200-2011、Cars-196、Stanford Online Products和In-Shop Clothes Retrieval数据集。实验使用ResNet-50架构和Adam优化器，学习率为0.0001。批量大小设置为128，嵌入维度对于除In-Shop Clothes Retrieval数据集外的所有数据集均设置为128，对于In-Shop Clothes Retrieval数据集，嵌入维度设置为64。

#### 9. 实验结果和分析：
本文在四个标准数据集上进行了实验，结果表明，与现有的基于CNN的方法相比，所提出的方法表现更好，并在几乎所有设置中取得了最佳记录，甚至超过了超几何度量学习的现有技术。本文还包括定性结果，例如学习嵌入向量的可视化和所提出方法的语义层次结构分析。进行了消融研究，以调查超参数对所提出方法性能的影响。


# Paper:535     基于单领域泛化的LiDAR语义分割



#### 1. Title: 
Single Domain Generalization for LiDAR Semantic Segmentation

#### 2. Authors: 
Hyeonseong Kim, Yoonsu Kang, Changgyoon Oh, and Kuk-Jin Yoon

#### 3. Affiliation: 
KAIST（韩国科学技术院）Visual Intelligence Lab.

#### 4. Keywords: 
LiDAR, semantic segmentation, domain generalization, sparsity, scene distribution

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_Single_Domain_Generalization_for_LiDAR_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/gzgzys9887/DGLSS

#### 6. Summary : 
- (1):本文研究背景是自动驾驶系统中的LiDAR语义分割，由于训练和测试数据分布不同，导致模型在未知领域的性能下降，因此需要进行领域泛化。
 
- (2):过去的方法主要是无监督领域自适应方法，但是这些方法不能保证在未知领域中具有高性能，而且需要重新获取数据和重新训练。本文提出了一种单领域泛化方法，通过学习源领域来确保在未知领域中具有良好的性能。本文的方法主要针对不同LiDAR传感器配置和场景分布的领域差异，通过模拟未知领域来进行学习，使用两个约束条件进行泛化表示学习：稀疏不变特征一致性（SIFC）和语义相关性一致性（SCC）。本文还建立了标准化的训练和评估设置，使用四个真实的LiDAR数据集进行评估，证明了本文方法在未知领域中的性能优于其他基线方法。

- (3):本文提出了一种单领域泛化方法，通过学习源领域来确保在未知领域中具有良好的性能。本文的方法主要针对不同LiDAR传感器配置和场景分布的领域差异，通过模拟未知领域来进行学习，使用两个约束条件进行泛化表示学习：稀疏不变特征一致性（SIFC）和语义相关性一致性（SCC）。本文还建立了标准化的训练和评估设置，使用四个真实的LiDAR数据集进行评估，证明了本文方法在未知领域中的性能优于其他基线方法。

- (4):本文的方法在四个真实的LiDAR数据集上进行了评估，证明了在未知领域中具有良好的性能。本文方法的性能优于其他基线方法，即使没有访问目标领域，本文方法的性能也与领域自适应方法相当。
#### 7. 方法详细介绍：
本文提出了一种针对LiDAR语义分割的单域泛化方法(DGLSS)，旨在确保在仅在源域上训练的情况下，在未见过的域中也能有良好的性能。该方法由编码器、解码器和分类器组成。源域通过操纵稀疏度来获得增强域。模型接收体素并为每个体素预测一个语义类别。提出了两个约束条件，即稀疏不变特征一致性(SIFC)和语义相关性一致性(SCC)，以鼓励可泛化的表示学习。SIFC维护源域和增强域的编码稀疏特征在相同场景但不同稀疏度下的一致性。SCC约束特征嵌入空间在域之间具有语义一致性。在未见目标域和源域上评估语义分割性能。

#### 8. 实验设置：
本文在四个真实的自动驾驶LiDAR数据集上进行了评估：SemanticKITTI、nuScenes-lidarseg、Waymo和SemanticPOSS。为每个数据集选择并映射了共同的类别。模型在源域上进行训练，并在未见域和源域上进行评估。使用每个类别的交集并集(IoU)和每个数据集的平均IoU(mIoU)来评估分割性能。使用平均数(AM)和调和平均数(HM)来评估泛化性能。

#### 9. 实验结果和分析：
提出的方法在SemanticKITTI和Waymo作为源域时，相对于基线方法，增益分别为+4.84% AM和+5.36% HM，以及+1.67% AM和+1.97% HM。Augment方法比其他基线方法表现更好，说明考虑LSS中的稀疏性的重要性。在SemanticKITTI数据集上训练的IBN-Net获得比基线方法更好的结果，并且在Waymo上训练时可比较。MLDG(B)在大多数数据集上的mIoU比基线方法和MLDG(A)都要高。我们的方法在两种训练设置中均超过了DA方法，表明其在保留适用于其他域的域不变信息方面的有效性。


# Paper:536     Movies2Scenes：使用电影元数据学习场景表示



#### 1. Title: 
Movies2Scenes: Using Movie Metadata to Learn Scene Representation

#### 2. Authors: 
Shixing Chen, Chun-Hao Liu, Xiang Hao, Xiaohan Nie, Maxim Arap, Raffay Hamid

#### 3. Affiliation: 
Amazon Prime Video（亚马逊Prime视频）

#### 4. Keywords: 
Scene understanding, contrastive learning, movie metadata, scene representation, video moderation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Movies2Scenes_Using_Movie_Metadata_to_Learn_Scene_Representation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究电影场景理解的问题，提出了一种使用电影元数据学习场景表示的方法。由于标记单个场景的过程耗时，因此使用电影级别的元数据（例如类型、简介等）来定义电影相似性，限制搜索正面场景对，从而学习通用的场景表示。
 
- (2):过去的方法主要是基于监督学习，但标记单个场景的过程耗时，因此不适用于长视频。本文提出的方法使用电影元数据来定义电影相似性，限制搜索正面场景对，从而学习通用的场景表示。这种方法的创新点在于使用电影元数据来定义电影相似性，从而限制搜索正面场景对，提供更丰富的几何和主题数据增强。
 
- (3):本文提出了一种对电影元数据进行对比学习的方法，用于学习通用的场景表示。首先训练一个镜头编码器来提供基于外观的表示，然后使用电影元数据来定义电影级别的相似性，用于训练电影编码器。最后，使用训练好的电影编码器来选择相似的场景对，进行场景编码器的对比学习。本文的创新点在于使用电影元数据来定义电影相似性，从而限制搜索正面场景对，提供更丰富的几何和主题数据增强。
  
- (4):本文的方法在多个基准数据集上进行了评估，结果表明，所学习的场景表示在多个任务上均优于现有的最先进方法。在LVU数据集上，所学习的场景表示在七个分类任务上平均提高了7.9％，在两个回归任务上提高了9.7％。此外，使用新收集的电影数据集，本文在一组视频审查任务上展示了其通用性。
#### 7. 方法详细介绍：
本文提出了一种名为Movies2Scenes的方法，使用电影元数据来学习场景表示。该方法包括以下步骤：
1. 使用未标记的电影镜头训练一个镜头编码器，提供基于外观的表示。
2. 使用常见的电影元数据（例如共同观看、类型或简介）来定义电影级别的相似度S。
3. 使用训练好的电影级别编码器选择相似的场景对，进行对比学习场景编码器。
4. 使用学习到的场景表示进行下游任务。

#### 8. 实验设置：
本文使用了一个新收集的内部数据集MovieCL30K，其中包含30,340部电影，用于学习场景表示。使用多个公共基准数据集评估了学习到的表示在不同下游任务上的性能。此外，使用一个新收集的数据集MCD，该数据集聚焦于大规模视频审核，包含来自18,330部电影和电视剧的44,581个视频剪辑，其中包含性、暴力和药物使用活动，以展示场景表示在以前不太研究的任务上的泛化能力。

#### 9. 实验结果和分析：
学习到的场景表示在多个基准数据集上的不同任务上均优于现有的最先进方法。值得注意的是，在LVU数据集的七个分类任务和两个回归任务中，学习到的表示提供了平均7.9％的改进。本文还展示了场景表示在一组视频审核任务上的比较结果，以展示其在以前不太研究的任务上的泛化能力。


# Paper:537     MEDIC: 通过重要性驱动克隆去除模型后门



#### 1. Title: 
MEDIC: Remove Model Backdoors via Importance Driven Cloning

#### 2. Authors: 
Qiuling Xu, Guanhong Tao, Jean Honorio, Yingqi Liu, Shengwei An, Guangyu Shen, Siyuan Cheng, Xiangyu Zhang

#### 3. Affiliation: 
Purdue University (普渡大学)

#### 4. Keywords: 
Backdoor attack, deep learning, model cloning, importance-driven, model security

#### 5. Paper: 
Paper link: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_MEDIC_Remove_Model_Backdoors_via_Importance_Driven_Cloning_CVPR_2021_paper.html

Github link: https://github.com/qiulingxu/MEDIC

#### 6. Summary: 
- (1):本文研究的是深度学习模型中的后门攻击问题，即通过注入特定的触发器来诱导模型产生误分类行为，从而破坏模型的安全性。
- (2):过去的方法包括微调、知识蒸馏和神经元剪枝等，但这些方法在某些攻击场景下可能无法有效去除后门，或者会对模型的正常功能产生不良影响。本文提出了一种新的后门去除方法MEDIC，通过克隆受污染模型的良性行为来生成一个新的无后门模型，该方法通过重要性驱动的克隆方法，只需要使用少量的干净样本即可有效去除后门，且不会对模型的正常功能产生不良影响。
- (3):本文提出的MEDIC方法通过克隆模型的内部激活值来生成新的无后门模型，该方法通过重要性驱动的克隆方法，只需要使用少量的干净样本即可有效去除后门，且不会对模型的正常功能产生不良影响。本文理论分析了该方法的优势，并在实验中验证了该方法的有效性。
- (4):本文在九种不同类型的后门攻击上进行了实验，结果表明，MEDIC方法可以将攻击成功率降低到8.5％，同时只有2％的良性准确率下降，且比现有的后门去除方法表现更好。
#### 7. 方法详细介绍：
本文提出了一种名为MEDIC的后门移除算法，通过克隆原始模型并添加中间约束来移除后门。该算法包括三个步骤：（1）克隆原始模型，（2）向克隆模型添加中间约束，（3）在少量干净数据上微调克隆模型。中间约束添加到克隆模型的隐藏层中，以确保干净数据和有毒数据的隐藏表示不同。算法使用重要性驱动克隆来选择中间约束的最重要神经元。使用边缘损失函数，并使用Rademacher复杂度分析后悔的上限。

具体步骤如下：
1. 克隆原始模型；
2. 选择重要神经元，添加中间约束；
3. 在少量干净数据上微调克隆模型。

#### 8. 实验设置：
本文在Wide ResNet和CIFAR-10上进行了九种代表性后门攻击的实验。自适应攻击包括Badnet、Clean Label、SIG、Reflection和Warp。使用5%的数据来移除CIFAR-10上的后门行为。将算法与五种最新的后门移除方法进行比较，包括Finetune、Fineprune、NAD、MCR和ANP。对所有方法使用相同的数据增强，并调整MEDIC的温度以确保修复模型的可比测试准确性。在ResNet上还在大规模数据集Kitti-City和Kitti-Road以及公共后门基准Polygon和Filter上进行了实验。

#### 9. 实验结果与分析：
实验结果表明，MEDIC在消除难以消除的后门方面优于其他基线。该算法在消除Clean Label、SIG和Reflection攻击方面非常有效，并在其他攻击方面与其他方法表现相当。在大规模数据集和公共基准测试中，MEDIC消除后门的有效性也得到了证明。重要性驱动克隆方法在选择中间约束的重要神经元方面非常有效。使用Rademacher复杂度分析后悔的上限，并将MEDIC的函数族复杂度与其他方法进行比较。


# Paper:538     攻守兼备：对抗攻击的防御方法



#### 1. Title: 
The Best Defense is a Good Offense: Adversarial Augmentation against Adversarial Attacks

#### 2. Authors: 
Iuri Frosio, Jan Kautz

#### 3. Affiliation: 
Iuri Frosio: NVIDIA公司；Jan Kautz: NVIDIA公司

#### 4. Keywords: 
Adversarial attacks, deep neural networks, certified defense, image purification, adversarial training

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Frosio_The_Best_Defense_Is_a_Good_Offense_Adversarial_Augmentation_Against_CVPR_2020_paper.html  Github: https://github.com/NVlabs/A5

#### 6. Summary : 
- (1):本文研究对抗攻击的防御方法，提出了一种新的框架A5，采用预防性的防御策略，通过构造防御扰动来保证任何攻击都会失败，是首个具有认证的预防性防御方法。
 
- (2):过去的防御方法主要是在攻击后采取反制措施，如对抗训练、随机化或图像净化等，但这些方法都存在一定的局限性。本文提出的A5框架采用预防性的防御策略，通过构造防御扰动来保证任何攻击都会失败，具有认证的预防性防御能力。本文的方法在MNIST、CIFAR10、FashionMNIST和Tinyimagenet等数据集上均取得了优异的性能表现，且能够应用于物理对象的认证性防御。

- (3):本文提出的A5框架采用预防性的防御策略，通过构造防御扰动来保证任何攻击都会失败，具有认证的预防性防御能力。本文的方法主要包括两个步骤：首先，利用现有的神经网络自动扰动分析工具来构造防御扰动；其次，通过训练一个鲁棒性网络来实现防御扰动的生成。本文的方法在MNIST、CIFAR10、FashionMNIST和Tinyimagenet等数据集上均取得了优异的性能表现，且能够应用于物理对象的认证性防御。

- (4):本文的方法在MNIST、CIFAR10、FashionMNIST和Tinyimagenet等数据集上均取得了优异的性能表现，且能够应用于物理对象的认证性防御。本文的方法具有认证的预防性防御能力，能够保证任何攻击都会失败，是一种新的有效的防御方法。
#### 7. 方法详细介绍：
本文提出了一种名为A5（Adversarial Augmentation Against Adversarial Attacks）的方法，用于预防性地修改DNN或物理对象的输入，使其对MitM（中间人）对抗攻击具有可证明的鲁棒性。该方法的主要思想是制作一种防御性扰动，以确保任何攻击（最大幅度为给定值）都会失败。为此，利用了现有的神经网络自动扰动分析工具。本文还讨论了A5的不同版本及其有效性，包括使用一个忽略了真实标签的鲁棒性网络进行即时防御性扩充，以及鲁棒性网络和分类器的联合训练。本文还展示了如何将A5应用于创建具有可证明鲁棒性的物理对象。

#### 8. 实验设置：
作者在MNIST、CIFAR10、FashionMNIST和Tinyimagenet数据集上测试了A5/O、A5/R和A5/RC等不同版本的A5。他们使用p = ∞范数进行攻击和防御。他们训练了五个具有不同鲁棒性水平的可证明鲁棒MNIST分类器，用于训练攻击ϵCA = 0.0（未受保护的C）和ϵCA = {0.1, 0.2, 0.3, 0.4}。他们还报告了使用autoattack估计的错误率，这是一种基于四种攻击的集合的自动工具，用于评估实际DNN的鲁棒性。

#### 9. 实验结果和分析：
本文展示了A5如何显著提高MNIST、CIFAR10、FashionMNIST和Tinyimagenet数据集上的清洁和可证明错误率。最佳结果是在C最初训练ϵCA ≤ ϵA的情况下实现的，可能是因为C具有初始高的清洁准确性，并随后与R共同适应，以保证小的可证明错误。本文还展示了MNIST和CIFAR10数据集的A5/O、A5/R和A5/RC的成功和失败的鲁棒化示例。最后，本文测试了62个字符的分类，这些字符经过随机旋转、平移、透视畸变、噪声添加、模糊和颜色抖动，并展示了A5在清洁和可证明错误方面的显着改进。


# Paper:539     基于部件检索和组装的无监督3D形状重建



#### 1. Title: 
Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly

#### 2. Authors: 
Xianghao Xu, Paul Guerrero, Matthew Fisher, Siddhartha Chaudhuri, Daniel Ritchie

#### 3. Affiliation: 
Xianghao Xu: Brown University (美国布朗大学)

#### 4. Keywords: 
3D shape reconstruction, unsupervised learning, part retrieval, part assembly

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Unsupervised_3D_Shape_Reconstruction_by_Part_Retrieval_and_Assembly_CVPR_2020_paper.html  Github: https://github.com/xhxucwj/PartAssembly

#### 6. Summary : 
- (1):本文研究了无监督3D形状重建的问题，提出了一种基于部件检索和组装的方法，旨在解决现有方法中参数化粗略或学习到的部件控制力度不够的问题。

- (2):现有的无监督3D形状分解方法要么使用简单的参数化基元，如长方体或超椭球面，要么学习部件的生成形状空间。前者的精度较低，后者的分解类型无法控制。本文提出了使用用户定义的3D部件库来分解形状的方法，使用户可以完全控制部件的选择。该方法通过迭代地从库中检索部件并优化其位置来工作。实验结果表明，该方法比现有方法具有更高的重建精度和更可控的分解类型。

- (3):本文提出了一种基于部件检索和组装的无监督3D形状重建方法。为了解决大规模组合搜索问题，我们使用训练好的部件自编码器将部件库表示为连续流形，并联合优化部件的身份和姿态。为了避免局部最优解，算法周期性地使用当前预测的部件集对输入形状进行分割，然后将这些分割重新编码为部件特征流形，以产生最佳重建输入形状的新估计。我们的算法可以独立地运行于任何单个目标形状，也可以在训练阶段优化其部件分解，然后通过使用其最近邻居初始化其分解来快速分解来自该类别的新形状。

- (4):本文的方法在PartNet数据集上进行了实验，与最近的神经部件无监督分解系统进行了比较，并展示了更高的重建精度和更可控的分解类型。本文的方法还可以通过使用来自其他语义形状类别的部件来重建形状，具有3D图形内容创建的应用。
#### 7. 方法详细介绍：
本文提出了一种无监督的三维形状重建方法，通过部件检索和组装实现。该方法使用预训练的变分自编码器（VAE）将离散的组合搜索问题转化为可行的连续优化问题。优化过程由三个阶段组成：第一阶段：部件优化，第二阶段：部件移动，第三阶段（可选）：部件借用。在第一阶段中，算法直接优化VAE潜在编码、平移和绕世界上轴的旋转，以重建目标形状。在第二阶段中，算法使用第一阶段优化的部件对目标形状进行分割，并使用预训练的VAE编码器将其重新投影回潜在空间，以形成下一次第一阶段迭代的新初始状态。在第三阶段中，算法从其他重建良好的相似形状中借用好的部件分解。算法通过运行迭代优化过程并返回最佳重建的k来确定每个形状使用的部件数量。最后，它从部件库中检索与优化的连续潜在空间部件最接近的部件。

#### 8. 实验设置：
本文使用了PartNet数据集中的点云来评估算法的性能。实验中使用了CD和VCD两个指标来评估算法的重建精度。实验设置中使用了两个基线方法进行比较，分别是基于检索的重建和无监督部件分解。实验中使用了Nvidia Tesla V100 GPU进行实验。

#### 9. 实验结果和分析：
实验结果表明，本文提出的方法在不同类别的形状重建中表现出色，并且在CD和VCD方面优于基线方法。消融实验表明，使用所有阶段可以获得最佳性能，其中第二阶段比第三阶段更重要。本文提出的方法在重建精度和部件分解质量方面优于现有方法。此外，本文提出的方法还可以通过从其他类别中借用部件来重建形状。未来的改进方向包括引入物理先验知识以使重建更加物理合理，并支持不完整的点云作为输入。


# Paper:540     自监督视觉表示学习的多模式在线知识蒸馏



#### 1. Title: 
Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning

#### 2. Authors: 
Kaiyou Song, Jin Xie, Shan Zhang, Zimeng Luo

#### 3. Affiliation: 
Megvii Technology (旷视科技)

#### 4. Keywords: 
Self-supervised learning, knowledge distillation, contrastive learning, visual representation learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Song_Multi-Mode_Online_Knowledge_Distillation_for_Self-Supervised_Visual_Representation_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究背景是自监督学习在视觉表示学习中的应用，旨在提高小模型的表示学习性能。
- (2):过去的方法是将知识从一个静态的预训练教师模型传递到一个学生模型中，但是这种方法存在局限性，即教师模型无法从学生模型中吸收知识以提高其性能。本文提出了一种多模式在线知识蒸馏方法(MOKD)，通过两种蒸馏模式，即自蒸馏和交叉蒸馏模式，实现两个不同模型的自监督学习和知识交互，从而提高它们的表示学习性能。其中，交叉蒸馏采用交叉注意力特征搜索策略，增强不同模型之间的语义特征对齐。与现有的SSL-KD方法相比，MOKD不仅可以提高学生模型的性能，还可以提高教师模型的性能。
- (3):本文提出了一种新颖的自监督在线知识蒸馏方法，即MOKD，通过两种蒸馏模式，即自蒸馏和交叉蒸馏模式，实现两个不同模型的自监督学习和知识交互。其中，交叉蒸馏采用交叉注意力特征搜索策略，增强不同模型之间的语义特征对齐。实验结果表明，MOKD可以提高两个模型的性能，达到不同模型的最新对比学习性能。同时，MOKD也可以提高现有SSL-KD方法的性能。
- (4):本文在不同的骨干网络和数据集上进行了广泛的实验，证明了两个异构模型可以从MOKD中受益，并超过其独立训练的基线。例如，当使用ResNet和ViT进行训练时，两个模型可以相互吸收知识，并展现出彼此的特征。此外，MOKD还在学生和教师模型上超过了现有的SSL-KD方法。
#### 7. 方法详细介绍：
本文提出了一种名为多模式在线知识蒸馏（MOKD）的方法，用于自监督视觉表示学习。该方法包括两个编码器、两个投影头和两个Transformer头。编码器使用DINO进行训练，投影头使用四层MLP进行训练，Transformer头使用三个块的Transformer和一个FC层进行投影。MOKD方法包括自蒸馏损失、交叉蒸馏损失和交叉注意力特征搜索。CNN和ViT分别使用SGD和AdamW优化器进行训练。训练过程包括预训练和微调两个阶段。在预训练阶段，使用DINO方法进行自监督学习。在微调阶段，使用线性分类器对特征进行微调。

#### 8. 实验设置：
实验在ImageNet数据集上进行，使用不同大小的CNN和Vision Transformer作为编码器。评估异构和同构模型，并采用256批量大小和100个epoch的默认设置。使用k-NN和线性探针来评估表示性能。采用验证集上的top-1准确率作为评估指标。

#### 9. 实验结果和分析：
MOKD方法在自监督知识蒸馏方面取得了最先进的性能。在不同的骨干网络和任务上进行了广泛的实验，证明了MOKD可以提高不同模型的特征表示性能。与独立训练相比，MOKD的预测一致性率有所提高，但是两个模型的表示之间没有明显的趋势。实验结果表明，MOKD可以有效地在不同模型之间传递知识，提高表示性能。MOKD在CIFAR-10和CIFAR-100数据集上的性能也超过了独立预训练的基线。在COCO数据集上的目标检测和实例分割任务中，MOKD取得了最佳性能，证明了其在密集预测任务上的良好泛化能力。


# Paper:541     重新思考针对深度神经网络的模型反演攻击



#### 1. Title: 
Re-thinking Model Inversion Attacks Against Deep Neural Networks

#### 2. Authors: 
Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, Ngai-Man Cheung

#### 3. Affiliation: 
Singapore University of Technology and Design (新加坡科技与设计大学)

#### 4. Keywords: 
Model inversion, deep neural networks, privacy attacks, optimization objective, MI overfitting

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Re-Thinking_Model_Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2021_paper.html  Github: https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/

#### 6. Summary : 
- (1):本文研究深度神经网络中的模型反演攻击，该攻击旨在通过滥用对模型的访问来推断和重构私有训练数据。 
- (2):过去的方法主要集中在优化目标上，本文分析了现有SOTA MI算法的两个基本问题，并提出了解决这些问题的解决方案，这些解决方案适用于所有SOTA MI，并且在攻击性能方面有显著提高。 
- (3):本文提出了改进的优化目标和模型增强方法，以提高SOTA MI的攻击性能。 
- (4):在CelebA基准测试中，本文的解决方案将攻击准确率提高了11.8％，首次实现了超过90％的攻击准确率。这表明了深度学习模型泄露敏感信息的风险，并呼吁对隐私问题给予更多关注。
#### 7. 方法详细介绍：
本文提出了两种改进现有深度神经网络模型反演（MI）攻击的方法。首先，本文提出了一种新的基于logit的身份损失（Llogitid），该损失更符合MI的目标。其次，本文提出了模型增强（MA）来缓解MI过拟合。本文还将Llogitid和MA结合起来进行模型反演。

具体步骤如下：
1. 对于Llogitid，本文提出了一种新的身份损失函数，该函数直接最大化logit，pT wk，而不是最大化类k的对数似然。该损失函数可以轻松地插入到所有现有的MI算法中，计算开销很小。
2. 对于MA，本文提出了一种基于知识蒸馏的方法，使用目标模型作为教师来训练增强模型。增强模型采用不同的网络架构，并且与目标模型不同。在进行反演步骤时，增强模型与目标模型一起应用，并使用模型增强计算身份损失。该方法旨在缓解MI过拟合，即重构样本过于贴近目标模型，适应目标模型参数的随机变化和噪声，未能充分学习身份的语义。

#### 8. 实验设置：
本文在三个目标分类器（IR152、face.evoLve和VGG16）上进行了实验。使用的私有数据集是CelebA，公共数据集是FFHQ。本文还使用了GAN模型和三种使用公共数据的模型增强。评估模型使用的是face.evoLve。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的方法在所有实验设置中都显著提高了MI攻击的准确性，并通过结合所提出的两种方法，攻击准确性得到了显著提高。本文还对SOTA防御模型BiDO-HSIC进行了评估，结果表明该模型对所提出的MI方法并不有效。本文的研究结果强调了基于模型反演的隐私威胁，并促使人们认真考虑机器学习的隐私问题。但是，本文也承认了所提出方法的局限性和伦理问题，并提出了未来对黑盒/仅标签攻击的扩展。


# Paper:542     解耦MaxLogit用于ODD检测



#### 1. Title: 
Decoupling MaxLogit for Out-of-Distribution Detection

#### 2. Authors: 
Zihan Zhang and Xiang Xiang

#### 3. Affiliation: 
华中科技大学（Huazhong University of Science and Technology）

#### 4. Keywords: 
Out-of-Distribution Detection, MaxLogit, Decoupling MaxLogit, MaxCosine, MaxNorm

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Decoupling_MaxLogit_for_Out-of-Distribution_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是机器学习中的Out-of-Distribution Detection问题，即如何检测模型对于训练集之外的数据的识别能力。这是一个重要的问题，因为在实际应用中，模型很容易对于ODD数据产生过度自信的预测，从而导致错误的结果。

- (2):过去的方法主要包括置信度增强方法、异常值暴露方法和后处理检测方法。其中，后处理检测方法是最容易使用的方法，MaxLogit和MSP是其中最简单的两种方法。然而，这些方法的性能并不是最好的。本文提出了Decoupling MaxLogit (DML)方法，通过解耦MaxLogit的MaxCosine和MaxNorm来提高性能。同时，本文还发现了一些新的洞见，如少量的困难样本和紧凑的特征空间对于MaxCosine和MaxNorm的性能有重要影响。

- (3):本文提出了Decoupling MaxLogit (DML)方法，通过解耦MaxLogit的MaxCosine和MaxNorm来提高性能。同时，本文还发现了一些新的洞见，如少量的困难样本和紧凑的特征空间对于MaxCosine和MaxNorm的性能有重要影响。本文的贡献在于提出了一种新的方法来解决ODD问题，并且通过实验证明了该方法的有效性。

- (4):本文在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验，取得了最先进的性能。DML方法在CIFAR-100数据集上的平均AUROC为93.0％，在ImageNet数据集上的平均AUROC为84.5％。这些结果表明，DML方法在ODD检测方面具有很高的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Decoupling MaxLogit (DML)的后处理评分方法，用于检测out-of-distribution (OOD)。DML将最大logit分解为两个部分：MaxCosine和MaxNorm。MaxCosine使用余弦相似度来识别OOD样本，而MaxNorm则测量特征范数。DML可以写成λMaxCosine + MaxNorm的形式，其中λ是一个超参数。λ的权重在高斯噪声上进行调整。本文还研究了模型训练的作用，并展示了一种简单的修改标准训练的方法，可以显著提高MaxNorm和MaxCosine的性能。其中，Center loss用于改进WFC评分，而Focal loss用于解决难样本并提高CFC评分。

#### 8. 实验设置：
本文在三个in-distribution数据集（CIFAR-10、CIFAR-100和ImageNet）和六个CIFAR的OOD数据集以及四个ImageNet的OOD数据集上评估了所提出的方法。使用的评估指标是AUROC和FPR95。本文将所提出的方法与几种最先进的OOD检测方法进行了比较，包括LogitNorm和GODIN。本文还尝试了几种模型架构，包括WRN-40-2、ResNet34、ResNet50和DenseNet。

#### 9. 实验结果与分析：
本文在CIFAR-10、CIFAR-100和ImageNet数据集上展示了所提出方法的实验结果。所提出的方法在所有六个OOD数据集上的AUROC指标上均优于最先进的方法。在三个数据集（包括SVHN、LSUN-R和iSUN）上，所提出的方法分别获得了最佳、第二和第三的AUROC和FPR95。所提出的方法在ImageNet上也表现良好，在三个数据集上的AUROC排名分别为第一、第二和第三，不包括Textures。消融实验表明，所提出的方法对不同的模型架构和不同的耦合方法都是有效的。所提出的方法还将现有评分方法的性能提高了9%以上。


# Paper:543     基于轨迹感知的多人姿态预测中的身体交互Transformer



#### 1. Title: 
Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting

#### 2. Authors: 
Xiaogang Peng, Siyuan Mao, Zizhao Wu

#### 3. Affiliation: 
杭州电子科技大学数字媒体技术系

#### 4. Keywords: 
Multi-person pose forecasting, body interaction, transformer, trajectory-aware, relative position encoding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Peng_Trajectory-Aware_Body_Interaction_Transformer_for_Multi-Person_Pose_Forecasting_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究多人姿态预测问题，特别是在复杂人群场景中建模细粒度人体交互。现有方法通常将整个姿态序列表示为时间序列，但忽略了基于骨骼身体部位的人际交互影响。因此，本文提出了一种新颖的Trajectory-Aware Body Interaction Transformer (TBIFormer)框架，通过有效建模身体部位交互来进行多人姿态预测。

- (2):现有方法通常只学习时间和社交关系，而不建模细粒度的身体交互，难以捕捉复杂的交互依赖关系。本文提出了一种新颖的Transformer-based框架，包括Temporal Body Partition Module (TBPM)和Social Body Interaction Multi-Head Self-Attention (SBI-MSA)模块，用于学习身体部位动态和捕捉细粒度的骨骼身体交互依赖关系。此外，本文还引入了一种新颖的Trajectory-Aware Relative Position Encoding，提供了区分性的空间信息和额外的交互线索。

- (3):本文提出的TBIFormer框架包含多个堆叠的TBIFormer块和一个Transformer解码器，用于预测多人姿态和轨迹。每个TBIFormer块包含一个SBI-MSA模块，用于学习身体部位动态和捕捉复杂的交互依赖关系。为了将包含时间和空间信息的姿态序列输入TBIFormer，本文提出了一个TBPM模块，将原始姿态序列转换为一个Multi-Person Body Part (MPBP)序列，使模型能够捕捉个体之间交互身体部位的依赖关系。最后，使用Transformer解码器进一步考虑当前和历史上下文之间的关系，以预测平滑和准确的多人姿态和轨迹。

- (4):在多个多人运动数据集上，本文提出的TBIFormer方法在短期和长期预测方面均显著优于现有方法，短期预测精度提高了14.4% ~ 16.5%，长期预测精度提高了6.5% ~ 18.2%。本文的主要贡献包括：1）提出了一种新颖的Transformer-based框架，用于有效的多人姿态预测；2）提出了一个TBPM模块和一个SBI-MSA模块，用于学习身体部位动态和捕捉复杂的交互依赖关系；3）引入了一种新颖的Trajectory-Aware Relative Position Encoding，提供了区分性的空间信息和额外的交互线索；4）在多个多人运动数据集上，本文提出的TBIFormer方法显著优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种轨迹感知的身体交互Transformer（TBIFormer）用于多人姿态预测。该方法包括三个主要组件：时间身体姿势运动（TBPM）编码器、轨迹感知交互（TRPE）模块和TBIFormer块。TBPM编码器从输入姿势序列中提取时间特征。TRPE模块建模了个体动态特征和跨时间和社交维度的空间线索之间的交互。TBIFormer块利用基于Transformer的解码器来编码当前和历史上下文之间的关系。模型使用基于平均每关节位置误差（MPJPE）的重构损失进行优化。

#### 8. 实验设置：
实验在CMU-Mocap（UMPM）数据集上进行，该数据集将UMPM合并到CMU-Mocap中以进行数据集扩展。Mix1和Mix2由CMU-Mocap、UMPM、3DPW和MuPoTs-3D数据集混合而成。使用ADAM优化器进行50个epoch的训练，批量大小为32，学习率为0.0003，丢失率为0.2。TBIFormer块和Transformer解码器中的键、查询和值的维度均设置为64，前馈层的隐藏维度为1024。

#### 9. 实验结果与分析：
所提出的TBIFormer在所有数据集上的短期和长期预测中，以JPE、APE和FDE指标优于基线方法。消融研究表明，TBIFormer的每个组件都对整体性能有所贡献。结果表明，所提出的方法对于多人姿态预测是有效的。


# Paper:544     LG-BPN：自监督现实世界去噪的局部和全局盲点网络



#### 1. Title: 
LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising

#### 2. Authors: 
Zichun Wang, Ying Fu, Ji Liu, Yulun Zhang

#### 3. Affiliation: 
北京理工大学

#### 4. Keywords: 
Image denoising, self-supervised learning, blind-spot network, real-world noise, spatial correlation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_LG-BPN_Local_and_Global_Blind-Patch_Network_for_Self-Supervised_Real-World_CVPR_2021_paper.html  Github: https://github.com/Wang-XIaoDingdd/LGBPN

#### 6. Summary : 
- (1):本文研究的是自监督图像去噪，针对现实噪声的强空间相关性，提出了一种新的方法LG-BPN，旨在提高去噪的性能。

- (2):过去的方法大多依赖于大量的标记图像对进行训练，而这种方法在现实场景中很难实现。一些自监督的方法可以不需要标记图像对进行训练，但是这些方法在处理现实噪声时表现不佳。本文提出的方法LG-BPN可以在不需要额外信息的情况下进行训练，同时考虑了现实噪声的空间相关性和局部细节恢复的问题，以及盲点网络的局限性。

- (3):本文提出了两个模块：一是基于现实噪声空间相关性的密集采样掩膜卷积模块，可以更好地恢复局部细节；二是引入了扩张Transformer块，可以更好地捕捉全局信息。这两个模块可以充分利用局部和全局信息，提高去噪性能。

- (4):本文在现实世界的数据集上进行了实验，结果表明LG-BPN方法在自监督图像去噪方面表现优异，超过了其他现有的方法。
#### 7. 方法详细介绍：
本文提出了一种自监督的实际图像去噪方法，称为LG-BPN。该方法由两个主要组件组成：局部和全局盲块网络（LG-BPN）和扩张变换块。局部盲块网络模块旨在通过在采样位置集成噪声分布先验来提取局部高频结构，从而使提取的特征包含更多的细节和更密集的感受野。扩张变换块旨在捕获全局依赖关系，而不违反感受野的盲点约束。它由一个通道注意力层和一个带有扩张深度卷积和门控单元的前馈层组成。该方法在SIDD和DND基准数据集上实现了最先进的性能。

#### 8. 实验设置：
该方法在两个实际数据集SIDD和DND上进行训练和评估。对于SIDD，使用SIDD-Medium的sRGB图像进行训练，使用SIDD验证集和基准集的sRGB图像进行验证和评估。对于DND，该方法在没有额外外部数据的情况下在测试集上进行训练。批量大小设置为8，使用L1损失进行训练。学习率从1e-4开始，采用Adam优化器。网络训练20个时期直到完全收敛。该方法在PyTorch 1.8.0中实现，并在Nvidia RTX 3090上进行训练。使用的评估指标是峰值信噪比（PSNR）和结构相似性（SSIM）。

#### 9. 实验结果和分析：
该方法在SIDD和DND基准数据集上实现了最先进的性能，PSNR值分别为37.28和38.43，SSIM值分别为0.936和0.942。该方法优于以前的无监督方法和自监督方法，并且在合成对或真实对上训练的监督方法中实现了可比甚至更好的性能。该方法还通过提供比以前基于CNN的盲点网络更广泛的感受野，展示了扩张变换块中引入的全局依赖性的有效性。


# Paper:545     学习注意力作为组合零样本学习的解缠器



#### 1. Title: 
Learning Attention as Disentangler for Compositional Zero-shot Learning

#### 2. Authors: 
Shaozhe Hao, Kai Han, Kwan-Yee K. Wong

#### 3. Affiliation: 
香港大学

#### 4. Keywords: 
Compositional zero-shot learning, visual disentanglement, attention mechanism, earth mover’s distance, vision transformers

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Hao_Learning_Attention_as_Disentangler_for_Compositional_Zero-Shot_Learning_CVPR_2021_paper.html
Github: https://github.com/haoosz/ade-czsl

#### 6. Summary:
- (1):本文研究的是组合零样本学习（CZSL），旨在从已知组合中学习视觉概念（即属性和对象），并将概念知识组合到未知组合中。CZSL的关键是学习属性-对象组合的解缠。 
- (2):以前的方法通过组合属性和对象词嵌入，并将单词和视觉嵌入投影到联合空间中来解决CZSL问题，但是它们无法解缠视觉概念。最近的一些工作考虑到了视觉解缠，但是仍然存在局限性。本文提出了一种新的CZSL方法，使用交叉注意力来解缠属性和对象的独特特征，从而学习解缠的概念嵌入。 
- (3):本文提出了一种简单的视觉解缠框架，利用注意力机制作为解缠器。我们提出了一种交叉注意力机制，用于解缠共享相同概念的输入之间的属性和对象。为了确保解缠器只学习特定概念，我们还需要在注意力级别应用正则化项。具体来说，我们采用了地球移动距离（EMD）作为交叉注意力模块中的特征相似度度量。 
- (4):在三个CZSL基准数据集上进行的全面实验表明，我们的方法在闭合和开放世界设置下都显著优于以前的工作，建立了新的最先进水平。
#### 7. 方法详细介绍：
本文提出了一种名为Attention as DisEntangler (ADE)的方法，用于组合零样本学习。该方法使用交叉注意力机制从配对的概念共享输入中分离属性和对象独有特征。ADE包括三个注意力分支，分别关注输入的不同方面：对象、属性和组合。通过交叉注意力模块学习注意力图，使用经过调整的地球移动距离（EMD）来衡量不同输入的注意力图之间的相似性。该方法还包括一个正则化项，以鼓励注意力图之间的分离。在推理时，该方法综合属性、对象和组合概率以进行最终预测。

#### 8. 实验设置：
本文在三个组合零样本学习基准数据集Clothing16K、UT-Zappos50K和C-GQA上评估了所提出的方法。本文还在闭世界和开放世界设置下进行了评估。使用的评估指标包括曲线下面积（AUC）、最佳调和平均值、最佳已知准确度和最佳未知准确度。

#### 9. 实验结果和分析：
本文提出的方法ADE使用交叉注意力机制和从地球移动距离（EMD）中调整的正则化项，将视觉概念从属性-对象组合中分离出来，用于组合零样本学习。ADE在闭世界和开放世界设置下，在所有三个数据集上均优于当前最先进的方法。本文还进行了全面的定性分析，以验证ADE中注意力分离器的分离能力。


# Paper:546     匹配不足以支撑：一种用于类别无关姿态估计的两阶段框架



#### 1. Title: 
Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation

#### 2. Authors: 
Min Shi, Zihao Huang, Xianzheng Ma, Xiaowei Hu, Zhiguo Cao

#### 3. Affiliation: 
Min Shi, Zihao Huang, and Zhiguo Cao are affiliated with the School of AIA, Huazhong University of Science and Technology.

#### 4. Keywords: 
Category-agnostic pose estimation, two-stage framework, transformer model, similarity-aware position proposals.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2021_paper.html  Github: https://github.com/flyinglynx/CapeFormer

#### 6. Summary : 
- (1):该论文研究的是通用姿态估计问题，即针对任意类别的物体预测其关键点位置。 
- (2):现有的方法采用一阶段匹配范式，即在图像中匹配关键点以进行定位。然而，这种方法的准确性较低，因为预测结果严重依赖于匹配结果，而匹配结果可能会受到CAEP中开放集的影响而产生噪声。因此，本文提出了一个两阶段框架，以校准不准确的匹配结果。第一阶段匹配的关键点被视为相似性感知的位置提议，第二阶段模型学习提取相关特征以校准初始提议。本文采用了一个定制的transformer模型，即CapeFormer，用于实现该框架。 
- (3):CapeFormer模型的第一阶段采用transformer编码器对支持关键点和查询图像进行编码，以提高表示质量。第二阶段生成相似性感知的位置提议，直接使用可学习的内积将支持关键点与查询图像特征进行匹配。在第二阶段，位置提议作为查询被打包到transformer解码器中，通过交叉注意力进行细化。 
- (4):在MP-100数据集上的实验结果表明，CapeFormer模型在1-shot和5-shot设置下的性能均优于之前的最佳方法POMNet，分别提高了5.6%和8.6%。当训练和测试类别几乎没有共同属性时，与POMNet相比，可以获得更显著的提高（高达10.5%）。
#### 7. 方法详细介绍：
本文提出了一种名为CapeFormer的两阶段框架，用于类别无关的姿态估计。第一阶段使用查询-支持联合精炼编码器生成相似度感知的提议，第二阶段使用提议精炼解码器对提议进行细化。相似度损失和偏移损失被用作监督信号，以促进表示和提议细化的学习。该方法使用ResNet-50作为骨干网络，并使用Adam优化器进行200个epoch的训练，批量大小为16。使用MP-100数据集进行训练和评估，使用正确关键点的概率（PCK）作为定量指标。

#### 8. 实验设置：
实验在MP-100数据集上进行，该数据集包含来自100个不同类别的超过18k张图像。样本被组织成五个拆分，训练、验证和测试类别没有重叠。模型使用批量大小为16进行200个epoch的训练，学习率设置为1e-5。为了公平比较，数据增强和预处理与POMNet相同。

#### 9. 实验结果和分析：
CapeFormer在1-shot和5-shot设置下在所有拆分上都取得了最佳PCK，优于之前最好的CAPE方法POMNet。该方法在1-shot设置下将平均PCK显着提高了5.6％，在5-shot设置下提高了8.6％。交叉超类别评估表明，CapeFormer在人体和车辆类别上实现了最先进的性能。消融研究证明了第二阶段和提议支持联合精炼编码器的有效性。


# Paper:547     OCTET: 基于物体的反事实解释



#### 1. Title: 
OCTET: Object-aware Counterfactual Explanations

#### 2. Authors: 
Mehdi Zemni, Mickaël Chen, Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, Matthieu Cord

#### 3. Affiliation: 
第一作者机构：Valeo.ai，法国巴黎

#### 4. Keywords: 
Counterfactual explanations, deep learning, autonomous driving, object-centric framework, generative modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zemni_OCTET_Object-Aware_Counterfactual_Explanations_CVPR_2021_paper.html  Github: https://github.com/valeoai/OCTET

#### 6. Summary : 
- (1):本文研究深度视觉模型的可解释性问题，提出了一种针对多物体场景的反事实解释方法，旨在找到最小且可解释的输入图像变化，以改变模型的输出结果，从而帮助用户理解模型的决策过程。
- (2):过去的反事实解释方法难以解释多物体场景的决策模型，本文提出的方法通过基于物体的框架和生成模型，将查询图像编码为一个结构化的潜在空间，使用户能够控制搜索方向，从而更好地解释模型的决策过程。
- (3):本文提出的方法是一种基于物体的反事实解释生成框架，通过生成模型将查询图像编码为潜在空间，然后在潜在空间中进行最小化修改，以生成反事实解释。该方法可以独立地评估场景中每个物体的贡献，并寻找与它们的位置、样式或两者的组合相关的解释。本文的创新点在于提出了一种针对多物体场景的反事实解释方法，能够更好地解释模型的决策过程。
- (4):本文在自动驾驶场景下进行了一系列实验，证明了所提出方法的有效性。实验结果表明，本文提出的方法可以生成高质量的反事实解释，并且能够帮助用户理解模型的决策过程。本文的方法在自动驾驶决策模型上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为OCTET的方法，用于生成面向对象的反事实解释。该方法使用了一种生成骨干网络（BlobGAN），该网络在BDD100k数据集上进行训练，具有40个blob。该方法的主要变体有两种：OCTET，其中同时优化了空间和样式特征；OCTET仅优化样式向量，而将空间向量保持与查询相同。该方法使用了一种组合式生成骨干网络，允许可编辑性、插入性和可移除性，并且使用了一种反演方法来获得与查询图像对应的潜在代码。该方法还使用了一种距离项，用于保持查询图像和解释图像之间的相似性。最后，该方法使用了一个优化问题来生成解释图像，该问题的目标是最小化损失函数，其中包括决策损失和距离损失。

#### 8. 实验设置：
本文使用了BDD100k数据集和BDD-OIA数据集，其中BDD-OIA数据集包含了四个标签：向左转、向右转、前进和停止。本文使用了一个基于BDD-OIA数据集训练的分类器作为决策模型，并将OCTET方法与STEEX方法进行了比较。

#### 9. 实验结果和分析：
本文的实验结果表明，OCTET方法可以生成更好的视觉质量的解释图像，并且可以更好地保持查询图像的特征。此外，用户研究表明，使用OCTET方法生成的解释图像可以帮助用户更好地理解决策模型。


# Paper:548     利用虚拟图像进行训练的渐进式转换学习



#### 1. Title: 
Progressive Transformation Learning for Leveraging Virtual Images in Training

#### 2. Authors: 
Yi-Ting Shen, Hyungtae Lee, Heesung Kwon, Shuvra S. Bhattacharyya

#### 3. Affiliation: 
Yi-Ting Shen: University of Maryland, College Park

#### 4. Keywords: 
UAV-based images, object detection, virtual images, Progressive Transformation Learning (PTL), domain gap

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shen_Progressive_Transformation_Learning_for_Leveraging_Virtual_Images_in_Training_CVPR_2021_paper.html  Github: https://gitlab.umiacs.umd.edu/dspcad/ptl-release

#### 6. Summary : 
- (1):本文研究的背景是如何有效地利用虚拟图像来训练目标检测模型，特别是在无人机图像中检测人类目标时，需要大规模的数据集来训练模型。

- (2):过去的方法主要是使用真实图像进行训练，但是真实图像的获取成本高昂，而且难以获取到包含各种姿势和视角的人类目标图像。本文提出了一种新的方法，即渐进式转换学习（PTL），通过逐步添加具有增强现实感的虚拟图像来扩充训练数据集。与传统的虚拟图像转换方法不同，PTL通过逐步迭代来处理真实图像和虚拟图像之间的领域差异，从而提高了模型的性能。

- (3):本文提出的PTL方法包括三个步骤：转换候选项选择、虚拟到真实的转换和数据集更新。其中，为了准确地量化领域差距，本文提出了一种新的方法，即使用目标检测器的特征表示空间来建模领域差距。实验结果表明，PTL方法在小数据和跨域情况下都能显著提高模型的性能。

- (4):本文的方法在UAV视角下的人类目标检测任务中取得了很好的性能，尤其是在小数据和跨域情况下。实验结果表明，PTL方法可以有效地利用虚拟图像来扩充训练数据集，从而提高模型的性能。
#### 7. 方法详细介绍：
本文提出了一种渐进式转换学习（Progressive Transformation Learning，PTL）方法，通过将虚拟图像转换为真实图像的形式来扩充训练集。该方法使用渐进式学习，以逐步添加转换后的虚拟图像到训练数据集中，同时避免转换质量的降低。该方法使用检测器的表示空间来测量真实图像和虚拟图像之间的领域差距，该表示空间是通过学习得到的，使得检测器在表示空间中具有不同属性的两个样本之间的距离较远。PTL方法包括三个步骤：1）从虚拟图像池中选择一部分虚拟图像作为转换候选集，根据其与当前训练集的相似度给予更高的权重；2）将所选虚拟图像转换为真实图像；3）将转换后的虚拟图像添加到训练集中，同时从虚拟图像池中排除所选虚拟图像。最终的人体检测模型可以通过在最终的真实图像集上进行训练来获得。

#### 8. 实验设置：
本文在三个真实无人机数据集（VisDrone、Okutama-Action和ICG）和一个虚拟数据集（Archangel）上进行了实验。评估指标为在IoU阈值为0.5时的平均精度（mean average precision，mAP）。

#### 9. 实验结果和分析：
本文的实验结果表明，PTL方法在所有三个数据集上的人体检测准确率均显著优于其他方法。在低样本检测情况下，PTL方法的准确率迅速提高，直到第三次迭代后不再显著变化。在AP@[.5:.95]指标上，Okutama-Action和ICG数据集上的准确率在第三次迭代后仍然持续提高。这表明，使用大领域差距的虚拟图像通过使用大τ添加到训练集中，当训练集和测试集在同一领域时，对准确性没有显著影响，甚至会产生负面影响。


# Paper:549     多个标注下的宝藏：一种不确定性感知的边缘检测器



#### 1. Title: 
The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector

#### 2. Authors: 
Caixia Zhou, Yaping Huang, Mengyang Pu, Qingji Guan, Li Huang, Haibin Ling

#### 3. Affiliation: 
北京交通大学交通数据分析与挖掘北京市重点实验室

#### 4. Keywords: 
Edge detection, uncertainty, multiple annotations, Gaussian distribution, adaptive weighting loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_The_Treasure_Beneath_Multiple_Annotations_An_Uncertainty-Aware_Edge_Detector_CVPR_2021_paper.html  Github: https://github.com/ZhouCX117/UAED

#### 6. Summary : 
- (1):本文研究的是边缘检测这一基础的低级视觉任务，旨在解决多个标注者提供的标注之间的主观性和不确定性问题。
 
- (2):现有的方法通常使用简单的投票过程来融合多个标注，忽略了边缘的固有模糊性和标注者的标注偏差。本文提出了一种新颖的不确定性感知边缘检测器（UAED），它利用不确定性来研究不同标注之间的主观性和模糊性。具体来说，本文将确定性标签空间转换为可学习的高斯分布，其方差测量不同标注之间的模糊程度。然后将学习到的方差视为预测边缘图像的估计不确定性，具有更高不确定性的像素往往是边缘检测的难样本。因此，本文设计了一种自适应加权损失，以强调从具有高不确定性的像素中学习，帮助网络逐渐集中于重要像素。UAED可以与各种编码器-解码器骨干结构相结合，广泛的实验表明，UAED在多个边缘检测基准上始终实现了卓越的性能。

- (3):本文提出了一种新颖的不确定性感知边缘检测器（UAED），它将确定性标签空间转换为可学习的高斯分布，利用学习到的方差来衡量标注的模糊性和不确定性，并设计了一种自适应加权损失，以强调从具有高不确定性的像素中学习。UAED可以与各种编码器-解码器骨干结构相结合，实验结果表明，UAED在多个边缘检测基准上始终实现了卓越的性能。

- (4):本文在多个边缘检测基准上进行了实验，结果表明，与现有方法相比，本文提出的UAED方法在边缘检测任务中取得了更好的性能。UAED方法可以有效地处理多个标注者提供的标注之间的主观性和不确定性问题，提高了边缘检测的准确性和鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种基于不确定性的边缘检测方法（UAED）。该方法将确定性标签空间转换为可学习的高斯分布，其方差测量不同注释之间的模糊程度。学习到的方差被视为预测边缘图的估计不确定性，具有更高不确定性的像素可能是边缘检测的难样本。设计了自适应加权损失，以强调从具有高不确定性的像素中学习，有助于网络逐渐集中于重要像素。UAED可以与各种编码器-解码器骨干网络相结合，提高性能。

具体步骤如下：
1. 使用编码器提取多尺度特征。
2. 使用两个独立的解码器和预测头获得学习高斯分布的均值和方差。
3. 从标签集中计算的方差监督学习到的方差。
4. 最后，从分布中采样作为预测，由从标签集中随机采样的注释进行监督。

#### 8. 实验设置：
本文在BSDS500和Multicue两个流行的边缘检测数据集上进行了实验，这些数据集包含每个图像的多个注释。数据增强遵循LPCB，将每个图像旋转25个不同的角度并选择最大矩形。然后，在每个角度上将每个图像翻转（水平，垂直和两者的组合）。使用包含10,103个图像的PASCAL VOC Context数据集作为额外的训练数据，其边缘注释是通过拉普拉斯检测器从语义掩码中获得的。

#### 9. 实验结果和分析：
本文提出的不确定性感知边缘检测器（UAED）在BSDS和Multicue数据集上取得了与基于Transformer的EDTER模型相当的性能，同时消耗相对较少的时间和资源。实验表明，UAED可以通过探索多个注释之间的不确定性带来一致的改进。然而，该方法仍需要耗费大量的像素级注释，利用更少的注释实现竞争性结果仍然是一个开放的问题。


# Paper:550     从有噪声标签的数据中学习



#### 1. Title: 
Learning to Learn from Noisy Labeled Data

#### 2. Authors: 
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj

#### 3. Affiliation: 
清华大学

#### 4. Keywords: 
Noisy labels, Learning to learn, Deep learning, Convolutional neural networks

#### 5. Paper: https://openaccess.thecvf.com/content_cvpr_2019/html/Liu_Learning_to_Learn_From_Noisy_Labeled_Data_CVPR_2019_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度学习中的噪声标签问题，即数据集中存在错误标注的情况，这会对模型的训练和泛化能力造成影响。
 
- (2):过去的方法主要是通过数据清洗或使用一些特殊的损失函数来解决噪声标签问题，但这些方法存在一些问题，如需要额外的标注数据或对噪声标签的分布做出假设。本文提出的方法是通过学习来解决噪声标签问题，具有更好的泛化能力和鲁棒性。

- (3):本文提出了一种基于元学习的方法，即学习如何学习来处理噪声标签问题。具体地，作者提出了一个元学习框架，通过在元训练中学习如何在噪声标签数据上训练模型，从而使得在元测试中的模型能够更好地适应新的噪声标签数据。本文的创新点在于将元学习应用于噪声标签问题，并且提出了一种新的元学习框架。

- (4):本文在多个数据集上进行了实验，结果表明本文提出的方法在噪声标签数据上具有更好的泛化能力和鲁棒性，相比于其他方法有更好的表现。
#### 7. 方法详细介绍：
本文提出了一种新的方法来解决问题。该方法包括以下步骤：
(1). xxx
(2). xxx
(3). xxx
(4). xxx
(5). xxx

#### 8. 实验设置：
我们使用了xxx数据集进行实验，并使用了xxx软件进行实验。我们设置了以下参数：
(1). xxx
(2). xxx
(3). xxx

#### 9. 实验结果与分析：
我们的方法在xxx指标上取得了优异的表现，比现有方法提高了xx%。我们还进行了详细的分析，发现我们的方法在xxx方面具有优势，但在xxx方面还有待改进。


# Paper:551     NeuralLift-360：将野外2D照片提升为具有360°视角的3D物体



#### 1. Title: 
NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views

#### 2. Authors: 
Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, Zhangyang Wang

#### 3. Affiliation: 
Dejia Xu: 德克萨斯大学奥斯汀分校VITA组
Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, Zhangyang Wang: 德克萨斯大学奥斯汀分校

#### 4. Keywords: 
3D content generation, Neural Radiance Fields, monocular depth estimator, diffusion model, view synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_NeuralLift-360_Lifting_An_In-the-Wild_2D_Photo_to_A_3D_Object_CVPR_2021_paper.html  Github: https://github.com/VITA-Group/NeuralLift-360

#### 6. Summary : 
- (1):本文研究了将单张图片转化为3D物体的挑战性任务，提出了一种新的框架NeuralLift-360，可以生成与给定参考图像相对应的具有360°视角的逼真3D物体。这项技术为3D艺术家和XR设计师提供了更加便捷的工作流程。
 
- (2):现有的3D内容生成方法需要大量的人工专家工作，或者需要大规模的数据集进行训练，且在测试图像不在分布范围内时性能会急剧下降。本文提出的方法可以在只有单张图片的情况下生成逼真的3D物体，且不需要大规模的数据集。与现有方法相比，本文提出的方法可以生成更多的视角，且不需要高质量的深度图作为几何指导。
 
- (3):本文提出了一种新的框架NeuralLift-360，它利用了深度感知的神经辐射表示（NeRF）和学习使用去噪扩散模型的场景。通过引入排名损失，我们的NeuralLift-360可以在野外进行粗略深度估计的指导。我们还采用了CLIP引导采样策略，以提供连贯的指导。实验结果表明，我们的NeuralLift-360明显优于现有的最先进的基线方法。
 
- (4):本文的方法可以将单张图片转化为具有360°视角的逼真3D物体，且在多个数据集上的实验结果表明，我们的方法在视角重建和几何重建方面都取得了优异的性能。
#### 7. 方法详细介绍：
本文提出了一种名为NeuralLift-360的方法，可以从单张图像中重建3D物体。该方法结合了NeRF和扩散模型，用于建模3D场景并虚拟未见视角。训练框架由两部分组成：从生成模型中提取的先验蒸馏损失和给定图像上的直接损失。该方法还引入了一种域自适应技术，以适应野外图像中的扩散先验。NeuralLift-360的训练目标使用贝叶斯规则和相机姿态作为潜在变量推导而来。损失函数使用扩散模型和混合参考引导实现，其中混合参考引导使用参考和非参考损失的混合来指导扩散过程。

具体步骤如下：
1. 使用NeRF建立深度感知的神经辐射表示。
2. 使用扩散模型学习场景，并使用CLIP引导采样策略提供连贯的指导。
3. 使用深度排序损失来减轻野外深度估计中的几何误差。

#### 8. 实验设置：
本文使用了两个数据集进行实验：ShapeNet和LLFF。在ShapeNet数据集上，使用了10个类别的物体，每个类别使用了24个视角。在LLFF数据集上，使用了4个场景，每个场景使用了200个视角。实验使用了Nvidia V100 GPU进行训练和测试，并使用了PyTorch框架实现。

#### 9. 实验结果和分析：
本文的实验结果表明，NeuralLift-360方法在重建3D物体方面表现出色，与其他方法相比具有更高的精度和更好的视觉效果。在ShapeNet数据集上，本文的方法在PSNR和SSIM指标上分别达到了29.5和0.92的高分数。在LLFF数据集上，本文的方法在PSNR和SSIM指标上分别达到了28.5和0.91的高分数。此外，本文还进行了消融实验，证明了所提出的方法的有效性。


# Paper:552     云-设备协作连续适应真实世界中不断变化的环境



#### 1. Title: 
Cloud-Device Collaborative Adaptation to Continual Changing Environments in the Real-world

#### 2. Authors: 
Yulu Gan, Mingjie Pan, Rongyu Zhang, Zijian Ling, Lingran Zhao, Jiaming Liu, Shanghang Zhang

#### 3. Affiliation: 
Yulu Gan, Mingjie Pan, Lingran Zhao, Jiaming Liu, Shanghang Zhang: School of Computer Science, Peking University, China

#### 4. Keywords: 
Continual domain adaptation, Cloud-Device Collaboration, Uncertainty Guided Sampling, Visual Prompt Learning Strategy

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gan_Cloud-Device_Collaborative_Adaptation_to_Continual_Changing_Environments_in_the_Real-World_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了在真实世界中不断变化的环境下，轻量级设备上的模型在分布转移下性能严重下降的问题。 
- (2):过去的方法主要集中在测试时间适应和云端-设备协作上，但是这些方法没有考虑到真实世界中不断变化的环境。本文提出了一种新的学习范式，即云-设备协作连续适应，旨在通过云端和设备之间的协作来提高设备模型的连续领域适应能力。同时，本文提出了一种基于不确定性的视觉提示适应的师生模型，将云端大模型的泛化能力转移到设备模型上。 
- (3):本文提出了一种基于不确定性引导采样和视觉提示学习策略的云-设备协作连续适应方法。具体来说，我们首先设计了不确定性引导采样来筛选出具有挑战性的数据，并将最具代表性的样本从设备传输到云端。然后，我们提出了一种基于视觉提示学习策略的师生模型，通过将视觉提示传输到设备并将其与输入数据连接，将设备测试分布拉近到云端训练分布。 
- (4):本文在两个目标检测数据集上进行了广泛的实验，结果表明，我们的方法在连续分布转移数据上优于现有的方法，并且在设备-云协作方面表现出了强大的能力。
#### 7. 方法详细介绍：
本文提出了一种云设备协同的持续自适应方法，旨在将大型云模型的泛化能力转移至轻量级设备模型。该方法包括两个主要策略：不确定性引导采样（UGS）和带不确定性引导更新的视觉提示学习策略（VPLU）。UGS旨在筛选出最具环境特异性的样本，减少与传输整个序列相比所需的带宽。VPLU旨在对齐源域和目标域的数据分布，并将大型教师模型的表示转移至轻量级学生模型。然后将轻量级学生模型和视觉提示传递到设备上，从而缓解真实世界中不断变化的场景。该方法在两个目标检测数据集上进行了评估，结果表明，所提出的U-VPA教师-学生框架优于以前的最先进的测试时间自适应和设备-云协作方法。

#### 8. 实验设置：
本文在两个测试时间自适应基准任务上评估了所提出的方法，分别是Cityscapes-to-Cityscapes-C和Cityscapes-to-ACDC-Detection数据集。Cityscapes-C数据集包含五种与自动驾驶场景相关的污染，而ACDC-Detection数据集则在各种不利的视觉条件下收集。预训练模型使用SGD优化器进行训练，学习率为1e-2。实验设置包括每个数据集中的图像数量、应用的特定污染以及重复相同序列组的次数，以评估所提出方法的连续泛化能力。

#### 9. 实验结果与分析：
本文使用PyTorch进行实验。Cityscapes-to-Cityscapes-C和Cityscapes-to-ACDC-Detection任务的批量大小均设置为8。预训练模型在源域（Cityscapes）上进行训练，遵循[3]中的方法。不确定性估计使用10个样本。云端的大型教师模型是具有ResNet-101骨干网络的FasterRCNN，而轻量级学生模型是具有ResNet-18骨干网络的FasterRCNN。UGS模块的不确定性阈值设置为0.008，VPA模块的视觉提示设置为3x200x200。评估指标为mAP@0.5（%）。实验结果表明，在目标检测任务中，所提出的U-VPA教师-学生框架的mAP@0.5提高了4.4-13.6%。未来的研究需要在具有特定带宽限制、特定云和特定设备的实际系统上测试该方法。


# Paper:553     基于视觉Transformer的内容感知令牌共享用于高效语义分割



#### 1. Title: 
Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers

#### 2. Authors: 
Chenyang Lu, Daan de Geus, Gijs Dubbelman

#### 3. Affiliation: 
Eindhoven University of Technology（荷兰埃因霍温科技大学）

#### 4. Keywords: 
Semantic Segmentation, Vision Transformers, Token Reduction, Content-aware Token Sharing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2022_paper.html  Github: https://tue-mps.github.io/CTS/

#### 6. Summary : 
- (1):本文研究背景是如何提高基于Vision Transformers（ViTs）的语义分割网络的计算效率。
 
- (2):过去的方法主要是通过减少或合并处理的token数量来提高效率，但这些方法并不适用于语义分割任务。本文提出了一种新的方法，即Content-aware Token Sharing（CTS），通过让语义相似的相邻图像块共享一个token来减少token数量，从而提高效率。本文的方法不需要修改ViT的架构或训练策略，因此与所有使用全局自注意力的backbones兼容，同时也适用于任何语义分割解码器或高级预训练策略。本文的方法通过一个高效的CNN模型来解决共享token的策略问题，该模型只会对大型ViTs应用于高分辨率图像时引入微小的计算开销。 
 
- (3):本文提出了一种新的方法Content-aware Token Sharing（CTS），通过让语义相似的相邻图像块共享一个token来减少token数量，从而提高基于Vision Transformers（ViTs）的语义分割网络的计算效率。本文的方法不需要修改ViT的架构或训练策略，因此与所有使用全局自注意力的backbones兼容，同时也适用于任何语义分割解码器或高级预训练策略。本文的方法通过一个高效的CNN模型来解决共享token的策略问题，该模型只会对大型ViTs应用于高分辨率图像时引入微小的计算开销。 
 
- (4):本文在ADE20K、Pascal Context和Cityscapes数据集上进行了实验，证明了CTS方法的有效性。本文的方法可以将处理的token数量降低44％，而不会降低分割质量。
#### 7. 方法详细介绍：
本文提出了一种基于内容感知的令牌共享(Content-aware Token Sharing, CTS)方法，用于提高ViT模型在语义分割任务中的效率。该方法通过使用一个策略网络来减少令牌数量，该网络在任何令牌进入ViT之前就能够确定哪些图像块可以共享一个令牌。策略网络是与ViT分开训练的，它预测2x2相邻块是否包含相同的类别，如果是，则它们可以共享一个令牌。然后，将减少的令牌集合输入ViT，使用这些令牌进行语义分割预测。该方法适用于所有仅使用全局自注意力的骨干网络以及任何语义分割解码器或高级预训练策略。

具体步骤如下：
1. 将原始图像块映射到具有相应位置嵌入的新块集合中。
2. 预测哪些超级块只包含一个语义类别，让这些2x2相邻块共享同一个令牌。
3. 骨干网络生成一组带有语义信息的预测令牌，然后通过解码器获得分割预测。
4. 策略模型预测超级块是否只包含一个语义类别，而不是显式预测类别，并使用可用的语义分割注释生成的真值进行训练。

#### 8. 实验设置：
本文在ADE20K、Pascal Context和Cityscapes数据集上进行了实验，以评估所提出的CTS方法的有效性。实验使用了各种ViT骨干网络和不同的分割解码器。还提供了数据集统计信息，显示了包含单个语义类别的超级块的百分比。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的CTS方法可以将处理的令牌总数至少减少30％，而不会降低分割质量。该方法适用于多个数据集和不同大小的变压器骨干网络，使用不同的预训练权重初始化，并使用各种分割解码器。结果表明，CTS方法在不降低分割质量的情况下实现了显着的效率提升。在ADE20K数据集上，CTS方法的吞吐量提高了64％，在Pascal Context和Cityscapes数据集上，分别提高了48％和110％。


# Paper:554     从图像域学习自适应稠密事件立体视觉



#### 1. Title: 
Learning Adaptive Dense Event Stereo from the Image Domain

#### 2. Authors: 
Hoonhee Cho, Jegyeong Cho, and Kuk-Jin Yoon

#### 3. Affiliation: 
KAIST（韩国科学技术院）Visual Intelligence Lab.

#### 4. Keywords: 
Event-based stereo matching, unsupervised domain adaptation, feature normalization, motion-invariant consistency module.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cho_Learning_Adaptive_Dense_Event_Stereo_From_the_Image_Domain_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是事件相机在低光条件下具有鲁棒性，但是现有的事件立体匹配网络在领域转移时性能严重下降。

- (2):过去的方法主要是基于帧的立体匹配，但是在领域转移时存在性能下降的问题。本文提出了一种新的无监督领域自适应（UDA）方法，将具有地面真实值的图像数据集用于源域，将没有地面真实值的事件数据集用于目标域，以解决领域转移的问题。本文的方法主要包括三个组件：自监督模块、特征归一化和运动不变一致性模块。

- (3):本文提出的自监督模块通过图像重建在事件目标域上训练网络，同时在源图像域上训练的伪影预测网络帮助消除重建图像中的间歇性伪影。其次，我们利用特征级归一化方案沿极线对齐提取的特征。最后，我们提出了运动不变一致性模块，以在扰动运动之间施加一致的输出。本文的方法在事件立体匹配领域自适应方面取得了显著的结果。

- (4):本文的方法在事件立体匹配任务上取得了优异的性能，证明了其在领域转移方面的有效性。
#### 7. 方法详细介绍：
本文提出了一种自适应密集事件立体视觉框架，称为ADES。该框架由三个模块组成：事件立体网络、伪影预测模块和运动不变一致性模块。事件立体网络接收事件对并输出密集视差图。伪影预测模块预测视差图中伪影的概率。运动不变一致性模块确保原始输入和运动扰动输入之间的像素级一致性。总损失定义为任务损失、伪影预测损失、重构损失和一致性损失的组合。

#### 8. 实验设置：
本文使用KITTI数据集、SceneFlow数据集和DSEC数据集进行评估。KITTI数据集包含394个带有稀疏地面真实值的立体图像用于训练。SceneFlow数据集提供了35k个带有密集地面真实值的立体图像。DSEC数据集提供了在室外驾驶场景中捕获的高分辨率立体事件流。每个损失项的权重分别设置为0.3、1和0.2。使用Adam优化器，学习率为1×10−3，批量大小为8，使用384×336的随机裁剪。

#### 9. 实验结果与分析：
本文提出的ADES框架在3D和4D成本体积网络上均取得了显著的性能，无论源域是合成数据集还是真实世界图像数据集。结果表明，我们的模块可以普遍应用于现有的事件立体网络中。我们的方法在DSEC基准测试集上的性能与其他有监督方法相当，尽管不依赖于目标域的地面真实值。与其他方法的定性比较表明，我们的方法可以预测准确、清晰的视差图。消融研究验证了每个组件的有效性，伪影预测有助于网络稳健地推断物体的边界，从而得到清晰的视差图。本文提出的特征归一化方法对事件立体匹配的域自适应能力比现有的无学习归一化方法更有效。


# Paper:555     促进语义连通性：双最近邻对比学习用于无监督域泛化



#### 1. Title: 
Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization

#### 2. Authors: 
Yuchen Liu, Yaoming Wang, Yabo Chen, Wenrui Dai, Chenglin Li, Junni Zou, and Hongkai Xiong

#### 3. Affiliation: 
第一作者：上海交通大学电子工程系

#### 4. Keywords: 
Unsupervised Domain Generalization, Contrastive Learning, Semantic Connectivity, Dual Nearest Neighbors, Strong Augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了无监督域泛化问题，即如何从不同领域的数据中学习到具有泛化能力的特征表示。由于标注数据通常昂贵且不易获得，因此本文研究了更实际的无监督域泛化问题。

- (2):目前的域泛化方法主要依赖于标注的源域数据，而本文研究了更实际的无监督域泛化问题。最近的无监督学习方法倾向于使用对比学习，即通过对同一图像的不同变换之间的相似性进行约束来学习语义表示。然而，大多数对比学习方法都是为i.i.d.数据集设计的，难以适应无监督域泛化问题。本文提出了语义连通性的概念，即抑制域内连通性并鼓励类内连通性有助于学习域不变的语义信息。本文提出了一种新的无监督域泛化方法，即具有强增强的双最近邻对比学习（DN2A），该方法利用强增强来抑制域内连通性，并提出了一种新的双最近邻搜索策略，以找到可信的跨域邻居和域内邻居，以鼓励类内连通性。

- (3):本文提出了一种新的无监督域泛化方法，即具有强增强的双最近邻对比学习（DN2A），该方法利用强增强来抑制域内连通性，并提出了一种新的双最近邻搜索策略，以找到可信的跨域邻居和域内邻居，以鼓励类内连通性。本文的贡献包括：提出了语义连通性的概念，提出了一种新的无监督域泛化方法，即DN2A，该方法利用强增强来抑制域内连通性，并提出了一种新的双最近邻搜索策略，以找到可信的跨域邻居和域内邻居，以鼓励类内连通性。

- (4):本文在PACS和DomainNet数据集上进行了实验，结果表明，与现有方法相比，本文的方法在线性评估上取得了12.01％和13.11％的准确率提高，仅使用1％的标签。此外，本文的方法在使用不到ImageNet数据集4％的情况下，就能够超越ImageNet预训练，为解决域泛化问题提供了一种有前途的方法。
#### 7. 方法详细介绍：
本文提出了一种名为“Dual Nearest Neighbors Contrastive Learning with Strong Augmentation”（DN2A）的无监督领域泛化方法。该方法包括两个步骤：通过强数据增强破坏领域内连通性，通过双重最近邻构建领域内类别连通性。强数据增强包括14种类型的增强，双重最近邻包括跨领域双重锁定最近邻和领域内循环最近邻。跨领域双重锁定最近邻利用领域内邻居和跨领域邻居来提高跨领域邻居的准确性，而领域内循环最近邻使用跨领域邻居作为中介来寻找更多不同的领域内邻居。该方法旨在促进语义连通性，学习领域不变的语义特征，以获得更好的领域泛化性能。

#### 8. 实验设置：
本文在PACS数据集上进行实验，该数据集包含四个领域：照片、艺术、卡通和素描。实验采用无监督领域泛化设置，其中模型在三个领域上进行训练，在剩余的一个领域上进行测试。评估指标是所有领域的平均分类准确率。

#### 9. 实验结果和分析：
本文将提出的DN2A方法与ERM基线和MoCo V2方法进行比较。结果表明，DN2A方法在PACS数据集上取得了最佳性能，比ERM基线和MoCo V2方法大幅优越。具体而言，DN2A方法的平均分类准确率为61.05％，而ERM基线和MoCo V2方法的分类准确率分别为13.82％和21.87％。


# Paper:556     相移编码器：面向对象检测中准确预测方向的方法



#### 1. Title: 
Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection

#### 2. Authors: 
Yi Yu, Feipeng Da

#### 3. Affiliation: 
第一作者：东南大学自动化学院，江苏省复杂系统测量与控制重点实验室

#### 4. Keywords: 
Oriented object detection, phase-shifting coder, boundary discontinuity, square-like problem, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Phase-Shifting_Coder_Predicting_Accurate_Orientation_in_Oriented_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/open-mmlab/mmrotate

#### 6. Summary : 
- (1):本文研究的背景是面向对象检测中的角度回归问题，旨在解决边界不连续和类似正方形的问题。
 
- (2):过去的方法主要是在水平对象检测的基础上添加一个输出通道来预测方向角度，但是这种方法面临边界不连续和类似正方形的问题。本文提出了一种新的可微分角度编码器——相移编码器（PSC），通过将不同周期的旋转周期性映射到不同频率的相位中，为面向对象检测中由旋转对称性引起的各种周期性模糊问题提供了一个统一的框架。在此框架下，边界不连续和类似正方形的问题都得到了优雅的解决。本文的方法是第一次将相移编码器应用于深度学习领域的角度回归问题，具有创新性和实用性。

- (3):本文提出的相移编码器通过将旋转周期性编码到周期相位中来解决边界不连续问题，同时通过混合不同频率的相位来解决类似正方形的问题。本文的方法在三个数据集上进行了实验，证明了其有效性和潜力。本文的方法在需要高质量边界框的场景下具有竞争性能。

- (4):本文的方法在三个数据集上进行了实验，结果表明，相较于其他方法，本文的方法在边界不连续和类似正方形的问题上具有更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为相移编码器（PSC）的新型可微角度编码器，用于准确预测物体的方向，并提出了增强的双频PSC（PSCD）来解决边界不连续和类似正方形的问题。PSC由编码、解码和映射过程组成，其中编码公式为xn = cos(φ + 2nπ/Nstep)，解码公式为φ = − arctan(∑n=1Nstep xn sin(2nπ/Nstep) / ∑n=1Nstep xn cos(2nπ/Nstep))。PSCD通过混合不同频率的相位来解决周期模糊问题。该方法被集成到深度神经网络中，并使用分类损失、框回归损失和角度回归损失进行训练。

#### 8. 实验设置：
本文在DOTA、HRSC和OCDPCB 1三个数据集上进行了实验。计算基础设施包括Intel i9-12900K CPU、Nvidia RTX3080 GPU、Windows 10操作系统、PyTorch 1.10.1、ultralytics/yolov5 6.0和MMRotate 0.3.2。高分辨率图像被分割成1024×1024的块，重叠200个像素进行训练，在推理期间，所有块的检测结果被合并以评估性能。HRSC中的图像被缩放到800×800进行训练和测试。本文使用了三种最先进的骨干网络进行实验：FCOS（无锚点）、RetinaNet（基于锚点）和YOLO（高效）。在DOTA数据集上设置了五个实验组，在HRSC或OCDPCB数据集上设置了两个实验组。每个组内的骨干网络和数据增强都相同，以确保公平比较。基于FCOS和RetinaNet的网络在DOTA上训练12个时期，在HRSC和OCDPCB上训练72个时期，而基于YOLO的组则训练120个时期。学习率最初设置为1e-3，最终降至1e-5。

#### 9. 实验结果和分析：
本文提出的PSC和PSCD方法在DOTA、HRSC和OCDPCB数据集上的表现优于现有的最先进方法。PSC和PSCD有效地解决了定向物体检测中的边界不连续和类似正方形的问题。与其他方法相比，PSC在mAP50方面与基于高斯分布的方法KLD相当，但在mAP75方面明显优于KLD。特别是，在DOTA数据集上，PSCD在mAP75方面比KLD平均高出2.36个百分点。当需要高质量的边界框时，PSC和PSCD预计将具有竞争力的性能。


# Paper:557     使用T形俄罗斯方块像素的图像超分辨率



#### 1. Title: 
Image Super-Resolution Using T-Tetromino Pixels

#### 2. Authors: 
Simon Grosche, Andy Regensky, J¨urgen Seiler, and Andr´e Kaup

#### 3. Affiliation: 
Friedrich-Alexander-Univerist¨at Erlangen-N¨urnberg, Chair of Multimedia Communications and Signal Processing（弗赖贝格-纽伦堡大学，多媒体通信与信号处理主席）

#### 4. Keywords: 
Image super-resolution, pixel binning, tetromino-shaped pixels, compressed sensing, locally fully connected reconstruction network

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Grosche_Image_Super-Resolution_Using_T-Tetromino_Pixels_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的背景是高分辨率成像传感器在低光照条件下和需要高帧率时进行像素合并，从而导致空间分辨率下降，需要进行单图像超分辨率技术进行放大恢复原始空间分辨率。
- (2):过去的方法包括经典的上采样算法和最近的神经网络，但是由于测量过程的规则性，单图像超分辨率存在固有的限制，会引入混叠伪影。为了解决这个问题，本文提出了一种新的像素布局概念，使用T形俄罗斯方块像素进行像素合并，以提高超分辨率后的图像质量。本文的方法在压缩感知框架下进行，计算了相干性以激励所使用的传感器布局。
- (3):本文提出了一种小型重复单元的T形俄罗斯方块像素布局，用于像素合并。本文使用局部全连接重建网络以及来自压缩感知领域的两种经典重建方法进行重建。本文的创新点在于使用小型重复单元的T形俄罗斯方块像素布局，以及使用局部全连接重建网络，实现了比传统单图像超分辨率更好的图像质量。
- (4):本文的方法在PSNR、SSIM和视觉上都优于传统单图像超分辨率方法，使用局部全连接重建网络和所提出的俄罗斯方块布局，PSNR提高了1.92 dB。本文的方法在超分辨率任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种使用T-tetromino像素进行图像超分辨率的方法。该方法涉及使用不同的传感器布局，包括先前研究中的tetromino传感器布局和两个提出的T-tetromino传感器布局。重建算法包括局部联合稀疏和字典学习（L-JSDE）、同时投影算法（SPL）以及使用本地完全连接重建（LFCR）网络和串联的类似VDSR的网络的数据驱动方法。LFCR网络是在与高分辨率参考的均方误差训练的，而VDSR-like网络则使用相同的损失函数进行训练。使用TECNICK和Urban100数据集对所提出的方法进行评估，并使用峰值信噪比（PSNR）和结构相似性指数（SSIM）作为比较指标。

#### 8. 实验设置：
无信息。

#### 9. 实验结果和分析：
本文将所提出的4x4 T-tetromino传感器布局与最先进的神经网络进行了比较。结果表明，使用所提出的传感器布局可以更好地重建具有强烈混淆伪影的区域。平均PSNR和SSIM结果也表明，所提出的传感器布局优于传统的方形像素布局。LFCR+VDSR网络与所提出的传感器布局相结合，实现了最高的重建质量。运行时间分析表明，LFCR（仅）是神经网络中最快的，同时在PSNR方面仍然优于VDSR。


# Paper:558     掩蔽图像是鲁棒Fine-tuning的反事实样本



#### 1. Title: 
Masked Images Are Counterfactual Samples for Robust Fine-tuning

#### 2. Authors: 
Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, Liang Lin

#### 3. Affiliation: 
Sun Yat-sen University (中山大学)

#### 4. Keywords: 
Deep learning, fine-tuning, out-of-distribution robustness, causal analysis, counterfactual samples

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xiao_Masked_Images_Are_Counterfactual_Samples_for_Robust_Fine-Tuning_CVPR_2021_paper.html  Github: https://github.com/Coxy7/robust-finetuning

#### 6. Summary : 
- (1):本文研究深度学习模型在训练数据和测试数据分布不同的情况下的鲁棒性问题，提出了一种基于因果分析的fine-tuning方法，旨在解决fine-tuning过程中的in-distribution和out-of-distribution性能的平衡问题。
 
- (2):现有的方法虽然可以提高模型的鲁棒性，但并没有明确解决out-of-distribution鲁棒性问题。本文提出的方法使用掩蔽图像作为反事实样本，有助于提高fine-tuning模型的鲁棒性。本文的方法创新性地使用了因果分析的思想，通过掩蔽图像中的语义相关或语义无关区域来打破虚假相关性，并使用其他图像的区域来填充掩蔽区域，从而生成反事实样本。实验结果表明，使用掩蔽图像进行fine-tuning可以在in-distribution和out-of-distribution性能之间取得更好的平衡，超过了现有方法的out-of-distribution性能。 

- (3):本文提出的方法使用掩蔽图像作为反事实样本，有助于保留预训练模型的稳定和可推广的知识。具体而言，我们使用基于类激活图的掩蔽策略来掩蔽图像中的语义相关或语义无关区域，并使用其他图像的区域来填充掩蔽区域，生成反事实样本。然后，我们使用预训练模型的特征进行特征蒸馏，从而fine-tuning模型。 

- (4):本文的方法在fine-tuning CLIP模型上进行了实验，结果表明，与现有方法相比，我们的方法在各种out-of-distribution数据集上取得了更好的平均准确率，而不依赖于模型集成或权重约束。本文的方法可以有效地解决fine-tuning过程中的in-distribution和out-of-distribution性能的平衡问题。
#### 7. 方法详细介绍：
本文提出了一种fine-tuning方法，使用掩蔽图像作为反事实样本来提高fine-tuning模型的OOD鲁棒性。该方法通过特定的操作打破下游数据中的虚假相关性，明确要求fine-tuning模型区分语义和非语义因素。该方法使用结构因果模型（SCM）来建模跨不同领域的物体中心图像数据生成的基本机制。该方法涉及基于补丁的图像掩蔽，其中图像的某些区域被掩蔽或替换以操纵领域相关或语义表示。使用交叉熵损失和均方误差损失训练fine-tuning模型以模仿预训练模型的特征表示。

#### 8. 实验设置：
本文在多个数据集上进行了广泛的实验，包括CIFAR-10、CIFAR-100、ImageNet和几个OOD数据集。实验中使用的预训练模型是CLIP和ResNet-50。fine-tuning模型使用不同的掩蔽策略和补丁填充策略进行训练。

#### 9. 实验结果和分析：
本文的实验结果表明，使用掩蔽图像进行fine-tuning的正则化可以在ID和OOD性能之间取得更好的平衡，超过了以前的方法在OOD性能上的表现。实验表明，大多数策略适用于构建有助于改善fine-tuning鲁棒性的反事实样本，而掩蔽对象通常可以实现最佳鲁棒性。本文还发现，将零-shot模型和fine-tuning模型的权重空间集成几乎不会改善ID和OOD准确性之间的平衡，这与以前的观察结果相矛盾，并暗示了所提出的方法可能与传统的fine-tuning方法产生本质上不同的模型。


# Paper:559     3D-POP-一种自动注释方法，以便使用基于标记的运动捕捉跟踪自由移动的鸟类的无标记2D-3D跟踪



#### 1. Title: 
3D-POP - An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture

#### 2. Authors: 
Hemal Naik, Alex Hoi Hang Chan, Junran Yang, Mathilde Delacoux, Iain D. Couzin, Fumihiro Kano, M´at´e Nagy

#### 3. Affiliation: 
Hemal Naik, Alex Hoi Hang Chan, Iain D. Couzin: Max Planck Institute of Animal Behavior, Collective Behavior and Ecology of Animal Societies Department, Germany

#### 4. Keywords: 
markerless pose tracking, motion capture, 3D annotation, animal behavior, dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Naik_3D-POP_-_An_Automated_Annotation_Approach_to_Facilitate_Markerless_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是机器学习和计算机视觉技术的发展，使得研究人员能够在不需要标记的情况下跟踪自由移动的动物的姿态和位置。
- (2):过去的方法主要是通过手动注释2D图像来跟踪动物的姿态，但是这种方法的局限性在于只能在2D空间内进行跟踪，而且对于多个动物的跟踪也存在困难。本文提出了一种使用运动捕捉系统来获取动物运动和姿态（2D和3D）的大量注释数据的方法，该方法可以在参考动物身上的标记位置的基础上提取形态关键点（例如眼睛、喙、尾巴）的3D位置。这种方法可以在相对较短的时间内生成大量的训练数据，并且可以跟踪多个自由移动的动物。
- (3):本文提出的方法是使用运动捕捉系统来获取大量的注释数据，包括2D和3D姿态、2D和3D运动轨迹和身份信息。本文的创新之处在于提取了形态关键点的3D位置，而不是直接在动物身上标记形态关键点。作者使用这种方法创建了一个新的数据集3D-POP，其中包含约300k帧（4百万个实例）的注释数据，可以用于解决2D到3D无标记姿态、轨迹跟踪和鸟类识别等问题。
- (4):本文的方法在鸟类跟踪任务上取得了良好的性能，可以支持他们的目标。
#### 7. 方法详细介绍：
本文提出了一种半自动化的方法，使用运动捕捉系统来获取动物运动和姿态的大量注释数据（2D和3D）。该方法提取了动物头部和身体的形态关键点的3D位置，参考了附着在动物身上的标记的位置。该方法克服了需要在动物身体上附着反射标记的挑战，这些标记通常位于难以访问的形态关键点上。该方法使得可以在半自动化的方式下生成大量的训练数据，且需要的时间和人力成本较少。

具体步骤如下：
1. 在动物头部和身体上附着标记，使用运动捕捉系统记录动物的运动轨迹。
2. 提取标记的3D位置，将其转换到全局坐标系中。
3. 将3D位置转换到每个摄像机的坐标系中，并将其投影到图像空间中，以获得2D注释。
4. 从关键点投影中派生出用于目标检测或跟踪任务的边界框注释。
5. 应用于多个试验，以获得所有个体的关键点。

#### 8. 实验设置：
本文的实验使用了18只鸽子在一个3.6m x 4.2m的区域内自由移动，摆放了四个不同的摄像机视角。实验设置包括运动跟踪红外摄像机、RGB摄像机和每只鸽子的定义坐标系。鸽子配备了头部和背包标记，摄像机放置在鸟上方，以最小化遮挡。

#### 9. 实验结果和分析：
本文提出的3D-POP数据集提供了准确的关键点注释，包括2D和3D关键点、边界框和个体身份。该数据集包括来自四个高分辨率摄像机的RGB图像和多个群体大小（1、2、5、10）的18只不同鸽子的多个视角的视频记录，共计约300k帧（400万个实例）。使用该数据集训练的CNN模型能够预测没有标记的鸟的姿态。该数据集将有助于解决鸟类的2D到3D无标记姿态、轨迹跟踪和识别问题。


# Paper:560     扩大开放式动作识别的实例特异性和类特异性信息



#### 1. Title: 
Enlarging Instance-specific and Class-specific Information for Open-set Action Recognition

#### 2. Authors: 
Jun Cen, Shiwei Zhang, Xiang Wang, Yixuan Pei, Zhiwu Qing, Yingya Zhang, Qifeng Chen

#### 3. Affiliation: 
第一作者：香港科技大学

#### 4. Keywords: 
Open-set action recognition, feature representation, instance-specific information, class-specific information, prototypical similarity learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cen_Enlarging_Instance-Specific_and_Class-Specific_Information_for_Open-Set_Action_Recognition_CVPR_2021_paper.html  Github: https://github.com/Jun-CEN/PSL

#### 6. Summary : 
- (1):本文研究的是开放式动作识别问题，即拒绝训练集分布之外的未知人类动作案例。现有方法主要集中于学习更好的不确定性分数，但忽略了特征表示的重要性。本文发现，具有更丰富语义多样性的特征可以显著提高相同不确定性分数下的开放式性能。
- (2):现有方法主要集中于学习更好的不确定性分数，但忽略了特征表示的重要性。本文提出了一种新的视角，即基于信息瓶颈理论分析开放式识别任务的特征表示行为，并提出了一种新的框架，即原型相似性学习（PSL），以保留更多的实例特异性（IS）信息和类特异性（CS）信息。PSL通过引入视频洗牌来学习原始和洗牌样本之间的不同时间信息，从而扩大CS信息。本文的方法在多个基准测试中取得了最先进的结果。
- (3):本文提出了一种新的框架，即原型相似性学习（PSL），以保留更多的实例特异性（IS）信息和类特异性（CS）信息。PSL通过引入视频洗牌来学习原始和洗牌样本之间的不同时间信息，从而扩大CS信息。本文的方法在多个基准测试中取得了最先进的结果。
- (4):本文的方法在多个基准测试中取得了最先进的结果，证明了PSL可以显著提高开放式和封闭式性能。
#### 7. 方法详细介绍：
本文提出了一种名为原型相似性学习（Prototypical Similarity Learning，PSL）的方法，旨在扩大特征表示中的实例特定（Instance-Specific，IS）和类特定（Class-Specific，CS）信息，以提高开放式动作识别（Open-set Action Recognition，OSAR）的性能。PSL基于经典的原型学习（Prototype Learning，PL）损失函数，鼓励同一类别的特征表示完全相同，但保留类内方差以保留IS信息。为了解决PSL收敛到平凡解的问题，作者将小批量内不同样本之间的相似度引入到损失函数的分母中，直接约束样本特征之间的关系，而不仅仅是监督样本特征与其原型之间的相似度。修改后的损失函数被命名为带对比项的PSL（PSL with contrastive terms，CT）。PSL还引入了视频洗牌，以学习原始和洗牌样本之间的不同时间信息，从而扩大CS信息。

#### 8. 实验设置：
本文在三种不同的骨干网络（TSM、I3D和SlowFast）上进行实验，使用UCF101作为InD数据集进行训练和闭集评估，使用HMDB51和MiT-v2作为OoD数据集进行开放式评估。在评估过程中，移除InD和OoD数据集之间的重叠类别。使用LARS优化器进行训练，所有方法的批量大小均为256。

#### 9. 实验结果与分析：
PSL在HMDB51和MiT-v2数据集上使用TSM骨干网络实现了显著更好的开放式和闭集性能，优于所有基线方法。InD和OoD样本的不确定性分布表明，PSL显著缓解了基线方法的过度自信问题。与几种知名的度量学习方法相比，PSL实现了最佳的开放式性能。K400预训练证明了更丰富的语义表示对OSAR的重要性，因为它实现了更高的开放式性能。


# Paper:561     视频中的少样本关系指代问题



#### 1. Title: 
Few-Shot Referring Relationships in Videos

#### 2. Authors: 
Yogesh Kumar and Anand Mishra

#### 3. Affiliation: 
Yogesh Kumar: 印度理工学院Jodhpur
Anand Mishra: 印度理工学院Jodhpur

#### 4. Keywords: 
Few-shot learning, visual relationships, video understanding, spatiotemporal localization, support set videos

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2021_paper.html  Github: https://vl2g.github.io/projects/refRelations/

#### 6. Summary : 
- (1):本文研究视频中的少样本关系指代问题，即如何在只有少量支持集视频的情况下，定位与查询视觉关系相关的主体和客体。这是一个具有挑战性的问题，因为在实际应用中，为每个视觉关系组合进行大规模注释是不可行的。

- (2):过去的方法主要依赖于强监督，而本文提出了一种基于T-部分随机场的目标函数最小化方法，其中T是测试视频中的帧数。该方法包括帧级和视觉关系相似性潜力，并使用关系网络进行学习。本文的方法在两个公共基准测试中进行了广泛的实验，并与其他方法进行了比较。

- (3):本文提出了一种新的问题设置，即在只有少量支持集视频的情况下，学习定位与查询视觉关系相关的主体和客体。为了解决这个问题，本文提出了一种基于T-部分随机场的目标函数最小化方法，其中T是测试视频中的帧数。该方法包括帧级和视觉关系相似性潜力，并使用关系网络进行学习。本文的方法在两个公共基准测试中进行了广泛的实验，并与其他方法进行了比较。

- (4):本文的方法在两个公共基准测试中进行了广泛的实验，并与其他方法进行了比较。实验结果表明，本文的方法在少样本关系指代问题上取得了较好的性能，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种针对视频的少样本关系推理方法。首先，使用FasterRCNN获取测试视频中最有信心的物体边界框。然后，将视频表示为一系列提取的边界框，并构建一个T-部分随机场来解决关系推理任务。通过学习神经网络的可学习参数来解决优化问题，以获得最佳的主体和客体轨迹。定义了帧级和视觉关系相似性潜力，以确保在帧之间选择一致的视觉关系。本文还描述了基于注意力的聚合技术，包括全局语义聚合和局部定位聚合，以丰富主体和客体的表示。最后，关系嵌入被定义为低维关系空间中的翻译向量。

#### 8. 实验设置：
本文在两个公共基准数据集VidOR和ImageNet-VidVRD上进行了实验。数据集根据谓词和视频随机分配到训练集和测试集中。本文的方法在相同的训练集和测试集上进行训练，支持集大小为4个视频。

#### 9. 实验结果和分析：
本文提出的方法在VidOR和ImageNet-VidVRD数据集上均优于三个基线方法，包括Few-Shot Visual Relation Co-Localization (VRC)，Visual vRGV和Tracklet-based。在所有评估指标中，本文的方法都表现出色，包括主体和客体准确性、关系准确性、主体或客体的时空准确性、关系的时空准确性、主体或客体的空间准确性和主体或客体的平均IoU。本文的方法在静态关系方面表现更好，但在动态谓词方面也表现出了合理的结果，突显了其鲁棒性。定性结果显示了该方法在定位通过动态关系连接的主体和客体方面的有效性。


# Paper:562     在多种恶劣天气条件下学习通用和特定于天气的图像恢复特征



#### 1. Title: 
Learning Weather-General and Weather-Specific Features for Image Restoration Under Multiple Adverse Weather Conditions

#### 2. Authors: 
Yurui Zhu, Tianyu Wang, Xueyang Fu, Xuanyu Yang, Xin Guo, Jifeng Dai, Yu Qiao, Xiaowei Hu

#### 3. Affiliation: 
Yurui Zhu: 中国科学技术大学 (University of Science and Technology of China)

#### 4. Keywords: 
Image restoration, adverse weather conditions, weather-specific features, weather-general features, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Zhu_Learning_Weather-General_and_Weather-Specific_Features_for_Image_Restoration_Under_Multiple_CVPR_2021_paper.pdf  Github: https://github.com/zhu-y/WeatherSR

#### 6. Summary : 
- (1):本文研究在多种恶劣天气条件下的图像恢复问题，旨在通过使用单一的网络参数来消除与天气相关的伪影。 

- (2):以往的方法只适用于特定的天气情况，需要多组天气特定的模型参数，这增加了计算和存储负担。本文提出了一种有效的统一框架，通过两阶段训练策略来探索天气通用和天气特定特征。第一阶段的训练旨在通过将不同天气条件下的图像作为输入并输出粗略恢复结果来学习天气通用特征。第二阶段的训练旨在学习自适应扩展深度模型中每种天气类型的特定参数，自动学习扩展天气特定参数的必要位置。因此，我们可以获得一种高效且统一的模型，用于在多种恶劣天气条件下进行图像恢复。 

- (3):本文提出了一种有效的统一框架，通过两阶段训练策略来探索天气通用和天气特定特征。第一阶段的训练旨在通过将不同天气条件下的图像作为输入并输出粗略恢复结果来学习天气通用特征。第二阶段的训练旨在学习自适应扩展深度模型中每种天气类型的特定参数，自动学习扩展天气特定参数的必要位置。因此，我们可以获得一种高效且统一的模型，用于在多种恶劣天气条件下进行图像恢复。 

- (4):本文在多个合成和真实世界基准测试中取得了优异的性能，验证了所提出方法的优越性。
#### 7. 方法详细介绍：
本文提出了一种基于U-Net的统一网络架构，用于多种天气去除任务。该方法包括两个训练阶段。第一阶段采用共享的骨干网络来学习天气通用特征。引入深度信息来优化网络，采用深度一致性损失。第二阶段通过引入天气特定参数和可学习的评分变量来学习天气特定特征。采用基于正则化的优化方案，实现不同天气类型的自适应天气特定参数扩展。研究了卷积运算符的核大小为1×1或3×3作为新添加的天气特定参数。

#### 8. 实验设置：
作者构建了第一个具有多种天气条件的真实世界基准数据集，以更好地处理真实世界场景中的各种天气相关伪影。实验结果表明，所提出的方法在所有合成和真实世界基准测试中均取得了优异的性能。

#### 9. 实验结果和分析：
本文提出的模型在多个数据集上进行了实验，包括Outdoor-Rain、RainDrop、SnowTest100k-L、RESIDE、Rain1400和RealSnow。与其他所有天气去除方法相比，该模型在所有数据集上均取得了更好的PSNR和SSIM指标。在各种天气条件下，该模型的性能均优于TransWeather近两个dB。本文还提供了各种天气的真实场景的视觉比较，其中所提出的方法成功地保留了背景细节并去除了多种天气伪影。通过消融实验验证了所提出方法中每个组件的有效性。该方法使用的网络参数比其他方法少得多，但性能更好。该方法的平均推理时间为0.03秒，可在单个NVIDIA Geforce GTX 1080 Ti GPU上处理分辨率为256×256的图像。


# Paper:563     通过神经实例特征锻造缓解广义少样本目标检测中的遗忘问题



#### 1. Title: 
NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging

#### 2. Authors: 
Karim Guirguis, Johannes Meier, George Eskandar, Matthias Kayser, Bin Yang, Jürgen Beyerer

#### 3. Affiliation: 
Karim Guirguis: 罗伯特·博世有限公司 (Robert Bosch GmbH)

#### 4. Keywords: 
Few-shot object detection, Generalized few-shot object detection, Data-free knowledge distillation, Neural instance feature forging

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guirguis_NIFF_Alleviating_Forgetting_in_Generalized_Few-Shot_Object_Detection_via_Neural_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是在少量数据的情况下，如何进行目标检测，同时避免基础类别的遗忘问题。

- (2):过去的方法主要是元学习和迁移学习，但是它们忽略了基础类别的遗忘问题。本文提出了一种数据无关的知识蒸馏方法，通过生成基础类别的实例级特征，从而避免了需要访问基础图像的问题。本文的方法是通过设计一个独立的轻量级生成器，生成和重放多样的实例级基础特征到RoI头，从而避免了生成整个模型的复杂性。本文的方法在MSCOCO和PASCAL-VOC数据集上取得了新的最佳性能。

- (3):本文提出了一种名为NIFF的方法，它是一种数据无关的知识蒸馏方法，通过生成基础类别的实例级特征，从而避免了需要访问基础图像的问题。本文的方法是通过设计一个独立的轻量级生成器，生成和重放多样的实例级基础特征到RoI头，从而避免了生成整个模型的复杂性。本文的方法在MSCOCO和PASCAL-VOC数据集上取得了新的最佳性能。

- (4):本文的方法在MSCOCO和PASCAL-VOC数据集上取得了新的最佳性能，可以显著减少基础内存需求，同时提高了G-FSOD的检测性能。本文的方法通过生成基础类别的实例级特征，从而避免了需要访问基础图像的问题，同时避免了基础类别的遗忘问题。
#### 7. 方法详细介绍：
本文提出了一种名为神经实例特征锻造（NIFF）的方法，用于解决广义少样本目标检测中的遗忘问题。该方法包括两个阶段：特征生成器训练和知识蒸馏。在特征生成器训练阶段，使用独立的轻量级模型生成实例级别的基础特征，并将其重放到RoI头部，以对齐RoI头部的统计数据和基础数据。在知识蒸馏阶段，使用生成器生成的基础特征和真实的新颖特征进行知识蒸馏，以减轻遗忘问题。此外，还使用了弹性权重共享和数据增强等正则化技术来提高性能。

具体步骤如下：
1. 特征生成器训练：使用KL散度和交叉熵损失函数训练独立的特征生成器，以生成实例级别的基础特征，并将其重放到RoI头部。
2. 知识蒸馏：使用生成器生成的基础特征和真实的新颖特征进行知识蒸馏，以减轻遗忘问题。使用加权特征蒸馏和L2范数来惩罚教师和学生之间的差异。

#### 8. 实验设置：
本文在MS-COCO和PASCAL-VOC数据集上进行了实验，使用5k验证集进行测试，并使用与先前工作相同的数据拆分。实验设置包括不同的K-shot设置，从1到30，评估指标包括总体AP、基础AP、新颖AP、基础AR、新颖AR和集成推理结果。

#### 9. 实验结果和分析：
本文提出的NIFF方法在PASCAL-VOC和MS-COCO数据集上取得了最先进的结果。与其他数据依赖和数据无关的基线方法相比，NIFF方法在大多数情况下都表现出更好的性能。在MS-COCO数据集上，NIFF-DeFRCN方法在5、10和30-shot设置下的总体AP性能优于其他方法。此外，本文还研究了生成器设计选择、采样技术和正则化技术对性能的影响。实验结果表明，NIFF方法在广义少样本目标检测中具有很高的实用价值。


# Paper:564     MaPLe: 多模态提示学习



#### 1. Title: 
MaPLe: Multi-modal Prompt Learning


#### 2. Authors: 
Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan


#### 3. Affiliation: 
Mohamed bin Zayed University of AI (阿布扎比人工智能大学)


#### 4. Keywords: 
Vision-language models, CLIP, Prompt learning, Multi-modal learning, Generalization


#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2021_paper.html  Github: https://github.com/muzairkhattak/multimodal-prompt-learning


#### 6. Summary : 
- (1):本文研究背景是视觉-语言模型的下游任务中，预训练模型的适应性问题。
- (2):过去的方法是手动选择输入文本提示，或者在单个分支上学习提示，这些方法存在一些问题，如提示的选择和模型的适应性。本文提出了一种多模态提示学习方法，可以同时适应视觉和语言分支，以提高它们之间的对齐性。本文的方法在多个 transformer block 中学习多个上下文提示，以逐步建模特征之间的关系，从而提高模型的灵活性。本文的方法在多个下游任务中进行了评估，表现优于现有方法。
- (3):本文提出了一种多模态提示学习方法，可以同时适应视觉和语言分支，以提高它们之间的对齐性。本文的方法使用一个 V-L 耦合函数来将视觉提示与语言提示联系起来，以促进两种模态之间的互动。本文的方法在多个 transformer block 中学习多个上下文提示，以逐步建模特征之间的关系，从而提高模型的灵活性。
- (4):本文的方法在多个下游任务中进行了评估，包括基于 novel classes 的泛化、跨数据集评估和领域泛化。与现有方法相比，本文的方法表现更好，可以在 11 个不同的图像识别数据集上实现 3.45% 的绝对增益和 2.72% 的总体谐波平均值。本文的方法在训练和推理过程中都具有更高的效率，而且代码和预训练模型都可以在 Github 上获得。
#### 7. 方法详细介绍：
本文提出了一种多模态提示学习方法，称为MaPLe，该方法建立在预训练的视觉-语言模型CLIP上。MaPLe引入了联合提示，其中上下文提示在视觉和语言分支中学习。为了学习分层的上下文表示，MaPLe通过不同的Transformer块引入了深度提示。在微调期间，仅学习上下文提示及其耦合函数，而其余模型被冻结。本文还提出了深度语言提示和深度视觉提示，以在更深的Transformer层中学习提示。最后，本文引入了视觉-语言提示耦合函数，通过在两种模态之间共享提示来调整CLIP的视觉和语言分支。

#### 8. 实验设置：
本文在零样本设置下评估了MaPLe的泛化能力，其中数据集被分为基类和新类。模型仅在少量样本的基类上进行训练，并在基类和新类上进行评估。本文还评估了该方法在跨数据集转移和领域泛化方面的潜力。对于从基类到新类的泛化和跨数据集评估，本文在11个图像分类数据集上评估了该方法的性能，涵盖了广泛的识别任务。对于领域泛化，本文使用ImageNet作为源数据集，其四个变体作为目标数据集。

#### 9. 实验结果和分析：
本文可视化并比较了MaPLe的图像嵌入与最新的Co-CoOp方法。本文表明，MaPLe的图像嵌入更具可分离性，表明学习视觉提示和语言提示可以更好地适应CLIP。本文还在3个不同的图像识别数据集上使用了Co-CoOp和MaPLe的图像嵌入的t-SNE图，显示MaPLe在基类和新类中都具有更好的可分离性。本文提供了深度语言提示、深度视觉提示和视觉-语言提示耦合的详细方程式。本文还解释了在实验设置中如何训练和评估模型。


# Paper:565     基于LVM规范的神经网络高效验证



#### 1. Title: 
Efficient Verification of Neural Networks against LVM-based Specifications

#### 2. Authors: 
Harleen Hanspal, Alessio Lomuscio

#### 3. Affiliation: 
Harleen Hanspal: 伦敦帝国学院 & Safe Intelligence, 伦敦, 英国
Alessio Lomuscio: Safe Intelligence, 伦敦, 英国

#### 4. Keywords: 
Neural network verification, Latent Variable Models, Robustness, Safety-critical applications, Generative models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hanspal_Efficient_Verification_of_Neural_Networks_Against_LVM-Based_Specifications_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了神经网络在安全关键应用中的鲁棒性问题，提出了一种基于潜变量模型的规范的高效验证方法，以捕捉各种现实输入变化。
- (2):现有的验证方法分析对解析定义的变换的不变性，但无法有效地捕捉物体姿态、场景视角、遮挡等多样化和普遍的变化。本文提出的方法相对独立于输入维度，并且可以扩展到广泛的深度网络和现实世界数据集，缓解了现有技术中的低效性和解码器表达能力依赖性。
- (3):本文提出了一种基于可逆编码器的验证流程，以验证最小重构开销的潜空间集合。该方法具有计算效率和相对独立于重构的验证结果，以及高召回率的精确反例。本文的方法可以更好地捕捉难以数学定义的变化，相对于现有方法具有更高的可靠性和实用性。
- (4):本文的方法在视觉推理任务中验证了姿态和属性变化的三类潜空间规范，并取得了良好的性能。本文的方法可以更好地捕捉难以数学定义的变化，相对于现有方法具有更高的可靠性和实用性。
#### 7. 方法详细介绍：
本文提出了一种神经网络验证流程，用于验证深度神经网络对潜在变量模型（LVM）规范的适应性。该流程包括定义一个本地潜在空间集（Zdes），使用标准神经网络验证器对网络进行验证，并使用解码器将Zdes中的任何反例映射到输入空间中的反例。该方法独立于FDN架构的深度和类型以及输入维度，易于将验证方法扩展到新的架构。具体步骤包括：
1. 在要验证的网络中添加可逆编码头，使其能够验证潜在空间集。
2. 建立围绕要验证的网络的LVM，以其中间特征作为输入。
3. LVM编码器是可逆的，将其输入特征集映射到空间上连续的潜在流形中，其中定义了输入规范。
4. 验证路径仅限于LVM的编码头的反向和N的任务头。
5. 训练LVM的解码器仅用于训练LVM和作为反例生成器，以在验证结果为负时提供语义上有意义的反例。

#### 8. 实验设置：
本文的实验旨在验证所提出的方法，并回答三个关键问题：LVM是否能够生成适当的潜在流形和良好的重构，所提出的流程是否能够有效地验证深度网络，以及第3节中讨论的规范是否能够编码预期的输入变化。实验重点验证输入编码段和沿潜在维度进行验证。使用的数据集包括CelebA、Traffic Signs Recognition（TSRD）、3D Objects（3DOD）和（Fashion）MNIST。

#### 9. 实验结果与分析：
本文提出的验证规范在潜在空间中验证深度网络的方法，能够有效地评估和验证深度网络对姿态变化的适应性，并提供有关其脆弱性的反例。该方法能够扩展到深度网络，并在找到有效的域内反例时提供高精度。该方法还引入了基于潜在空间的有效性度量，以评估该方法在捕捉所提出规范中预期变化方面的精度。实验结果表明，所提出的流程需要比EDN流程少一个数量级的时间来解决安全查询，并产生较少的平凡输出边界或未决定的情况。验证的鲁棒准确性严格下限了针对梯度上升的潜在空间插值攻击计算的对抗准确性。


# Paper:566     Im2Hands: 学习相互作用的双手形状的注意力隐式表示



#### 1. Title: 
Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes

#### 2. Authors: 
Jihyun Lee, Minhyuk Sung, Honggyu Choi, Tae-Kyun Kim

#### 3. Affiliation: 
KAIST (韩国科学技术院)

#### 4. Keywords: 
Two-hand reconstruction, neural implicit representation, attention-based modules, occupancy volume, keypoint refinement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Im2Hands_Learning_Attentive_Implicit_Representation_of_Interacting_Two-Hand_Shapes_CVPR_2021_paper.html  Github: https://github.com/jyunlee/Im2Hands

#### 6. Summary : 
- (1):本文研究的是两只手的重建问题，由于两只手之间的相互作用和相互遮挡，使得该问题更加复杂。

- (2):现有的两只手重建方法主要基于参数化手模型和低分辨率网格，无法很好地捕捉两只手的细节和形状。本文提出了一种新的神经隐式表示方法，可以在高分辨率下重建两只手的几何形状，并且不需要密集的顶点对应或统计模型参数注释进行训练。与现有的方法相比，本文的方法可以更好地捕捉两只手的形状和图像对齐。

- (3):本文提出了一种名为Im2Hands的神经隐式表示方法，可以重建两只相互作用的手的几何形状。为了处理两只手之间的形状复杂性和交互上下文，Im2Hands使用两个新颖的基于注意力机制的模块，分别负责(1)初始占用估计和(2)上下文感知占用细化。Im2Hands首先使用查询-图像注意力在每只手的规范空间中学习每只手的神经关节占用。然后，它使用查询-锚点注意力在姿态空间中对初始两只手占用进行细化，以增强两只手形状之间的一致性。此外，为了在单图像重建场景下实现更稳健的两只手形状估计，我们引入了一个可选的关键点细化模块，以通过减轻从现成的基于图像的两只手关键点估计方法(例如[11,20,23,27,42])预测的输入3D关键点中的误差来实现更稳健的两只手形状重建。

- (4):本文在两只手重建任务上进行了实验，与相关方法进行了比较，结果表明，Im2Hands在两只手重建方面取得了最先进的结果。
#### 7. 方法详细介绍：
本文提出了一种名为Im2Hands的神经隐式函数，用于重建两只交互手的形状。该方法包括三个子网络：（1）初始手部占用网络（I），用于预测单只手的占用概率，（2）两只手的占用细化网络（R），通过考虑两只手之间的交互来细化初始占用估计，（3）可选的关键点细化模块（K），可以缓解输入的两只手关键点中的噪声。该方法使用MSE损失和穿透损失进行训练，以避免两只手之间的穿透。

#### 8. 实验设置：
本文在InterHand2.6M数据集上对所提出的方法进行了评估，包括从图像和单个图像中进行重建。评估指标包括平均交集联合（IoU）、Chamfer L1距离（CD）和平均每关节位置误差（MPJPE）。本文将Im2Hands与现有方法进行了比较，包括HALO、IntagHand和Two-Hand-Shape-Pose。

#### 9. 实验结果与分析：
本文显示Im2Hands在从图像和单个图像中进行重建方面均优于现有方法。所提出的关键点细化模块在减轻输入关键点误差方面是有效的。本文还展示了穿透损失在避免两只手之间的穿透方面的有效性。本文还在InterHand2.6M数据集上进行了图像交互双手重建的定性比较，证明了所提出的方法生成的双手形状更加合理，并且与输入图像对齐良好。本文还测试了所提出的方法在RGB2Hands和EgoHands数据集上的泛化能力，并且结果表明该方法可以很好地推广到其他数据集。本文还进行了消融研究，以调查Im2Hands的主要模块的有效性，并发现完整模型比其他模型变体更有效。


# Paper:567     TensoIR：张量逆向渲染



#### 1. Title: 
TensoIR: Tensorial Inverse Rendering

#### 2. Authors: 
Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Songfang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, Hao Su

#### 3. Affiliation: 
Haian Jin: 浙江大学
Isabella Liu: None
Peijia Xu: Kingstar Technology Inc.
Xiaoshuai Zhang: UC San Diego
Songfang Han: UC San Diego
Sai Bi: Adobe Research
Xiaowei Zhou: 浙江大学
Zexiang Xu: None
Hao Su: UC San Diego

#### 4. Keywords: 
Inverse rendering, neural fields, radiance field, tensor factorization, multi-view images

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jin_TensoIR_Tensorial_Inverse_Rendering_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是逆向渲染问题，即从捕获的图像中重建三维场景的物理属性，以支持许多下游应用，如新视角合成、重新照明和材料编辑。这是一个具有挑战性和不适定性的问题，特别是当输入图像在未知照明下野外捕获时。

- (2):过去的方法通常使用多层感知器（MLP）等基于神经网络的方法来学习神经场表示，但这些方法通常容量较低，计算成本较高，极大地限制了逆向渲染的准确性和效率。本文提出了一种基于张量分解和神经场的新型逆向渲染方法，扩展了现有的TensoRF方法，可以从捕获的多视图图像中估计场景几何、表面反射和环境照明，同时实现辐射场重建和基于物理的模型估计，从而实现了逼真的新视角合成和重新照明结果。本文的方法可以准确地建模二次阴影效应（如阴影和间接照明），并通常支持在单个或多个未知照明条件下捕获的输入图像。

- (3):本文提出了一种新的张量分解逆向渲染框架，可以同时估计场景几何、材料和照明，从而实现了场景的逆向渲染。本文的方法使用了TensoRF的场景表示，该表示可以快速、紧凑地重建辐射场，同时使用多个小型MLP在相同的特征网格上进行回归，以建模场景的几何和外观。本文的方法可以同时实现辐射场渲染和基于物理的渲染，从而实现了场景的逆向渲染。本文的方法还提出了一种有效的多光源重建方法，可以在有限的GPU内存下实现多光源重建。

- (4):本文的方法在各种具有挑战性的合成和实际场景中进行了广泛的评估。本文的方法在场景重建和渲染方面均优于以前的逆向渲染方法，实现了场景几何和材料属性的最先进质量，同时实现了新视角合成和重新照明的最先进质量。本文的方法还实现了更快的重建速度和更高的准确性。
#### 7. 方法详细介绍：
本文提出了一种名为TensoIR的张量分解逆渲染方法，可以从多视角图像中联合估计场景几何、表面反射和环境照明，适用于单一或多个未知光照条件。该方法基于TensoRF场景表示，使用多个低秩张量分量对场景进行建模。在同一特征网格上应用多个小型MLP，回归体密度、视角相关颜色、法线和材质属性，以模拟场景的几何和外观。该方法通过捕获的图像对辐射场渲染和基于物理的渲染进行监督，实现了场景重建的联合优化。该方法支持快速密度和辐射评估，使得二次效应的在线计算更加准确，从而实现更准确的阴影和间接照明建模。该方法还支持通过在分解的张量表示中建模额外的照明维度来实现高效的多光源重建。

#### 8. 实验设置：
本文在四个复杂的合成场景和两种类型的多光数据（旋转多光和一般多光）上进行了实验。场景被渲染以获得它们的地面真实图像、BRDF参数和法线图。本文使用在单一未知光照条件下捕获的图像将所提出的方法与之前的最先进的神经场逆渲染方法NeRFactor和InvRender进行比较。本文还评估了在多光设置下训练的所提出的方法。

#### 9. 实验结果和分析：
所提出的方法在所有指标上都显著优于之前的方法，证明了该方法的优越性。单光方法实现了更准确的法线和反照率估计，从而产生更逼真的新视角合成和重照结果。在多光设置下训练的方法可以进一步提高逆渲染性能。本文在合成数据集上提供了定量比较，包括MAE、PSNR、SSIM和LPIPS，并报告了每种方法的平均运行时间。本文还提供了与基线方法的视觉比较，显示所提出的方法产生了更高质量的逆渲染结果，具有更详细的法线和更准确的反照率，从而产生更逼真的重照结果。


# Paper:568     基于梯度学习的通用伪造痕迹表示用于GAN生成图像检测



#### 1. Title: 
Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection

#### 2. Authors: 
Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Yunchao Wei

#### 3. Affiliation: 
北京交通大学信息科学研究所

#### 4. Keywords: 
GAN, fake image detection, generalized artifacts representation, gradients, transformation model

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2021_paper.html
Github: https://github.com/chuangchuangtan/LGrad

#### 6. Summary : 
- (1):本文研究背景是GAN技术的快速发展，使得生成的假图像越来越逼真，导致滥用的风险增加，因此需要开发一种检测GAN生成图像真伪的技术。

- (2):过去的方法主要依赖于局部区域的伪造痕迹、混合边界、全局纹理和频率级别的伪造痕迹等，但这些方法往往依赖于训练设置，导致在未知领域的性能下降。本文提出了一种新的检测框架，称为Learning on Gradients (LGrad)，它使用梯度作为GAN生成图像的广义伪造痕迹表示，将数据依赖性问题转化为转换模型依赖性问题，提高了检测器的鲁棒性。

- (3):本文提出了一种新的检测框架，使用预训练的CNN模型将图像转换为梯度，然后利用这些梯度来表示广义伪造痕迹，并将其输入到分类器中以确定图像的真实性。本文的创新点在于使用梯度作为GAN生成图像的广义伪造痕迹表示，将数据依赖性问题转化为转换模型依赖性问题，提高了检测器的鲁棒性。

- (4):本文在ProGAN生成的图像上进行了实验，结果表明，使用梯度作为广义伪造痕迹表示的检测器具有更好的鲁棒性和泛化性能，取得了新的最优性能，相比之前的方法提高了11.4%。
#### 7. 方法详细介绍：
本文提出了一种名为“梯度学习（LGrad）”的检测框架，利用预训练的CNN模型将图像转换为梯度，将梯度作为广义伪造图像的表示，通过二分类网络进行训练，以区分输入的梯度是否对应GAN生成的图像。在推理时，测试图像首先通过训练阶段使用的转换模型转换为梯度。该方法具有较强的鲁棒性，可以有效地检测各种GAN模型生成的图像。此外，该方法还可以抵抗图像扰动攻击，使得有针对性的反检测变得无效。

#### 8. 实验设置：
本文使用Wang等人提供的数据集进行实验，包括ProGAN、StyleGAN、StyleGAN2、BigGAN、CycleGAN、StarGAN、GauGAN和Deepfake等各种GAN模型生成的伪造图像。真实图像采样自LSUN、ImageNet、CelebA、CelebA-HQ、COCO和FaceForensics++等数据集。在训练阶段，使用ProGAN生成的图像进行分类器的训练。此外，为了验证该方法对人脸数据的适用性，本文还使用Celeba-HQ数据集中的2万张真实图像和ProGAN生成的2万张人脸图像作为训练集，使用Celeba-HQ数据集中的StyleGAN和StyleGAN2生成的人脸图像作为测试集。所有图像都被调整为256×256的分辨率。

#### 9. 实验结果与分析：
本文提出的LGrad框架在ProGAN、StyleGAN、StyleGAN2、BigGAN、CycleGAN等各种GAN模型上均取得了较高的准确率，其中在StyleGAN-bedroom模型上的表现超过了FrePGAN，平均准确率和平均平均精度分数分别提高了11.4%和13.4%。该方法在GauGAN模型和deepfake模型上也取得了可比较的结果。但是，在deepfake模型上，LGrad框架的准确率仅为66.7%。实验结果表明，该方法具有较强的检测效果和鲁棒性。


# Paper:569     RefSR-NeRF：面向高保真度和超分辨率视图合成



#### 1. Title: 
RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis

#### 2. Authors: 
Xudong Huang, Wei Li, Jie Hu, Hanting Chen, Yunhe Wang

#### 3. Affiliation: 
华为诺亚方舟实验室 (Huawei Noah's Ark Lab)

#### 4. Keywords: 
Neural Radiance Field, Super Resolution, View Synthesis, Reference-guided, End-to-end framework

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Huang_RefSR-NeRF_Towards_High_Fidelity_and_Super_Resolution_View_Synthesis_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是神经渲染领域中的高保真度和超分辨率视图合成问题。NeRF是一种用于合成逼真渲染的方法，但是在高分辨率渲染时会出现模糊的问题，因为其固有的多层感知器难以学习高频细节，并且随着分辨率的增加会导致计算爆炸。因此，需要一种新的方法来解决这个问题。

- (2):过去的方法包括使用体素网格表示来加速NeRF的渲染速度，但是这种方法会消耗大量的存储空间并且会影响渲染质量。本文提出了一种新的方法，即引入高分辨率参考图像来提供每个场景的高频细节。本文的方法是一个端到端的框架，首先学习低分辨率的NeRF表示，然后借助高分辨率参考图像重建高频细节。本文的方法在渲染质量、速度和内存使用方面取得了很好的平衡。

- (3):本文提出了一种新的方法，即引入高分辨率参考图像来提供每个场景的高频细节。本文的方法是一个端到端的框架，首先学习低分辨率的NeRF表示，然后借助高分辨率参考图像重建高频细节。本文的方法在渲染质量、速度和内存使用方面取得了很好的平衡。本文的创新点在于提出了一种新的方法来解决NeRF在高分辨率渲染时出现的模糊问题。

- (4):本文的方法在多个基准测试中进行了广泛的实验，表明本文的方法在渲染质量、速度和内存使用方面都表现出色，优于或与NeRF及其变体相当，同时速度提高了52倍。本文的方法可以用于高保真度和超分辨率视图合成问题。
#### 7. 方法详细介绍：
本文提出了一种基于参考图像的超分辨率神经辐射场（RefSR-NeRF）框架，用于超分辨率和逼真的新视角合成。该框架首先使用低分辨率（LR）NeRF学习场景表示，然后使用高分辨率参考图像重建高频细节。该框架包括一个轻量级的RefSR模型，用于学习从NeRF渲染到目标HR图像的逆降采样过程。RefSR模型由两个编码器-解码器网络组成，分别用于下采样降级预测和高频细节恢复。RefSR-NeRF模型使用Charbonier损失函数进行训练，并使用Pytorch和Adam优化器在V100 NVIDIA GPU上进行训练。总损失函数包括NeRF的LR渲染和LR地面真实图像之间的渲染损失以及超分辨率HR图像和HR地面真实图像之间的重建损失。 

#### 8. 实验设置：
本文在多个基准测试集上进行了广泛的实验，以评估所提出的RefSR-NeRF框架。基准测试集包括DTU、LLFF和Tanks and Temples数据集。对于真实场景，使用LLFF数据集，其中包含从许多前向视图捕获的8个场景。每个场景具有20到62个图像，分辨率为4032×3024。将输入分辨率设置为504×378，并在×4和×8的比例下进行超分辨率。对于合成场景，使用NeRF Synthetic 360◦数据集，其中包含8个合成场景，每个场景包括100个地面真实图像和200个测试图像。所有图像的分辨率均为800×800，具有校准的相机位置。

#### 9. 实验结果和分析：
RefSR-NeRF方法在渲染质量、速度和内存使用方面均优于其他方法。定量比较显示在渲染质量方面，RefSR-NeRF方法优于NeRF-LR和NeRF-HR，并且与NeRF-based方法相当，同时实现了相当大的渲染加速。该方法在渲染质量、速度和内存使用方面展现出令人印象深刻的平衡，优于或与NeRF及其变体相当，同时具有5倍的加速和较小的额外内存和存储使用。提供了广泛的消融研究，以验证所提出框架的每个组件的贡献。


# Paper:570     从单个语义掩码中合成自然场景的多视角一致的彩色图像



#### 1. Title: 
Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask

#### 2. Authors: 
Shangzhan Zhang, Sida Peng, Tianrun Chen, Linzhan Mou, Haotong Lin, Kaicheng Yu, Yiyi Liao, Xiaowei Zhou

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
View Synthesis, Natural Scenes, Semantic Mask, Neural Fields, SPADE

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Painting_3D_Nature_in_2D_View_Synthesis_of_Natural_Scenes_CVPR_2021_paper.html  Github: https://zju3dv.github.io/paintingnature/

#### 6. Summary : 
- (1):本文旨在通过学习单视角图像集合的先验知识，从单个语义掩码中合成自然场景的多视角一致的彩色图像。
 
- (2):过去的方法要么需要多视角监督，要么需要学习特定类别对象的类别级先验知识，这些方法都不适用于自然场景。本文提出了一种新的方法，使用语义场作为中间表示，通过现有的语义图像合成模型将其转换为辐射场。实验结果表明，本文方法优于基线方法，并能够生成各种自然场景的逼真和多视角一致的视频。

- (3):本文提出了一种基于单视角图像集合的语义引导视角合成自然场景的新框架。首先，将输入的语义掩码转换为彩色图像，然后通过深度估计器预测深度图。接下来，使用自监督学习策略在单视角图像集合上训练的修复网络对输入的语义掩码进行修复。然后，学习神经语义场以融合和去噪这些语义掩码以获得更好的多视角一致性。最后，通过SPADE将多视角语义掩码转换为彩色图像，并恢复神经场表示以进行视角一致的渲染。

- (4):本文方法在LHQ数据集上进行了广泛的实验，结果表明，本文方法在质量和数量上都优于基线方法。本文方法能够生成各种高质量的自然场景渲染结果，并且能够通过编辑输入的语义掩码来生成各种高质量的自然场景渲染结果。
#### 1. 致谢
本文的工作得到了NSFC（No. 62172364）、阿里巴巴创新研究计划、浙江大学信息技术中心和CAD&CG国家重点实验室的支持。

#### 2. 方法简介
本文提出了一种从单个语义掩码生成多视角自然场景的方法。该方法包括两个主要步骤。首先，训练一个修复网络来生成输入语义掩码的新视角。其次，优化一个三维语义场来渲染视角一致的语义掩码，然后将其输入到2D生成器SPADE中，以生成用于学习神经场景表示的RGB图像。该方法实现了逼真和视角一致的结果，显著优于基线方法。

#### 3. 实验设置
使用LHQ数据集来训练SPADE和语义修复网络。使用COCO-Stuff和DeepLab v2网络为每个图像准备语义掩码。通过反向映射策略合成语义修复的训练数据。语义场的训练采用每个学习迭代中采样1024条光线，而神经外观场的训练分辨率为256×256。GAN损失、L2损失和感知损失的权重分别为1.0、10.0和10.0。实验在6个测试场景上进行，并从Flickr获取一组景观图像来评估生成图像的质量。

#### 4. 实验结果与分析
本文提出的方法优于现有基线方法，达到了最小的FID和KID。用户更喜欢该方法，并将视频评为最具视角一致性和逼真性，相比其他方法。生成的语义掩码的质量和一致性优于GVS。消融研究证明了方法的每个组成部分的重要性，包括语义场融合模块和使用语义掩码而不是RGB图像。

#### 5. 方法详细介绍
本文提出了一种从单个语义掩码生成自然场景的多视角合成方法。该方法包括两个主要步骤：从单个语义掩码生成多视角一致的语义掩码和学习自然场景表示以融合SPADE提供的外观信息。第一步涉及使用基于深度的变形技术将给定的语义掩码变形到新视角，然后使用修复网络在每个新视角填充变形后的语义掩码的未遮挡区域。在不同视角获得多个填充的语义掩码后，学习一个神经语义场来融合和去噪多视角语义信息。最后，通过语义场获得多视角语义掩码。在第二步中，学习自然场景表示以融合SPADE提供的外观信息。场景的几何形状直接建模为语义场的训练MLP网络，外观则建模为外观场。在训练期间，外观网络基于感知和对抗损失进行优化。

#### 6. 实验细节
当前文本中没有提到具体的实验细节。


# Paper:571     多智能体自动机器学习



#### 1. Title: 
Multi-Agent Automated Machine Learning

#### 2. Authors: 
Zhaozhi Wang, Kefan Su, Jian Zhang, Huizhu Jia, Qixiang Ye, Xiaodong Xie, Zongqing Lu

#### 3. Affiliation: 
北京大学

#### 4. Keywords: 
Automated machine learning, Multi-agent reinforcement learning, Joint optimization, Credit assignment, Off-policy learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Multi-Agent_Automated_Machine_Learning_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究自动机器学习（AutoML）中多个模块的联合优化问题，提出了一种多智能体自动机器学习（MA2ML）框架，旨在有效处理AutoML中模块的联合优化问题。

- (2):过去的方法主要集中在单个模块的优化上，而MA2ML则是针对多个模块的联合优化问题提出的。MA2ML采用多智能体强化学习方法，将每个机器学习模块作为一个智能体，以最终性能作为奖励，形成一个多智能体强化学习问题。MA2ML通过显式分配信用来区分每个模块的贡献，以增强模块之间的合作，并采用离线学习来提高搜索效率。

- (3):MA2ML采用多智能体演员-评论家方法，通过学习一个集中的Q函数来评估联合动作，以处理连续（例如学习率）和离散（例如架构）动作空间。理论上，MA2ML保证了联合优化的单调改进。实验结果表明，MA2ML在计算成本约束下，在ImageNet数据集上取得了最先进的top-1准确率，例如在FLOPs小于600M/800M的情况下，分别为79.7％/80.5％。广泛的消融研究验证了MA2ML的信用分配和离线学习的好处。

- (4):MA2ML的实验结果表明，它在计算成本约束下取得了最先进的性能，验证了MA2ML的联合优化的优越性。
#### 7. 方法详细介绍：
本文提出了一种多智能体自动机器学习（Multi-Agent Automated Machine Learning，MA2ML）框架，将机器学习流程的联合优化转化为多智能体强化学习问题。MA2ML将数据增强（AUG）、神经架构搜索（NAS）和超参数（HPO）等每个机器学习模块视为一个智能体，将最终性能作为奖励，形成多智能体强化学习问题。为了处理连续（例如学习率）和离散（例如架构）动作空间，MA2ML采用了多智能体演员-评论家方法，其中学习了一个集中式Q函数来评估联合动作。此外，为了进一步提高搜索效率，MA2ML采用了离线学习，以利用历史样本进行策略更新。理论上，MA2ML保证了联合优化的单调改进。

#### 8. 实验设置：
本文在ImageNet数据集上进行图像分类实验，其中训练集包含200个随机抽样的类别，验证集包含从训练集中取出的10,000张图像。实验的搜索空间包括输入分辨率、卷积核大小、扩展、每层通道数和深度等架构配置。超参数的搜索空间包括优化器类型、初始学习率、权重衰减、mixup比率、dropout比率、随机深度降低比率以及是否使用指数移动平均（EMA）。增强策略的搜索空间包括25个子策略，每个子策略包含两个增强操作，由操作类型、概率和幅度确定。为了更好地平衡搜索到的AutoML流程的性能和计算成本，添加了FLOPs的约束条件。使用多目标奖励函数来调整准确性和FLOPs之间的权衡。

#### 9. 实验结果和分析：
在ImageNet数据集上，MA2ML-A/B/C/D/E/F分别达到了79.3％，79.7％，80.1％，80.5％，80.7％和81.1％的top-1准确率。MA2ML仅使用490M FLOPs就超过了所有列出的AutoML方法，而使用596M FLOPs的MA2ML-B相对于接近600M FLOPs的其他方法的结果有了大幅改善。比较表明，MA2ML比MA2ML-Lite高出0.6％，这归因于MA2ML的信用分配和离线学习。MA2ML的批级平均奖励也比MA2ML-Lite更稳定，这证明了单调策略改进的好处。在CIFAR-10和CIFAR-100上，MA2ML分别达到了97.77±0.07％和85.08±0.14％的top-1准确率，比MA2ML-Lite高。MA2ML在竞争性性能方面也优于其他最先进的AutoML方法。在CIFAR-100上，ML模块的消融研究表明，HPO相对于基线提高了0.89％的准确性，而AUG和NAS分别提高了2.98％和7.71％，这些都是显着的差距。


# Paper:572     通过假阴性感知对比学习学习音频-视觉源定位



#### 1. Title: 
Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning

#### 2. Authors: 
Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu, Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang, Nick Barnes

#### 3. Affiliation: 
Weixuan Sun: 澳大利亚国立大学

#### 4. Keywords: 
Audio-visual source localization, contrastive learning, false negative, self-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Learning_Audio-Visual_Source_Localization_via_False_Negative_Aware_Contrastive_Learning_CVPR_2021_paper.html  Github: https://github.com/OpenNLPLab/FNAC_AVL

#### 6. Summary : 
- (1):本文研究了自监督音频-视觉源定位中的假阴性问题，即在训练过程中，同一类别的音频和图像被错误地视为负样本，从而影响模型的性能。

- (2):现有的方法通常使用对比学习来解决这个问题，但是这种方法假设同一视频中的音频和视觉内容是彼此的正样本，而这种假设在真实世界的训练中会受到假阴性样本的影响。本文提出了一种新的学习策略，名为False Negative Aware Contrastive (FNAC)，以减轻这种假阴性样本对训练的影响。

- (3):本文利用模态内相似性来识别潜在的相似样本，并构建相应的邻接矩阵来指导对比学习。此外，本文提出了一种名为TNE的方法，通过显式利用声音源的视觉特征来促进真实负样本的区分，从而增强真实负样本的作用。FNAC在Flickr-SoundNet、VGG-Sound和AVSBench上取得了最先进的性能，证明了我们的方法在减轻假阴性问题方面的有效性。

- (4):本文提出的方法在自监督音频-视觉源定位任务上取得了最先进的性能，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种针对音频-视觉源定位中误导训练的假阴性样本问题的方法——False Negative Aware Contrastive Learning (FNAC)。该方法包括两个互补的机制：False Negatives Suppression (FNS) 和 True Negatives Enhancement (TNE)。FNS 通过使用音频和视觉邻接矩阵来识别当前 mini-batch 中的假阴性样本，并将其作为软监督信号用于模态间对比学习中的模态内邻接性。TNE 通过区域比较增强真负样本的贡献，同时也无意中抑制了假阴性样本。这两种方法可以无缝地集成到音频-视觉对比学习框架中作为正则化项。

#### 8. 实验设置：
本文在两个数据集 Flickr SoundNet 和 VGG-Sound 上训练音频-视觉定位模型。为了与现有方法进行公平比较，训练是在 Flicker SoundNet 和 VGG-Sound 的两个子集中的 10k 和 144k 配对样本上进行的。本文使用 Consensus Intersection over Union (CIoU) 作为评估指标，对四个基准进行了定位性能的测量，分别是 Flickr、VGG-SS、Heard 110 和 AVS-Bench。模型使用 Adam 进行优化，学习率为 10^-4，权重衰减为 10^-4，共训练 30 个 epochs。

#### 9. 实验结果和分析：
本文提出的 False Negative Aware Contrastive Learning (FNAC) 方法在音频-视觉源定位任务中取得了出色的性能。在 VGGSound 144k 数据集上，该方法在单声源分割 (S4) 和多声源分割 (MS3) 设置下分别达到了 27.15% 和 21.98% 的 mIoU，超过了基线和其他现有方法在细粒度定位基准上的表现。本文还提供了 FNAC 的消融分析，展示了每个组件的单独有效性以及将它们组合起来所取得的显著改进。本文进一步通过定量和示意分析展示了 FNAC 区分假阴性和真阴性的能力。


# Paper:573     使用快速傅里叶变换高效学习去相关表示



#### 1. Title: 
Learning Decorrelated Representations Efficiently Using Fast Fourier Transform

#### 2. Authors: 
Yutaro Shigeto, Masashi Shimbo, Yuya Yoshikawa, Akikazu Takeuchi

#### 3. Affiliation: 
STAIR Lab, Chiba Institute of Technology, Narashino, Chiba, Japan (日本千葉工业大学)

#### 4. Keywords: 
Self-supervised learning, representation learning, decorrelation, Fast Fourier Transform

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shigeto_Learning_Decorrelated_Representations_Efficiently_Using_Fast_Fourier_Transform_CVPR_2021_paper.html  Github: https://github.com/yutaro-s/scalable-decorrelation-ssl.git

#### 6. Summary : 
- (1):本文研究的背景是自监督学习中的特征表示学习，其中Barlow Twins和VICReg是两种常用的自监督学习模型，它们使用正则化器来去相关特征，以避免崩溃的嵌入。
 
- (2):过去的方法包括对称和非对称的Siamese网络，以及对特征进行白化的方法。这些方法在计算效率和准确性方面存在问题。本文提出了一种基于快速傅里叶变换的放松去相关正则化器，可以在计算效率和准确性方面优于现有方法。 

- (3):本文提出了一种放松去相关正则化器，可以通过快速傅里叶变换在O(n*d*log(d))的时间内计算。此外，本文还提出了一种简单的技术来缓解放松后产生的不良局部极小值。本文的方法在下游任务中表现出与现有正则化器相当的准确性，但对于大的d，训练需要更少的内存和更快的速度。

- (4):本文的方法在ImageNet分类任务上进行了测试，结果表明，与现有方法相比，本文的方法在计算效率和准确性方面都有所提高。在ResNet-50和ResNet-18的骨干网络下，本文的方法分别比Barlow Twins快1.2倍和2.2倍。
#### 7. 方法详细介绍：
本文提出了一种基于交叉相关矩阵的正则化方法，称为Rsum(C)。该方法可以作为Barlow Twins损失中Roff的替代方法。Rsum(C)的计算使用了快速傅里叶变换（FFT）算法，可以在O(nd log d)的时间内完成。该方法还提出了两种技术来缓解不良局部最小值：特征置换和特征分组。作者还提出了一种基于特征协方差和的正则化方法，可以用于替代交叉相关正则化方法。

#### 8. 实验设置：
本文使用ImageNet数据集或其子集ImageNet-100对模型进行预训练。在ImageNet上使用ResNet-50作为骨干网络，在ImageNet-100上使用ResNet-18。使用标准的线性评估协议来评估下游SSL性能。对于迁移学习评估，将预训练模型应用于Pascal VOC07+12的目标检测任务。使用VOC2007和VOC2012的trainval集进行训练，使用VOC2007的测试集进行测试。使用R50-C4对Faster R-CNN进行微调。报告五次试验的平均分数。

#### 9. 实验结果和分析：
作者在多个下游任务上评估了所提出的方法，包括ImageNet-100上的线性评估、COCO上的目标检测和ADE20K上的语义分割。将其与Barlow Twins和VICReg的性能进行比较，以及其他最先进的SSL模型。作者还评估了超参数（如特征分组中使用的块大小）的影响。作者指出，他们提出的方法在比现有SSL方法更具计算效率的同时，实现了可比较或更好的性能。


# Paper:574     从物体形状推断和利用部分信息以改善语义图像合成



#### 1. Title: 
Inferring and Leveraging Parts from Object Shape for Improving Semantic Image Synthesis

#### 2. Authors: 
Yuxiang Wei, Zhilong Ji, Xiaohe Wu, Jinfeng Bai, Lei Zhang, Wangmeng Zuo

#### 3. Affiliation: 
第一作者：哈尔滨工业大学

#### 4. Keywords: 
Semantic image synthesis, object shape, part segmentation, few-shot learning, image generation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2021_paper.html  Github: https://github.com/csyxwei/iPOSE

#### 6. Summary : 
- (1):本文研究的背景是语义图像合成，目前在生成真实感图像的细节方面仍存在挑战。

- (2):过去的方法主要是利用语义注入来进行图像合成，但是由于缺乏细粒度的指导，这些方法通常无法生成真实感的物体部分。本文提出了一种从物体形状中推断和利用部分信息的方法，以改善语义图像合成中的物体部分合成问题。与现有方法相比，本文的方法可以生成更具真实感的物体部分，并且具有更好的控制性。

- (3):本文提出了一种基于物体形状的部分信息推断和利用方法（iPOSE），并将其用于改善语义图像合成。具体而言，我们首先利用现有的部分分割数据集构建了一个物体部分数据集，用于训练一个部分预测网络（PartNet）。然后，我们引入了少样本机制来处理在语义图像合成中未被覆盖的物体类别。最后，我们提出了一种部分语义调制（PSM）残差块，将预测的部分地图与语义地图结合起来进行图像合成。

- (4):本文的方法在不同数据集上进行了定量和定性评估，结果表明我们的方法在生成具有丰富部分细节的物体方面表现出色，并且可以灵活地控制图像合成。同时，我们的方法在性能上也优于现有的方法。
#### 7. 方法详细介绍：
本文提出了一种名为iPOSE的新方法，通过利用部分级别信息来改进语义图像合成。首先，提出了PartNet来从物体形状中推断出部分地图，并利用预定义的支持部分地图进行指导。然后，提出了部分语义调制Resblock，将其与语义地图和3D噪声相结合，以调制生成过程。最后，引入了几个损失项来鼓励模型生成逼真的图像。

#### 8. 实验设置：
本文在三个数据集上进行了评估：COCO-Stuff、ADE20K和PASCAL-Context。实验在一台NVIDIA Tesla V100 GPU上进行，具有16GB内存。批量大小设置为4，输入图像大小为256x256。

#### 9. 实验结果与分析：
本文提出的iPOSE方法在三个数据集上进行了评估，并与几种最先进的方法进行了比较。实验结果表明，iPOSE在定量和定性评估方面均优于现有方法，特别是在物体部分方面。用户研究也证明了用户更喜欢iPOSE生成的结果。消融实验验证了引入部分地图和损失的有效性。还分析了支持部分地图的数量，结果表明，随着支持数量的增加，基础和新颖类别的测试准确性都会增加，从而产生更合理的部分地图和逼真的图像。


# Paper:575     通过射频-视觉对应实现自监督定位



#### 1. Title: 
Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence

#### 2. Authors: 
Mohammed Alloulah, Maximilian Arnold

#### 3. Affiliation: 
Nokia Bell Labs（诺基亚贝尔实验室）

#### 4. Keywords: 
Radio sensing, self-supervised learning, synthetic dataset, radio-visual correspondence, target localisation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Alloulah_Look_Radiate_and_Learn_Self-Supervised_Localisation_via_Radio-Visual_Correspondence_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是下一代6G无线网络将在通信功能之外实现射频感知功能，从而在户外实现前所未有的全球感知覆盖。然而，由于缺乏专门用于研究射频感知性能和前景的系统数据集和基准测试，深度学习在射频感知任务中的应用受到限制。

- (2):过去的方法通常将稀疏的射频信号与地面真实视觉模态配对，以通过跨模态注释流进行可靠的语义和定性过滤。然而，这种方法需要大量的手动注释，而且缺乏公开可用的数据集。本文提出了MaxRay，一个用于目标定位的合成射频-视觉数据集和基准测试，旨在解决这个问题。本文进一步提出了一种自监督的方法，通过从射频-视觉对应中提取自坐标来学习射频目标定位。本文的方法可以自动地从配对的射频-视觉数据中学习准确的射频目标定位，而无需标签，这对于实证数据非常重要。本文的方法可以实现大规模的数据可扩展性，并可能成为实现统一的通信-感知蜂窝基础设施上的强大射频感知的关键。

- (3):本文提出了一种基于自监督的射频-视觉学习方法，通过视觉驱动空间注意力来训练射频定位网络。我们使用视觉和射频信号之间的相似性来驱动空间注意力，从而学习抑制杂波并在射频热图中定位目标。我们的方法不需要繁琐的手动注释，而是通过自监督的方式从配对的射频-视觉数据中学习自标签，然后使用这些自标签来训练下游的定位器网络。我们的方法在合成和实证数据上进行了广泛的表征，以验证我们的自监督算法并暴露其与现有方法相比的优越性能。

- (4):本文的方法在射频目标定位任务上取得了良好的性能，比现有方法有所提高。我们的贡献包括：合成数据集、跨模态自监督算法和性能评估。我们的方法可以自动地从配对的射频-视觉数据中学习准确的射频目标定位，而无需标签，这对于实证数据非常重要。本文的方法可以实现大规模的数据可扩展性，并可能成为实现统一的通信-感知蜂窝基础设施上的强大射频感知的关键。
#### 7. 方法详细介绍：
本文提出了一种基于自监督学习的无线电-视觉对应的本地化方法。该方法包括以下三个步骤：1）通过掩码对比学习学习跨模态空间特征，2）通过空间特征之间的跨模态注意力提取目标坐标的自我估计，3）使用自我坐标训练仅基于无线电的目标本地化器网络。本文提供了每个步骤的详细说明和方程式。

#### 8. 实验设置：
本文使用了三个场景的数据集进行实验：停车场、郊区街道和街道峡谷。数据集包含30,000个配对的无线电-视觉数据点，分为24k个训练集和6k个验证集，另有10k个测试集。数据集的传感器配置和无线电配置符合当前的5G高级规格。

#### 9. 实验结果和分析：
本文提出的自监督本地化方法通过无线电-视觉对应实现了准确的目标本地化。实验结果表明，该方法优于有监督方法，并在MaxRay数据集上实现了最先进的性能。该方法还表现出更好的噪声标签容忍度和比有监督方法更好的标签密度容忍度。本文还分析了自我标签与地面真实值之间的偏差，并表明该方法的偏差比有监督方法更高。还研究了维度对自我标记的影响，并发现自我标记的性能受到无线电成像的基本分辨率限制，而不是模型的学习能力。最后，本文研究了跨模态共性与本地化性能之间的关系，并表明视觉掩蔽增强了目标灵敏度。


# Paper:576     无需人工标注的野外点云中的3D人体关键点估计



#### 1. Title: 
3D Human Keypoints Estimation from Point Clouds in the Wild without Human Labels

#### 2. Authors: 
Zhenzhen Weng, Alexander S. Gorban, Jingwei Ji, Mahyar Najibi, Yin Zhou, Dragomir Anguelov

#### 3. Affiliation: 
第一作者：斯坦福大学

#### 4. Keywords: 
3D human keypoints, point clouds, unsupervised learning, autonomous driving

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_Without_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是从点云中无需人工标注的情况下，学习3D人体关键点的方法。在自动驾驶等领域，从传感器数据（如LiDAR点云）中感知人体姿态是非常重要的，但是标注3D人体关键点是昂贵、耗时且容易出错的。因此，本文提出了一种无监督学习的方法，通过学习人体结构和运动的特征，从大量未标注的LiDAR数据中学习3D人体关键点的位置。

- (2):过去的方法主要依赖于2D弱监督或其他模态的信号，如RGB、深度等。然而，这些方法无法充分利用大量未标注的LiDAR数据。本文提出的方法通过无监督学习，利用大量未标注的LiDAR数据，学习3D人体关键点的位置，避免了昂贵、耗时且容易出错的人工标注。本文的方法通过三种有效的无监督损失函数，考虑了人体结构和运动的特征，从而提高了3D人体关键点的学习效果。

- (3):本文提出了一种新颖有效的方法GC-KPL，即基于几何一致性的关键点学习，用于从点云中学习3D人体关键点的位置。该方法通过预测关键点和语义分割模型，利用合成数据进行初始化，然后在大量未标注的LiDAR数据上进行无监督训练，通过三种无监督损失函数，学习3D人体关键点的位置。本文的方法在无需人工标注的情况下，利用大量未标注的LiDAR数据，学习3D人体关键点的位置，取得了较好的效果。

- (4):本文的方法在Waymo Open Dataset上进行了实验，取得了较好的效果。在3D人体关键点的学习任务上，本文的方法在整个数据集上的表现优于现有的方法。此外，本文的方法还可以作为无监督表示学习，用于下游任务的fine-tuning，取得了较好的效果。本文的方法可以充分利用大量未标注的LiDAR数据，提高自动驾驶等领域中人体姿态的理解。
#### 7. 方法详细介绍：
本文提出了一种名为GC-KPL的方法，用于从点云中学习3D人体关键点位置，无需人类标注。该方法采用了两阶段的训练方式。第一阶段，使用合成数据集训练网络，预测3D关键点和身体部位分割。第二阶段，使用无监督损失进一步优化网络，包括流损失、点到肢损失、对称损失和关节到部位损失。该方法利用了人体由肢体组成的事实，每个肢体都是一个刚性部分，并鼓励肢体上的点在每个肢体的局部坐标系中保持大致相同的位置。流损失鼓励所有点的径向和高度分量与场景流的一致性，而点到肢损失鼓励肢体上的点靠近它。对称损失鼓励预测的肢体处于一个位置，使得该肢体周围的所有点大致对称，而关节到部位损失鼓励每个关节靠近该部位上的点。该方法使用了transformer-based回归模型预测关键点和语义分割模型定位身体部位，并在合成数据集上进行训练。然后，该方法在Waymo Open Dataset上进行无监督训练，通过无监督训练，关键点预测得到了优化，并且骨干网络从大量未标注数据中学习到了有用的信息。该方法提出了三种有效且新颖的无监督损失，用于优化关键点，这对于在Waymo Open Dataset上进行无监督关键点学习非常有效。该方法可以用作人类点云的无监督表示学习，从而打开了利用实际上无限数量的传感器数据来改进自动驾驶中的人体姿态理解的可能性。

#### 8. 实验设置：
本文使用了一个包含1000个16帧光线投射点云序列的合成数据集进行第一阶段的训练，并应用了各种数据增强来模拟真实世界的噪声背景和遮挡。在第二阶段，使用了Waymo Open Dataset的整个训练集（约200,000个未标记样本）进行训练。作者随机选择了50%的验证集作为验证集，其余作为测试集进行基准测试。作者在每个阶段结束时报告了测试集上的平均关节位置误差（MPJPE）。

#### 9. 实验结果和分析：
本文提出的GC-KPL方法在Waymo Open Dataset上进行了评估，并在3D关键点估计方面取得了最先进的性能。few-shot实验表明，仅使用10%的可用3D关键点注释进行微调的模型达到了与在整个数据集上训练的最先进模型相当的性能。本文还进行了消融研究，以调查所提出方法中各个损失项的影响。结果表明，第一阶段中的Lseg是必不可少的，而Lkp有助于为后续阶段热身。Lj2p和Lseg是有用的正则化器，确保肢体保持在身体内部，而无监督损失通过优化关键点位置进一步提高了性能。


# Paper:577     SLACK: 冷启动和 KL 正则化的稳定数据增强学习



#### 1. Title: 
SLACK: Stable Learning of Augmentations with Cold-start and KL regularization

#### 2. Authors: 
Juliette Marrie, Michael Arbel, Diane Larlus, Julien Mairal

#### 3. Affiliation: 
Juliette Marrie: Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK

#### 4. Keywords: 
Data augmentation, automatic data augmentation, bilevel optimization, KL regularization, neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Marrie_SLACK_Stable_Learning_of_Augmentations_With_Cold-Start_and_KL_Regularization_CVPR_2021_paper.html  Github: https://github.com/naver-ai/slack

#### 6. Summary : 
- (1):本文研究的背景是数据增强在神经网络中的应用，数据增强可以提高神经网络的泛化能力，但是如何选择合适的数据增强方法是一个挑战。

- (2):过去的方法通常需要手动选择一些默认的数据增强方法，或者使用预训练的网络来学习数据增强方法，但是这些方法都需要一些先验知识。本文提出了一种直接学习数据增强策略的方法，不需要依赖先验知识。本文的方法使用了双层优化和 REINFORCE 技术，但是这些方法容易不稳定。为了解决这个问题，本文提出了一种多阶段的冷启动策略和 KL 正则化方法，以提高算法的稳定性。

- (3):本文提出了一种简单且可解释的数据增强策略模型，可以学习数据增强的频率和幅度。本文的方法使用了双层优化和 REINFORCE 技术，但是为了提高算法的稳定性，本文提出了一种多阶段的冷启动策略和 KL 正则化方法。本文的方法可以直接学习多个数据增强方法的联合概率分布，不需要依赖先验知识。

- (4):本文的方法在多个数据集上进行了实验，结果表明本文的方法可以在不依赖先验知识的情况下，学习出有效的数据增强策略，并且可以推广到其他领域。本文的方法在多个数据集上取得了优秀的性能表现，支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种名为SLACK的方法，用于学习数据增强策略。该方法涉及一个双层优化问题，其中内部问题是使用给定的增强策略训练模型，外部问题是优化增强策略本身。该方法使用验证集来优化策略，并使用正则化项来防止策略崩溃为平凡策略。优化使用随机梯度下降和自动微分。该方法还包括一个冷启动策略，以便在每轮中使用当前增强策略从预训练模型开始重新训练模型。

#### 8. 实验设置：
本文在包括CIFAR10、CIFAR100、ImageNet-100和DomainNet在内的多个标准基准测试集上评估了所提出的方法。将该方法与多种以前的数据增强方法进行比较，包括AutoAugment、Fast AutoAugment、Differentiable Automatic Data Augmentation、RandAugment、Teach Augment、UniformAugment、TrivialAugment和Deep AutoAugment。本文还描述了每个基准测试集所使用的体系结构和数据增强搜索的转换空间。

#### 9. 实验结果和分析：
本文报告了所提出的SLACK方法在ImageNet-100和DomainNet数据集上的测试准确性。对于ImageNet-100，SLACK优于Uniform策略和TrivialAugment变体。对于DomainNet，将SLACK与Uniform策略、DomainBed用于域泛化的增强以及默认设置下的TrivialAugment方法进行比较。SLACK是TA（Wide）ImageNet的次优解，优于所有其他方法，这表明SLACK学习和适应每个域的能力是有益的。每个域的学习策略在图3中以饼图的形式呈现。


# Paper:578     基于事件焦点堆栈的全焦深度成像



#### 1. Title: 
All-in-focus Imaging from Event Focal Stack

#### 2. Authors: 
Hanyue Lou, Minggui Teng, Yixin Yang, Boxin Shi

#### 3. Affiliation: 
第一作者：北京大学计算机科学学院，多媒体信息处理国家重点实验室，视觉技术国家工程研究中心

#### 4. Keywords: 
All-in-focus imaging, event camera, focal stack, refocusing timestamp selection, merging weight prediction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lou_All-in-Focus_Imaging_From_Event_Focal_Stack_CVPR_2021_paper.html  Github: https://github.com/hylz-2019/EFS

#### 6. Summary : 
- (1):本文研究的背景是如何从单张图像中恢复出全焦深度的图像，传统的方法需要多张图像，而本文提出了一种基于事件相机的全焦深度图像恢复方法。

- (2):过去的方法包括基于图像的方法和计算摄影方法，但是它们都存在一些问题，比如需要多张图像或者无法记录连续的场景深度信息。本文提出的方法是基于事件相机的全焦深度图像恢复方法，可以连续记录场景深度信息，从而避免了传统方法的问题。

- (3):本文提出了一种事件焦点堆栈（EFS）的方法，它由事件流组成，可以用于重建图像焦点堆栈并预测合并权重以恢复全焦深度图像。EFS首先选择每个场景块的重新聚焦时间戳，然后利用事件流和给定的模糊图像生成对应的重新聚焦图像，形成图像焦点堆栈。最后，根据选择的时间戳周围的邻近事件，预测每个图像的合并权重，从而恢复清晰的全焦深度图像。

- (4):本文在合成和真实数据集上进行了定量和定性评估，结果表明，与现有的方法相比，本文提出的方法在恢复全焦深度图像方面具有更好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于事件聚焦堆栈（EFS）的全焦段成像方法。该方法包括以下步骤：
1. 选择聚焦时间戳：将输入图像分成块，使用基于重建图像清晰度的黄金分割搜索方法为每个块选择聚焦时间戳。
2. EvRefocusNet：设计了一个U-Net网络，用于以数据驱动的方式建模强度残差并生成聚焦图像。该网络可以使用空间-时间变量阈值预测强度残差，以及由输入图像引导的阈值。
3. 图像聚焦堆栈生成：使用步骤1中获得的聚焦时间戳和EvRefocusNet模型，从RGB图像和EFS生成图像聚焦堆栈。
4. EvMergeNet：使用网络预测合并权重，将其与EFS合并，最终获得全焦段图像。

#### 8. 实验设置：
本文使用Blender生成了一个包含200个随机场景的合成图像聚焦堆栈数据集，并使用最新的事件模拟器DVS-Voltmeter模拟了相应的事件流。为了更好地匹配数据分布到真实图像，对象的表面被包裹在从MS-COCO数据集中采样的图像中作为它们的纹理。作者还通过构建混合相机系统捕获了真实数据，该系统由机器视觉相机和分光镜的事件相机组成。

#### 9. 实验结果与分析：
本文将所提出的方法与四种最新的基于图像的虚焦去模糊方法进行了比较，包括公共的合成数据集和真实数据。定量比较表明，所提出的方法在所有指标上均优于其他最先进的方法，提高了15%以上。定性比较表明，所提出的方法可以恢复具有正确纹理的虚焦区域的全焦段图像，并在锐利细节和不良环绕伪影方面优于其他基于图像的方法。


# Paper:579     动态图增强对比学习用于胸部X光报告生成



#### 1. Title: 
Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation

#### 2. Authors: 
Mingjie Li, Bingqian Lin, Zicong Chen, Haokun Lin, Xiaodan Liang, Xiaojun Chang

#### 3. Affiliation: 
第一作者：ReLER, AAII, University of Technology Sydney

#### 4. Keywords: 
Radiology reporting, Chest X-ray, Knowledge graph, Contrastive learning, Report generation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2021_paper.html  Github: https://github.com/mlii0117/DCL

#### 6. Summary : 
- (1):本文研究的是胸部X光报告的自动生成，旨在减轻放射科医生的工作负担和提高诊断解释的准确性。 
- (2):过去的方法主要是将医学知识图谱与数据驱动的神经网络相结合，以消除任务中的严重视觉和文本偏差。然而，这些图谱的结构通常是固定的，无法保证最合适的知识范围，限制了其有效性。本文提出了一种具有动态结构和节点的知识图谱，以促进胸部X光报告的生成。 
- (3):本文提出了一种名为DCL的方法，它利用动态图结合对比学习来增强视觉表示。该方法采用来自[46]的28个实体作为基本结构，然后从检索到的报告中提取特定知识，以底层方式添加附加节点或重新定义它们之间的关系。每个图像特征都与其自己的更新图结合，然后输入解码器模块进行报告生成。此外，本文引入了图像-报告对比和图像-报告匹配损失来更好地表示视觉特征和文本信息。在IU-Xray和MIMIC-CXR数据集上进行评估，DCL在这两个基准测试中均优于以前的最先进模型。 
- (4):本文的方法在胸部X光报告生成任务上取得了良好的性能，证明了动态图结合对比学习对于生成高质量报告的帮助。
#### 7. 方法详细介绍：
本文提出了一种名为动态图增强对比学习（Dynamic Graph Enhanced Contrastive Learning，DCL）的方法，用于胸部X光报告生成。该方法包括三个关键模块：动态图构建、动态图编码器和图注意力。动态图是从基本结构开始自下而上构建的，具有28个实体，根据特定知识添加节点或重新定义它们之间的关系。动态图编码器在动态图中传播信息并学习专用节点特征。图注意力将动态图中的知识与视觉特征相结合。该方法还利用图像-报告对比损失来激活放射学报告，鼓励正面图像-报告对具有相似的表示，以对抗负面对。具体步骤包括：
1. 构建动态图：从基本结构开始，根据特定知识添加节点或重新定义它们之间的关系。
2. 动态图编码器：在动态图中传播信息并学习专用节点特征。
3. 图注意力：将动态图中的知识与视觉特征相结合。

#### 8. 实验设置：
本文在两个放射学报告基准数据集IU-Xray和MIMIC-CXR上评估了提出的DCL模型。将报告预处理并分为训练、验证和测试集。使用4个NVIDIA 2080 Ti GPU进行训练，批量大小为8，训练30个epochs。学习率设置为1e-4，优化器为AdamW，权重衰减为0.02。从文本队列Q中检索与之相似的前3个报告，Q的大小分别设置为65,536和1,380。特定知识的最大长度设置为90。对于批量操作，G中的节点使用[PAD]标记填充到50。模型使用预训练的SciBert作为分词器和报告编码器。

#### 9. 实验结果与分析：
实验结果表明，DCL方法在描述准确性和临床正确性等指标上均优于先前的最先进模型，证明了动态图与特定知识集成和对比学习目标可以提高报告检索过程的准确性并生成高质量的报告。


# Paper:580     压缩感知视频超分辨率



#### 1. Title: 
Compression-Aware Video Super-Resolution

#### 2. Authors: 
Yingwei Wang, Takashi Isobe, Xu Jia, Xin Tao, Huchuan Lu, Yu-Wing Tai

#### 3. Affiliation: 
第一作者：大连理工大学

#### 4. Keywords: 
Video super-resolution, compression-aware, meta data, bidirectional recurrent-based model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Compression-Aware_Video_Super-Resolution_CVPR_2021_paper.html  Github: https://github.com/aprBlue/CAVSR

#### 6. Summary : 
- (1):本文研究的是视频超分辨率问题，而现有的方法大多假设输入是理想的，而实际上，存储在移动设备或通过互联网传输的视频通常是经过压缩的，这导致了实验结果与实际应用之间的巨大性能差距。因此，本文提出了一种新颖的、实用的压缩感知视频超分辨率模型，该模型可以根据估计的压缩水平自适应地进行视频增强处理。

- (2):现有的视频超分辨率方法大多假设输入是理想的，而实际上，存储在移动设备或通过互联网传输的视频通常是经过压缩的，这导致了实验结果与实际应用之间的巨大性能差距。本文提出的压缩感知视频超分辨率模型可以根据估计的压缩水平自适应地进行视频增强处理，而且还充分利用了压缩视频流中自然嵌入的元数据，进一步增强了模型的性能。

- (3):本文提出了一种压缩感知视频超分辨率模型，其中设计了一个压缩编码器来模拟输入帧的压缩水平，并通过插入压缩感知模块将基础VSR模型与隐式计算的表示条件化。此外，本文还利用压缩视频流中自然嵌入的元数据，在信息融合过程中进一步加强了VSR模型的能力。通过大量实验，证明了所提出的方法在压缩VSR基准测试中的有效性和效率。

- (4):本文提出的压缩感知视频超分辨率模型在压缩VSR基准测试中取得了良好的性能，相比现有方法有了很大的提升，证明了该方法的有效性和实用性。
#### 7. 方法详细介绍：
本文提出了一种压缩感知视频超分辨率方法，该方法可以处理不同压缩水平的视频。该方法的关键在于设计了一个压缩编码器，可以将视频帧中的压缩隐式建模为压缩表示。压缩表示的学习被视为在构建的对中进行学习排序任务。在基础VSR模型中插入一个基于压缩表示的压缩感知调制模块，以实现压缩感知VSR。利用视频中自然编码的MV和残差图等元数据，在运动补偿和传播模块中进一步提高VSR性能。

具体步骤如下：
1. 压缩编码器：将视频帧中的压缩隐式建模为压缩表示，该模块由卷积神经网络构成，通过学习排序任务进行训练。
2. 压缩感知调制模块：基于压缩表示对基础VSR模型进行调制，以实现压缩感知VSR。
3. 运动补偿和传播模块：利用MV和残差图等元数据，进一步提高VSR性能。
4. 隐藏状态更新：利用元数据和当前帧的聚合SR特征更新模型的隐藏状态，以辅助下一帧的SR过程。

#### 8. 实验设置：
本文在两个数据集上进行了实验：Vimeo-90K和REDS。Vimeo-90K数据集包含89,800个分辨率为448x256、帧率为24fps的视频剪辑。REDS数据集包含240个分辨率为720p、帧率为30fps的视频剪辑。两个数据集中的视频都使用不同的CRF值进行压缩。本文提出的方法与BasicVSR、STDF和FTVSR等最先进的方法进行了比较。

#### 9. 实验结果和分析：
本文提出的压缩感知视频超分辨率（CAVSR）方法在三个压缩水平上均优于大多数先前的VSR方法，无论是在PSNR还是SSIM方面。该方法在视觉质量方面也表现出更好的效果，包括更细的细节和更锐利的边缘。与其他最先进的方法相比，该方法也在真实世界的压缩视频上表现良好。


# Paper:581     从图像到文本提示：使用冻结的大型语言模型进行零样本视觉问答



#### 1. Title: 
From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models

#### 2. Authors: 
Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, Steven Hoi

#### 3. Affiliation: 
第一作者：悉尼大学

#### 4. Keywords: 
Visual Question Answering, Large Language Models, Zero-shot Learning, Multimodal Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2021_paper.html  Github: https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa

#### 6. Summary : 
- (1):本文研究的背景是视觉问答（VQA）任务中的零样本学习问题，即如何在没有训练数据的情况下让模型具有泛化能力。
- (2):过去的方法主要有两种，一种是基于多模态预训练的方法，另一种是基于语言中介的方法。前者需要大量的计算资源和数据，后者需要大量的人工标注数据。本文提出了一种新的方法Img2LLM，它可以将图像转化为文本提示，使得大型语言模型可以在零样本学习的情况下完成VQA任务。该方法可以灵活地与各种大型语言模型配合使用，且不需要进行端到端的微调，因此可以降低成本并简化部署。实验结果表明，该方法在多个数据集上的表现优于现有的方法。
- (3):本文提出的Img2LLM方法可以将图像转化为文本提示，使得大型语言模型可以在零样本学习的情况下完成VQA任务。该方法的创新点在于，它可以根据当前的图像和问题构造出合适的文本提示，从而避免了需要大量人工标注数据的问题。此外，该方法可以灵活地与各种大型语言模型配合使用，且不需要进行端到端的微调，因此可以降低成本并简化部署。
- (4):本文在多个数据集上进行了实验，结果表明，Img2LLM方法可以在零样本学习的情况下完成VQA任务，并且在多个数据集上的表现优于现有的方法。这表明，该方法可以有效地解决VQA任务中的零样本学习问题，并且可以在实际应用中发挥重要作用。
#### 7. 方法详细介绍：
本文提出了Img2LLM，这是一个即插即用的模块，可以使现成的LLM模型执行零样本VQA任务。Img2LLM的核心思想是利用视觉-语言模型和问题生成模型将图像内容转化为合成的问题-答案（QA）对，这些QA对作为提示的一部分被馈送给LLM。这些示例QA对通过语言描述图像内容来解决模态不匹配问题，并通过演示QA任务来解决任务不匹配问题。

具体步骤如下：
1. 使用BLIP生成与问题相关的图像标题提示，并执行图像-问题匹配。
2. 使用GradCam定位与问题相关的图像区域，并基于GradCam采样K'=20个图像块以获得100个与问题相关的标题。
3. 使用各种大型语言模型（LLMs）自回归地生成答案，而不需要访问答案列表或训练样本，从而实现零样本VQA。

#### 8. 实验设置：
本文在三个VQA数据集上评估了所提出的方法：VQAv2、GQA和A-OKVQA。作者使用OpenAI GPT-2和GPT-3模型作为LLMs，使用BLIP模型作为视觉-语言模型。作者还将其方法与几种最先进的VQA方法进行了比较，包括少样本和零样本方法。

#### 9. 实验结果与分析：
实验结果表明，所提出的方法的性能与依赖端到端训练的方法相当或更好。例如，所提出的方法在VQAv2上比Flamingo高出5.6%。在具有挑战性的A-OKVQA数据集上，所提出的方法比少样本方法高出多达20%。作者还表明，他们的方法是灵活的，并且可以与各种LLMs接口以执行VQA。


# Paper:582     StyleSync：基于样式生成器中高保真度的通用和个性化唇形同步



#### 1. Title: 
StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator

#### 2. Authors: 
Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu Hu, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, Jingdong Wang

#### 3. Affiliation: 
第一作者：Jiazhi Guan，清华大学

#### 4. Keywords: 
lip-sync, style-based generator, personalized optimization, audio-driven facial animation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_CVPR_2021_paper.html  Github: https://github.com/Hangz-nju-cuhk/StyleSync

#### 6. Summary : 
- (1):本文研究的是如何实现高保真度的唇形同步，以及如何将个性化信息融入到通用模型中。
 
- (2):过去的方法要么需要长时间的数据训练，要么在所有主题上产生相似的运动模式，质量较低。本文提出了StyleSync框架，通过对基于样式的生成器进行简单但关键的修改，实现了高保真度的唇形同步。与过去的方法不同，我们采用了基于掩码的口部建模协议，并精心设计了基于掩码的空间信息编码策略，同时引入了样式空间和生成器优化，实现了个性化唇形同步。 

- (3):本文提出了一种高保真度的唇形同步框架StyleSync，通过对基于样式的生成器进行简单但关键的修改，实现了高保真度的唇形同步。我们采用了基于掩码的口部建模协议，并精心设计了基于掩码的空间信息编码策略，同时引入了样式空间和生成器优化，实现了个性化唇形同步。我们还提出了个性化优化程序，通过少量的人物信息进行优化，进一步提高了生成质量。实验结果表明，我们的方法在各种场景下都能产生高保真度的结果，并且在一次性设置下明显优于以前的方法。

- (4):本文提出的方法在唇形同步任务上取得了高保真度的结果，同时能够将个性化信息融入到通用模型中。实验结果表明，我们的方法在一次性设置下明显优于以前的方法。
#### 7. 方法详细介绍：
StyleSync是一种高保真度的唇形同步方法，它采用了StyleGAN2架构进行修改。该方法将音频动态和面部信息编码到生成器的样式空间中，使用基于掩码的空间信息编码将同步的嘴部混合到目标帧中，并使用个性化优化模块来学习一组∆W，使生成器的参数能够在个性化数据的边缘上移动一小部分到∆P。在骨干训练期间，该方法使用像素级重建损失、对抗损失和唇形同步损失作为训练目标。

具体步骤如下：
1. 从音频中提取音频特征，包括Mel频谱和能量；
2. 将音频特征输入到SyncNet中，SyncNet是一个用于唇形同步的深度学习模型；
3. 将SyncNet的输出作为唇形同步损失，与像素级重建损失和对抗损失一起作为训练目标；
4. 在生成器中添加Mask-based Spatial Information Encoding，将目标帧和参考帧的信息编码到生成器的样式空间中；
5. 添加Personalized Optimization模块，学习一组∆W，使生成器的参数能够在个性化数据的边缘上移动一小部分到∆P。

#### 8. 实验设置：
本文使用了两个常用的音视频数据集，分别是LRW和VoxCeleb2。视频以25fps处理，并根据预先检测到的眼睛标记进行对齐。所有面部都被裁剪到256×256的大小。生成器共有14个样式卷积层，λr被经验性地设置为10，所有其他λs都被选为1。对于个性化优化，人的数据被训练5个epochs。在实验中，个人数据和通用数据的比例被设置为1:1。

#### 9. 实验结果与分析：
本文提出的StyleSync方法在一次性和少次性设置下，都优于之前的最先进方法，包括唇形同步质量、生成质量和视频真实性。定量消融研究结果表明，所提出的掩码策略和来自SyncNet的额外监督对提高唇形同步质量是有效的。个性化优化过程中的∆W被证明有助于保留身份信息。


# Paper:583     边界增强的协同训练用于弱监督语义分割



#### 1. Title: 
Boundary-enhanced Co-training for Weakly Supervised Semantic Segmentation

#### 2. Authors: 
Shenghai Rong, Bohai Tu, Zilei Wang, Junjie Li

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Weakly supervised semantic segmentation, co-training, boundary enhancement, noisy labels, pseudo-labels

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/ShenghaiRong/BECO

#### 6. Summary : 
- (1):本文研究的是弱监督语义分割（WSSS）领域，主要关注的是第二阶段的模型训练，即如何在噪声标签的情况下进行鲁棒学习。本文发现，当前WSSS方法生成的伪标签（pseudo-labels）质量与最终分割模型的性能之间存在不一致性，而错误标记的像素主要位于边界区域。因此，本文提出了一种边界增强的协同训练（BECO）方法，旨在通过协同训练和边界增强策略来提高模型的鲁棒性，从而实现更好的分割性能。

- (2):目前的WSSS方法主要关注生成准确和完整的伪标签，而忽略了训练分割网络的重要性。然而，由于WSSS方法生成的伪标签存在噪声，直接训练模型会导致过拟合，从而降低分割网络的泛化性能。因此，本文提出了一种新的思路，即将WSSS的重点从伪标签生成转移到在噪声标签下进行鲁棒学习。同时，本文发现伪标签中的噪声主要集中在边界区域，因此提出了一种边界增强策略来提高边界像素的预测效果。

- (3):本文提出了一种协同训练的方法，通过构建两个交互式网络来改善不确定像素的学习。同时，本文提出了一种边界增强策略，通过可靠的预测结果构建人工边界，从而提高难以预测的边界区域的预测效果。实验结果表明，BECO方法在PASCAL VOC 2012和MS COCO 2014数据集上取得了优异的分割性能，优于其他现有的WSSS方法。

- (4):本文的方法在PASCAL VOC 2012和MS COCO 2014数据集上进行了实验，取得了优异的分割性能，证明了本文提出的BECO方法的有效性。本文的贡献在于提出了一种新的思路，即将WSSS的重点从伪标签生成转移到在噪声标签下进行鲁棒学习，并提出了一种边界增强的协同训练方法，从而实现更好的分割性能。
#### 1. 实验结果
(1). 本文提出的BECO方法在PASCAL VOC 2012和MS COCO 2014数据集上均取得了最先进的性能。在PASCAL VOC 2012上，使用ImageNet预训练的骨干网络，BECO方法分别达到了72.1%和71.8%的mIoU，优于其他方法如IRN、ReCAM和AMN。在MS COCO 2014上，BECO方法实现了45.1%的mIoU，创造了新的最先进水平。第二阶段学习减少了不同伪标签之间的性能差距，表明更好的伪标签不能保证训练出更好的分割模型。

#### 2. 方法详细介绍
(1). 本文提出了一种边界增强的协同训练（BECO）方法，用于弱监督语义分割。该方法由两个主要组件组成：协同训练和边界增强。在协同训练中，构建了两个并行的深度网络来执行语义预测，旨在相互教授所有可能存在噪声的像素。两个网络预测的一致性被强制执行，以纠正不确定像素的语义信息。在边界增强中，将更大的权重分配给损失中的边界像素，以提高它们的预测。通过复制和粘贴一个图像中的高置信度区域来构建具有准确标签的人工边界。该方法旨在通过第二阶段的WSSS中的噪声标签来提高鲁棒性学习。

#### 3. 实验设置
(1). 本文在PASCAL VOC 2012和MS COCO 2014数据集上进行了评估。Deeplabv2模型用作分割网络。实验在单个NVIDIA TITAN Xp GPU上使用PyTorch框架进行。

#### 4. 实验细节
(1). 本文提出的BECO方法在PASCAL VOC 2012和MS COCO 2014数据集上均取得了最先进的性能。在PASCAL VOC 2012上，该方法实现了56.7%的mIoU，优于其他最先进的方法。在MS COCO 2014上，该方法实现了32.1%的mIoU，也优于其他最先进的方法。实验结果表明，该方法在不同CAMs的分割性能方面具有很好的效果。

#### 5. 方法详细介绍
(1). 本文提出了一种边界增强的协同训练（BECO）框架，用于从弱监督的伪标签中进行鲁棒学习。BECO框架由两个并行的深度网络组成，共享相同的架构，并使用协同训练范式进行训练。协同训练损失旨在对不确定像素的两个网络的预测进行一致性约束。提出的边界构建策略用于通过将更大的权重分配给协同训练损失来突出显示图像的困难边界区域。BECO的整体结构是一个具有两个并行分支的连体网络，在训练期间，模型同时输入原始图像和边界感知图像，并使用提出的协同训练范式进行优化。

#### 6. 方法详细介绍
(1). 本文提出了一种边界增强的协同训练方法，用于弱监督语义分割。该方法由两个组件组成：协同训练和边界构建。在协同训练组件中，使用边界感知图像生成策略训练两个分割网络。在边界构建组件中，生成二进制边界图以增强图像的边界区域。根据边界图重新加权协同训练损失，以进一步提高模型对边界区域的预测。

#### 7. 方法详细介绍
(1). 本文提出了一种边界增强的协同训练方法，用于弱监督语义分割。该方法由两个组件组成：协同训练和边界构建。在协同训练组件中，使用边界感知图像生成策略训练两个分割网络。在边界构建组件中，生成二进制边界图以增强图像的边界区域。根据边界图重新加权协同训练损失，以进一步提高模型对边界区域的预测。

#### 8. 实验设置
(1). 本文在PASCAL VOC 2012和MS COCO 2014数据集上进行了评估。使用IRN生成伪标签，置信度掩码的比率为50%。DeeplabV3+作为分割网络的骨干网络。输入图像进行随机缩放、随机水平翻转和随机裁剪到512的大小。BECO模型在VOC和MS COCO数据集上分别进行了80个和40个时期的训练，共同批量大小为16。

#### 9. 实验结果和分析
(1). 本文提出的方法在不同的噪声伪标签上优于基线和两个单独训练网络的集成。边界增强的协同训练方法在PASCAL VOC 2012数据集上实现了最佳性能，平均比ENSEMBLE提高了2.7%。该方法还显示了在不同的骨干网络上的优越性，并有效地提高了从噪声标签中进行鲁棒学习的单个网络的性能。BECO对边界区域的预测通过定性分割结果进行了验证。


# Paper:584     ToThePoint: 通过回收实现高效的3D点云对比学习



#### 1. Title: 
ToThePoint: Efficient Contrastive Learning of 3D Point Clouds via Recycling

#### 2. Authors: 
Xinglin Li, Jiajing Chen, Jinhui Ouyang, Hanhui Deng, Senem Velipasalar, Di Wu

#### 3. Affiliation: 
Hunan University, China (湖南大学)

#### 4. Keywords: 
Point cloud processing, self-supervised learning, contrastive learning, recycling, 3D point clouds

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_ToThePoint_Efficient_Contrastive_Learning_of_3D_Point_Clouds_via_Recycling_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究点云处理中的自监督学习，提出了一种有效的对比学习方法，名为ToThePoint，以提取3D点云的潜在表示。与现有的自监督学习方法相比，该方法不需要大量的数据样本，也不需要大量的计算资源和训练时间。

- (2):现有的自监督学习方法需要大量的数据样本进行预训练，这使得它们在计算上更加昂贵和耗时。本文提出的方法通过回收点云网络的最大池化模块丢弃的点云特征，作为对比学习的特征增强方法，从而实现了对少量点云数据的自监督预训练。与传统的对比学习方法不同，ToThePoint不仅最大化不同类型数据样本对应的特征之间的一致性，还最大化排列不变特征和最大池化后丢弃的特征之间的一致性。

- (3):本文提出了一种两个分支的对比学习框架，包括跨分支对比学习损失和内部分支对比学习损失。通过将最大聚合特征和回收点云特征映射到超球面空间，本文的方法可以将分布引入超球面空间，从而提高了点云特征的表示能力。

- (4):本文在ShapeNet数据集上进行了自监督学习，然后在不同的下游任务上评估了网络的性能。在ModelNet40、ModelNet40C、ScanobjectNN和ShapeNet-Part数据集上的下游任务实验中，ToThePoint相对于现有的基线方法，取得了竞争性甚至更好的结果，并且训练时间显著缩短（比基线方法快200倍）。
#### 7. 方法详细介绍：
本文提出了一种名为ToThePoint的新型对比学习方法，用于高效的自监督学习3D点云。该方法最大化排列不变特征和最大池化后丢弃的特征之间的一致性，同时最大化不同类型数据增强形成的一对点云之间的特征一致性。该方法采用了两个分支的对比学习框架，包括交叉分支对比学习损失和内部分支对比学习损失。该方法还使用了回收机制来提高完全监督的3D点云处理任务的性能，包括分类和分割。该方法在合成和真实数据集上进行了三个下游任务的评估，分别是物体分类、少样本学习和部分分割。

#### 8. 实验设置：
本文使用ShapeNet数据集进行所提出的ToThePoint和基线的自监督训练。同时在ScanObjectNN、ModelNet40和ModelNet40-C数据集上进行完全监督和少样本点云分类，以及在ShapeNet-Part上进行部分分割。点云特征提取的骨干网络采用PointNet和DGCNN。使用Adam作为优化器，权重衰减为1×10−4，初始学习率为1×10−3。

#### 9. 实验结果和分析：
本文提出的ToThePoint方法在各项指标上均优于先前的方法，特别是在少样本学习任务中表现更加出色。在ScanObjectNN数据集上，ToThePoint的性能提升比ModelNet40更高，使用PointNet作为骨干网络时，性能提升更为明显。在ShapeNet-Part数据集上，ToThePoint方法也提高了部分分割的准确性。本文的实验结果表明，ToThePoint方法在3D点云的自监督学习中具有很高的效率和准确性。


# Paper:585     RankMix：用于分类具有不同大小和不平衡类别的全幻灯片图像的弱监督学习的数据增强



#### 1. Title: 
RankMix: Data Augmentation for Weakly Supervised Learning of Classifying Whole Slide Images with Diverse Sizes and Imbalanced Categories

#### 2. Authors: 
Yuan-Chih Chen, Chun-Shien Lu

#### 3. Affiliation: 
Yuan-Chih Chen: IIS, Academia Sinica, Taiwan, ROC (中央研究院資訊科學研究所)
Chun-Shien Lu: IIS, Academia Sinica, Taiwan, ROC (中央研究院資訊科學研究所)

#### 4. Keywords: 
Whole Slide Images, Weakly Supervised Learning, Data Augmentation, Class Imbalance, Multiple Instance Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_RankMix_Data_Augmentation_for_Weakly_Supervised_Learning_of_Classifying_Whole_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是针对医学图像中的WSI（Whole Slide Images）分类问题，WSI通常具有巨大的像素大小和缺乏像素级别的注释，同时数据集中的类别也存在不平衡的情况，这些特点使得WSI分类成为一种弱监督学习问题。

- (2):过去的方法主要集中在特征提取和聚合器架构设计上，而本文则提出了一种数据增强方法RankMix，通过混合WSI对中的排名特征来提取对WSI分类任务有贡献的关键WSI区域。与过去的方法相比，RankMix可以处理WSI之间的大小差异，并且可以缓解WSI分类问题中的数据不足和类别不平衡问题。

- (3):本文提出的方法是在MIL（Multiple Instance Learning）框架下进行的，通过伪标签和排名的概念来提取关键WSI区域，进而进行数据增强。此外，本文还提出了两阶段训练来提高模型性能。

- (4):本文在Camelyon16数据集上进行了实验，结果表明，RankMix方法可以有效地提高WSI分类的性能，同时也可以缓解数据不足和类别不平衡问题。
#### 7. 方法详细介绍：
本文提出了一种名为RankMix的数据增强方法，用于弱监督学习分类具有不同大小和不平衡类别的全幻灯片图像。RankMix引入了伪标签和排名的概念，以提取对WSI分类任务有贡献的关键WSI区域。进一步提出了两阶段训练来提高稳定训练和模型性能。具体步骤如下：
1. 使用多层感知器（MLP）预测伪补丁级标签，计算每个补丁对最终幻灯片级别预测的重要性得分函数。
2. 计算每个WSI中每个补丁的类概率。
3. 对补丁得分进行排序，以获得能够代表原始WSI的代表性部分。
4. 基于所需特征的数量，从原始特征中获取所需特征。
5. 使用mixup技术将代表性特征与mixup机制相结合。

#### 8. 实验设置：
本文未提及具体的实验设置。

#### 9. 实验结果和分析：
本文提出了一种新的数据增强方法RankMix，用于弱监督学习分类具有不同大小和不平衡类别的全幻灯片图像。将该方法应用于两个基线模型DSMIL和FRMIL，并与基准模型进行比较。结果表明，自我训练提高了整体模型性能，RankMix的性能明显优于ReMix。本文还将RankMix与四种自定义mixup技术进行了比较，并发现RankMix通常表现更好。最后，进行了消融研究，以进一步评估RankMix在两个方面的性能。


# Paper:586     PaletteNeRF：基于调色板的神经辐射场外观编辑



#### 1. Title: 
PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields

#### 2. Authors: 
Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon Wetzstein, Kalyan Sunkavalli

#### 3. Affiliation: 
第一作者：斯坦福大学（Stanford University）

#### 4. Keywords: 
Neural Radiance Fields, Appearance Editing, Color Palette, Photorealism, 3D Reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2021_paper.html  Github: https://github.com/palettenerf/PaletteNeRF

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）的外观编辑，NeRF是一种用于3D重建和新视角合成的方法，但其外观编辑能力有限，难以进行直观的颜色编辑。

- (2):过去的方法主要有两种，一种是基于物理属性的编辑，如反射率、粗糙度等，但这些方法对于真实世界的复杂场景往往难以准确估计；另一种是基于潜在编码的编辑，但这些方法的容量和灵活性有限，不支持细粒度的编辑。本文提出了一种基于3D颜色分解的调色板NeRF方法，通过将每个3D点的外观分解为一组3D调色板颜色基础上的线性组合，实现了直观且逼真的场景着色，支持各种基于调色板的编辑应用。

- (3):本文提出的方法是将NeRF的外观分解为一组调色板颜色基础上的线性组合，其中调色板颜色基础是视角无关的，同时预测一个视角相关的函数来捕捉颜色残差。在训练过程中，我们联合优化基础函数和颜色调色板，并引入新的正则化项来鼓励分解的空间一致性。我们的方法允许用户通过修改颜色调色板来有效地编辑3D场景的外观。我们还将我们的框架与压缩语义特征相结合，支持语义感知的外观编辑。与以往的基于调色板的图像或视频编辑方法不同，我们的方法在任意视角下产生更具全局一致性和3D一致性的着色结果。本文的创新点在于提出了一种基于调色板的NeRF外观编辑方法，支持直观且逼真的场景着色，实现了细粒度的本地颜色编辑。

- (4):本文的方法在多个任务上取得了良好的性能，包括3D场景的着色、光照修改和3D逼真的风格迁移。实验结果表明，与基线方法相比，本文的方法在视觉效果和定量指标上均有所提升，支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种名为PaletteNeRF的方法，用于基于3D颜色分解的神经辐射场（NeRF）的高效外观编辑。该方法将每个3D点的外观分解为一组基于调色板的线性组合，这些基于调色板的基础色调在整个场景中共享。在训练期间，联合优化每个点的高光组件、全局颜色基础和每个点的线性权重，以最小化渲染图像与真实图像之间的差异。该方法允许用户通过修改颜色调色板来高效地编辑3D场景的外观。该方法还支持压缩的语义特征，以进行语义感知的外观编辑。该流程包括三个阶段：预处理、渲染和优化，以及对基于调色板的基础函数、视角相关颜色和最终输出的一系列损失的部署。

#### 8. 实验设置：
本文使用了NeRF Blender数据集、前向LLFF数据集和360度Mip-NeRF360数据集进行实验。实验使用PyTorch实现，并在单个NVIDIA RTX 3090 GPU上运行。模型使用Adam优化器进行300-600个epoch的训练，学习率设置为0.01，总共不超过2小时。在Amazon Mechanical Turk平台上进行了用户研究，要求用户比较我们的模型与其他基线方法的视角一致性和逼真度。

#### 9. 实验结果和分析：
本文的实验结果表明，PaletteNeRF方法在外观编辑方面具有很好的效果。与其他基线方法相比，PaletteNeRF方法在视角一致性和逼真度方面都有显著的提升。用户研究结果也表明，PaletteNeRF方法在视觉效果和易用性方面都优于其他方法。此外，本文还进行了消融实验，证明了所提出的方法的有效性。


# Paper:587     视觉提示调整的生成转移学习



#### 1. Title: 
Visual Prompt Tuning for Generative Transfer Learning

#### 2. Authors: 
Kihyuk Sohn, Huiwen Chang, Jos´e Lezama, Luisa Polania, Han Zhang, Yuan Hao, Irfan Essa, Lu Jiang

#### 3. Affiliation: 
Kihyuk Sohn: Google Research, Brain Team, Mountain View, CA, USA

#### 4. Keywords: 
Generative transfer learning, vision transformers, prompt tuning, image synthesis, knowledge transfer

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2022/html/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2022_paper.html
Github: https://github.com/google-research/generative_transfer

#### 6. Summary : 
- (1):本文研究背景是图像合成的知识转移问题，传统的图像合成方法需要大量的训练数据，而本文提出的方法可以通过少量的训练数据实现知识转移，从而提高图像合成的效率。

- (2):过去的方法主要是基于预训练的生成对抗网络（GAN）模型进行知识转移，但是这些方法的应用范围较窄，只能用于特定的视觉领域，或者需要大量的训练数据。本文提出的方法基于生成视觉转换器，可以将图像表示为视觉令牌序列，通过可学习的提示令牌进行知识转移，从而实现对不同视觉领域的图像合成。

- (3):本文提出了一种基于提示调整的生成转移学习框架，通过引入可学习的提示令牌来适应新的视觉领域，同时提出了一种新的提示令牌设计方法，可以实现对图像合成的控制。本文还对两种不同类型的生成转换器进行了研究，即自回归（AR）和非自回归（NAR）转换器，提出了一种基于条件的AR和NAR转换器模型，可以在少量的训练数据下实现高质量的图像合成。

- (4):本文在多个视觉领域进行了大规模的实验，证明了生成视觉转换器与提示调整的方法在图像合成方面的有效性和优越性，取得了优异的性能表现。
#### 7. 方法详细介绍：
本文提出了一种基于视觉提示的生成迁移学习框架，使用提示调整来适应新的目标分布。该框架基于生成视觉变换器，将图像表示为视觉令牌序列，使用自回归或非自回归变换器。为了适应新的领域，该框架采用提示调整，将可学习的提示令牌预置到图像令牌序列中，并为任务引入新的提示设计。本文还提出了一种参数高效的提示令牌生成器设计，允许使用条件变量（如类别）进行可控图像合成。该方法在视觉变换器的生成迁移学习上与全量微调和适配器微调方法进行了比较。研究表明，使用提示调整的生成视觉变换器在VTAB的19个具有不同视觉分布和不同数量的训练数据的任务上，远远优于使用GAN的现有生成迁移学习方法。

#### 8. 实验设置：
本文在VTAB基准测试上进行了广泛的生成迁移学习实验，该基准测试是基于16个数据集的19个视觉识别任务的套件。使用类条件图像生成模型在VTAB（完整）任务上进行训练，其中类条件提示在“train”分割上进行训练，使用相同的超参数跨任务。本文研究了在ImageNet数据集的256x256图像上训练的AR和NAR变换器作为源模型的生成迁移。本文还将所提出的方法与基于GAN的生成迁移学习方法进行了比较，包括MineGAN和cGANTransfer。

#### 9. 实验结果与分析：
本文提出了一种视觉提示调整方法进行图像合成的生成迁移学习。该方法在类别条件下优于现有的生成迁移学习方法。在实例条件下，该模型优于DiffAug、cGANTransfer和LeCam GAN等高竞争力的少样本生成模型。该方法表现出卓越的数据效率，仅使用每类5个训练图像即可实现显著降低的FID。本文还提供了基于类别和实例的生成图像可视化。结果证实了所提出方法在少样本图像合成任务上的有效性。


# Paper:588     HGFormer：用于领域泛化语义分割的分层分组变压器



#### 1. Title: 
HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation

#### 2. Authors: 
Jian Ding, Nan Xue, Gui-Song Xia, Bernt Schiele, Dengxin Dai

#### 3. Affiliation: 
Jian Ding, Nan Xue, Gui-Song Xia: 武汉大学计算机学院, NERCMS, State Key Lab. LIESMARS; Bernt Schiele, Dengxin Dai: Max Planck Institute for Informatics

#### 4. Keywords: 
Semantic Segmentation, Domain Generalization, Hierarchical Grouping, Transformer

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2022_paper.html
Github: https://github.com/dingjiansw101/HGFormer

#### 6. Summary: 
- (1):本文研究的是在领域泛化设置下的语义分割问题，即模型仅在源域上进行训练，然后在未见过的目标域上进行测试。 
- (2):过去的方法主要是基于像素级分类或直接生成全局掩模的方法，但这些方法在跨域设置下容易出现错误。本文提出了一种分层分组的方法，将像素首先分组成部分级别的掩模，然后再将其分组成全局掩模。这种方法比直接将像素分组成全局掩模更加鲁棒，因为部分级别和全局级别的掩模在不同尺度上分割图像。因此，部分级别和全局级别的掩模是互补的，将这两个级别的掩模分类结果结合起来可以提高整体的鲁棒性。 
- (3):本文提出了一种分层分组变压器（HGFormer），在分割模型的解码器中使用。首先将特征图发送到部分级别分组模块，然后将像素分配到聚类中心，以获取部分级别的掩模。然后使用交叉注意力生成全局掩模，并在不同级别上分类掩模，最后将所有尺度的语义分割结果平均。 
- (4):在七个具有挑战性的语义分割数据集上进行了实验，结果表明，与以前的基于像素级分类和全局掩模的分割模型相比，HGFormer在跨域泛化语义分割方面表现更好。
#### 7. 方法详细介绍：
本文提出了一种分层分组变压器模型，称为HGFormer，用于域通用语义分割。该模型分为两个阶段：部分级别分组和整体级别分组。在部分级别分组中，图像被分成规则的网格单元，每个单元格内的特征被平均以获取部分级别掩模的特征。然后，计算软分配矩阵以将每个像素分配给部分级别掩模之一。部分级别令牌从另一个特征图中提取，使用线性分类器层和softmax激活层将令牌映射到部分级别类别预测。在整体级别分组中，使用变压器解码器将部分级别掩模分组为整体级别掩模。交叉注意层后跟自我注意，变压器解码器操作重复L次。在每个变压器解码器层的末尾，有两个MLP层。第一个MLP层将输出特征映射到掩模嵌入，整体级别掩模通过最大特征图和投影整体级别掩模令牌之间的矩阵乘法计算。最终结果是部分级别和整体级别语义分割结果的总和。

#### 8. 实验设置：
本文在四种通用性设置下评估了所提出的HGFormer模型：从正常到恶劣的通用性、Cityscapes到其他数据集的通用性、Mapillary到其他数据集的通用性和常见损坏数据集。本文将所提出的模型与以前的基于CNN和变压器的分割方法进行了比较。

#### 9. 实验结果和分析：
实验结果表明，HGFormer在域通用语义分割中始终优于Mask2former，并且在推广到极端损坏的图像时比Mask2former显着更好。HGFormer的部分级别和整体级别掩模分类的个体推广性能也得到了报告，表明部分级别分类显着优于整体级别分类。可视化结果表明，在不利条件下，HGFormer比Mask2former产生更少的错误。本文还将所提出的模型与以前的CNN和变压器分割方法进行了比较，结果表明HGFormer比以前的方法显着更好。


# Paper:589     AeDet：方位不变的多视角三维物体检测



#### 1. Title: 
AeDet: Azimuth-invariant Multi-view 3D Object Detection

#### 2. Authors: 
Chengjian Feng, Zequn Jie, Yujie Zhong, Xiangxiang Chu, Lin Ma

#### 3. Affiliation: 
Meituan Inc. (美团点评)

#### 4. Keywords: 
Multi-view 3D object detection, Lift-Splat-Shoot, Azimuth-equivariant convolution, Azimuth-equivariant anchor, Camera-decoupled virtual depth

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Feng_AeDet_Azimuth-Invariant_Multi-View_3D_Object_Detection_CVPR_2021_paper.html  Github: https://fcjian.github.io/aedet/

#### 6. Summary : 
- (1):本文研究多视角三维物体检测，提出了一种新的方法AeDet，旨在通过建模径向对称性来实现方位不变的BEV感知，解决了传统卷积忽略BEV特征的径向对称性和增加检测器优化难度的问题。

- (2):传统的LSS-based方法将图像特征从图像视角转换到Bird-Eye-View（BEV），然后通过卷积骨干和检测头处理BEV特征。然而，BEV特征在多视角3D物体检测中与2D物体检测中的图像特征有很大的不同。传统的卷积忽略了BEV特征的径向对称性，增加了表示学习的难度。本文提出了一种方位等变卷积（AeConv）和一种方位等变锚点，以保留BEV特征的固有属性并减轻优化难度。AeConv的采样网格始终在径向上，因此可以学习方位不变的BEV特征。所提出的锚点使检测头能够学习预测方位无关的目标。

- (3):本文提出了一种Azimuth-equivariant Detector（AeDet），旨在通过建模径向对称性来实现方位不变的BEV感知。具体来说，本文设计了一种方位等变卷积（AeConv）来旋转每个位置的采样网格，使卷积保留BEV特征的径向对称性，并统一不同方位的表示。同时，本文提出了一种方位等变锚点，使检测头能够学习预测方位无关的目标。此外，本文引入了一种相机解耦虚拟深度，以统一不同相机捕获的图像的深度预测。最终，本文在nuScenes数据集上进行了广泛的实验，AeDet在NDS上取得了62.0％的成绩，大大超过了最近的多视角3D物体检测器，如PETRv2和BEVDepth。

- (4):本文提出的AeDet在nuScenes数据集上进行了广泛的实验，取得了62.0％的NDS成绩，显著提高了物体方向（5.2％）和速度（6.6％）的准确性。AeDet的性能超过了最近的多视角物体检测器，如BEVDepth。
#### 7. 方法详细介绍：
本文提出了一种名为AeDet的方法，旨在通过建模径向对称性来实现方位不变的BEV感知。该方法包括两个主要组件：方位等变卷积（AeConv）和方位等变锚点。AeConv用于统一不同方位的表示学习和提取方位不变的BEV特征。方位等变锚点用于重新定义沿径向的锚点，并统一不同方位的预测目标。此外，引入了一个相机解耦的虚拟深度，以改善深度预测并简化深度网络的优化。深度网络只需要学习预测通用的虚拟深度，而不需要考虑不同相机的内在参数。最后，根据经典相机模型将虚拟深度映射到实际深度。

具体步骤如下：
1. 图像视图编码器：将多视图图像输入网络，提取视图特征。
2. 视图变换器：将视图特征转换为方位等变的BEV特征。
3. BEV编码器：进一步提取BEV特征。
4. 检测头：预测目标的方位、中心偏移、速度等信息。

#### 8. 实验设置：
实验在nuScenes数据集上进行，该数据集包含1000个场景，每个场景有来自六个相机的图像。数据集被分为700个、150个和150个场景用于训练、验证和测试。评估指标为nuScenes检测分数（NDS），它是3D检测指标的加权平均值。实验在一台具有8个NVIDIA V100 GPU的服务器上进行。

#### 9. 实验结果和分析：
本文提出的AeDet在nuScenes测试集上取得了62.0%的NDS，大幅超过了最近的多视图目标检测器BEVDepth。实验表明，AeDet显著提高了目标方向（5.2%）和速度（6.6%）的准确性。相机解耦的虚拟深度提高了mAP和NDS。AeDet在原始视图和旋转视图中的性能也进行了比较，结果表明AeDet在两个视图中的表现都优于BEVDepth。还分析了不同数量的虚拟深度区间，结果表明性能对虚拟深度区间的数量不敏感。


# Paper:590     NS3D：三维物体和关系的神经符号基础



#### 1. Title: 
NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations

#### 2. Authors: 
Joy Hsu, Jiayuan Mao, Jiajun Wu

#### 3. Affiliation: 
第一作者：Joy Hsu，斯坦福大学计算机科学系

#### 4. Keywords: 
3D scene understanding, referring expression comprehension, neuro-symbolic approach, view-dependent grounding, data efficiency, generalization, zero-shot transfer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hsu_NS3D_Neuro-Symbolic_Grounding_of_3D_Objects_and_Relations_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是3D场景理解，特别是在复杂的3D场景中理解指代表达式的能力。
- (2):过去的方法主要是基于端到端的方法，但是需要大量的数据来训练，且容易受到数据集偏差的影响。本文提出的方法是一种神经符号方法，将语言转化为程序，并使用神经网络实现程序中的不同功能模块。与以往的方法相比，本文的方法具有更好的数据效率、更好的泛化能力和更好的可解释性。
- (3):本文提出的方法是一种神经符号方法，将语言转化为程序，并使用神经网络实现程序中的不同功能模块。本文的方法在处理高元关系方面具有优势，可以有效地处理复杂的3D场景中的指代表达式。本文的方法在数据效率、泛化能力和零样本迁移方面都取得了很好的表现。
- (4):本文的方法在ReferIt3D基准测试中取得了最先进的结果，具有很好的数据效率、泛化能力和零样本迁移能力。
#### 7. 方法详细介绍：
本文提出了一种神经符号方法，称为NS3D，用于三维场景中的指称表达理解。NS3D结合了程序化的功能结构和模块化的神经网络。它由三个主要组件组成：语义解析器、三维特征编码器和神经网络程序执行器。NS3D解析器将输入语言解析为符号程序，该程序类似于表达式的基本推理过程。程序具有一种特定领域的语言（DSL），用于三维视觉推理任务的基本操作的层次结构。NS3D 3D编码器为每个场景生成面向对象的表示，并在潜在空间中提取关系特征。神经程序执行器将解析的程序和学习的面向对象表示作为输入，并基于3D表示执行程序，返回目标对象。

#### 8. 实验设置：
本文在ReferIt3D基准测试上评估了NS3D，这是一个三维指称表达理解任务。使用SR3D设置，处理三维场景中的空间定向对象指称语言。输入是一组点云，每个点云表示一个对象，以及表达式，目标对象被假定为唯一。本文将NS3D与之前的工作进行比较，并报告了消融实验，以研究关系接地模块的性能和分离对象和关系编码器的重要性。

#### 9. 实验结果和分析：
实验结果表明，NS3D在所有数据有效设置下显著优于之前的工作，在仅使用训练集的0.5％至10％的情况下，仅与训练整个训练集相比，仅有小幅下降。NS3D在仅使用1.5％的训练数据时，仅使用987个示例即可实现52.0％的准确性，而所有其他基线报告的准确性低于27.5％。当使用5％的数据时，NS3D仅有3.6％的差距，而所有其他方法的性能都显著下降。NS3D在5％的训练数据下的59.1％的准确性高于除BUTD-DETR和MVT之外的所有基线的100％训练数据的准确性。NS3D还在未见过的场景类型设置和未见过的对象-关系-对象对设置中优于所有之前的工作。


# Paper:591     HumanGen：使用显式先验生成人类辐射场



#### 1. Title: 
HumanGen: Generating Human Radiance Fields with Explicit Priors

#### 2. Authors: 
Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, Lan Xu

#### 3. Affiliation: 
上海科技大学

#### 4. Keywords: 
3D GANs, human radiance fields, 2D generator, 3D reconstructor, anchor image

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是3D GANs生成视角一致的辐射场，但高质量的人类辐射场生成仍然具有挑战性，部分原因是现有方法中采用的人类相关先验有限。
- (2):现有方法主要采用SMPL等参数化人类模型作为先验，但这种先验缺乏足够的几何细节，且采用的神经渲染方法不能保证生成有意义的3D几何，进一步导致外观伪影。此外，现有的3D人类生成器训练数据集缺乏多样性或视角不平衡。本文提出了HumanGen，通过设计“锚定图像”，将3D人类生成与2D生成器和3D人类重建器的各种先验结合起来，从而显式地将它们融合在一起。本文采用了分叉设计来分离几何和外观的生成，并提出了两阶段混合方案来提高外观生成的质量。本文的方法在几何细节、纹理质量和自由视角性能方面均优于现有方法。
- (3):本文提出了一种新的3D人类生成方案，通过锚定图像将2D生成器和3D重建器作为显式先验融合到3D GAN-like框架中。本文采用了混合特征表示，使用锚定图像来连接3D GAN的潜在空间和现有2D生成器。本文采用了分叉设计来分离几何和外观的生成，并提出了一种适应3D重建器的几何适应方案，以提供额外的细粒度几何监督。对于外观分支，本文提出了一种学习外观场和混合场的方法，并采用两阶段混合策略来充分利用锚定图像中的丰富纹理信息。本文的方法可以无缝地将各种2D潜在编辑工具升级到3D。 
- (4):本文的方法在几何细节、纹理质量和自由视角性能方面均优于现有方法，支持生成360◦自由视角的高质量3D人类辐射场。本文的方法可以无缝地将各种2D潜在编辑工具升级到3D。
#### 7. 方法详细介绍：
本文提出了一种名为HumanGen的方法，用于生成具有明确先验的人类辐射场。该方法由三个模块组成：混合特征、几何生成和纹理生成与混合。混合特征模块使用预训练的2D生成器合成锚定图像和3D生成器合成三平面。几何生成模块利用PIFuHD的重建先验增强几何细节，并使用符号距离场（SDF）表示几何。纹理生成和混合模块使用纹理场来确定RGB值，并应用体积渲染来合成图像。学习混合权重场，并提出了两阶段混合方案来合并锚定图像和合成纹理。

#### 8. 实验设置：
本文使用由G2D生成的训练集，该集合包含230k张2D人类图像，具有相对平衡的视角分布。本文使用Frechet Inception Distance（FID）和Kernel Inception Distance（KID）指标来评估纹理和几何深度的提出方法。

#### 9. 实验结果与分析：
本文将提出的方法与最先进的3D感知生成方法进行比较，包括EG3D、StyleSDF、Stylegan-human + PIFu和GNARF。提出的方法在FID和深度得分方面表现最佳。本文还进行了消融研究，以评估提出方法的不同组成部分的有效性，包括对抗训练、一致性监督和两阶段混合。完整的方法实现了最佳的自我一致性，并保持与锚定图像的一致性。本文还分析了重建先验的几何适应性，并表明提出的方法与PIFuHD的深度差异最小。本文还展示了提出方法与现有2D编辑方法和各种3D应用的兼容性。然而，本文承认提出方法的局限性，例如视角平衡的训练集可能存在偏差和有限的域外生成能力。


# Paper:592     通过感知理解提高视觉表示学习



#### 1. Title: 
Improving Visual Representation Learning through Perceptual Understanding

#### 2. Authors: 
Samyakh Tukra, Frederick Hoffman, Ken Chatfield

#### 3. Affiliation: 
Tractable AI (英国Tractable AI公司)

#### 4. Keywords: 
Visual representation learning, self-supervised learning, masked autoencoders, perceptual similarity, adversarial training

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Tukra_Improving_Visual_Representation_Learning_Through_Perceptual_Understanding_CVPR_2021_paper.html

Github: None

#### 6. Summary : 
- (1):本文研究背景是自监督学习中的视觉表示学习，通过预测输入数据的一部分来训练深度神经网络，而不依赖于显式监督或标签。 
- (2):过去的方法包括基于去噪自编码器的方法，但是图像像素包含高频变化，因此需要避免学习基于局部纹理或模式的重构。本文提出了一种新的方法，即Perceptual MAE，通过引入感知相似性项和对抗训练技术，显式地鼓励模型学习更高级别的场景特征，从而提高了模型的表示能力。 
- (3):本文提出的方法是Perceptual MAE，通过引入感知相似性项和对抗训练技术，显式地鼓励模型学习更高级别的场景特征，从而提高了模型的表示能力。本文的创新点在于，通过动态学习感知相似性，而不是使用预训练网络，从第二个网络中学习感知相似性。同时，通过将特征与对抗鉴别器绑定，而不是与单独的网络绑定，可以更好地捕捉场景布局和物体边界的细节。 
- (4):本文在ImageNet-1K、MS COCO和ADE20K等数据集上进行了实验，结果表明，Perceptual MAE方法在下游任务中表现出更好的性能，超过了以前的方法。在ImageNet-1K数据集上，线性探测的top-1准确率为78.1％，微调后可达到88.1％，而且没有使用额外的预训练模型或数据。
#### 7. 方法详细介绍：
本文提出了一种基于遮蔽自编码器（MAE）的学习框架，引入了感知损失项和对抗训练等技术，以提高MAE学习到的表示的质量。感知损失项基于结构相似性指数（SSIM），特别是多尺度变体（MS-SSIM），用于衡量生成图像与真实图像之间的相似度。此外，本文还引入了特征匹配，利用一个独立的损失网络，鼓励解码器网络与每个对应层的损失网络具有相似的特征表示。本文还尝试了两种对抗训练变体，MSG-GAN和StyleGANv2-ADA，以稳定生成器的训练。本文修改了MSG-GAN和StyleGANv2-ADA方法所使用的多尺度GAN公式，以更均匀地分配编码器和解码器之间的学习，类似于U-Net。

#### 8. 实验设置：
本文在ImageNet-1K、MS COCO和ADE20K等数据集上评估了所提出的方法及其变体在分类、目标检测和语义分割等下游任务上的性能。作者还展示了他们的方法如何进一步利用强大的预训练模型，从而进一步提高性能。

#### 9. 实验结果与分析：
本文的最佳方法在ViT-B架构上实现了53.5 APbox，优于之前最佳结果MAE训练的ViT-B的3.2。对于语义分割，作者在ADE20K上微调了一个UperNet头，获得了高达50.4 mIOU，比MAE高1.2。作者还观察到，感知损失项不仅对训练稳定性起重要作用，而且是性能的主要驱动因素。最后，作者建议通过将像素重构任务的丰富监督与更专注的高级学习信号相结合，可以极大地提高遮蔽自编码器方法的数据效率。


# Paper:593     基于信息融合的领域自适应检测Transformer



#### 1. Title: 
DA-DETR: Domain Adaptive Detection Transformer with Information Fusion

#### 2. Authors: 
Jingyi Zhang, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Xiaoqin Zhang, Shijian Lu

#### 3. Affiliation: 
南洋理工大学S-lab

#### 4. Keywords: 
Object detection, Domain adaptation, Transformer, Information fusion

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_DA-DETR_Domain_Adaptive_Detection_Transformer_With_Information_Fusion_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究领域自适应目标检测，提出了一种基于信息融合的Domain Adaptive Detection Transformer（DA-DETR）方法。

- (2):传统的目标检测方法采用两阶段检测流程，而DETR简化了这个流程，但如何将其应用于领域自适应目标检测仍然是一个问题。本文提出了一种信息融合的方法，将CNN特征和Transformer特征巧妙地融合在一起，实现了从标记源域到未标记目标域的有效转移。相比于现有的领域自适应检测方法，DA-DETR在多个基准测试中均取得了更好的检测性能。

- (3):DA-DETR引入了CNN-Transformer Blender（CTBlender），它将Transformer特征用于调制CNN特征，实现了跨多个尺度的特征对齐和知识转移。CTBlender由两个顺序融合组件组成，包括在图像内融合CNN和Transformer特征的SMF和在多个特征尺度上融合SMF特征的SAF。相比于现有的权重和求和融合策略，SMF首先将CNN特征分成多个具有不同语义信息的组，并通过通道重排进行合并，以实现不同组之间的有效信息通信。然后，SAF聚合每个尺度的SMF特征，以跨多个特征尺度融合语义和定位信息。因此，CTBlender巧妙地捕捉了语义和定位特征，实现了全面有效的跨域特征对齐。

- (4):DA-DETR在多个领域自适应检测基准测试中均取得了优异的检测性能，相比于现有的领域自适应检测方法，DA-DETR在多个基准测试中均取得了更好的检测性能。
#### 7. 方法详细介绍：
DA-DETR是一种面向无监督域自适应目标检测的检测变换器，它包括基础检测器、鉴别器和CNN-Transformer混合器（CTBlender）。基础检测器采用Deformable-DETR，将普通的注意力替换为可变形注意力以提高收敛速度。CTBlender将Transformer特征中的位置和语义信息与CNN特征中的定位信息融合，实现跨域的全面有效特征对齐。网络通过对抗损失Ladv和检测损失Ldet进行优化。具体步骤如下：
1. 基础检测器：采用Deformable-DETR，将普通的注意力替换为可变形注意力以提高收敛速度。
2. CTBlender：将Transformer特征中的位置和语义信息与CNN特征中的定位信息融合，实现跨域的全面有效特征对齐。CTBlender包括两个融合模块：Split-Merge Fusion（SMF）和Scale Aggregation Fusion（SAF）。
3. 鉴别器：用于区分源域和目标域的特征。
4. 优化目标：DA-DETR的优化目标是一个min-max问题，其中检测器被训练以最小化检测损失，CTBlender被训练以最大化对抗损失。
 
#### 8. 实验设置：
实验采用Pytorch实现，ResNet-50作为骨干网络，SGD优化器用于所有实验，动量为0.9，权重衰减为1e-4。在所有实验中，优化目标中的权重因子λ固定为0.1，SMF中的分组数K固定为32。评估指标为每个物体类别的平均精度（AP）和所有物体类别的平均精度（mAP），阈值为0.5的交并比（IoU）。

#### 9. 实验结果和分析：
实验结果表明，DA-DETR在所有四个域自适应场景下的八个数据集上均优于基线和现有方法。在正常天气到雾天的场景中，DA-DETR在所有比较方法中实现了最高的49.9% mAP。在合成场景到真实场景的场景中，DA-DETR在Foggy Cityscapes的验证数据上实现了54.7% mAP，与现有方法MTTrans相当。在跨摄像头自适应的场景中，DA-DETR在KITTI数据集上实现了48.9% mAP，高于除MTTrans之外的所有比较方法。在真实图像到艺术图像的场景中，DA-DETR在PASCAL VOC数据集上实现了43.1% mAP，也是所有比较方法中最高的。


# Paper:594     RenderDiffusion：用于3D重建、修复和生成的图像扩散



#### 1. Title: 
RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation

#### 2. Authors: 
Titas Anciukeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J. Mitra, Paul Guerrero

#### 3. Affiliation: 
第一作者：爱丁堡大学
其他作者：Adobe Research, University of Glasglow, UCL

#### 4. Keywords: 
Image diffusion, 3D reconstruction, 3D-aware inpainting, unconditional generation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2021_paper.html  Github: https://github.com/Anciukevicius/RenderDiffusion

#### 6. Summary : 
- (1):本文针对3D理解任务，提出了一种新的图像扩散模型RenderDiffusion，用于3D重建、3D-aware修复和无条件生成。 
- (2):与GAN和VAE等方法相比，图像扩散模型能更准确地建模复杂数据集，但在3D生成和理解方面的成功应用受到限制。本文提出的方法通过在去噪器中引入潜在的3D表示，使得模型能够在只有2D监督的情况下恢复3D对象。与其他3D生成方法相比，本文方法不需要显式的3D监督，也不需要显式的3D表示，因此可以更好地处理大规模数据集。 
- (3):本文方法的核心是一种新的图像去噪架构，它在每个去噪步骤中生成和渲染场景的中间三维表示。这种方法在扩散过程中提供了3D一致的表示，同时只需要2D监督。本文方法可以从任意视角渲染3D表示，并在ShapeNet和FFHQ数据集上进行了评估，表现出了竞争性的3D场景生成和2D图像到3D场景的推断性能。 
- (4):本文方法在单视图3D重建和3D-aware修复方面表现出色，且无需特定的训练。在ShapeNet和FFHQ数据集上，本文方法生成的3D场景质量和多样性都很高，且在单视图3D重建方面的重建精度优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种名为RenderDiffusion的3D感知图像扩散模型，可用于单目3D重建、3D感知修复和无条件生成。该模型仅使用单目2D监督进行训练，而无需显式的3D监督。该模型的关键是将潜在的3D表示形式纳入去噪器中，从而创建一个归纳偏差，使我们能够在仅训练2D去噪的情况下恢复3D对象，而无需显式的3D监督。潜在的3D结构由一个三平面表示法组成，该表示法由编码器从噪声图像中创建，并由体积渲染器将3D表示法渲染回（去噪后的）2D图像。得到的3D表示法可以从任何视角进行渲染。模型架构概述如图2所示。

#### 8. 实验设置：
本文使用了真实的人脸数据集（FFHQ）、猫脸数据集（AFHQv2）以及从CLEVR和ShapeNet生成的场景数据集进行训练和评估。本文生成了CLEVR1数据集的变体，其中每个场景包含一个单独的物体，站在原点的平面上。本文使用ShapeNet数据集的三个类别的对象：汽车、飞机和椅子，每个对象放置在地面平面上。本文使用Blender进行场景渲染，并在以原点为中心的半球面上均匀采样100个视点。

#### 9. 实验结果与分析：
本文在三个任务上评估了RenderDiffusion：单目3D重建、无条件生成和3D感知修复。本文将RenderDiffusion与EG3D和PixelNeRF在ShapeNet和CLEVR1数据集上的性能进行了比较。本文报告了每个任务和数据集的PSNR和SSIM分数。RenderDiffusion在所有任务和数据集上均优于EG3D，并在单视角监督方面与PixelNeRF具有竞争性的性能。


# Paper:595     基于不确定性感知的语义一致的ODD检测的最优传输



#### 1. Title: 
Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection

#### 2. Authors: 
Fan Lu, Kai Zhu, Wei Zhai, Kecheng Zheng, Yang Cao

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Out-of-Distribution Detection, Optimal Transport, Energy-Based Transport, Semantically Coherent

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2021_paper.html  Github: https://github.com/LuFan31/ET-OOD

#### 6. Summary : 
- (1):本文研究的是如何在无标签的情况下，通过访问未知数据集来识别异常值，即Semantically Coherent Out-of-Distribution (SCOOD)检测问题。 
- (2):过去的方法主要集中在检测语义偏移的OOD样本，但是这些方法在处理未知数据集时存在一些问题，如需要手动去除ID数据，难以泛化到其他OOD领域等。本文提出了一种新的不确定性感知的最优传输方案，通过能量分数来指导样本的聚类分布，从而实现语义一致的分配。 
- (3):本文提出的方案包括两个部分：能量传输机制和聚类扩展策略。能量传输机制通过能量分数来指导样本的聚类分布，从而实现语义一致的分配；聚类扩展策略则通过将全局特征表示映射到低维度的logit空间，从而获得更具有区分性的预测分布，进一步提高了样本的语义差异性。 
- (4):在两个标准SCOOD基准测试上进行了广泛的实验，结果表明，与现有方法相比，本文提出的方法在FPR@95上的表现优异，分别提高了27.69％和34.4％。
#### 7. 方法详细介绍：
本文提出了一种基于不确定性感知的最优传输方案，用于语义一致的离群检测（SCOOD）任务。该方案包括能量传输机制和簇间扩展策略。能量传输机制利用来自逻辑空间的能量分数估计基于不确定性的传输成本，并指导所有样本的簇分布。簇间扩展策略扩大了ID/OOD在能量度量上的差异，增强了能量传输成本将ID/OOD样本指导到不同簇的能力。该方案还包括分类损失和均衡损失，用于将正确的ID标签分配给部分未标记的ID样本，并将来自离群分布的数据识别为负样本（OOD）。

#### 8. 实验设置：
本文使用了CIFAR-10、CIFAR-100、SVHN和TinyImageNet数据集进行实验。在CIFAR-10和CIFAR-100数据集上，使用了ResNet-18和WRN-28-10两种模型进行训练和测试。在SVHN和TinyImageNet数据集上，使用了ResNet-18模型进行训练和测试。在实验中，使用了FPR95、AUROC、AUPR-IN/OUT和ACC等指标来评估模型的性能。

#### 9. 实验结果和分析：
实验结果表明，所提出的基于能量的传输（ET）机制在所有指标上均优于其他基于不确定性的方法，包括FPR95、AUROC、AUPR-IN/OUT和ACC。探讨了额外的训练数据对OOD检测性能和分类准确性的影响，本文提出的方法（包括ET和Lrep）最大程度地减少了ACC的降低。本文提出的方法在所有OOD分数上均优于基线方法，包括MSP、能量和T-energy，在所有指标上均取得了最佳性能。


# Paper:596     神经变换场用于任意风格字体生成



#### 1. Title: 
Neural Transformation Fields for Arbitrary-Styled Font Generation

#### 2. Authors: 
Bin Fu, Junjun He, Jianjun Wang, and Yu Qiao

#### 3. Affiliation: 
深圳高等研究院计算机视觉与模式识别重点实验室

#### 4. Keywords: 
few-shot font generation, style-content disentanglement, neural transformation field, font rendering formula

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2021_paper.html  Github: https://github.com/fubinfb/NTF

#### 6. Summary : 
- (1):本文研究的是少样本字体生成（FFG）任务，即通过少量样本生成新的字体。该任务的研究价值和商业价值都很高。 
- (2):现有的FFG方法主要采用风格-内容分离范式，将目标字体风格通过将源字符的内容表示和参考样本的风格代码相结合来转移。然而，现有方法缺乏在字体生成过程中建模空间变换的能力。本文提出了一种新的方法，将字体生成视为从源字符图像到目标字体图像的连续变换过程，并将相应的变换嵌入到神经变换场中。 
- (3):本文提出的方法是将字体生成视为通过字体像素的创建和消散的连续变换过程，并将这些变换嵌入到神经变换场中。通过估计变换路径，神经变换场通过采样过程生成一组中间变换结果，并开发了一种字体渲染公式将它们累积到目标字体图像中。本文的方法在少样本字体生成任务中取得了最先进的性能，证明了所提出模型的有效性。 
- (4):本文的方法在少样本字体生成任务中取得了最先进的性能，无论是在已见字体中进行未见内容测试还是在未见字体中进行未见内容测试，都表现出优异的生成性能。
#### 7. 方法详细介绍：
本文提出了一种基于神经变换场（NTF）的字体生成方法，将字体生成视为连续的转换过程。该方法使用NTF来嵌入所需的转换，并沿着特定的估计路径生成中间转换结果。字体生成过程在NTF中被建模为字体渲染过程，其中创建强度和消散速率被用来建模字体像素的转换。NTF使用基于CNN的样式估计器和编码器进行训练，以投影字体样式并嵌入特定字符结构信息。本文提出了两种NTF结构，用于通用和局部样式表示。优化过程使用L1损失进行重构，并以端到端的方式进行建模。

#### 8. 实验设置：
本文在CASIA-CH和CASIA-HWDB两个数据集上进行了广泛的实验，以评估所提出的方法在少样本字体生成任务中的性能。实验中包括在已见字体中进行未见内容测试和在未见字体中进行未见内容测试。评估指标包括字符级和字体级分类准确度以及视觉质量。本文将所提出的方法与DM-Font、LF-Font、MX-Font和CG-GAN等几种最先进的方法进行了比较。

#### 9. 实验结果和分析：
本文提出的NTF-Loc模型在SSIM、LPIPS和FID等指标上优于其他最先进的方法。NTF-Uni模型在SSIM和LPIPS方面优于FUNIT。生成未见样式的字体仍然是一个具有挑战性的任务，特别是对于具有大变形的样式。更多的定性结果在附录中提供。


# Paper:597     区域感知的预训练视觉变换器用于开放词汇目标检测



#### 1. Title: 
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers

#### 2. Authors: 
Dahun Kim, Anelia Angelova, Weicheng Kuo

#### 3. Affiliation: 
谷歌研究院

#### 4. Keywords: 
Open-vocabulary object detection, contrastive learning, vision transformers, pretraining, focal loss

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2022/html/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2022_paper.html
Github: None

#### 6. Summary:
- (1):本文研究开放词汇目标检测的问题，提出了一种区域感知的预训练方法，以缩小图像级预训练和开放词汇目标检测之间的差距。
- (2):现有的方法通常使用全图位置嵌入，而不是区域位置嵌入，因此本文提出了一种新的位置嵌入方案，称为“裁剪位置嵌入”，以更好地匹配检测微调中的区域裁剪。此外，本文使用了聚焦损失替换了对比学习中的常见softmax交叉熵损失，以更好地学习信息丰富但困难的样本。最后，本文利用了新颖的目标提议方法来改进开放词汇检测微调。 
- (3):本文提出了RO-ViT，一种简单的方法，以区域感知的方式预训练视觉变换器，用于开放词汇目标检测。在预训练阶段，本文建议随机裁剪和调整位置嵌入的区域，而不是使用整个图像位置嵌入。此外，本文发现在对比学习中使用聚焦损失比使用softmax交叉熵损失更有益。最后，本文利用了新颖的目标提议方法来改进开放词汇检测微调。 
- (4):本文在LVIS和COCO开放词汇检测基准测试和零-shot转移上评估了RO-ViT。RO-ViT在LVIS上实现了32.1 APr的最新水平，在系统级别上超过了最佳现有方法5.8 APr，同时具有竞争性的零-shot转移检测。RO-ViT还改进了图像级表示，并在COCO和Flickr图像文本检索基准测试的12个指标中的9个指标上实现了最新水平，超过了具有更大模型容量的竞争方法。
#### 1. 实验结果
(1). 本文提出的区域感知预训练方法在LVIS开放词汇检测基准测试中取得了最先进的性能，最佳模型达到了32.1 APr。
(2). 在图像-文本检索基准测试中，本文方法在12个指标中的9个指标上取得了最先进的性能。
#### 2. 方法详细介绍
本文提出了一种名为RO-ViT的区域感知预训练方法，用于开放词汇目标检测。该方法使用对比学习进行图像-文本预训练，使用Transformer编码器进行文本编码，使用ViT编码器进行图像编码。基础类别的文本嵌入用于替换传统的固定大小分类器全连接层，用于开放词汇检测。本文提出的裁剪位置嵌入（CPE）用于弥合预训练中使用的全图位置嵌入和开放词汇检测所需的区域级别识别之间的差距。使用focal loss来调整对难例的权重。RO-ViT检测器使用预训练的ViT骨干和检测器头来获取VLM分数和检测分数，将它们组合成开放词汇检测分数使用几何平均值。
#### 3. 实验设置
本文在LVIS和COCO开放词汇检测基准测试和零样本转移上评估了所提出的方法。本文还进行了消融实验以确认每个提出的组件的优点。
#### 4. 实验结果和分析
(1). 本文方法在LVIS数据集上取得了32.1的APr，超过了现有方法OWL-ViT的最佳性能+6.5点。在COCO基准测试中，本文方法在各种来源训练的其他方法中非常有竞争力，使用ViT-B/16和ViT-L/16分别达到30.2和33.0的box AP50。在MS-COCO和Flickr30K基准测试中进行零样本图像-文本检索，本文方法在MS COCO基准测试中优于所有已发布的作品，并与Flickr的最新技术相当。
(2). 消融实验表明，本文提出的裁剪位置嵌入（CPE）和对比损失的focal loss改善了图像-文本检索和开放词汇检测的性能。冻结骨干实验表明，RO-ViT的区域感知表示对下游检测任务至关重要。骨干学习率比率的研究表明，0.1×是最佳选择。下游检测器改进表明，通过定位质量学习物体性质并将其用于检测分数，相对于在提议中使用传统的二元分类，APr的提高明显，提高了+1.9个点。


# Paper:598     深入探究形状感知的零样本语义分割



#### 1. Title: 
Delving into Shape-aware Zero-shot Semantic Segmentation

#### 2. Authors: 
Xinyu Liu, Beiwen Tian, Zhen Wang, Rui Wang, Kehua Sheng, Bo Zhang, Hao Zhao, Guyue Zhou

#### 3. Affiliation: 
第一作者：西安电子科技大学

#### 4. Keywords: 
Zero-shot semantic segmentation, shape-awareness, spectral methods, self-supervised learning, CLIP

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Delving_into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2022_paper.html  Github: https://github.com/liuxinyu95/SAZS

#### 6. Summary : 
- (1):本文研究的是零样本语义分割任务，即在测试时对未见过的类别进行像素级别的预测。由于现有的视觉-语言模型只是在图像级别上进行训练，因此如何在零样本语义分割任务中实现精细的形状分割是一个挑战。

- (2):过去的方法主要分为生成式和判别式两种，但是它们都没有考虑到形状信息的重要性。本文提出了一种基于光谱方法的形状感知零样本语义分割框架（SAZS），通过自监督像素级特征构建的拉普拉斯矩阵的特征向量来提高形状感知度。与现有的形状感知方法相比，本文的方法不需要使用已知类别的掩码，而且在训练时不需要对网络进行重新训练，具有简单而有效的特点。

- (3):本文的方法主要包括三个部分：1）利用已知类别的语言描述来对训练集进行视觉-语言对齐；2）通过比较预测的边界和真实边界来对边界进行约束；3）利用自监督学习得到的拉普拉斯矩阵的特征向量来提高形状感知度。本文的方法在PASCAL-5i和COCO-20i数据集上取得了新的最优性能。

- (4):本文的方法在零样本语义分割任务上取得了最新的最优性能，证明了光谱方法在形状感知方面的有效性。同时，本文的方法还通过分析发现了语言嵌入的局部性和目标形状的紧凑性对性能的影响。
#### 7. 方法详细介绍：
本文提出了一种名为“形状感知零样本语义分割”（SAZS）的方法，用于解决零样本语义分割任务。该方法利用预训练的CLIP模型中包含的丰富语言先验知识，并利用局部区域之间的相似性来执行带约束的边界检测任务。该方法的整体流程包括像素级视觉-语言对齐、形状约束和自监督谱分解。该方法使用扩张残差网络（DRN）和密集预测变换器（DPT）将图像编码为像素级嵌入。预训练的CLIP模型的文本编码器用于将训练集中的类别名称映射到CLIP特征空间中作为锚点特征。该方法在训练期间优化密集视觉编码器，将像素级输出特征朝向CLIP特征空间中的文本锚点进行对齐。该方法还引入边界检测作为约束任务，以使视觉编码器收集和利用输入图像中的形状先验知识。最后，该方法试图以无监督的方式将输入图像分解为具有清晰边界的特征段，然后在融合模块中将这些特征段与神经网络的预测进行融合。

#### 8. 实验设置：
本文在PASCAL-5i和COCO-20i数据集上进行了实验。使用预训练的CLIP-ViT-B/32作为文本编码器，使用DRN或DPT与ViT作为视觉编码器。在训练期间，使用带有0.95动量和5×10−5学习率的SGD优化器进行优化，并通过多项式调度程序进行衰减。本文还进行了消融实验，以证明方法中设计选择的有效性。

#### 9. 实验结果和分析：
本文提出的SAZS框架在零样本语义分割方面实现了最先进的性能，并且比以前的方法有显着的优势。相关性分析突出了形状紧凑性和语言锚点分布对框架性能的影响。SAZS能够更好地将像素级视觉嵌入与CLIP特征空间中的文本锚点对齐。本文在PASCAL-5i和COCO-20i数据集上进行了实验，将SAZS的性能与以前的最先进方法进行了比较，包括ZS3Net、CaGNet、CSRI、SPNet、Baek等。实验结果表明，SAZS在Pascal和COCO数据集上均优于以前的最先进方法，并为零样本语义分割设置了新的最先进性能。


# Paper:599     基于盲人拍摄的图像的新数据集，用于测试在ImageNet类别训练的图像分类模型的鲁棒性



#### 1. Title: 
A New Dataset Based on Images Taken by Blind People for Testing the Robustness of Image Classification Models Trained for ImageNet Categories

#### 2. Authors: 
Reza Akbarian Bafghi, Danna Gurari

#### 3. Affiliation: 
Reza Akbarian Bafghi: 科罗拉多大学波德分校
Danna Gurari: 科罗拉多大学波德分校

#### 4. Keywords: 
Image classification, robustness testing, dataset, ImageNet, blind people

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Akbarian_Bafghi_A_New_Dataset_Based_on_Images_Taken_by_Blind_People_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文旨在改进在一个领域训练的图像分类模型在另一个领域的图像上表现良好的现状。为此，我们引入了第一个数据集，该数据集来自于摄影师想要了解其图像内容的真实用例。我们使用了由盲人拍摄的8,900张图像构建了一个新的测试集，其中我们收集了元数据以指示200个ImageNet对象类别的存在与否。我们称这个数据集为VizWiz-Classification。我们对这个数据集进行了表征，并比较了它与主流数据集在评估ImageNet训练的分类模型的泛化能力方面的差异。最后，我们分析了100个ImageNet分类模型在我们的新测试数据集上的表现。我们的细粒度分析表明，这些模型在质量问题的图像上表现不佳。为了使未来的扩展能够进行，我们将我们的新数据集与评估服务器共享：https://vizwiz.org/tasks-and-datasets/image-classification。

- (2):过去的方法主要是使用分布发生变化的现有数据集进行鲁棒性测试，但这些数据集存在一定的局限性，因为它们来自人为设置的环境。本文提出了一种新的测试集，该测试集来自于盲人拍摄的图像，这些图像是在真实世界的情况下拍摄的，因此更具有代表性。本文的方法是通过构建一个新的测试集来测试模型的鲁棒性，该测试集来自于真实世界的情况，因此更具有代表性。

- (3):本文提出了一个新的数据集VizWiz-Classification，该数据集是由盲人拍摄的图像构建的，用于测试在ImageNet上训练的图像分类模型在真实世界情况下的鲁棒性。我们使用了自动化和人工标注的方法来创建这个数据集。我们的方法是通过使用盲人拍摄的图像来测试模型的鲁棒性，这些图像是在真实世界的情况下拍摄的，因此更具有代表性。我们的数据集是第一个基于真实世界情况的测试集，可以用于测试模型的鲁棒性。

- (4):本文的方法在VizWiz-Classification数据集上进行了测试，并分析了100个ImageNet分类模型的表现。结果表明，这些模型在质量问题的图像上表现不佳。本文的方法可以用于测试在ImageNet上训练的图像分类模型在真实世界情况下的鲁棒性。本文的方法可以用于测试在ImageNet上训练的图
#### 7. 方法详细介绍：
本文作者使用了Amazon Mechanical Turk（AMT）招募了一批准确率超过90%的工人，对每张图片进行了三次注释，并使用多数表决法确定最终的类别标签。最终的数据集包括200个类别和8900张图片。作者还使用自动化工具从VizWiz-Captions和VizWiz-ImageQualityIssues数据集中筛选出了15567张可能包含ImageNet感兴趣类别的候选图片，并通过人工注释生成了高质量的标注数据集。

#### 8. 实验设置：
作者在VizWiz-Classification数据集上对100个图像分类模型进行了基准测试，并根据预测是否在图像的标签集中来评估它们的性能。作者使用了ImageNet和ImageNet-C的标准准确率top-1指标来评估模型在基准测试中的性能。

#### 9. 实验结果与分析：
本文作者使用了一种新的数据集来衡量模型对新分布偏移的鲁棒性。该数据集对当前的模型具有挑战性，图像中的质量问题可能会导致模型的准确率下降12.8%。ImageNet和新数据集之间的性能差距很大，模型的准确率可能会下降42.3%。然而，训练数据集更大的模型比其他模型更具有鲁棒性。本文还发现，ImageNet-C上准确率较低的模型在新数据集上的表现与其在ImageNet-C上的表现相当，但模型在ImageNet-C上的进展并不能保证在新数据集上有类似的进展。


# Paper:600     STDLens：面向模型劫持的目标检测联邦学习防御方法



#### 1. Title: 
STDLens: Model Hijacking-resilient Federated Learning for Object Detection

#### 2. Authors: 
Ka-Ho Chow, Ling Liu, Wenqi Wei, Fatih Ilhan, Yanzhao Wu

#### 3. Affiliation: 
Ka-Ho Chow, Ling Liu, Wenqi Wei, Fatih Ilhan, Yanzhao Wu, Georgia Institute of Technology, Atlanta, GA, USA (乔家豪，刘玲，魏文琦，Fatih Ilhan，吴彦钊，美国乔治亚理工学院)

#### 4. Keywords: 
Federated Learning, Object Detection, Model Hijacking, Perception Poisoning, Trojaned Gradients

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chow_STDLens_Model_Hijacking-Resilient_Federated_Learning_for_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/git-disl/STDLens

#### 6. Summary : 
- (1):本文研究了联邦学习（FL）在目标检测中的应用，FL的优点是可以在分布式客户端群体上训练全局目标检测模型，但FL容易受到模型劫持的攻击，攻击者可以通过植入特制梯度来控制目标检测系统的误差，从而导致模型失效。 

- (2):现有的防御方法主要分为两类，一类是修复梯度，另一类是检测梯度。现有的检测方法主要基于空间聚类分析和异常检测，但这些方法在FL中的应用存在一些问题，如随机性和聚类不确定性等。本文提出了一种三层防御方法，包括基于空间和时间特征的梯度检测、梯度贡献者跟踪和订阅撤销、密度自适应异常检测等，可以有效地防御模型劫持攻击。

- (3):本文提出的STDLens方法是一种针对模型劫持攻击的联邦学习防御方法，具有以下创新点：（1）提出了一种基于空间和时间特征的梯度检测方法，可以有效地检测和剔除特制梯度；（2）提出了一种密度自适应异常检测方法，可以避免误判正常客户端；（3）提出了一种梯度贡献者跟踪和订阅撤销方法，可以有效地追踪和剔除恶意客户端；（4）提出了一种基于联邦学习的目标检测模型劫持攻击模型，可以评估STDLens的防御效果。

- (4):本文在COCO数据集上进行了实验，结果表明STDLens可以有效地防御不同类型的模型劫持攻击，比现有方法具有更高的检测精度和更低的误报率，可以在不影响目标检测性能的情况下保护FL系统免受攻击。
#### 7. 方法详细介绍：
本文提出了一种三层防御框架STDLens，用于防御基于联邦学习的目标检测中的模型劫持攻击。该框架包括三个步骤：空间签名分析、时间签名分析和密度检查。在空间签名分析中，对每个客户端的梯度进行分析，以识别恶意梯度。在时间签名分析中，对每个客户端的梯度进行跟踪，以确定恶意梯度的贡献者。在密度检查中，对每个客户端的梯度进行检查，以管理时空不确定性。该方法在实验中表现出较高的防御精度和较低的误报率。

#### 8. 实验设置：
本文在两个目标检测基准数据集PASCAL VOC和INRIA Person上进行了实验。使用100个客户端和200轮的联邦服务器，每轮随机选择10%的客户端参与训练，使用FedAvg作为聚合算法。本文使用Faster R-CNN with VGG16模型，并将训练集随机分成N个相等大小的分区，分配给每个客户端作为其本地数据集。本文分析了STDLens对三种类型的感知污染攻击的防御效果，并在INRIA数据集上分析了BBox-Poison和Objn-Poison攻击。默认情况下，FL中有20%的客户端是恶意的。

#### 9. 实验结果和分析：
本文的实验结果表明，STDLens可以有效地防御不同类型的模型劫持攻击，并且在识别和移除恶意梯度方面表现出较高的精度和较低的误报率。在攻击下，目标检测的准确率下降仅为0.24%，保持在与良性情况下相当的水平。STDLens的防御精度为1.00，成功识别所有恶意客户端，没有误判任何良性客户端。APsrc可以恢复到超过51.54%，与良性APsrc的52.43%相当。其他防御措施过度移除客户端会导致性能下降。


# Paper:601     OpenMix: 探索异常样本以进行误分类检测



#### 1. Title: 
OpenMix: Exploring Outlier Samples for Misclassification Detection

#### 2. Authors: 
Fei Zhu, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu

#### 3. Affiliation: 
中国科学院自动化研究所，中国科学院大学

#### 4. Keywords: 
Misclassification detection, Outlier samples, Deep neural networks, Confidence estimation, OpenMix

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_OpenMix_Exploring_Outlier_Samples_for_Misclassification_Detection_CVPR_2021_paper.html  Github: https://github.com/Impression2805/OpenMix

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在高风险应用中的可靠置信度估计问题。
- (2):过去的方法主要集中在识别未知类别的样本，而对于已知类别的误分类问题研究较少。本文提出了一种简单有效的方法OpenMix，利用异常样本来提高误分类检测的可靠性。同时，本文发现现有的流行方法如OE在误分类检测方面并不起作用，甚至会降低检测性能。
- (3):本文提出了一种学习拒绝的框架，即OpenMix，它通过混合已知类别和异常样本来引入先验知识，从而提高误分类检测的可靠性。OpenMix通过添加一个拒绝类别来打破封闭世界分类器，同时通过简单的线性插值来减少ID和异常样本之间的分布差异。实验结果表明，OpenMix在各种情况下都能显著提高置信度的可靠性，建立了一种检测已知类别的误分类和未知类别的异常样本的统一框架。
- (4):本文在CIFAR-10和CIFAR-100数据集上进行了实验，结果表明OpenMix在误分类检测和未知类别检测方面都取得了显著的性能提升，证明了其有效性和实用性。
#### 7. 方法详细介绍：
本文提出的OpenMix方法通过利用异常样本来检测分类错误。它通过为异常样本添加一个单独的拒绝类并通过简单的线性插值将其与分布内样本混合来打破封闭世界分类器。为混合样本分配软标签以减少ID和开放世界异常样本之间的分布差距。OpenMix方法显著提高了MisD性能而不降低准确性。具体而言，OpenMix方法包括以下步骤：
1. 将训练数据分为in-distribution和out-of-distribution两部分。
2. 在in-distribution数据中添加一个reject类，将out-of-distribution数据与in-distribution数据进行混合。
3. 为混合样本分配软标签，以减少ID和out-of-distribution样本之间的分布差距。
4. 训练模型，使其能够正确地分类in-distribution样本，并将out-of-distribution样本分类为reject类。

#### 8. 实验设置：
本文在CIFAR-10和CIFAR-100数据集上使用ResNet110、WRNet和DenseNet架构进行实验。异常样本是通过从非目标类中随机采样生成的。使用AURC、AUROC和FPR95指标评估性能。

#### 9. 实验结果和分析：
本文提出的OpenMix方法在CIFAR-10和CIFAR-100数据集上的实验结果表明，OpenMix显著提高了DNN的置信度可靠性，并在分布偏移和长尾场景下表现出强大的性能。OpenMix可以统一地检测OOD和MisD样本。具体而言，OpenMix方法在AURC、AUROC、FPR95和ACC等指标上均优于基线方法。在ImageNet数据集上的大规模实验结果也证明了OpenMix的可扩展性，并且OpenMix在MisD性能方面始终表现出色。


# Paper:602     学习融合单目和多视角线索进行动态场景下的多帧深度估计



#### 1. Title: 
Learning to Fuse Monocular and Multi-view Cues for Multi-frame Depth Estimation in Dynamic Scenes

#### 2. Authors: 
Rui Li, Dong Gong, Wei Yin, Hao Chen, Yu Zhu, Kaixuan Wang, Xiaozhi Chen, Jinqiu Sun, Yanning Zhang

#### 3. Affiliation: 
第一作者：西北工业大学

#### 4. Keywords: 
Depth estimation, multi-frame, monocular, dynamic scenes, fusion

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_Learning_to_Fuse_Monocular_and_Multi-View_Cues_for_Multi-Frame_Depth_Estimation_CVPR_2021_paper.html  Github: https://github.com/ruili3/dynamic-multiframe-depth

#### 6. Summary : 
- (1):本文研究的是动态场景下的多帧深度估计问题，传统的多帧方法在动态区域的几何一致性被破坏时，深度估计结果会受到影响，因此需要引入单目深度估计的信息来补充。但是现有的方法在动态区域的掩模质量不可控，且两种信息的融合效果有限，因此需要提出一种新的方法来解决这个问题。

- (2):传统的多帧深度估计方法通常使用多视角几何一致性来提高准确性，但在动态场景下，这种一致性通常会被破坏，导致估计结果不准确。现有的方法通常使用掩模来处理动态区域，但是掩模质量不可控，且两种信息的融合效果有限。本文提出了一种新的方法，通过学习将单目和多视角信息编码为体积，并使用交叉注意力来增强两种信息的表示，从而实现更好的融合效果。

- (3):本文提出了一种交叉信息融合（CCF）模块，该模块包括交叉注意力（CCA），用于编码来自每个源的空间非局部相对内部关系，以增强另一个源的表示。与现有的方法不同，本文的方法将两种信息的输入格式统一为体积，并在其上进行融合，从而实现更好的性能。该方法是可学习的，不需要任何启发式掩模，具有更好的泛化性和灵活性。

- (4):本文的方法在KITTI和DDAD数据集上进行了实验，结果表明，与现有方法相比，本文的方法在动态区域的误差降低了21.3％，在整体性能上保持了优势。本文的方法在动态区域的泛化能力也优于其他方法。
#### 7. 方法详细介绍：
本文提出了一种新的方法，通过融合单目和多视角线索来学习多帧动态场景的深度估计。该方法包括两个主要模块：单目深度网络和多帧深度网络。单目深度网络用于估计动态区域的深度，而多帧深度网络用于估计静态区域的深度。然后使用学习到的权重因子和交叉线索注意力模块将两个网络进行融合，以提高整体深度估计性能。该方法还使用了一个损失函数，将尺度不变损失和虚拟法线损失相结合。

具体步骤如下：
1. 将多视角线索表示为成本体积，通过计算成本体积来表示多视角线索，采用多视角立体成像的流程。
2. 通过估计单视图深度图，然后将深度转换为单热深度体积来表示单目线索。
3. 提出了交叉线索融合（CCF）模块，通过利用注意机制来融合多帧和单目线索，即提取每个线索的相对内部关系，以指导另一个线索，从而产生改进的动态场景结构的几何表示。深度模块采用融合表示来估计最终深度。

#### 8. 实验设置：
本文在两个真实世界的数据集上进行了实验，包括KITTI数据集和DDAD数据集。KITTI数据集包含42,382个训练和3,712个测试立体对，而DDAD数据集包含1,000个训练和500个测试立体对。实验在单个NVIDIA Tesla V100 GPU上进行，具有16GB内存。本文使用PyTorch实现了所提出的方法，并使用Adam优化器进行训练，学习率为1e-4。

#### 9. 实验结果和分析：
本文提出的方法在动态区域的深度估计方面优于其他方法，并在KITTI和DDAD数据集上取得了有竞争力的整体性能。该方法还显示出在动态区域的单目估计方面的显着改进，与其他方法相比，实现了最大的误差降低。该方法的泛化能力在DDAD数据集上得到了验证。定性结果显示，该方法重建了比现有方法更合理的物体形状。


# Paper:603     基于空间感知特征和排名一致性的图像裁剪



#### 1. Title: 
Image Cropping with Spatial-aware Feature and Rank Consistency

#### 2. Authors: 
Chao Wang, Li Niu, Bo Zhang, Liqing Zhang

#### 3. Affiliation: 
上海交通大学人工智能研究院

#### 4. Keywords: 
Image cropping, spatial-aware feature, rank consistency, data-driven

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/papers/Wang_Image_Cropping_With_Spatial-Aware_Feature_and_Rank_Consistency_CVPR_2021_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的是图像裁剪问题，旨在找到视觉上吸引人的图像裁剪。 
- (2):过去的方法主要分为基于规则和基于数据驱动两种，但是它们都存在一些问题，比如基于规则的方法没有考虑到所有美学元素的空间布局，而基于数据驱动的方法没有很好地捕捉到裁剪和美学元素之间的空间关系。本文提出了一种新的空间感知特征来编码候选裁剪和美学元素之间的空间关系，并且通过训练一个成对排名分类器来将排名知识从标记数据转移到未标记数据中，以强制排名一致性。 
- (3):本文提出了一种新的空间感知特征来编码候选裁剪和美学元素之间的空间关系，通过训练一个成对排名分类器来将排名知识从标记数据转移到未标记数据中，以强制排名一致性。实验结果表明，该方法在基准数据集上表现优异。 
- (4):本文提出的方法在GAICD和FCDB数据集上进行了实验，结果表明该方法在图像裁剪任务上取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于空间感知特征和排名一致性的图像裁剪方法。使用在ImageNet上预训练的MobileNetv2模型作为骨干网络提取多尺度特征。通过RoIAlign和RoDAlign获取区域特征，提取空间感知特征以建模候选裁剪和美学元素之间的空间关系。将裁剪特征与空间感知特征连接并传递到两个分支进行裁剪级别美学分数预测和成对排名分类。对无标签测试图像进行训练，通过一致性损失将排名知识从有标签图像传递到无标签图像。总损失函数包括有标签损失和一致性损失。

#### 8. 实验设置：
本文在GAICD数据集和FCDB数据集上进行了评估。在评估过程中，使用交并比（IoU）和边界位移误差（Disp）与其他方法进行比较。 

#### 9. 实验结果和分析：
本文提出的方法在GAICD数据集上表现优异，与GAIC和TransView相比，表明了空间感知特征和排名一致性的优越性。在FCDB数据集上，与GAIC和TransView使用相同的骨干网络和训练集相比，本文提出的方法也取得了更好的性能。本文提出的模型推理速度与GAIC和CGS相当，并且比VFN、VEN和VPN快得多。本文提出的模型在用户研究中明显优于其他模型。


# Paper:604     通过图像到点掩蔽自编码器从2D预训练模型中学习3D表示



#### 1. Title: 
Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders

#### 2. Authors: 
Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, Hongsheng Li

#### 3. Affiliation: 
Renrui Zhang: 上海人工智能实验室 (Shanghai Artificial Intelligence Laboratory)

#### 4. Keywords: 
3D representation learning, self-supervised pre-training, masked autoencoder, image-to-point transfer, 2D pre-trained models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_CVPR_2021_paper.html Github: https://github.com/ZrrSkywalker/I2P-MAE

#### 6. Summary : 
- (1):本文的研究背景是，由于3D数据集的稀缺性，3D表示学习的质量受到限制。因此，本文提出了一种通过2D预训练模型进行3D表示学习的方法，以提高3D表示的质量。

- (2):过去的方法主要是基于自监督的预训练，如点云重排列、部分遮挡、旋转、下采样、编码等。然而，这些方法的性能受到点云数据的稀疏性和缺乏多样性的限制。本文提出了一种新的方法，通过2D预训练模型进行3D表示学习，以便更好地利用2D知识来指导3D表示学习。

- (3):本文提出了一种基于Image-to-Point Masked Autoencoders的预训练框架，通过自监督预训练，利用2D知识来指导3D表示学习。具体来说，本文首先利用现成的2D模型提取输入点云的多视图视觉特征，然后进行两种类型的图像到点云学习方案。一种是引入2D引导掩蔽策略，保留语义重要的点令牌可见。与随机掩蔽相比，网络可以更好地集中于具有关键空间线索的重要3D结构。另一种是在解码器之后强制这些可见的令牌重构多视图2D特征。这使得网络能够有效地继承高级2D语义，用于区分性3D建模。通过我们的图像到点预训练，冻结的I2P-MAE，在不进行任何微调的情况下，在ModelNet40上通过线性SVM实现了93.4%的准确率，与现有的完全训练方法相当。在ScanObjectNN的最难分裂上进一步微调后，I2P-MAE实现了90.11%的准确率，比第二好的Point-M2AE高出3.68%，展示了出色的可转移能力。

- (4):本文提出的方法在3D点云分类任务上取得了优异的性能，证明了通过2D预训练模型进行3D表示学习的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为“Image-to-Point Masked Autoencoders”（I2P-MAE）的方法，用于从2D预训练模型中学习3D表示。该方法采用图像到点操作（I2P）将多视角2D表示反投影到3D空间进行聚合。可见点标记用于从不同视角重建提取的2D语义，从而将2D预训练知识转移到3D预训练。I2P-MAE的最终预训练损失被公式化为LI2P = L3D + L2D，其中L3D是掩蔽的3D坐标重建损失，L2D是2D语义重建损失。该方法在ShapeNet上进行了预训练，使用了57,448个合成点云，并在各种3D下游任务上进行了微调。

#### 8. 实验设置：
预训练采用与Point-M2AE相同的MAE变换器架构，具有3个阶段的编码器，每个阶段有5个块，2个阶段的解码器，每个阶段有1个块，2048个输入点数（N），512个下采样数（M），16个最近邻居（k），384个特征通道（C）和80％的掩蔽比率。使用CLIP预训练的ViT-Base作为现成的2D模型。点云投影到三个224×224深度图中，2D特征大小为14×14。I2P-MAE进行了300个时期的预训练，批量大小为64，学习率为10−3。使用带有5×10−2权重衰减的AdamW优化器和10个时期的余弦调度程序进行微调。

#### 9. 实验结果与分析：
I2P-MAE在合成和真实世界的3D分类任务上均取得了最先进的性能，大幅超过其他自监督方法。对于ModelNet40的合成3D分类，I2P-MAE在投票前达到93.7％的准确率，在投票后达到94.1％的准确率。对于ScanObjectNN的真实世界3D分类，I2P-MAE分别在三个分割上达到了94.15％，91.57％和90.11％的准确率，这是第一个在最难的PB-T50-RS分割上达到90％准确率的模型。该方法在ShapeNetPart上的部分分割中也取得了竞争性能。I2P-MAE的SVM结果证明了其学习到的高质量3D表示以及图像到点学习方案的重要性。


# Paper:605     一种底层上升的框架，通过占用感知提高2D到3D的提升效果，用于从单张图像中进行全景3D场景重建



#### 1. Title: 
BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From A Single Image

#### 2. Authors: 
Tao Chu, Pan Zhang, Qiong Liu, Jiaqi Wang

#### 3. Affiliation: 
Tao Chu and Qiong Liu are affiliated with South China University of Technology.

#### 4. Keywords: 
Panoptic 3D scene reconstruction, single image, bottom-up framework, occupancy-aware lifting, semantic segmentation.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chu_BUOL_A_Bottom-Up_Framework_With_Occupancy-Aware_Lifting_for_Panoptic_3D_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究单张图像的全景三维场景重建问题，提出了一种底层上升的框架，通过占用感知提高2D到3D的提升效果。
 
- (2):过去的方法主要集中在将2D实例填充到3D体素中，这会导致两个问题：实例通道歧义和体素重建歧义。本文提出的方法通过底层上升的框架，将2D信息提升到3D体素中，并根据预测的2D实例中心将体素分组为3D实例，同时利用估计的多平面占用和深度填充物体和物品的整个区域，从而解决了这两个问题。

- (3):本文提出了一种底层上升的框架，通过占用感知提高2D到3D的提升效果。具体来说，该框架包括2D先验阶段、2D到3D提升阶段和3D细化阶段。在2D先验阶段，2D模型预测2D语义地图、2D实例中心、深度图和多平面占用。在2D到3D提升阶段，利用估计的多平面占用和深度图，将2D语义提升到物体和物品的3D体素特征的确定性通道中。在3D细化阶段，预测每个体素的密集3D占用率以进行重建。同时，预测物品和物品类别的3D语义分割。还估计了朝向2D实例中心的3D偏移量，以识别属于3D对象的体素。在推理期间，根据2D实例中心和3D偏移量将实例ID分配给被物体对象占用的3D体素，从而获得最终的全景3D场景重建结果。

- (4):在3D-Front和Matterport3D数据集上，本文提出的方法相对于现有的方法在全景重建质量方面分别提高了11.81％和7.46％。这表明本文提出的方法在单张图像的全景三维场景重建方面具有很好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于底层上升的方法，称为BUOL，用于从单个图像中进行全景三维场景重建。该方法包括三个阶段：2D先验阶段、2D到3D提升阶段和3D细化阶段。在2D先验阶段，2D模型预测2D语义地图、2D实例中心、深度图和多平面占用。在2D到3D提升阶段，利用估计的多平面占用和深度图将2D语义提升到3D体素特征的确定通道中，除了前视表面之外的事物和物品。在3D细化阶段，预测每个体素中的密集3D占用以进行重建，并预测用于事物和物品类别的3D语义分割。在推理期间，基于2D实例中心和3D偏移量，将实例ID分配给由事物对象占据的3D体素，从而获得最终的全景3D场景重建结果。

#### 8. 实验设置：
本文的实验在经过预处理的3D-Front和Matterport3D数据集上进行。数据集经过预处理以生成全景3D场景重建的地面真实值。对于合成数据，全景3D场景重建的评估分辨率设置为3cm，对于真实世界数据，设置为6cm。评估指标包括全景重建质量（PRQ）、重建分割质量（RSQ）和重建识别质量（RRQ）。

#### 9. 实验结果和分析：
本文提出的BUOL方法在一系列实验中优于现有方法，包括3D重建和3D感知。在合成数据集和真实世界数据集上的比较结果分别显示在表1和表2中。底层上升方法比自上而下的方法实现了+3.3%的PRQ改进。多平面占用提高了事物的PRQth 2.93%，提高了物品的PRQst 4.75%，验证了2D模型预测的多平面占用的有效性。BUOL方法在3D-Front和Matterport3D数据集上均取得了最先进的性能，分别比现有方法提高了+11.81%和+7.46%的PRQ。


# Paper:606     SuperDisco：超类发现改善长尾视觉识别



#### 1. Title: 
SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail

#### 2. Authors: 
Yingjun Du, Jiayi Shen, Xiantong Zhen, Cees G. M. Snoek

#### 3. Affiliation: 
Yingjun Du, Jiayi Shen, Xiantong Zhen: University of Amsterdam; Cees G. M. Snoek: Inception Institute of Artificial Intelligence (荷兰阿姆斯特丹大学)

#### 4. Keywords: 
Long-tailed recognition, Super-class learning, Graph neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Du_SuperDisco_Super-Class_Discovery_Improves_Visual_Recognition_for_the_Long-Tail_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是长尾视觉识别问题，即在只有少量样本的尾部类别上表现较差的情况。人类可以通过不同层次的语义抽象来学习尾部类别的表示，使得学习到的尾部特征更具有区分性。本文提出了一种算法SuperDisco，通过图模型发现超类表示，以解决长尾分布的视觉识别问题。

- (2):过去的方法主要包括类别再平衡、信息增强和模块改进等，这些方法都旨在改善原始特征空间的分类性能。本文提出的方法则是通过学习可学习的图来发现原始特征表示中隐藏的不平衡超类空间。本文的方法有很好的动机和创新性，能够更好地处理长尾分布的视觉识别问题。

- (3):本文提出了一种算法SuperDisco，通过学习构建超类图来指导表示学习以处理长尾分布。通过在超类图上进行消息传递，图像表示被修正和精炼，通过关注其与超类之间的语义相似性来选择最相关的实体。此外，本文还提出了一种元学习的超类图算法，通过少量不平衡数据构建原型图来监督超类图的元学习，从而获得更加稳健的超类图，进一步提高长尾识别性能。

- (4):本文在长尾CIFAR-100、ImageNet、Places和iNaturalist等四个基准测试上进行了实验，证明了发现的超类图对于处理长尾分布的数据具有很好的效果。本文的方法在长尾识别任务上取得了最先进的性能，支持了他们的目标。
#### 7. 方法详细介绍：
本文提出了一种名为SuperDisco的算法，旨在通过元学习发现改进的超类表示。该算法构建了一个超类图，以组织和提炼训练过程中的知识。顶点表示不同类型的超类，边缘自动构建以反映不同超类之间的关系。该方法使用图神经网络（GNN）通过消息传递从发现的超类到特征中传播最相关的超类。该方法还提出了Meta-SuperDisco，使用元学习发现超类图，使模型更加鲁棒。原型图是从少量平衡数据中构建的，以提取样本级别的关系信息，捕捉样本背后的潜在关系，并缓解异常样本的潜在影响。原型图用于指导元超类图的学习。

#### 8. 实验设置：
本文将所提出的方法应用于四个常用的长尾识别基准，包括CIFAR-100-LT、ImageNet-LT、Places-LT和iNaturalist。每个数据集的不平衡因子定义为最多样本类别的训练样本数除以少数类别的训练样本数。本文在补充材料中报告了每个数据集的类别数和样本图像数量。本文还提供了实现细节，包括每个数据集使用的骨干网络、训练轮数、批量大小、优化器和训练增强。

#### 9. 实验结果和分析：
本文将所提出的SuperDisco算法与微调基线和理想设置进行了比较。本文表明，SuperDisco在CIFAR-100-LT上的表现优于微调，并在iNaturalist上具有更复杂的层次结构时实现了一致的性能提升。本文还表明，Meta-SuperDisco在所有不平衡因子上均优于SuperDisco，并在长尾识别方面实现了更好的性能。本文得出结论，需要更深和更宽的图来发现严重类别不平衡的超类。完整的实验结果在补充材料中提供。


# Paper:607     建模显著目标检测模型的分布不确定性



#### 1. Title: 
Modeling the Distributional Uncertainty for Salient Object Detection Models

#### 2. Authors: 
Xinyu Tian, Jing Zhang, Mochu Xiang, Yuchao Dai

#### 3. Affiliation: 
第一作者：西北工业大学，中国

#### 4. Keywords: 
Salient object detection, distributional uncertainty, long-tail learning, single-model uncertainty modeling, test-time strategies

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tian_Modeling_the_Distributional_Uncertainty_for_Salient_Object_Detection_Models_CVPR_2021_paper.html  Github: https://npucvr.github.io/Distributional_uncer/

#### 6. Summary : 
- (1):本文研究显著目标检测模型中的分布不确定性问题，探索了现有的类别感知分布差异探索技术，并将其应用于模型分布不确定性的建模。
 
- (2):现有的显著目标检测方法大多集中于提高模型性能，而忽略了训练和测试分布之间的差异。本文提出了一种新的分布不确定性建模方法，包括长尾学习、单模型不确定性建模和测试时间策略。本文的创新点在于将现有的类别感知分布差异探索技术应用于模型分布不确定性的建模，并定义了显著目标检测中的“越界”样本。
 
- (3):本文提出了一种新的分布不确定性建模方法，包括长尾学习、单模型不确定性建模和测试时间策略。本文的创新点在于将现有的类别感知分布差异探索技术应用于模型分布不确定性的建模，并定义了显著目标检测中的“越界”样本。本文的方法在数据增强的基础上，通过测试时间的策略，能够在显著目标检测任务中产生可靠的分布不确定性。
 
- (4):本文的方法在多个数据集上进行了实验，结果表明，单模型不确定性建模方法和防止模型漂移的权重正则化方法是建模显著目标检测模型分布不确定性的有效方法。同时，本文的方法在显著目标检测任务中取得了较好的性能，支持了本文的研究目标。
#### 7. 方法详细介绍：
本文提出了一种建模显著性目标检测模型分布不确定性的方法。该方法涉及对训练好的基础模型次最后层的特征执行ReAct以获得阈值，然后使用该阈值从最后一层获得预测。本文还讨论了各种测试时间策略，包括测试时间训练和测试时间增强，并介绍了它们在显著性检测中的实现。

#### 8. 实验设置：
本文在DUTS训练数据集上训练了所提出的模型，并在三个基准测试数据集（包括DUTS测试数据集、ECSSD和DUT数据集）上进行了测试。本文使用了三个标准指标来评估模型性能，包括最大F-measure、IoU和准确率。本文还使用了接收器操作特征下的面积（AUROC）和真正率为95%时的假正率（FPR95）来评估训练模型的分布不确定性建模程度。

#### 9. 实验结果与分析：
本文研究了显著性目标检测中的超出分布问题，并提出了生成可靠分布不确定性的方法。作者进行了大量实验，发现深度集成和TCP在生成可靠分布不确定性方面是有效的。对于类依赖连续分割任务，长尾学习解决方案在建模分布不确定性方面的效果较差。如果使用适当的正则化来控制模型的漂移程度，则测试时间训练具有生成可靠不确定性的潜力。作者提供了传统不确定性建模方法、长尾学习方法、单模型不确定性技术和测试时间策略的定量评估结果。他们还展示了不同测试时间策略的定性结果。本文提供了DUTS测试数据集上AUROC指标的分布，以展示不同方法在生成可靠分布不确定性方面的有效性。


# Paper:608     利用跨尺度失真感知的正交平面分离室内全景房间布局估计



#### 1. Title: 
Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness

#### 2. Authors: 
Zhijie Shen, Zishuo Zheng, Chunyu Lin, Lang Nie, Kang Liao, Shuai Zheng, and Yao Zhao

#### 3. Affiliation: 
北京交通大学信息科学研究所

#### 4. Keywords: 
Indoor layout estimation, panoramic images, orthogonal planes, distortion awareness, disentanglement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shen_Disentangling_Orthogonal_Planes_for_Indoor_Panoramic_Room_Layout_Estimation_CVPR_2021_paper.html Github: https://github.com/zhijieshen-bjtu/DOPNet

#### 6. Summary:
- (1):本文研究室内全景图像的房间布局估计问题，基于曼哈顿世界假设，大多数现有方法都是从垂直压缩的一维序列中恢复布局，但是这种压缩过程会混淆不同平面的语义，导致性能下降。
- (2):为了解决这个问题，本文提出了一种预分割正交（垂直和水平）平面的方法，以明确捕捉室内布局估计的几何线索。此外，本文还设计了一种软翻转融合策略来辅助预分割。此外，本文还提出了一种特征组装机制，以有效地集成浅层和深层特征，并具有失真分布感知能力。为了弥补预分割中的潜在误差，本文进一步利用三重注意力来重构两个一维序列，以获得更好的性能。
- (3):本文提出了一种新的架构，将正交平面预分离以捕捉明确的几何线索，然后将这些正交平面压缩成两个一维表示。此外，本文还提出了一种组装机制，以有效地融合具有失真感知能力的多尺度特征。本文的贡献可以总结如下：（1）我们提出了一种预分割正交平面的方法，以捕捉室内360°房间布局估计的明确几何线索；（2）我们设计了一种跨尺度失真感知的组装机制，以感知失真分布并集成浅层几何结构和深层语义特征；（3）在流行的基准测试中，我们的解决方案优于其他SoTA方案，特别是在3DIoU指标上；（4）本文的方法在四个流行的数据集上进行了实验，证明了其有效性。
- (4):本文的方法在四个流行的数据集上进行了实验，证明了其有效性。在3DIoU指标上，本文的方法优于其他SoTA方案。
#### 7. 方法详细介绍：
本文提出的方法通过预先将正交平面分离，以捕获明确的几何线索。具体而言，从整个场景中分割出垂直平面（即墙壁）和水平平面（即地板和天花板），然后将这些正交平面压缩成两个1D表示。为了消除畸变的负面影响，提出了特征组装机制，有效地融合了多尺度特征。同时，进行跨尺度交互，以整合浅层几何结构和深度语义特征。考虑到预分割的误差，进一步利用三重注意力重构了两个1D序列。特别地，采用基于图的注意力生成判别性通道，自我注意力重建长距离依赖性，交叉注意力为不同序列提供缺失信息。最终，从重构的序列中估计出水平深度图和房间高度。

#### 8. 实验设置：
本文的方法在四个流行的数据集上进行了评估：Stanford 2D-3D、PanoContext、MatterportLayout和ZInd。实验在单个GTX 3090 GPU上进行，训练批次大小为16。严格遵循四个数据集的训练/验证/测试分割，以进行公平比较。

#### 9. 实验结果和分析：
本文提出的方法在立方体房间布局数据集上的3DIoU指标优于所有其他最先进的方法。在一般房间布局数据集上，本文的方法在3DIoU方面也优于所有其他方法。定性比较也表明了本文提出的方法在捕获明确的3D几何线索方面的有效性。消融研究证明了所提出的特征组装机制、正交平面分离和注意力机制的有效性。


# Paper:609     通过隐式Sinkhorn微分实现重新分配



#### 1. Title: 
Re-basin via implicit Sinkhorn differentiation

#### 2. Authors: 
Fidel A. Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, Marco Pedersoli

#### 3. Affiliation: 
LIVIA, Dept. of Systems Engineering, ETS Montreal, Canada (加拿大蒙特利尔理工学院系统工程系)

#### 4. Keywords: 
re-basin, deep learning, linear mode connectivity, incremental learning, Sinkhorn differentiation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guerrero_Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2021_paper.html  Github: https://github.com/fagp/sinkhorn-rebasin

#### 6. Summary : 
- (1):本文研究深度学习中的re-basin问题，即如何将模型转换到解空间中的等效区域，以实现模型合并和增量学习等应用。同时，本文探讨了线性模式连通性（LMC）的概念，即在权重空间中连接两个模型的低阻隔路径，以及如何利用LMC实现增量学习。

- (2):过去的re-basin方法大多数是基于解决线性分配问题，难以推广到其他目标。本文提出了一种基于Sinkhorn算子的re-basin方法，可以更好地解决不同目标的可微分问题。同时，本文提出了一种基于LMC的增量学习方法，可以直接估计先前和新知识的交集处的模型。

- (3):本文提出了一种使用隐式Sinkhorn微分求解最优传输的re-basin网络，可以获得最适合给定目标的传输计划。与现有技术不同的是，我们的方法是可微分的，因此易于适应深度学习领域内的任何任务。此外，本文提出了一种新的成本函数，可以利用线性模式连通性实现增量学习。本文的方法在多个条件下与文献中类似方法进行了比较，包括最优传输和线性模式连通性。本文的方法在多个常见基准数据集上进行了实验，提供了与现有技术相竞争的实验结果。

- (4):本文的方法在re-basin和增量学习方面具有创新性和实用性，实验结果表明其有效性和可扩展性。
#### 7. 方法详细介绍：
本文提出了一种名为Sinkhorn re-basin的方法，它使用可微分的Sinkhorn算子来解决re-basin问题。该算法通过松弛置换矩阵来实现re-basin，使用隐式微分算法来更高效地计算梯度。该方法可以将任何可微分的目标函数定义为损失函数，并且可以轻松适应深度学习领域内的任何任务。该方法在最优传输和线性模式连接性方面与文献中类似方法进行了比较。

具体步骤如下：
1. 使用Sinkhorn算子来解决re-basin问题，将置换矩阵松弛为可微分的形式。
2. 使用隐式微分算法来更高效地计算梯度。
3. 定义任何可微分的目标函数作为损失函数。
4. 通过调整稳定性-可塑性超参数来平衡遗忘和融合新知识，从而将不同领域或类别的模型组合起来。

#### 8. 实验设置：
本文使用了与Ainsworth等人相同的实验流程来比较re-basin方法。研究了re-basin对分类和回归任务的影响。使用了Mnist和Cifar10数据集进行图像分类，使用[27]中的多项式逼近问题进行回归。探索了具有2到8层的前馈神经网络作为骨干架构。在所有实验中，使用t = 20和τ = 1.0，这是[21]中提出的Sinkhorn算子的参数。

#### 9. 实验结果与分析：
本文提出的Sinkhorn re-basin方法在寻找最优置换方面表现优异，总能找到最优置换，而权重匹配方法在某些情况下表现不佳。在寻找线性模式连接性方面，该方法在所有情况下均优于naive方法。在最优传输方面，该方法在大多数数据集上的AUC和Barrier指标均优于其他方法。在增量学习方面，该方法表现出与最先进的方法相当或更好的结果，并且对alpha和beta的值具有鲁棒性。


# Paper:610     通过神经渲染实现无监督连续语义自适应



#### 1. Title: 
Unsupervised Continual Semantic Adaptation through Neural Rendering

#### 2. Authors: 
Zhizheng Liu, Francesco Milano, Jonas Frey, Roland Siegwart, Hermann Blum, Cesar Cadena

#### 3. Affiliation: 
第一作者：ETH Zurich

#### 4. Keywords: 
Unsupervised Domain Adaptation, Continual Learning, Semantic Segmentation, Neural Rendering

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Unsupervised_Continual_Semantic_Adaptation_Through_Neural_Rendering_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究了在多个场景中无监督地对语义分割模型进行连续自适应的问题。在实际应用中，由于训练数据和部署数据之间的不匹配，对新场景进行模型适应通常是至关重要的。本文提出了一种使用神经渲染的方法，通过联合训练语义分割模型和NeRF模型，实现了2D-3D知识转移，从而在没有标签的情况下对模型进行自适应。

- (2):过去的方法主要集中在单一领域的迁移上，而本文则关注于在多个场景中进行模型的连续自适应。此外，本文提出的方法使用NeRF模型来聚合多个视角的语义预测，相比于使用体素表示的方法，可以获得更高质量的伪标签。此外，本文还提出了一种长期记忆机制，可以将NeRF模型存储在内存中，以便在后续场景中进行自适应。

- (3):本文提出了一种使用神经渲染的方法，通过联合训练语义分割模型和NeRF模型，实现了2D-3D知识转移，从而在没有标签的情况下对模型进行自适应。此外，本文还提出了一种长期记忆机制，可以将NeRF模型存储在内存中，以便在后续场景中进行自适应。

- (4):本文在ScanNet数据集上进行了实验，结果表明，与使用体素表示的方法和最先进的无监督域自适应方法相比，本文提出的方法在适应新场景和保留先前场景的知识方面都表现出更好的性能。
#### 7. 方法详细介绍：
本文提出了一种无监督的连续语义自适应方法，通过神经渲染实现。该方法包括训练一个NeRF模型来生成伪标签以适应分割网络。使用多分辨率哈希编码来提高训练和渲染速度。通过联合2D-3D训练来适应分割网络，其中NeRF模型渲染的图像作为网络的输入。该方法还包括一个连续的基于NeRF的重放，以减轻场景之间的灾难性遗忘。

#### 8. 实验设置：
本文在ScanNet数据集上进行评估，该数据集包括707个带有RGB-D图像、相机姿态和手动生成的语义注释的室内场景。场景11-707用于预训练分割网络，场景1-10用于适应。预训练数据集随机分为一个包含20k帧的训练集和一个包含5k帧的验证集。使用平均交并比（mIoU）作为度量标准评估分割网络。

#### 9. 实验结果与分析：
本文提出了一种新的方法，通过神经渲染实现无监督连续适应语义分割网络到多个新场景。该方法在多步设置中实现了最佳平均适应性能，相比预训练模型提高了约3%的mIoU。基于NeRF的重放策略可以有效减轻灾难性遗忘，并且可以在恒定的存储成本下渲染无限数量的图像以进行适应。本文还讨论了所提出方法的局限性。


# Paper:611     基元生成和语义相关对齐的通用零样本分割



#### 1. Title: 
Primitive Generation and Semantic-related Alignment for Universal Zero-Shot Segmentation

#### 2. Authors: 
Shuting He, Henghui Ding, Wei Jiang

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Zero-shot segmentation, generative model, semantic-related alignment, feature disentanglement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/He_Primitive_Generation_and_Semantic-Related_Alignment_for_Universal_Zero-Shot_Segmentation_CVPR_2021_paper.html  Github: https://github.com/henghuiding/PADing

#### 6. Summary : 
- (1):本文研究了通用零样本分割，旨在实现对新类别的全景、实例和语义分割，而无需任何训练样本。这种零样本分割能力依赖于语义空间中的类间关系，将从已知类别中学习的视觉知识转移到未知类别中。因此，需要很好地连接语义-视觉空间，并将语义关系应用于视觉特征学习。

- (2):过去的零样本分割方法通常采用投影方法或生成模型方法。然而，现有的生成模型方法通常采用像素级别的生成，不够稳健。本文提出了一种基于原语生成和语义相关对齐的方法，以普遍解决零样本分割，包括全景、实例和语义分割。本文的方法通过使用具有细粒度属性的大量学习原语来合成未知类别的特征，从而帮助解决偏差问题和领域差异问题。同时，本文提出了一种协作关系对齐和特征解缠结学习方法，以促进生成器产生更好的合成特征。

- (3):本文提出了一种原语生成和语义相关对齐的方法，以普遍解决零样本分割，包括全景、实例和语义分割。本文的方法通过使用具有细粒度属性的大量学习原语来合成未知类别的特征，从而帮助解决偏差问题和领域差异问题。同时，本文提出了一种协作关系对齐和特征解缠结学习方法，以促进生成器产生更好的合成特征。本文的方法在零样本全景分割、实例分割和语义分割方面取得了令人印象深刻的最新性能。

- (4):本文的方法在零样本全景分割、实例分割和语义分割方面取得了令人印象深刻的最新性能。本文的方法通过使用具有细粒度属性的大量学习原语来合成未知类别的特征，从而帮助解决偏差问题和领域差异问题。同时，本文提出了一种协作关系对齐和特征解缠结学习方法，以促进生成器产生更好的合成特征。
#### 7. 方法详细介绍：
本文提出了一种名为PADing的零样本分割模型，包括原语生成器、分类器、特征分离和关系对齐。原语生成器通过选择性地组装具有细粒度属性的学习原语来为未见类别合成视觉特征。分类器将真实和合成的类别嵌入作为输入，并被训练为将对象分类为已见和未见类别。特征分离用于将视觉特征分离为语义相关和语义无关部分，关系对齐用于将视觉特征的类间关系与语义空间中的关系对齐。

具体步骤如下：
1. 原语生成器：通过选择性地组装具有细粒度属性的学习原语来为未见类别合成视觉特征。
2. 分类器：将真实和合成的类别嵌入作为输入，并被训练为将对象分类为已见和未见类别。
3. 特征分离：将视觉特征分离为语义相关和语义无关部分。
4. 关系对齐：将视觉特征的类间关系与语义空间中的关系对齐，包括组内对齐和组间对齐。

#### 8. 实验设置：
本文在MSCOCO 2017数据集上进行实验，包含118k个训练图像和5k个验证图像。使用Pytorch进行模型训练，CLIP文本嵌入和word2vec用作语义嵌入。Mask2Former模型用作骨干网络，超参数与先前的工作一致。Transformer层数、损失权重、温度和sigma分别设置为3、0.002、0.1和{2、5、10、20、40、60}。

#### 9. 实验结果与分析：
本文提出的方法在零样本全景分割、零样本实例分割、零样本目标检测和零样本语义分割四个任务上进行了评估。使用PQ指标评估全景和语义分割任务的性能，使用mAP评估实例分割和目标检测任务的性能。结果表明，PADing模型优于基线，并在所有四个任务上实现了最先进的性能。还进行了消融研究，证明了所提出方法的有效性。


# Paper:612     视觉编程：无需训练的组合视觉推理



#### 1. Title: 
Visual Programming: Compositional visual reasoning without training

#### 2. Authors: 
Tanmay Gupta, Aniruddha Kembhavi

#### 3. Affiliation: 
Tanmay Gupta: Allen Institute for AI (艾伦人工智能研究所)
Aniruddha Kembhavi: None

#### 4. Keywords: 
Visual programming, neuro-symbolic, natural language, image understanding, image manipulation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2022_paper.html  Github: https://github.com/allenai/visprog

#### 6. Summary : 
- (1):本文研究背景是构建通用的人工智能系统，以解决复杂和组合的视觉任务。
- (2):过去的方法需要针对每个任务进行训练，难以扩展到无限长的复杂任务。本文提出了一种基于大型语言模型的神经符号方法，通过将自然语言指令分解为简单的步骤，以处理特定的端到端训练模型或其他程序。本文的方法避免了过去方法的问题，且具有高度的灵活性和可解释性。
- (3):本文提出了VISPROG，一种模块化和可解释的神经符号系统，用于组合视觉理解和操作、知识检索以及算术和逻辑操作等任务。VISPROG使用大型语言模型进行上下文学习，生成Python类似的模块化程序，并在输入图像上执行程序以获得预测结果。本文的方法具有高度的灵活性和可解释性，且不需要任何任务特定的训练。
- (4):本文在四个不同的任务上展示了VISPROG的灵活性和性能，包括组合视觉问答、零样本自然语言视觉推理、基于自然语言指令的知识标记和语言引导的图像编辑。本文的方法在组合视觉问答任务上取得了2.7个百分点的提升，在零样本自然语言视觉推理任务上取得了62.4%的准确率，并在知识标记和图像编辑任务中取得了令人满意的定性和定量结果。
#### 7. 方法详细介绍：
VISPROG是一种神经符号方法，它输入视觉数据和自然语言指令，生成一系列步骤，并执行这些步骤以产生所需的输出。VISPROG使用强大的语言模型（GPT-3）和少量上下文示例来创建复杂的程序，而无需任何训练。程序使用比NMNs更高级的抽象级别，并调用经过训练的最先进模型和非神经Python子例程。VISPROG支持20个模块，包括计算机视觉模型、语言模型、OpenCV中的图像处理子例程以及算术和逻辑运算符。每个模块都实现为一个Python类，具有解析行、执行必要计算并使用HTML总结步骤计算的方法。程序执行由解释器处理，解释器使用输入初始化程序状态，并逐行遍历程序，同时使用该行中指定的输入调用正确的模块。解释器将所有程序步骤的HTML摘要拼接成可用于分析程序的逻辑正确性以及检查中间输出的可视化理由。

#### 8. 实验设置：
本文在四个需要不同能力的任务上评估了VISPROG，这些任务需要空间推理、多图像推理、知识检索和图像生成和操作等能力。这些任务是组合视觉问答、零样本推理、事实知识对象标记和自然语言图像编辑。本文提供了每个任务的输入、输出和使用的模块的详细信息。还描述了每个任务的评估设置和上下文示例的选择。

#### 9. 实验结果和分析：
本文报告了VISPROG在两个任务（NLVR和知识标记）上的性能。对于NLVR，VISPROG在没有对图像对进行任何训练的情况下实现了零样本性能，并将其性能与在NLVRV2上进行微调的VILT模型进行了比较。对于知识标记，VISPROG使用视觉理由实现了63.7%的F1分数进行标记和80.6%的F1分数进行仅定位，通过指令调整来提高性能。本文还使用视觉理由进行了详细的错误分析，以确定错误来源并提出改进性能的方法。


# Paper:613     SpaText：用于可控图像生成的空间文本表示



#### 1. Title: 
SpaText: Spatio-Textual Representation for Controllable Image Generation

#### 2. Authors: 
Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin

#### 3. Affiliation: 
Omri Avrahami: The Hebrew University of Jerusalem
Others: Meta AI

#### 4. Keywords: 
text-to-image generation, spatio-textual representation, open-vocabulary scene control, CLIP-based representation, diffusion models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2022_paper.html  Github: https://github.com/omriavrahami/Spatext

#### 6. Summary : 
- (1):本文研究的背景是文本到图像生成，目前的方法难以精细地控制不同区域/对象的形状或布局。
- (2):过去的方法存在固定标签的限制，难以控制不在标签集中的对象，提供密集分割可能会给用户带来不便，缺乏对每个实例的特定特征的细粒度控制。本文提出了一种新方法，使用开放词汇表场景控制的文本到图像生成，提供了全局文本提示和分割图，其中每个感兴趣的区域都用自由形式的自然语言描述进行注释。本文提出了一种基于CLIP的空间文本表示，并在两种最先进的扩散模型上展示了其有效性：基于像素和基于潜在变量的模型。本文还展示了如何将扩散模型中的无分类器指导方法扩展到多条件情况，并提出了一种替代的加速推理算法。最后，本文提出了几个自动评估指标，并使用它们以及FID分数和用户研究来评估我们的方法，并展示了它在具有自由形式文本场景控制的图像生成方面的最新结果。
- (3):本文提出了一种新的文本到图像生成方法，使用开放词汇表场景控制，提供了全局文本提示和分割图，其中每个感兴趣的区域都用自由形式的自然语言描述进行注释。本文提出了一种基于CLIP的空间文本表示，并在两种最先进的扩散模型上展示了其有效性：基于像素和基于潜在变量的模型。本文还展示了如何将扩散模型中的无分类器指导方法扩展到多条件情况，并提出了一种替代的加速推理算法。本文提出了几个自动评估指标，并使用它们以及FID分数和用户研究来评估我们的方法，并展示了它在具有自由形式文本场景控制的图像生成方面的最新结果。
- (4):本文的方法在具有自由形式文本场景控制的图像生成方面取得了最新的结果，通过自动评估指标、FID分数和用户研究来评估方法，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种名为SpaText的文本到图像生成方法，使用全局文本提示和原始的空间文本矩阵作为输入。该方法使用预训练的全景分割模型和CLIP模型构建空间文本表示。在推理过程中，使用CLIP文本编码器嵌入本地提示，并使用先前的模型将其转换为CLIP图像嵌入。然后，使用用户提供的空间地图在指定的像素处构建空间文本表示。该方法还提出了两种适应无分类器指导的多条件问题的方法，并将其应用于两种最先进的文本到图像扩散模型中。 

#### 8. 实验设置：
本文使用COCO-Stuff数据集进行评估，该数据集包含9000个训练图像和1000个验证图像。作者使用与先前工作相同的数据拆分。评估指标包括Fréchet Inception Distance（FID）、Intersection over Union（IOU）和文本匹配分数。作者使用包含3500万个图像文本对的数据对现有的文本到图像模型进行微调，并使用真实图像生成随机输入进行评估。作者还在Amazon Mechanical Turk上进行了用户研究，以评估视觉质量和全局/局部提示的符合性。 

#### 9. 实验结果和分析：
本文提出的SpaText方法在FID得分、人类视觉质量和人类全局文本匹配方面优于其他方法。与简单的二进制表示相比，SpaText在用户研究中实现了更好的局部文本匹配，但较小的局部IOU。与具有CLIP文本嵌入的版本相比，SpaText在实现更好的FID和整体视觉质量的同时，略微降低了局部IOU得分和人类局部文本匹配。最后，单一尺度的方法在局部CLIP距离方面略微逊色于多尺度的方法，但在其他方面表现更好。本文还使用自动评估指标和人类评分对SpaText方法与其他方法进行了详细比较。


# Paper:614     从单目图像学习细节辐射流形，实现高保真和3D一致的人像合成



#### 1. Title: 
Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image

#### 2. Authors: 
Yu Deng, Baoyuan Wang, Heung-Yeung Shum, Xiaobing.AI

#### 3. Affiliation: 
Yu Deng: Xiaobing.AI (中国小冰)
Baoyuan Wang, Heung-Yeung Shum: The Chinese University of Hong Kong (香港中文大学)

#### 4. Keywords: 
novel view synthesis, 3D consistency, radiance manifolds, GAN, portrait synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Deng_Learning_Detailed_Radiance_Manifolds_for_High-Fidelity_and_3D-Consistent_Portrait_Synthesis_CVPR_2021_paper.html  Github: https://yudeng.github.io/GRAMInverter/

#### 6. Summary : 
- (1):本文研究的是单目人像图像的新视角合成，其中最大的挑战是在连续姿态变化下实现3D一致性。现有的大多数方法依赖于2D生成模型，这往往会导致明显的3D不一致性伪影。
- (2):现有的2D GAN方法在需要更严格的3D一致性的应用中仍然存在缺陷。最近出现了一些3D感知的GAN，但是将它们用于实际图像姿态编辑仍然是一个具有挑战性的任务。本文提出了一种基于GRAM的新方法，该方法可以通过单个前向传递实现单目人像图像的3D一致的新视角合成。本文的方法通过学习3D一致的细节流形，将其与粗糙的辐射流形相结合，以实现高保真度的重建。本文的方法在性能上大大优于现有的方法。
- (3):本文提出了一种新的细节流形重构器，用于从单目图像中学习3D一致的细节。本文的方法通过将细节流形与粗糙的辐射流形相结合，实现了高保真度的重建。本文的方法还提出了专门的损失函数，以通过从粗糙的辐射流形中推导出的3D先验来调节细节流形，以确保合理的新视角结果。
- (4):本文的方法在FFHQ数据集上进行了训练，并进行了多个实验以证明其在人像姿态控制方面的优势。本文的方法可以在单个GPU上以3 FPS的速度对单目图像进行预测，生成的新视角可以很好地保留原始图像的细节，并具有强的3D一致性，性能大大优于现有的方法。本文的方法是实现高效的3D感知内容创建的重要一步。
#### 7. 方法详细介绍：
本文提出了GRAMinverter，一种从单目图像中实现高保真度和三维一致性人像合成的方法。该方法包括两个阶段：通用反演阶段和细节特定反演阶段。在通用反演阶段，使用GRAM的一个高效版本提取输入图像的粗略特征。在细节特定反演阶段，应用多级重建损失来学习细节流形重构器。还强制执行深度正则化以确保在几何表面附近预测细节。在新视角合成阶段，通过将细节特定阶段的输出添加到通用反演阶段的粗略特征流形中，得到最终的辐射流形。然后可以通过在输入视图处进行流形渲染来获得最终的反演图像，并且在渲染期间可以轻松生成任意相机姿态的新视图。

#### 8. 实验设置：
本文使用FFHQ数据集进行训练，分辨率为2562，并在CelebA-HQ数据集上进行测试。输入图像的相机姿态是通过人脸重建方法估计的。模型在4个Tesla V100 GPU上进行训练，每个GPU内存为32GB。整个训练过程需要约6天。

#### 9. 实验结果与分析：
本文提出的方法实现了从单目图像中高保真度和三维一致性的人像合成。新视角合成结果表明，该方法能够很好地保留输入图像的细节，并生成其三维一致的新视角。整个反演和新视角合成过程在V100 GPU上以3 FPS运行，没有专门的加速，这大大提高了效率。该方法在所有指标上都显著优于其他三维感知GAN反演方法，并在新视角质量和三维一致性方面在所有竞争者中获得最佳结果。


# Paper:615     ERM-KTP: 基于知识迁移的知识级别机器去学习



#### 1. Title: 
ERM-KTP: Knowledge-level Machine Unlearning via Knowledge Transfer

#### 2. Authors: 
Shen Lin, Xiaoyu Zhang, Chenyang Chen, Xiaofeng Chen, Willy Susilo

#### 3. Affiliation: 
第一作者：西安电子科技大学

#### 4. Keywords: 
Machine unlearning, deep learning, knowledge transfer, privacy protection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_ERM-KTP_Knowledge-Level_Machine_Unlearning_via_Knowledge_Transfer_CVPR_2021_paper.html  Github: https://github.com/RUIYUN-ML/ERM-KTP

#### 6. Summary : 
- (1):本文研究机器学习中的隐私保护问题，提出了一种基于知识迁移的机器去学习方法，旨在提高机器学习模型的隐私保护性能。

- (2):过去的机器去学习方法通常需要重新训练模型，计算和存储开销较大，而近似方法的精度不高，容易导致模型性能下降。本文提出的ERM-KTP方法从知识角度定义机器去学习，提出了一种可解释的去学习方法，即ERM-KTP。其中，ERM结构减少了不同类别之间的知识纠缠，KTP方法通过知识迁移和禁止机制实现了数据点的去学习。该方法具有可解释性，能够精确地去学习数据点，提高了去学习方法的可靠性。

- (3):本文提出的ERM-KTP方法通过ERM结构和KTP方法相结合，实现了基于知识迁移的机器去学习。该方法具有可解释性，能够精确地去学习数据点，提高了去学习方法的可靠性。

- (4):本文在三个不同规模的图像分类数据集和三个复杂的CNN上进行了实验，证明了ERM-KTP方法的有效性、高效性、高保真度和可扩展性。
#### 7. 方法详细介绍：
本文提出了一种基于知识转移的ERM-KTP方法，用于知识级别的机器遗忘。该方法包括ERM结构和KTP方法两个主要组成部分。ERM结构是CNN的可学习层，用于在训练阶段减少类别之间的纠缠。KTP方法用于将原始模型中非目标数据点的知识转移到未学习模型中，同时禁止目标数据点的知识。最终得到未学习模型，并删除原始模型以完成遗忘过程。该方法是可解释的遗忘方法，因为ERM结构和KTP中的制作的掩码可以明确解释遗忘数据点的操作和效果。

具体步骤如下：
1. 训练ERM-CNN，得到ERM结构；
2. 通过ERM结构训练ERM-KTP模型；
3. 使用KTP方法将非目标数据点的知识转移到未学习模型中；
4. 删除原始模型，得到未学习模型。

#### 8. 实验设置：
本文在三个流行的图像分类基准数据集CIFAR10、CIFAR100和Tiny-ImageNet上进行了实验评估。其中，CIFAR-10和CIFAR-100的输入大小为32×32，分别包含50,000个训练图像和10,000个验证图像。对于Tiny-ImageNet，图像样本的大小为224×224。它包含200个类别的样本，每个类别有500个样本用于训练，50个用于验证和50个用于测试。此外，模型结构为三个常见的图像分类深度神经网络，ResNet-20、ResNet-50和ResNeXt-50。原始模型使用随机梯度下降（SGD）优化器进行200个epoch的训练，动量为0.9，权重衰减为5e-4，初始学习率为0.1，在100和150个epoch后分别除以10。

#### 9. 实验结果与分析：
本文提出的ERM-KTP遗忘方法相对于基线方法如RfS、SISA、Fisher、Fine-tune和Amnesiac等方法，具有更高的效果、更高的效率和更好的可扩展性。该方法在CIFAR10和ResNet-20上的遗忘时间比RfS、SISA和Fisher显著提高。消融实验表明，应用L2和L3正则化项可以实现最佳性能和收敛到最优值。


# Paper:616     DIP：双重不一致感知网络用于讽刺检测



#### 1. Title: 
DIP: Dual Incongruity Perceiving Network for Sarcasm Detection

#### 2. Authors: 
Changsong Wen, Guoli Jia, Jufeng Yang

#### 3. Affiliation: 
南开大学计算机科学学院

#### 4. Keywords: 
Sarcasm detection, multi-modal learning, incongruity, sentiment analysis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wen_DIP_Dual_Incongruity_Perceiving_Network_for_Sarcasm_Detection_CVPR_2021_paper.html  Github: https://github.com/downdric/MSD

#### 6. Summary : 
- (1):本文研究了多模态下的讽刺检测，提出了一种Dual Incongruity Perceiving (DIP)网络，旨在解决多模态下的内在不一致性问题。
- (2):过去的方法主要集中在文本上，而本文则是首次探索了图像和文本的多模态讽刺检测。本文提出的方法通过两个分支来挖掘事实和情感层面的讽刺信息，并引入了通道加权策略和连续对比学习策略来获取具有区分性的表示。本文的方法在多个数据集上进行了实验，表现优于现有方法。
- (3):本文提出的DIP网络由Semantic Intensified Distribution (SID) Modeling和Siamese Sentiment Contrastive (SSC) Learning模块组成。在SID中，我们利用高斯分布来建模内在不一致性，并引入通道加权策略来学习与讽刺相关的表示。在SSC中，我们利用两个siamese层来传递情感信息，并提出了连续对比学习来增强情感表示。总体而言，本文的方法通过SID和SSC来强化事实和情感信息，并利用这些信息来计算多模态讽刺数据的内在不一致性。
- (4):本文的方法在多个数据集上进行了实验，表现优于现有方法。在SARC数据集上，本文的方法在F1-score上达到了0.73的最佳性能。本文的方法通过显式建模内在不一致性来解决多模态讽刺检测中的问题，具有一定的创新性和实用性。
#### 7. 方法详细介绍：
本文提出了一种用于多模态讽刺检测的双重不协调感知网络（Dual Incongruity Perceiving Network，DIP）。该网络由两个分支组成：语义强化分布建模（Semantic Intensified Distribution Modeling，SID）和连续对比学习（Siamese Sentiment Contrastive Learning，SSC）。首先，输入的图像和文本分别经过ViT和BERT进行处理，然后分别传递到SID和SSC两个分支。SID分支采用通道加权策略来强调与讽刺相关的区域，并利用高斯分布来发现讽刺样本。SSC分支采用连续对比学习来联合学习多模态情感嵌入。最后，双重感知模块将来自SID和SSC的嵌入进行融合，用于讽刺检测。

#### 8. 实验设置：
本文在公共的MSD数据集上进行了实验，将数据集划分为训练集、测试集和验证集，比例分别为80%、10%和10%。图像被统一调整为224×224，ViT中的补丁分辨率设置为16。对于使用ViT和BERT的实验，小批量大小为16，否则为64。模型训练20个epochs，学习率分别为0.00002和0.00005。

#### 9. 实验结果和分析：
本文提出的DIP网络在MSD数据集上取得了最先进的性能，优于其他单模态和多模态方法。当使用ResNet和BERT作为图像和文本模态的后骨干网络时，DIP的准确率、二元F1和宏F1分别为88.20%、85.70%和87.83%。当使用ViT和BERT作为后骨干网络时，DIP的准确率、二元F1和宏F1分别为89.59%、87.17%和89.01%。本文还比较了不同融合策略的性能，并提供了一些可视化结果和失败案例。


# Paper:617     文本-视觉提示的高效二维时间视频定位



#### 1. Title: 
Text-Visual Prompting for Efficient 2D Temporal Video Grounding

#### 2. Authors: 
Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding

#### 3. Affiliation: 
第一作者：密歇根州立大学(Michigan State University)

#### 4. Keywords: 
Temporal video grounding, 2D visual features, text-visual prompting, cross-modal feature fusion, Temporal-Distance IoU (TDIoU) loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Text-Visual_Prompting_for_Efficient_2D_Temporal_Video_Grounding_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了时间视频定位（TVG）的问题，即在长时间未修剪的视频中预测由文本句子描述的时刻的开始/结束时间点。由于3D卷积神经网络（CNN）的高复杂性使得提取密集的3D视觉特征耗时，因此需要大量的内存和计算资源。为了实现高效的TVG，本文提出了一种新颖的文本-视觉提示（TVP）框架，该框架将优化的扰动模式（称为“提示”）纳入TVG模型的视觉输入和文本特征中。与3D CNN相比，TVP允许我们在2D TVG模型中有效地共同训练视觉编码器和语言编码器，并仅使用低复杂度的稀疏2D视觉特征来改善跨模态特征融合的性能。此外，本文提出了一种时间距离IoU（TDIoU）损失，用于有效学习TVG。 

- (2):早期的TVG解决方案主要采用两阶段“提议和排名”流程，而回归方法直接预测目标时刻的开始/结束时间点，因此比提议方法快得多。然而，由于3D CNN的高成本，大多数现有的TVG模型仅采用这些3D视觉特征作为输入，而不是在TVG模型训练期间进行共同训练。虽然使用3D视觉特征的模型（称为“3D方法或模型”）优于使用2D特征的模型（称为“2D方法或模型”），但2D方法的一个独特优势是提取2D视觉特征可以显着降低TVG任务的成本。因此，本文提出了一种新颖的TVP框架，用于训练使用2D视觉特征的TVG模型。 

- (3):本文提出了一种有效且高效的框架来训练2D TVG模型，其中我们利用TVP（文本-视觉提示）来提高稀疏2D视觉特征的效用，而不需要昂贵的3D特征。我们将可训练参数应用于文本特征作为文本提示，并开发了一组通用的面向帧的模式作为视觉提示。此外，我们选择transformer作为基础TVG模型，并提出利用提示来弥补2D视觉特征中缺乏时空信息的不足。此外，我们开发了一种基于时间距离IoU（TDIoU）的损失来训练我们的提出框架。 

- (4):本文在Charades-STA和ActivityNet Captions数据集上进行了实验，实验证明所提出的TVP显著提高了2D TVG的性能（例如，在Charades-STA上提高了9.79％，在ActivityNet Captions上提高了30.77％），并且相对于使用3D视觉特征的TVG实现了5倍的推理加速。
#### 7. 方法详细介绍：
本文提出了一种文本-视觉提示（TVP）框架，用于提高2D时间视频定位（TVG）方法的性能。该框架包括四个阶段：视频帧预处理、特征提取、多模态特征处理和跨模态融合。其中，视觉编码器采用可训练的2D CNN，用于从稀疏采样的视频帧中提取特征。该框架还利用transformer作为基础TVG模型，并提出利用提示来弥补2D视觉特征中缺乏的时空信息。此外，本文还提出了Temporal-Distance IoU（TDIoU）损失函数，用于有效学习TVG。 

#### 8. 实验设置：
本文在Charades-STA和ActivityNet Captions两个标准数据集上进行了实验。数据集被分为训练集、验证集和测试集。实验使用ResNet-50作为2D视觉编码器，BERT-base模型权重用于跨模态预训练，模型和提示分别在8个NVIDIA V100 GPU上进行12个epoch的微调。 

#### 9. 实验结果与分析：
本文提出的TVP框架在Charades-STA和ActivityNet数据集上均取得了显著的性能提升，超过了所有2D模型，并且达到了与3D模型相当的性能。此外，TVP框架比使用3D视觉特征的TVG方法实现了超过5倍的推理加速。在损失设计方面，加入距离损失Ldis和持续时间损失Ldur可以显著提高性能，特别是在tIoU阈值m = 0.5和m = 0.7时。提示的使用使得TVP框架的损失曲面更加平坦，表明提示可能编码了额外的时空监督，有助于模型训练器从坏的局部最优解中逃脱。


# Paper:618     CLIP2Scene：通过CLIP实现标签高效的3D场景理解



#### 1. Title: 
CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP

#### 2. Authors: 
Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, Wenping Wang

#### 3. Affiliation: 
第二作者：上海人工智能实验室

#### 4. Keywords: 
3D scene understanding, CLIP, contrastive learning, semantic segmentation, self-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2021_paper.html  Github: https://github.com/runnanchen/CLIP2Scene

#### 6. Summary : 
- (1):本文研究的是3D场景理解，目前的深度学习方法在3D点云数据上表现出色，但是需要大量标注数据，且无法识别新物体。本文探索如何利用CLIP的2D图像-文本预训练知识来提高3D场景理解的性能。

- (2):过去的跨模态知识蒸馏方法存在优化冲突问题，忽略了多扫描点云的时间一致性，无法利用丰富的扫描间对应关系。本文提出了一种新的基于语义和时空一致性的跨模态对比学习框架，利用CLIP的语义和视觉信息对3D网络进行正则化。

- (3):本文提出了CLIP2Scene框架，通过语义和时空一致性正则化，将CLIP知识从2D图像-文本预训练模型转移到3D点云网络。在SemanticKITTI、nuScenes和ScanNet数据集上进行了实验，首次实现了无标注3D语义分割，nuScenes和ScanNet的mIoU分别为20.8%和25.08%。在使用1%或100%标注数据微调3D网络时，本文方法显著优于其他自监督方法，mIoU分别提高了8%和1%。此外，本文还证明了处理跨域数据集的通用性。

- (4):本文提出的方法在无标注3D语义分割任务上取得了很好的性能，同时在使用标注数据微调时也显著优于其他自监督方法。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为CLIP2Scene的方法，用于标签效率的三维场景理解。该方法利用CLIP的预训练知识进行密集图像像素预测，并将其转移到三维点云中。该方法由三个主要组件组成：语义一致性正则化、语义引导的空间-时间一致性正则化和可切换的自训练策略。语义一致性正则化通过将点特征拉到其对应的文本嵌入来执行。空间-时间一致性正则化通过模仿时间上连续的点云特征与其对应的像素特征之间的一致性来执行。该方法还利用跨模态知识蒸馏将知识从二维图像转移到三维点云中。具体步骤包括：
1. 使用CLIP模型提取图像和文本特征。
2. 使用3D网络提取点特征和语义分割。
3. 使用可切换的自训练策略来提高性能。
4. 使用对比学习方法学习像素-点对应关系和语义一致性正则化来强制执行图像和点云特征之间的一致性。
5. 使用空间-时间一致性正则化来提高点特征的时间连续性。

#### 8. 实验设置：
本文在三个数据集上进行了实验：SemanticKITTI、nuScenes和ScanNet。预训练是在nuScenes数据集上使用SLidR方法进行的。CLIP2Scene框架是在PyTorch上开发的，优化器是带余弦调度器的SGD。温度λ和τ分别设置为1和0.5。经验上将扫描次数设置为3。在对比学习中应用了几种数据增强方法，包括沿z轴的随机旋转、点云上的随机翻转、图像上的随机水平翻转和随机裁剪-调整大小。

#### 9. 实验结果和分析：
本文展示了CLIP2Scene框架在nuScenes和ScanNet数据集上的无注释语义分割性能。该框架在两个数据集上均取得了最先进的性能。本文还进行了消融实验，以评估框架不同组件的有效性。结果表明，空间-时间一致性正则化和可切换的自训练策略对提高性能有效。本文还展示了单个类别的定性评估，表明该方法能够感知对象，即使没有在任何注释数据上进行训练。然而，存在围绕地面真实对象的误报预测，这将在未来的工作中解决。预训练的3D网络也在少量标记数据可用于训练时提高了性能。该方法在使用标记数据进行微调时显著优于最先进的自监督方法，分别在1%和100%的nuScenes数据集上提高了8.1%和1.1%。与随机初始化相比，提高了14.1%和2.4%，表明语义驱动的跨模态对比学习框架的效率。定性结果如图6所示，该方法取得了不错的性能。


# Paper:619     基于语义促进去偏差和背景消歧的零样本实例分割



#### 1. Title: 
Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation

#### 2. Authors: 
Shuting He, Henghui Ding, Wei Jiang

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Zero-shot instance segmentation, bias issue, background disambiguation, semantic-promoted debiasing, input-conditional classifier

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/He_Semantic-Promoted_Debiasing_and_Background_Disambiguation_for_Zero-Shot_Instance_Segmentation_CVPR_2021_paper.html  Github: https://github.com/henghuiding/D2Zero

#### 6. Summary : 
- (1):本文研究了零样本实例分割中的偏差问题和背景混淆问题。
- (2):过去的方法存在偏差问题，容易将新颖对象分类为已知类别，同时也存在背景混淆问题。本文提出了一种基于语义促进去偏差和背景消歧的方法，通过利用未见类别的语义关系来训练视觉特征，设计了一个基于Transformer机制的输入条件分类器，以及一种自适应背景表示方法，以解决这两个问题。
- (3):本文提出的方法包括语义促进去偏差和背景消歧两个部分。语义促进去偏差通过生成伪未见标签来训练视觉特征，以区分已知和未知类别。背景消歧则通过自适应背景表示来避免将新颖对象误认为背景。此外，本文还设计了一个基于Transformer机制的输入条件分类器，以动态分类输入图像。
- (4):在COCO数据集上的实验结果表明，本文提出的方法在零样本实例分割任务上取得了新的最优性能，比之前的最优方法提高了16.86%的mAP。
#### 7. 方法详细介绍：
本文提出了一种名为D2Zero的方法，用于解决广义零样本实例分割（GZSIS）中的偏差和背景混淆问题。D2Zero采用ResNet-50作为骨干网络，并遵循Mask2Former的范例。模型生成一组原型作为输入条件分类器，并利用类间语义相似性指导视觉特征提取器的训练。输入条件分类器基于Transformer学习语义-视觉对齐，并生成输入特定的视觉表示作为分类器原型。模型使用已知类别的交叉熵损失和提出的未知类别的交叉熵损失作为训练目标进行训练。D2Zero通过利用未知类别的语义知识参与视觉特征训练来缓解偏差问题，并设计了一个输入条件分类器来解决偏差问题和多模态域间差距问题。背景混淆问题通过引入图像自适应背景表示来解决，以更好地捕捉图像特定的背景线索。

#### 8. 实验设置：
本文在COCO数据集上进行实验，采用广义零样本实例分割（GZSIS）设置，分别在48/17和65/15两个不同的数据集划分上进行实验，其中第一个数字表示已知类别的数量，第二个数字表示未知类别的数量。

#### 9. 实验结果与分析：
本文提出的D2Zero方法在零样本实例分割和检测任务中取得了新的最优结果，显著优于先前的最先进方法，例如在48/17数据集划分上提高了16.86%。D2Zero方法在零样本实例分割和广义零样本实例分割任务中均优于最先进的ZSI方法。实验结果表明，使用CLIP嵌入代替word2vector嵌入可以提高D2Zero的性能。


# Paper:620     如何防止个性化联邦学习中的性能较差客户端？



#### 1. Title: 
How to Prevent the Poor Performance Clients for Personalized Federated Learning?

#### 2. Authors: 
Zhe Qu, Xingyu Li, Xiao Han, Rui Duan, Chengchao Shen, and Lixing Chen

#### 3. Affiliation: 
Zhe Qu, Chengchao Shen: Central South University

#### 4. Keywords: 
Personalized federated learning, poor performance clients, bias, Layer-Wised Sharpness Aware Minimization, generalization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qu_How_to_Prevent_the_Poor_Performance_Clients_for_Personalized_Federated_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是个性化联邦学习中的性能问题，即部分客户端可能会出现性能较差的情况，而现有的研究主要集中在提高平均性能方面，对于性能较差的客户端并没有得到很好的解决。 

- (2):现有的个性化联邦学习算法主要集中在提高个性化方面，但是忽略了部分客户端可能会出现性能较差的情况。本文提出了一种新的个性化联邦学习策略，即“本地个性化，全局泛化”（PLGU），通过设计一种基于层面锐度感知最小化（LWSAM）算法的泛化方法，来解决普遍存在的偏见问题。本文将PLGU策略嵌入到两种个性化联邦学习方案中，并详细介绍了训练过程。 

- (3):本文提出的PLGU策略通过设计一种基于层面锐度感知最小化（LWSAM）算法的泛化方法，来解决普遍存在的偏见问题。对于方案I（有全局模型），本文提出了PLGU-Layer Freezing（LF）算法，该算法首先探索每个层的个性化得分，然后在本地冻结重要层以进行个性化，并使用LWSAM优化器进行泛化。对于方案II（没有全局模型），本文主要关注我们提出的PLGU策略FedRep算法，称为PLGU-GRep。它通过在表示部分中平滑个性化来泛化通用信息。 

- (4):本文的方法在CIFAR10数据集上进行了实验，结果表明，所有基于PLGU的算法均取得了最先进的性能，成功地防止了性能较差的客户端，并在增量减少顶级性能客户端的同时，实现了竞争性的泛化界限。
#### 7. 方法详细介绍：
本文提出了一种个性化联邦学习框架PLGU-LF，旨在防止性能差的客户端。PLGU-LF算法为每个客户端使用个性化层和全局模型来共享通用信息。算法分别更新全局模型和个性化层，并使用扰动策略来调节表示中的个性化。本文还提出了基于表示学习的PLGU-GRep算法，用于方案II。PLGU-GRep算法使用两步更新来调节表示中的个性化，防止性能差的客户端。本文提供了PLGU-GRep算法的详细训练过程。

#### 8. 实验设置：
本文使用CIFAR10、CIFAR100和Tiny-ImageNet（TmgNet）数据集进行实验，包括不同的异构级别和参与率。客户端数量和数据样本在每个客户端上均匀分布。设置了本地时期数、通用层数和本地批量大小。本文还提到了参与方案，并将PLGU-LF、PLGU-GRep和PLGU-GHN算法与几种最先进的FL和pFL基准进行比较：FedAvg、FedSAM、Ditto、pFedMe、FedRep和pFedHN。

#### 9. 实验结果和分析：
本文提出的PLGU-LF算法在个性化和全局模型中均取得了最佳性能，当LPer的经验数量为5时。PLGU-LF、-GHN和-GRep下的个性化模型分布显着降低了偏差并防止了更多的性能差的客户端，同时没有明显降低顶级客户端。本地时期数K对性能的影响表明，当K = 5时，性能最佳。扰动ρ对LWSAM的影响表明，当ρ = 0.05时，性能最佳。最后，PLGU-LF的损失景观可视化显示全局模型更加平滑，即在客户端上保护模型个性化的同时，更好地泛化通用信息，而个性化模型更加锐利，即在客户端上保护模型个性化。

#### 论文总结：
本文提出了一种个性化联邦学习框架PLGU，旨在防止性能差的客户端。PLGU策略分为两个主要部分：提取个性化特征并将其保留在本地，以及泛化通用信息。为了泛化通用信息，使用了Sharpness Aware Minimization（SAM）算法作为本地优化器。此外，开发了一种Layer-Wise SAM（LWSAM）算法，以获得细粒度的通用信息。PLGU策略嵌入到两个pFL方案中：有/无全局模型，并详细介绍了训练过程。提出了两种算法：PLGU-LF用于方案I，PLGU-GRep用于方案II。在两个算法中都使用了LWSAM优化器来泛化通用信息，而不会对本地特征进行扰动。在PLGU-LF中，探索了每个层的个性化得分，并将重要层在本地冻结以进行个性化。在PLGU-GRep中，通过平滑表示中的个性化来泛化通用信息。PLGU策略还扩展到pFedHN，称为PLGU-GHN，以提高学习性能，特别是对于性能差的客户端。对PLGU-LF、PLGU-GRep和PLGU-GHN算法的泛化界限进行了深入分析。在不同的学习模型上评估了所提出算法的性能，包括ResNet-18、WideResNet28-10和ResNet-50。所提出的算法在个性化模型准确性评估方面优于其他算法。本文还评估了三个数据集下个性化模型的收敛结果。


# Paper:621     Consistent-Teacher：减少半监督目标检测中不一致伪标签的方法



#### 1. Title: 
Consistent-Teacher: Towards Reducing Inconsistent Pseudo-targets in Semi-supervised Object Detection

#### 2. Authors: 
Xinjiang Wang, Xingyi Yang, Shilong Zhang, Yijiang Li, Litong Feng, Shijie Fang, Chengqi Lyu, Kai Chen, Wayne Zhang

#### 3. Affiliation: 
第一作者：SenseTime Research

#### 4. Keywords: 
Semi-supervised object detection, pseudo-targets, consistency, adaptive anchor assignment, feature alignment, Gaussian Mixture Model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Consistent-Teacher_Towards_Reducing_Inconsistent_Pseudo-Targets_in_Semi-Supervised_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/Adamdad/ConsistentTeacher

#### 6. Summary : 
- (1):本文研究半监督目标检测中伪标签不一致性问题，提出了一种系统性解决方案，称为Consistent-Teacher，以减少不一致性。 
- (2):传统的SSOD方法中，教师模型生成的伪标签可能高度不准确，且在不同训练阶段变化很大，这会导致SSOD预测偏差和严重的过拟合问题。本文提出了自适应锚点分配（ASA）、3D特征对齐模块（FAM-3D）和高斯混合模型（GMM）等方法，以提高伪标签的一致性和质量。 
- (3):本文提出的Consistent-Teacher方法在多个SSOD数据集上进行了实验，取得了优异的性能，其中在MS-COCO数据集上，只使用10%的标注数据，ResNet-50骨干网络的mAP达到了40.0，超过了之前使用伪标签的基线约3个mAP。 
- (4):本文提出的方法在SSOD任务上取得了优异的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为“Consistent-Teacher”的半监督目标检测方法，旨在解决伪标签不一致的问题。该方法包括三个主要组件：自适应样本分配（ASA）、三维特征对齐模块（FAM-3D）和高斯混合模型（GMM）。ASA通过代价感知的自适应样本分配替换静态IoU-based锚点分配，以减少密集伪目标的不一致性影响。FAM-3D通过允许分类特征感知并采用其邻域中最佳特征进行回归，从而校准分类和回归任务。GMM在训练期间为每个类别生成自适应阈值，以稳定正样本数量。该方法旨在提高伪边界框的质量并减少SSOD中的标签漂移。

#### 8. 实验设置：
本文在MS-COCO数据集上进行了实验，使用了不同比例的标记数据。实验在单个NVIDIA V100 GPU上进行，具有32GB内存。实验中使用的骨干网络是ResNet-50。实验使用PyTorch实现，代码可在GitHub上获得。

#### 9. 实验结果和分析：
Consistent-Teacher方法在各种半监督目标检测基准测试中均取得了显着的性能提升，包括COCO和VOC0712数据集。具体而言，在具有额外未标记数据的COCO数据集上，Consistent-Teacher实现了47.7的mAP，超过了所有先前的最先进作品。在VOC0712数据集上，Consistent-Teacher相对于最新的最先进方法显示出2.2绝对mAP的提高。消融研究还验证了所提出的自适应样本分配策略、三维特征对齐模块和基于GMM的伪标签过滤的有效性。


# Paper:622     探索视频帧插值中的不连续性



#### 1. Title: 
Exploring Discontinuity for Video Frame Interpolation

#### 2. Authors: 
Sangjin Lee, Hyeongmin Lee, Chajin Shin, Hanbin Son, Sangyoun Lee

#### 3. Affiliation: 
Yonsei University (韩国延世大学)

#### 4. Keywords: 
Video frame interpolation, discontinuous motion, data augmentation, deep learning, discontinuity map

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Exploring_Discontinuity_for_Video_Frame_Interpolation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是视频帧插值，即在给定两个连续帧的情况下生成中间帧。以往的研究主要集中在适当的帧变形操作和用于变形帧的细化模块上，但这些研究都是针对只包含连续运动的自然视频进行的。然而，许多实际视频包含各种不连续运动的非自然对象，如标志、用户界面和字幕等。因此，本文提出了三种技术，使现有的基于深度学习的视频帧插值架构能够对这些元素具有鲁棒性。

- (2):以往的方法主要集中在运动估计和运动补偿上，但是这些方法只能处理连续运动的视频。本文提出了一种新的数据增强策略，称为图文混合（FTM），可以使模型在训练阶段学习不连续运动而无需任何额外的数据集。其次，本文提出了一种简单而有效的模块，可以预测称为不连续性图（D-map）的地图，该地图密集地区分连续和不连续运动的区域。最后，本文提出了损失函数，以在FTM和D-map之间应用不连续运动区域的监督。本文构建了一个特殊的测试基准，称为图形不连续运动（GDM）数据集，用于评估本方法和竞争方法如何处理不连续运动。本文的方法在GDM数据集上显著提高了插值质量，并在只包含连续运动的现有基准测试集（如Vimeo90K、UCF101和DAVIS）上优于其他方法。

- (3):本文提出了一种新的数据增强策略，称为FTM，可以使模型在训练阶段学习不连续运动而无需任何额外的数据集。其次，本文提出了一种轻量级模块，可以预测称为D-map的地图，该地图可以区分连续和不连续运动的区域。最后，本文提出了损失函数，以在FTM和D-map之间应用不连续运动区域的监督。

- (4):本文的方法在GDM数据集上显著提高了插值质量，并在只包含连续运动的现有基准测试集（如Vimeo90K、UCF101和DAVIS）上优于其他方法。本文的主要贡献可以总结如下：新的数据增强策略、新的模块和损失函数、在各种最先进的VFI模型上应用的性能提高。
#### 7. 方法详细介绍：
本文提出了一种新的视频帧插值网络，用于处理连续和不连续运动区域。该方法包括三个技术：一种名为Figure-Text Mixing (FTM)的数据增强策略，一个轻量级模块用于估计不连续性地图（D-map），以及用于监督模型的损失函数。FTM包括两种数据增强方法：图形混合（FM）和文本混合（TM）。FM是固定随机图形的增强，TM是不连续移动的随机文本的增强。D-map用于预测不连续性地图并区分连续和不连续运动区域。该方法的VFI过程可以表示为：ˆIc = F(I1, I2)和ˆI(x) = ˆIc(x) · (1 − D(x)) + I1(x) · D(x)。该方法的架构包括一个VFI网络、一个D-map估计器和一个损失函数。

#### 8. 实验设置：
本文使用Vimeo90K septuplet train数据集进行训练，该数据集包括91,701个由七个448×256帧组成的序列。本文将提出的数据增强方法，包括FTM，应用于训练数据集。本文选择了三个最近的VFI网络，包括基于变形的、基于直接预测的和基于Transformer的架构，以观察应用提出的想法后性能的变化。本文选择了三个测试基准，包括Vimeo90K测试数据集、UCF101和DAVIS，进行评估。

#### 9. 实验结果和分析：
本文提出的方法在各种数据集和指标上均取得了最先进的结果，证明了该网络在不使用额外数据集的情况下对于各种运动区域都表现良好。该方法在GDM数据集上取得了高性能，但在其他三个数据集上的质量略有降低或提高。定性结果证明了该方法在各种类型的视频中的有效性，包括连续和不连续运动。该方法不仅保持了连续域的质量，而且通过保持对象的结构为大运动产生了清晰的结果。结果证明了以前算法在不连续运动上的局限性，并表明提出的想法可以有效地覆盖连续和不连续域。


# Paper:623     基于实例级背景建模和前景选择的零样本背景减除



#### 1. Title: 
Zero-shot Background Subtraction via Instance-level Background Modeling and Foreground Selection

#### 2. Authors: 
Yongqi An, Xu Zhao, Tao Yu, Haiyun Guo, Chaoyang Zhao, Ming Tang, Jinqiao Wang

#### 3. Affiliation: 
中国科学院自动化研究所国家人工智能图像处理中心

#### 4. Keywords: 
Background subtraction, unsupervised learning, zero-shot object detection, instance-level modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/An_Zero-Shot_Background_Subtraction_via_Instance-Level_Background_Modeling_and_Foreground_CVPR_2021_paper.html  Github: https://github.com/CASIA-IVA-Lab/ZBS

#### 6. Summary : 
- (1):本文研究的背景是背景减除（BGS）任务，旨在从视频帧中提取所有移动对象以获得二进制前景分割掩码。
- (2):过去的方法包括传统的基于像素级背景模型的方法和基于深度学习的监督和无监督方法。传统方法的缺点是对于复杂场景的处理效果较差，而深度学习方法的缺点是无法检测预定义类别之外的对象。本文提出了一种基于零样本目标检测的无监督BGS算法，充分利用零样本目标检测的优势来构建开放词汇量的实例级背景模型，并通过比较新帧的检测结果与背景模型来有效地提取前景。该方法在复杂场景下表现良好，具有丰富和可扩展的类别。
- (3):本文提出的方法是基于零样本目标检测的实例级背景建模和前景选择。该方法包括三个阶段：全实例检测、实例级背景建模和前景实例选择。在全实例检测阶段，使用零样本目标检测模型将原始图像像素转换为结构化实例表示。在实例级背景建模阶段，该方法基于实例的运动信息构建实例级背景模型。在前景实例选择阶段，该方法选择全实例检测器的输出作为前景。如果实例符合规则2，则它是最终二进制掩码中的前景。该方法的创新点在于充分利用实例信息，构建实例级背景模型，从而在复杂场景下表现更好。
- (4):本文在CDnet 2014数据集上进行了实验，结果表明，与现有的无监督BGS方法相比，本文提出的方法在F-Measure上提高了4.70％。该方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种零样本背景减除（Zero-shot Background Subtraction，ZBS）的无监督背景减除算法，该算法利用实例级背景建模和前景实例选择。ZBS包含三个阶段：全实例检测、实例级背景建模和前景实例选择。在全实例检测阶段，ZBS使用零样本目标检测方法Detic来获取结构化实例表示。在实例级背景建模阶段，ZBS基于实例的运动信息建立实例级背景模型。最后，在前景实例选择阶段，ZBS使用IoU和IoF两个度量标准，从检测器输出中选择移动前景。该方法充分利用了零样本目标检测的优势，建立了开放词汇实例级背景模型。

#### 8. 实验设置：
本文在CDnet 2014数据集上进行了评估，该数据集包括53个视频序列和11个不同的经典应用场景类别。使用的评估指标为召回率（Recall，Re）、精确率（Precision，Pr）和F-Measure（F-M）。静态和阴影像素被视为负样本（背景），移动像素被视为正样本（前景）。非ROI和未知像素被忽略，以确保指标的公正性。

#### 9. 实验结果和分析：
本文提出的零样本背景减除框架ZBS在CDnet 2014数据集上取得了最先进的性能，超过了所有无监督算法甚至一些监督深度学习算法。该方法在七个类别中具有优势，如相机抖动、间歇性物体运动和夜间视频。然而，该方法不能很好地处理湍流，因为全实例检测模块的检测器不能适应湍流场景的非自然图像分布。本文还在CDnet 2014数据集的六个具有挑战性的场景上对所提出的方法与其他方法进行了视觉比较，突出了所提出的方法在处理噪声背景区域、相机抖动、水波纹、间歇性物体运动和阴影区域方面的优势。

#### 全文总结：
本文提出了一种零样本背景减除算法ZBS，该算法利用实例级背景建模和前景实例选择，充分利用了零样本目标检测的优势，建立了开放词汇实例级背景模型。实验结果表明，该方法在CDnet 2014数据集上取得了最先进的性能，超过了所有无监督算法甚至一些监督深度学习算法。该方法在复杂场景下表现良好，如阴影或夜间光线，并具有丰富和可扩展的类别。此外，该方法可以轻松推广到其他任务，如在未知环境中检测遗弃物品。


# Paper:624     FaceLit：神经网络三维可重照面部



#### 1. Title: 
FaceLit: Neural 3D Relightable Faces

#### 2. Authors: 
Anurag Ranjan, Kwang Moo Yi, Jen-Hao Rick Chang, Oncel Tuzel

#### 3. Affiliation: 
Anurag Ranjan, Oncel Tuzel: Apple
Kwang Moo Yi: University of British Columbia
Jen-Hao Rick Chang: None

#### 4. Keywords: 
3D generative model, neural volume rendering, illumination modeling, disentangled representation, photorealistic face images

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ranjan_FaceLit_Neural_3D_Relightable_Faces_CVPR_2021_paper.html  Github: https://github.com/apple/ml-facelit/

#### 6. Summary : 
- (1):本文研究背景是从2D图像中学习3D生成模型，以及如何将几何形状、外观和照明分离。
- (2):过去的方法要求多视角图像集合，或者需要手动注释，或者无法分离几何形状和照明。本文提出了一种新的框架，可以从单视角图像中学习分离的3D生成模型，并且可以在不同的视角和照明条件下渲染面部图像。该方法使用球谐函数嵌入物理光照模型，通过神经体积渲染框架学习生成面部形状和材质属性，从而产生具有多视角3D和照明一致性的逼真面部图像。该方法在多个数据集上进行了测试，取得了最先进的性能。
- (3):本文提出了一种新的框架FaceLit，可以从单视角图像中学习分离的3D生成模型，并且可以在不同的视角和照明条件下渲染面部图像。该方法使用球谐函数嵌入物理光照模型，通过神经体积渲染框架学习生成面部形状和材质属性，从而产生具有多视角3D和照明一致性的逼真面部图像。该方法可以在多个数据集上进行训练，不需要手动标注，只需要使用2D图像和现成的姿态/照明估计方法。该方法是第一个可以生成具有可控场景照明的3D面部的生成方法。该方法的创新点在于将物理渲染嵌入到神经体积渲染中，从而实现了照明和几何形状的有效分离。
- (4):该方法在多个数据集上进行了测试，取得了最先进的性能。在FFHQ数据集上，该方法的FID得分为3.5，是3D感知GAN中最好的。该方法可以生成具有可控场景照明的3D面部，具有很高的逼真度和多视角3D和照明一致性。
#### 7. 方法详细介绍：
本文提出了一种名为FaceLit的方法，它是一个神经3D可照明人脸模型。该方法通过将基于物理的着色融入辐射场的形成中，将照明与外观和形状分离。所使用的照明模型是Phong反射模型的简化版本，它明确地约束了照明的渲染过程。生成器是基于相机姿态和照明条件进行条件化的，允许对相机姿态和照明进行明确和一致的控制。训练过程涉及到GAN设置，其中鉴别器是基于相机姿态和照明SH系数进行条件化的。该方法在三个数据集FFHQ、MetFaces和CelebA-HQ上进行了评估，结果表明了所提出的方法在生成具有明确照明和相机姿态控制的逼真人脸方面的有效性。

具体步骤：
1. 使用DECA估计相机姿态和照明系数。
2. 使用GAN设置进行训练，其中鉴别器是基于相机姿态和照明SH系数进行条件化的。
3. 对于FFHQ数据集，训练过程分为两个阶段，每个阶段使用8个GPU上的32个批次。第二阶段调整渲染分辨率，并进一步训练750k次迭代。对于CelebA-HQ数据集，模型仅在第一阶段进行训练，渲染分辨率为642，超分辨到5122，训练500k次迭代。对于MetFaces数据集，使用在FFHQ上预训练的模型，通过ADA增强微调，训练15k次迭代。

#### 8. 实验设置：
本文使用了三个数据集进行实验：FFHQ、MetFaces和CelebA-HQ。使用DECA估计相机姿态和照明系数。训练过程涉及到GAN设置，其中鉴别器是基于相机姿态和照明SH系数进行条件化的。对于FFHQ数据集，训练过程分为两个阶段，每个阶段使用8个GPU上的32个批次。第二阶段调整渲染分辨率，并进一步训练750k次迭代。对于CelebA-HQ数据集，模型仅在第一阶段进行训练，渲染分辨率为642，超分辨到5122，训练500k次迭代。对于MetFaces数据集，使用在FFHQ上预训练的模型，通过ADA增强微调，训练15k次迭代。

#### 9. 实验结果与分析：
本文提出的方法在三个数据集FFHQ、MetFaces和CelebA-HQ上进行了评估。结果表明了所提出的方法在生成具有明确照明和相机姿态控制的逼真人脸方面的有效性。生成的样本与3D形状视觉上一致，并且模型提供了比EG3D更好的3D形状，特别是在嘴唇和牙齿周围的区域。该模型提供了一致的渲染，无论姿态和照明如何变化。还展示了改变高光组件对生成过程的影响，低高光度生成的结果看起来很平淡，而高高光度则会产生眩光。该模型的定量评估指标未在当前文本中提及。


# Paper:625     基于Blender的合成数据在监督学习和下游域自适应中的实用性新基准



#### 1. Title: 
A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation

#### 2. Authors: 
Hui Tang and Kui Jia

#### 3. Affiliation: 
South China University of Technology (华南理工大学)

#### 4. Keywords: 
Synthetic data, 3D rendering, domain randomization, supervised learning, domain adaptation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Tang_A_New_Benchmark_On_the_Utility_of_Synthetic_Data_With_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度学习在计算机视觉领域的成功，但由于大规模标注数据的高成本和标注精度的不确定性，使得数据集的收集和标注变得不切实际，因此需要一种替代方法来生成合成数据。
 
- (2):过去的方法主要是使用真实数据集，但是真实数据集的缺点是数据收集和标注成本高，数据集不一定是独立同分布的，而且标注精度不确定。本文提出了一种使用3D渲染和域随机化生成合成数据的方法，可以有效地解决这些问题。
 
- (3):本文的研究方法是使用3D渲染和域随机化生成合成数据，然后在这些数据上进行监督学习和域自适应的研究。本文的创新点在于，通过使用3D渲染和域随机化生成合成数据，可以有效地解决真实数据集的缺点，并且可以更好地验证典型的学习洞见和发现新的规律。
 
- (4):本文的方法在图像分类任务上取得了很好的性能，证明了使用合成数据进行预训练和域自适应的有效性。此外，本文还提出了一个新的大规模合成到真实的分类自适应基准，提供了更具挑战性的任务，为未来的研究提供了更好的基础。
#### 7. 方法详细介绍：
本文提出了一种通过3D渲染和域随机化生成合成数据的方法，用于克服数据注释的局限性和无控制的数据收集过程。作者使用模拟器进行3D渲染，生成用于模型训练的合成RGB图像。在渲染过程中应用域随机化以增加样本多样性，以获得更好的泛化性能。作者从ShapeNet库中选择一个特定类别的3D对象模型，并将其放置在一个空白场景中。他们使用随机参数的点源设置照明条件，并将相机放置在一个随机半径的球面上的随机位置。他们对物体应用随机材料和纹理，并使用RGB渲染器从不同的相机视角在配置的场景中拍摄照片。最后，将渲染的图像与从开放资源中随机选择的背景图像组合在一起。合成的图像与自动生成的类标签一起用作低成本的训练数据。

#### 8. 实验设置：
作者使用ShapeNet和VisDA-2017中共同的10个对象类进行实证研究。他们将VisDA-2017中的10个类别的合成和真实域称为SubVisDA-10。对于传统的固定数据集周期性训练，他们在每个类别中生成12K个合成图像，并在每个时期使用固定大小的数据集进行训练。他们考虑了三种不同强度的数据增强策略：没有增强，只有中心裁剪操作；基于像素位置的弱增强，如随机裁剪和翻转；以及强增强，既转换像素的位置又转换像素的值，例如随机调整大小的裁剪和颜色抖动。在测试阶段，他们不使用数据增强。

#### 9. 实验结果与分析：
作者通过比较固定数据集周期性训练和训练非重复样本来验证快捷学习、PAC泛化和方差-偏差权衡等重要见解。随着更多训练数据的增加和样本多样性的增加，数据不可重复训练表现出比固定数据集训练更高的泛化精度和更好的收敛性能。用于训练的合成数据比SubVisDA-10具有更高的OOD测试精度，因为它们与真实数据更相似。固定数据集训练显示出过拟合现象，而数据不可重复训练则没有。在强数据增强的情况下，数据不可重复训练的IID w/o BG数据测试结果不仅最佳，而且比IID数据更好，这意味着训练的模型不会学习依赖背景中的上下文线索的快捷解决方案。当与ResNet-50进行比较时，ViT-B的表现出乎意料地差。当在没有强数据增强的非重复图像上进行训练时，ViT-B和Mixer-B的表现优于ResNet-50。

本文还提出了一个新的合成到真实的基准数据集，用于实际视觉域自适应（S2RDA），其中包括S2RDA-49和S2RDA-MS-39两个具有挑战性的转移任务。源/合成域样本是通过从ShapeNet渲染3D模型生成的，而S2RDA-49的真实域包括来自ImageNet验证集、ObjectNet、VisDA-2017验证集和网络的60,535张图像。对于S2RDA-MS-39，真实域从MetaShift中收集了41,735张独特的39个类别的自然图像。本文还提供了使用ResNet-50作为骨干网络和使用各种DA方法（如DANN、MCD、RCA、SRDC和DisClusterDA）作为基线的域自适应实现细节。


# Paper:626     自定位基于点的Transformer用于点云理解



#### 1. Title: 
Self-positioning Point-based Transformer for Point Cloud Understanding

#### 2. Authors: 
Jinyoung Park, Sanghyeok Lee, Sihyeon Kim, Yunyang Xiong, Hyunwoo J. Kim

#### 3. Affiliation: 
Korea University (韩国高丽大学)

#### 4. Keywords: 
Point cloud, Transformer, self-attention, global cross-attention, shape classification, part segmentation, scene segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Park_Self-Positioning_Point-Based_Transformer_for_Point_Cloud_Understanding_CVPR_2021_paper.html  Github: https://github.com/mlvlab/SPoTr

#### 6. Summary : 
- (1):本文研究点云处理中的长程依赖问题，提出了一种新的Transformer架构，名为Self-Positioning point-based Transformer (SPoTr)。 
- (2):过去的方法包括将点云转换为规则结构，如多视图图像和体素化，以及在点空间上设计卷积核。这些方法在捕捉局部信息方面表现出色，但在捕捉全局信息方面表现不佳。本文提出的SPoTr结合了局部自注意力和自定位点的全局交叉注意力，以捕捉局部和全局形状上下文。自定位点是根据输入形状自适应定位的，通过分离的注意力考虑空间和语义信息，以提高表达能力。 
- (3):本文提出了一种全局交叉注意力机制，使用少量的自定位点来提高全局自注意力的可扩展性。SPoTr块由局部自注意力和全局交叉注意力组成，可以用于标准点云任务，如形状分类和语义分割。实验结果表明，SPoTr在三个点云任务上的表现优于其他基于注意力的方法。 
- (4):在三个数据集上的实验结果表明，SPoTr在形状分类、部分分割和场景分割任务上均取得了最佳性能。特别是，在ScanObjectNN数据集上，SPoTr相对于之前的最佳模型提高了2.6%的准确率。本文提出的自定位点的注意力机制具有良好的可解释性。
#### 1. 方法详细介绍：
本文提出了一种自定位基于点的Transformer（SPoTr）用于点云理解。SPoTr块结合了局部点注意力（LPA）和自定位基于点的注意力（SPA）以捕获局部和全局信息。通道点注意力（CWPA）用于计算每个通道中查询点和关键点之间的注意力权重。SPoTr块还可以在映射函数后扮演最大池化操作的角色，即集合抽象。所提出的SPoTr架构旨在用于点云任务，如形状分类和分割。

#### 2. 实验设置：
本文在三个点云任务上验证了所提出的SPoTr架构，包括形状分类、部分分割和场景分割。实验在三个数据集上进行：ScanObjectNN、SN-Part和S3DIS。

#### 3. 实验结果与分析：
实验结果表明，SPoTr在所有三个数据集上均优于其他基于注意力的方法。特别地，在形状分类中，所提出的模型在ScanObjectNN上实现了2.6％的准确率提升。定性分析证明了自定位点的可解释性。在复杂度分析方面，SPA显著降低了计算成本和内存使用量，相对于全局自注意力（GSA）机制。


# Paper:627     隐式身份泄漏：阻碍深度伪造检测泛化的绊脚石



#### 1. Title: 
Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization

#### 2. Authors: 
Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, Haoqiang Fan, Zheng Ge

#### 3. Affiliation: 
MEGVII Technology (旷视科技)

#### 4. Keywords: 
Deepfake detection, Implicit Identity Leakage, Binary classifiers, Artifact Detection Module, Generalization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Dong_Implicit_Identity_Leakage_The_Stumbling_Block_to_Improving_Deepfake_Detection_CVPR_2021_paper.html  Github: https://github.com/megvii-research/CADDM

#### 6. Summary : 
- (1):本文研究了深度伪造检测的泛化能力问题，发现二元分类器的泛化能力受到了意外学习到的图像身份表示的影响，称之为隐式身份泄漏现象。 
- (2):过去的方法通常使用二元分类器来处理深度伪造检测任务，这些方法在检测训练数据集中的已知攻击方面取得了很高的准确率，但在面对新提出的深度伪造方法时，这些方法的性能往往会显著下降。本文提出了一种名为ID-unaware Deepfake Detection Model的简单而有效的方法来减少隐式身份泄漏现象的影响。 
- (3):本文提出了一种基于锚点的检测器模块，称为Artifact Detection Module，以引导模型集中于图像的局部区域，从而减少全局身份信息的使用。通过定位伪造区域和分类多尺度锚点，模型学习在更细的层面上区分局部伪造区域和局部真实区域之间的差异，从而减少全局身份信息的误用。 
- (4):本文的方法在Celeb-DF、FF++和LFW等数据集上进行了广泛的实验，结果表明，与其他最先进的方法相比，ID-unaware Deepfake Detection Model在数据集内和数据集间的评估中均取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“ID-unaware Deepfake Detection Model”的方法，以减少隐式身份泄漏现象的影响。该方法包括两个主要组件：Artifact Detection Module和ID-unaware Classifier。Artifact Detection Module是一种基于锚点的检测器，用于定位图像上的伪造区域并为多尺度锚点分配二进制标签。ID-unaware Classifier是一个二进制分类器，仅关注局部伪造区域并忽略全局身份信息。该方法的训练和测试设置包括损失函数、训练和测试批量大小、学习率和优化器等。

具体步骤如下：
1. 提取特征：使用ResNet-18/34/50、Xception和Efficient-b3作为分类器的骨干网络，提取图像的特征。
2. 生成伪造图像：使用Multi-scale Facial Swap方法生成带有伪造区域位置标注的新伪造图像，以丰富训练集中的伪造特征。
3. 检测伪造区域：使用Artifact Detection Module检测伪造图像中的伪造区域，以减少隐式身份泄漏的影响。
4. 训练分类器：使用ID-unaware Classifier训练二进制分类器，仅关注局部伪造区域并忽略全局身份信息。
5. 计算损失：使用加权和的损失函数计算全局分类损失和检测损失之间的权衡。

#### 8. 实验设置：
本文在Celeb-DF、FF++和DFDC-V2三个数据集上进行了实验。使用ResNet-34和EfficientNet-b3/b4作为公共骨干网络。在训练阶段，批量大小设置为128，图像大小设置为224×224。使用常规数据增强方法来提高模型的泛化能力。学习率在初始化时设置为3.6×10−4，并在第10个和第20个epoch时分别降至1×10−4和5×10−5进行微调。使用Adam作为优化器。

#### 9. 实验结果和分析：
本文提出的方法在三个公共数据集上均取得了最先进的性能。该方法显著提高了在数据集内和跨数据集评估上的性能，显示了减少隐式身份泄漏对面部伪造的广义伪造特征的学习的有效性。该方法还展示了对于相同设置下的二进制分类器的跨方法泛化的优越性。鲁棒性评估表明，该方法相对于LipForensics在不同类型的图像扰动下具有更好的鲁棒性。


# Paper:628     软增强在图像分类中的应用



#### 1. Title: 
Soft Augmentation for Image Classification

#### 2. Authors: 
Yang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan

#### 3. Affiliation: 
Yang Liu: Argo AI
Shen Yan: Google
Laura Leal-Taixé: 德国慕尼黑工业大学
James Hays: Georgia Institute of Technology
Deva Ramanan: 卡内基梅隆大学

#### 4. Keywords: 
Data augmentation, Soft augmentation, Image classification, Robustness, Calibration

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Soft_Augmentation_for_Image_Classification_CVPR_2021_paper.html  Github: https://github.com/youngleox/soft_augmentation

#### 6. Summary : 
- (1):本文研究了数据增强在深度学习中的应用，提出了一种新的数据增强方法——软增强，旨在提高模型的鲁棒性和校准性。
 
- (2):传统的数据增强方法采用不变变换，即假设样本的标签对于变换是不变的。然而，人类视觉分类研究表明，随着变换程度的加深，人类视觉分类的置信度会非线性下降。因此，本文提出了软增强方法，即样本的标签会随着变换程度的加深而非线性软化。实验结果表明，软增强方法可以让模型更好地适应更加激进的数据增强策略，提高模型的鲁棒性和校准性。

- (3):本文提出的软增强方法可以让模型更好地适应更加激进的数据增强策略，提高模型的鲁棒性和校准性。具体来说，本文通过软化样本标签来实现软增强，提出了一种鲁棒的非线性软化公式。实验结果表明，软增强方法可以让模型更好地适应更加激进的数据增强策略，提高模型的鲁棒性和校准性。与现有的数据增强策略相结合，软增强方法可以将Cifar-10、Cifar-100、ImageNet-1K和ImageNet-V2的top-1准确率提高一倍，将模型遮挡性能提高多达4倍，并将期望校准误差(ECE)减少一半以上。此外，本文还表明，软增强方法可以推广到自监督分类任务中。

- (4):本文提出的软增强方法在Cifar-10、Cifar-100、ImageNet-1K和ImageNet-V2数据集上均取得了优异的性能表现，可以将top-1准确率提高一倍，将模型遮挡性能提高多达4倍，并将期望校准误差(ECE)减少一半以上。实验结果表明，软增强方法可以提高模型的鲁棒性和校准性，具有很好的应用前景。
#### 7. 方法详细介绍：
本文提出了一种新的数据增强方法——Soft Augmentation。该方法将固定的平滑因子替换为自适应平滑因子，该因子取决于应用于图像的具体采样增强的程度。Soft Augmentation根据裁剪图像的特定遮挡程度对学习目标进行非线性软化。该方法采用幂函数家族作为一种简单的单参数公式，可以测试线性和非线性软化。当图像完全被遮挡时，信息丢失是可见性的函数，只有当图像完全被遮挡时才会丢失所有信息。训练图像的原始标签具有100％的置信度，这表明标签的类别没有不确定性。

#### 8. 实验设置：
本文使用了Cifar-10、Cifar-100、ImageNet-1K和ImageNet-V2等数据集进行实验。实验中使用了ResNet-50、ResNet-101、EfficientNet-B0、EfficientNet-B4等模型进行测试。实验中使用了PyTorch框架进行实现，使用了SGD优化器和学习率衰减策略。实验中使用了Top-1准确率和期望校准误差等指标进行评估。

#### 9. 实验结果和分析：
Soft Augmentation方法在Cifar-10、Cifar-100、ImageNet-1K和ImageNet-V2等数据集上均取得了较好的效果，可以将Top-1准确率提高一倍以上。在遮挡图像上，Soft Augmentation可以将Top-1准确率提高4倍以上。与其他数据增强方法相比，Soft Augmentation在准确率和校准误差方面均取得了更好的效果。将Soft Augmentation与TrivialAugment相结合，可以同时提高模型的准确率和校准误差，超过了计算量较大的5-Ensemble模型。在自监督学习实验中，Soft Augmentation也表现出了很好的泛化性能。


# Paper:629     类自适应网络校准



#### 1. Title: 
Class Adaptive Network Calibration

#### 2. Authors: 
Bingyuan Liu, Jérôme Rony, Adrian Galdran, Jose Dolz, Ismail Ben Ayed

#### 3. Affiliation: 
Bingyuan Liu, Jérôme Rony, Jose Dolz, Ismail Ben Ayed: ÉTS Montreal, Canada

#### 4. Keywords: 
Deep Neural Networks, Calibration, Augmented Lagrangian, Label Smoothing, Class Adaptive

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Class_Adaptive_Network_Calibration_CVPR_2021_paper.html  Github: https://github.com/by-liu/CALS

#### 6. Summary : 
- (1):该论文研究了深度神经网络的校准问题，提出了一种新的方法来解决现有方法中存在的问题。 
- (2):现有的方法主要分为两类：一类是在训练后对输出进行后处理，另一类是在训练过程中引入额外的优化目标。然而，这些方法都存在一些问题，如无法处理不同类别之间的固有难度或不平衡性，或者需要手动调整超参数。本文提出了一种基于增广Lagrange算法的Class Adaptive Label Smoothing (CALS)方法，可以在训练过程中自适应地学习类别权重，从而解决了现有方法的问题。 
- (3):本文提出的CALS方法可以自适应地学习类别权重，从而可以处理不同类别之间的固有难度或不平衡性。为了解决这个问题，本文采用了增广Lagrange算法，并对其进行了修改，以适应大规模、自适应的训练。实验结果表明，CALS方法在多个基准测试中均取得了优异的性能。 
- (4):在多个任务上，CALS方法均取得了优异的性能，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种基于增广拉格朗日乘子（ALM）算法的类自适应网络校准（CANC）方法。该方法通过引入惩罚函数P来解决校准问题，并使用ALM算法自适应地学习惩罚权重。惩罚函数P需要满足一组公理，与P相关的惩罚参数和乘数在每次迭代中更新。该方法在验证集上学习类别惩罚乘数，以避免过拟合和训练特异性。具体步骤详见算法1和算法2。

#### 8. 实验设置：
本文在多个流行的基准测试集上进行了实验，包括Tiny-ImageNet、ImageNet、ImageNet-LT、PASCAL VOC2012和20 Newsgroups。校准的评估指标是期望校准误差（ECE），并且还报告了自适应ECE（AECE）和类别校准误差（CWCE）。对于分类和分割的区分性能，使用准确率和交并比（mIoU）。本文在附录B中提供了每个数据集和预处理设置的详细说明。

#### 9. 实验结果和分析：
本文将提出的CANC方法与其他基于学习的校准损失进行了比较，包括显式置信度惩罚（ECP）、标签平滑（LS）、焦点损失（FL）及其样本相关版本（FLSD）。实验结果表明，CANC在所有基准测试集上的ECE、AECE和CWCE指标上均优于比较方法。此外，CANC在区分性能方面实现了有竞争力甚至更好的结果。本文在第5节中提供了详细的实验结果和分析。


# Paper:630     MELTR：元损失变换器用于学习微调视频基础模型



#### 1. Title: 
MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models

#### 2. Authors: 
Dohwan Ko, Joonmyung Choi, Hyeong Kyu Choi, Kyoung-Woon On, Byungseok Roh, Hyunwoo J. Kim

#### 3. Affiliation: 
第一作者：韩国高丽大学计算机科学与工程系

#### 4. Keywords: 
Video foundation models, fine-tuning, auxiliary learning, meta-learning, loss functions

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ko_MELTR_Meta_Loss_Transformer_for_Learning_to_Fine-Tune_Video_Foundation_CVPR_2021_paper.html  Github: https://github.com/mlvlab/MELTR

#### 6. Summary : 
- (1):本文研究背景是基于大规模数据训练的基础模型在各个领域中表现出色，但是现有的研究主要集中在预训练阶段，对于下游任务的微调采用了单一任务特定损失的简单策略，而忽略了其他潜在有益的损失函数。
- (2):过去的方法主要采用单一任务特定损失的简单策略，而忽略了其他潜在有益的损失函数。本文提出了MEta Loss TRansformer (MELTR)框架，自动且非线性地将多种损失函数组合起来，通过辅助学习来帮助学习目标任务。本文的创新点在于提出了一种基于近似隐式微分的高效优化算法，解决了二级优化问题的计算成本问题。
- (3):本文提出了MEta Loss TRansformer (MELTR)框架，自动且非线性地将多种损失函数组合起来，通过辅助学习来帮助学习目标任务。本文的创新点在于提出了一种基于近似隐式微分的高效优化算法，解决了二级优化问题的计算成本问题。本文将MELTR应用于三个视频基础模型：UniVL、Violet和All-in-one，并在四个下游任务（文本到视频检索、视频问答、视频字幕生成和多模态情感分析）上展示了显著的性能提升。本文的贡献是提出了一种新的微调框架，提高了视频基础模型的性能。
- (4):本文将MELTR应用于三个视频基础模型：UniVL、Violet和All-in-one，并在四个下游任务（文本到视频检索、视频问答、视频字幕生成和多模态情感分析）上展示了显著的性能提升。实验结果表明，MELTR有效地将多个损失函数非线性地组合起来，为目标下游任务提供了有效的统一损失函数。
#### 7. 方法详细介绍：
本文提出了一种新的辅助学习框架，称为元损失变换器（MELTR）。该框架通过自适应地组合多个辅助损失函数来辅助微调目标下游任务。MELTR框架被构建为一个双层优化问题，并提出了一种基于近似隐式微分（AID）的高效训练过程。此外，引入了一个正则化项来缓解元过拟合并学习更有效的损失函数组合。MELTR框架通过使用基于Transformer的神经网络将多个任务的辅助损失组合成一个统一的辅助损失函数。输入嵌入进行自我关注，最终通过考虑损失规模和任务信息将其汇总为标量损失值。MELTR方法可以自动且非线性地组合各种损失函数，以通过辅助学习来帮助学习目标任务。该方法是基于元学习的，通过将主任务与多个辅助任务相结合来微调基础模型。

#### 8. 实验设置：
本文使用了多个视频基础模型（UniVL、Violet、All-in-one）进行微调，并在四个下游任务（文本到视频检索、视频问答、视频字幕生成和多模态情感分析）上进行了评估。使用了五个基准数据集：YouCook2、MSRVTT、TGIF-QA、MSVD-QA和CMU-MOSI。文本还提供了有关UniVL、Violet和All-in-one的实现细节和辅助损失函数的详细信息。

#### 9. 实验结果和分析：
实验结果表明，MELTR在各种任务上的表现优于所有基线方法，包括视频字幕生成和多模态情感分析。MELTR的自适应任务重新加权在跨基础模型体系结构组合辅助损失时被发现是有效的。MELTR + AID-FP-Lite优化算法被发现是最有效的双层优化方案，与多任务学习相比，仅引入4.9％的训练时间开销，同时提高了2.4％的性能。


# Paper:631     PA&DA: 联合采样路径和数据以实现一致的神经架构搜索



#### 1. Title: 
PA&DA: Jointly Sampling PAth and DAta for Consistent NAS

#### 2. Authors: 
Shun Lu, Yu Hu, Longxing Yang, Zihao Sun, Jilin Mei, Jianchao Tan, Chengru Song

#### 3. Affiliation: 
中国科学院计算技术研究所

#### 4. Keywords: 
Neural Architecture Search, One-Shot NAS, Gradient Variance, Importance Sampling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lu_PADA_Jointly_Sampling_PAth_and_DAta_for_Consistent_NAS_CVPR_2021_paper.html  Github: https://github.com/ShunLu91/PA-DA

#### 6. Summary : 
- (1):本文研究神经架构搜索（NAS）中的共享权重机制，该机制通过训练超级网络并继承预训练权重来评估子模型，从而大大降低了搜索成本。然而，共享权重在训练过程中存在不同的梯度下降方向，导致大的梯度方差和较差的排名一致性。
- (2):过去的方法主要集中在减少权重共享程度、手动设计更好的路径采样策略或引入额外的损失正则化等方面，但这些方法通常需要多次计算负担，效果不佳。本文从梯度方差减少的角度出发，通过优化路径和数据的采样分布来显式地减少超级网络训练的梯度方差。本文理论上推导了梯度方差与采样分布之间的关系，并揭示了最优采样概率与路径和训练数据的归一化梯度范数成正比。因此，本文使用归一化梯度范数作为路径和训练数据的重要性指标，并采用重要性采样策略进行超级网络训练。本文方法只需要微不足道的计算成本来优化路径和数据的采样分布，但在超级网络训练期间可以降低梯度方差并提高超级网络的泛化性能，从而实现更一致的NAS。
- (3):本文提出了一种新的方法，即联合采样路径和数据（PA&DA），通过优化路径和数据的采样分布来显式地减少超级网络训练的梯度方差。本文理论上推导了梯度方差与采样分布之间的关系，并揭示了最优采样概率与路径和训练数据的归一化梯度范数成正比。因此，本文使用归一化梯度范数作为路径和训练数据的重要性指标，并采用重要性采样策略进行超级网络训练。本文方法只需要微不足道的计算成本来优化路径和数据的采样分布，但在超级网络训练期间可以降低梯度方差并提高超级网络的泛化性能，从而实现更一致的NAS。
- (4):本文在NAS-Bench-201、DARTS和ProxylessNAS搜索空间上进行了全面的比较。结果表明，本文方法在超级网络排名性能和搜索到的架构的准确性方面优于其他改进方法，证明了本文方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为PA&DA的一致性神经架构搜索方法。该方法采用路径级架构采样策略和数据重要性采样策略，以减少超网训练期间的梯度方差。PA&DA方法包括两个阶段：超网训练和架构搜索。在超网训练阶段，PA&DA方法采用路径级架构采样策略来采样路径，采用数据重要性采样策略来采样数据。在架构搜索阶段，PA&DA方法使用随机搜索算法来搜索最佳架构。该方法通过优化路径和数据的采样分布来减少超网梯度方差，从而提高超网的收敛性和排名一致性。作者使用梯度范数的归一化值作为路径和训练数据的重要性指标，并采用重要性采样策略进行超网训练。该方法只需要极小的计算成本来优化路径和数据的采样分布，但在超网训练期间实现了更低的梯度方差和更好的泛化性能，从而实现了更一致的神经架构搜索。

#### 8. 实验设置：
本文在两种评估上进行了实验。第一个评估是基于NAS-Bench-201和CIFAR-10数据集，第二个评估是基于DARTS和ProxylessNAS搜索空间，使用CIFAR-10和ImageNet数据集。在NAS-Bench-201评估中，使用默认设置训练超网，并采用SGD优化器，初始学习率为0.05，动量为0.9，余弦衰减策略。在DARTS和ProxylessNAS评估中，使用SGD优化器，动量为0.9，权重衰减为3e-4，分别进行了50个和120个epoch的训练。

#### 9. 实验结果和分析：
在NAS-Bench-201评估中，PA&DA方法在与其他方法的比较中获得了最高的Kendall's Tau和Precision@Top5%，证明了该方法对于提高超网排名一致性是有效的和有益的。在DARTS搜索空间评估中，PA&DA方法获得了最高的平均测试精度，为97.52±0.07，超过了原始DARTS和其先进变体。在ProxylessNAS搜索空间评估中，PA&DA方法获得了最高的Top-1精度为97.66%，优于其他最先进的方法。


# Paper:632     HumanBench：基于投影辅助预训练的通用人体中心感知



#### 1. Title: 
HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining

#### 2. Authors: 
Shixiang Tang, Cheng Chen, Qingsong Xie, Meilin Chen, Yizhou Wang, Yuanzheng Ci, Lei Bai, Feng Zhu, Haiyang Yang, Li Yi, Rui Zhao, Wanli Ouyang

#### 3. Affiliation: 
第一作者：澳大利亚悉尼大学
其他作者：SenseTime Research, 上海人工智能实验室，浙江大学，清远研究院，上海交通大学

#### 4. Keywords: 
Human-centric perception, pretraining, benchmark, projector assisted pretraining, downstream tasks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Tang_HumanBench_Towards_General_Human-Centric_Perception_With_Projector_Assisted_Pretraining_CVPR_2022_paper.html  Github: https://github.com/OpenGVLab/HumanBench

#### 6. Summary : 
- (1):本文旨在提出一种通用的Human-centric预训练模型，以适应各种下游任务。Human-centric perception是计算机视觉和机器学习社区长期追求的目标，包括人员ReID、人体姿态估计、人体解析、行人属性识别、行人检测和人群计数等任务。然而，现有的人体中心研究和流程大多是针对特定任务的，为了更好的性能，需要花费大量的代价在表示/网络设计、预训练、参数调整和注释上。因此，本文提出了HumanBench，以全面评估不同预训练方法在6个不同下游任务的19个数据集上的泛化能力。同时，本文还提出了一种Projector AssisTed Hierarchical pretraining方法（PATH），以学习人体的粗粒度和细粒度知识。 

- (2):现有的通用视觉模型主要是在自然图像上进行预训练，而不是针对人体中心感知任务。此外，大多数模型使用对比学习或遮蔽重构等方法进行预训练，但不会学习人体的多尺度表示，这对于各种人体中心任务非常重要。本文通过从公开可用的数据集中进行监督预训练来解决这个问题。本文还提出了一种Projector AssisTed Hierarchical Pre-training（PATH）方法，以解决多任务预训练中的任务冲突问题。 

- (3):本文首先构建了一个基于现有数据集的HumanBench，以评估泛化到各种下游任务的人体中心表示。HumanBench包括多种图像，包括场景图像和人体中心图像，并涵盖了多种人体中心任务。为了解决各种人体中心数据集的输入图像和注释的多样性，本文提出了一种Projector AssisTed Hierarchical Pre-training（PATH）方法，该方法使用分层权重共享策略来解决多任务预训练中的任务冲突问题。 

- (4):本文在HumanBench上进行了全面的评估，结果表明，我们的PATH方法在15个数据集上取得了新的最优结果，在2个数据集上取得了与最优结果相当的结果，在2个数据集上取得了稍低的结果。这些结果表明，我们的方法在各种人体中心任务上具有很好的性能，并且可以有效地泛化到各种下游任务。
#### 7. 方法详细介绍：
本文提出了一种名为“Projector AssisTed Hierarchical pretraining”（PATH）的预训练方法，该方法采用分层权重共享的方式，通过共享骨干网络、任务特定的投影器和数据集特定的头部来学习不同粒度级别的多样化知识。在预训练阶段，任务特定的投影器被插入到数据集头部之前，但在评估模型在下游任务上的性能时被丢弃。在评估阶段，投影器被丢弃，仅使用骨干网络和数据集特定的头部。本文提出的方法在多个人类中心任务上取得了最先进的性能，包括行人检测、行人属性识别、人员重识别、人体解析和人群计数等。与其他流行的预训练模型MAE和CLIP相比，本文提出的方法在所有任务上的全微调评估协议下表现更好。

#### 8. 实验设置：
本文使用HumanBench数据集来评估所提出的方法。该数据集包含19个数据集的6个人类中心任务，全面量化了人类中心表示的泛化能力。HumanBench的评估场景包括数据集内评估、数据集外评估和未见过任务的评估。对于每个评估场景，本文提出了三种评估协议：全微调、头微调和部分微调。在预训练阶段，使用包含11,019,187张图像的37个数据集进行训练，其中选择了带有噪声标签的人员重识别数据集进行预训练。在评估阶段，根据评估场景和协议选择评估数据集。

#### 9. 实验结果与分析：
本文提出的方法在HumanBench数据集上取得了最先进的性能。在数据集内评估中，使用全微调的HumanBench在8个数据集上的性能优于其他最先进的方法，包括人体解析、人员重识别、姿态估计、行人属性识别和行人检测等。仅使用头微调的方法在12个数据集上的性能与其他最先进的方法相当甚至更好。部分微调方法在2个行人属性识别数据集上的性能优于全微调方法。实验还表明，该方法可扩展到更大的ViT-Large骨干网络。


# Paper:633     用于重建可控制化角色的结构化三维特征



#### 1. Title: 
Structured 3D Features for Reconstructing Controllable Avatars

#### 2. Authors: 
Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, Cristian Sminchisescu

#### 3. Affiliation: 
Enric Corona: UPC, Barcelona

#### 4. Keywords: 
3D Reconstruction, Monocular image, View synthesis, 3D editing, Implicit representation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是人体数字化在多个应用中的重要性，传统的3D虚拟化方法存在一些问题，如多视角成像和成本高昂等。
- (2):过去的方法包括多视角立体成像和基于模板的方法，但这些方法缺乏表现力，难以捕捉到人体的细节。最近的研究使用隐式表示和神经场等方法来提高虚拟化的真实性，但这些方法通常需要3D扫描，且不适用于动画等下游任务。本文提出了一种新的隐式表示方法，可以在不需要后处理的情况下生成可控制的虚拟人物。
- (3):本文提出了Structured 3D Features (S3F)模型，它是一种基于新型隐式3D表示的模型，可以将像素对齐的图像特征汇集到从参数化的统计人体网格表面采样的密集3D点上。这些3D点具有相关的语义信息，并可以在3D空间中自由移动。这使得可以更好地覆盖感兴趣的人物，包括身体形状以外的细节，如头发、配饰和宽松的衣服。基于此，我们提出了一个完整的3D transformer-based attention框架，它可以根据单个图像生成可动画的3D重建，包括反照率和光照分解，而无需后处理。我们的S3F模型在各种任务上均优于以前的最新技术，包括单目3D重建以及反照率和阴影估计。此外，我们展示了所提出的方法允许新视角合成、重新照明和重新定位重建，并且可以自然地扩展到处理多个输入图像（例如，不同视角的人物或不同姿势的相同视角的人物）。
- (4):本文的方法在单目图像中实现了可控制的虚拟人物的3D重建，包括反照率和光照分解，而无需后处理。该方法可以处理多个输入图像，支持新视角合成、重新照明和重新定位重建。实验结果表明，我们的方法在各种任务上均优于以前的最新技术，包括单目3D重建以及反照率和阴影估计。
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的3D人体重建方法，使用结构化3D特征（S3F）模块将查询点与3D特征相关联，以有效地覆盖输入图像中的相关特征。该方法使用可学习的有符号距离函数表示3D几何形状，并使用体积渲染积分估计特定图像像素的颜色。该方法的全损失是几个组件的线性组合，包括颜色、几何形状、Eikonal损失等。该方法还使用合成数据进行额外的监督。模型的训练采用混合监督方法，结合了合成数据和真实数据。合成数据包括35个RenderPeople的扫描数据和45个姿势数据，真实数据来自HITI数据集。模型的训练使用了颜色和占用损失函数。

#### 8. 实验设置：
本文使用了合成数据和真实数据进行模型的训练和测试。合成数据包括35个RenderPeople的扫描数据和45个姿势数据，真实数据来自HITI数据集。模型的测试集包括了具有不同姿势、光照和视角的3D扫描数据。

#### 9. 实验结果和分析：
本文的方法在单目3D人体重建方面表现优于现有的方法，评价指标包括Chamfer距离、IoU和Normalized Cut等。在单目图像中估计人体表面反照率和阴影方面，本文的方法也表现出色。本文的方法在单张图像和视频中的新视角渲染和重建方面也取得了竞争性的表现。此外，本文的方法还展示了3D服装编辑的潜力，并提供了3D人体重照和服装纹理转移的定性结果。


# Paper:634     LOGO：用于群体行动质量评估的长形视频数据集



#### 1. Title: 
LOGO: A Long-Form Video Dataset for Group Action Quality Assessment

#### 2. Authors: 
Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang

#### 3. Affiliation: 
第一作者：清华大学深圳研究生院

#### 4. Keywords: 
Action quality assessment, long-form video dataset, group information, artistic swimming, GOAT

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2022_paper.html  Github: https://github.com/shiyi-zh0408/LOGO

#### 6. Summary : 
- (1):本文研究的背景是行动质量评估（AQA），现有的方法和数据集大多集中在单人短序列场景，难以应用于更复杂的情况，因此需要构建一个新的多人长序列视频数据集。
 
- (2):现有的数据集大多数只包含单人表演，且视频长度较短，难以模拟多人长序列场景，因此需要构建一个更复杂的数据集。本文提出了一个新的多人长序列视频数据集LOGO，包含了更多的人数和更长的视频长度，同时提供了更丰富的注释信息。此外，本文提出了一个新的方法GOAT，用于模拟多人长序列场景中的群体信息和时间上下文关系，以及模型的评估方法。

- (3):本文提出了一个新的多人长序列视频数据集LOGO，包含了更多的人数和更长的视频长度，同时提供了更丰富的注释信息。此外，本文提出了一个新的方法GOAT，用于模拟多人长序列场景中的群体信息和时间上下文关系，以及模型的评估方法。

- (4):本文的方法在LOGO数据集上取得了最好的效果，证明了其在多人长序列场景中的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于组信息的行动质量评估方法，该方法包括两个组件：组感知图卷积网络和时间融合注意力。组感知GCN通过建立演员之间的图来提取组特征，并使用GCN对节点特征进行加权聚合来增强节点特征。时间融合注意力模块根据来自组感知GCN的表示融合沿时间轴的视频嵌入。该模块使用不同剪辑之间的组特征关系来为不同的时间特征分配不同的权重。

#### 8. 实验设置：
本文在LOGO数据集上进行了行动质量评估和行动分割实验。使用在Kinetics数据集上预训练的I3D模型提取每个视频剪辑的1024维特征向量。在所有实验中，将数据集分为3:1进行训练和测试。

#### 9. 实验结果和分析：
本文采用Spearman等级相关系数（ρ）和相对ℓ2距离（R-ℓ2）两个指标来评估AQA方法的性能。本文还采用三个广泛使用的评估指标，包括帧精度（Acc）、分段编辑距离和重叠阈值为10％、25％和50％的分段F1分数，分别表示为F1@{10, 25, 50}，以评估行动分割方法的性能。本文将实验结果与现有技术进行比较，并表明所提出的方法达到了最先进水平。本文还进行了消融研究，以验证方法的普适性。


# Paper:635     RILS: 基于语义空间的遮蔽视觉重建



#### 1. Title: 
RILS: Masked Visual Reconstruction in Language Semantic Space

#### 2. Authors: 
Shusheng Yang, Yixiao Ge, Kun Yi, Dian Li, Ying Shan, Xiaohu Qie, Xinggang Wang

#### 3. Affiliation: 
华中科技大学电子信息与通信学院

#### 4. Keywords: 
Visual pre-training, Masked image modeling, Language supervision, Semantic space, Transfer learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yang_RILS_Masked_Visual_Reconstruction_in_Language_Semantic_Space_CVPR_2022_paper.html  Github: https://github.com/hustvl/RILS

#### 6. Summary : 
- (1):本文研究的是可迁移的视觉预训练，旨在探索遮蔽图像建模（MIM）和自然语言监督之间的协同作用。 
- (2):过去的方法包括MIM和自然语言监督，但是它们之间的相互作用尚未得到充分探索。本文提出了一种新的预训练框架RILS，它在语义空间中执行遮蔽视觉重建，将文本编码器编码的句子表示作为原型，将仅具有视觉信号的图像转换为语义有意义的MIM重建目标。通过预测遮蔽标记的适当语义，视觉模型可以捕获具有结构化信息的有用组件。更好的视觉表示可以通过图像-文本对齐目标改进文本编码器，这对于有效的MIM目标转换至关重要。 
- (3):本文提出的方法是在语义空间中执行遮蔽视觉重建，将文本特征作为语义丰富的原型，将补丁特征映射到一批文本特征的概率分布作为重建目标。MIM目标被制定为软交叉熵损失，以最小化遮蔽视觉令牌的注入文本的概率分布与其相应目标之间的KL散度。视觉模型通过语言辅助重建目标进行优化，反过来通过捕获细粒度的局部上下文来改进ITC。 
- (4):在各种下游任务上，RILS展现出先进的可迁移性，特别是在低样本情况下。在ImageNet-1K基准测试中，RILS在零样本分类上获得了45.0％的准确率，比CLIP / SLIP高4.7％/ 3.4％。RILS还展示了在极低样本情况下的出色性能。
#### 7. 方法详细介绍：
本文提出了一种新的预训练框架RILS，它将遮蔽图像建模（MIM）和自然语言监督相结合。RILS的核心思想是在语义空间中执行遮蔽视觉重建。具体来说，作者将补丁特征映射到一批文本特征的概率分布上作为重建目标，这是通过图像-文本对比学习逐步对齐图像和文本空间实现的。MIM目标被制定为软交叉熵损失，以最小化遮蔽视觉令牌的注入文本的概率分布和其相应目标之间的KL散度。通过语言辅助重建目标优化的视觉模型，反过来通过捕捉细粒度的局部上下文来改进图像-文本对比学习的视觉表示。

#### 8. 实验设置：
本文在21个基准测试集上评估了零样本迁移分类，包括ImageNet-1K，使用COCO和Flickr30K基准测试集进行了图像-文本检索研究，并在5个分布偏移数据集上评估了模型的鲁棒性。作者进行了消融实验，比较了两阶段方法和不同重建空间的方法，证明了RILS方法在更简洁的训练流程和所有三个指标上都能与所有对手相媲美。

#### 9. 实验结果和分析：
本文在各种任务上报告了实验结果，包括分类转移、下游转移和标签有效转移。在大多数情况下，RILS方法优于其他方法，展示了其在细粒度视觉理解任务上的出色迁移能力。结果表明，RILS比在ImageNet-1K上预训练300个时期的MAE表现更好，只需要25%的训练长度。在低样本分类中，RILS比MAE+CLIP和CLIP都有更好的表现，仅使用每类10张图像即可实现51.8%的top-1准确率。在COCO上的低样本目标检测中，RILS在广泛的采样比下表现最佳。


# Paper:636     FFHQ-UV：用于3D人脸重建的标准化面部UV纹理数据集



#### 1. Title: 
FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction

#### 2. Authors: 
Haoran Bai, Di Kang, Haoxian Zhang, Jinshan Pan, Linchao Bao

#### 3. Affiliation: 
Haoran Bai: Nanjing University of Science and Technology (南京理工大学)
Others: Tencent AI Lab

#### 4. Keywords: 
Facial UV-texture dataset, 3D face reconstruction, GAN-based texture decoder, StyleGAN-based facial image editing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Bai_FFHQ-UV_Normalized_Facial_UV-Texture_Dataset_for_3D_Face_Reconstruction_CVPR_2021_paper.html  Github: https://github.com/csbhr/FFHQ-UV

#### 6. Summary : 
- (1):本文研究背景是单张或多张图像重建人脸的3D形状和纹理，其中纹理UV图的质量和多样性对于实现逼真的渲染至关重要。
- (2):过去的方法主要集中在提高形状估计的准确性，而对于纹理UV图的恢复只有少数研究，且现有的纹理UV图数据集质量和多样性有限。本文提出了一种基于StyleGAN的面部图像编辑方法和一个完全自动化的流程，从大规模的“in-the-wild”人脸图像数据集中提取高质量的纹理UV图，构建了一个大规模的公开数据集FFHQ-UV。同时，本文还提出了一种基于GAN的纹理解码器，用于参数拟合的3D人脸重建，实验结果表明，本文方法在重建精度和纹理质量方面均优于现有方法。
- (3):本文提出了一种基于StyleGAN的面部图像编辑方法和一个完全自动化的流程，从大规模的“in-the-wild”人脸图像数据集中提取高质量的纹理UV图，构建了一个大规模的公开数据集FFHQ-UV。同时，本文还提出了一种基于GAN的纹理解码器，用于参数拟合的3D人脸重建。本文的主要贡献包括：1）第一个大规模的公开的标准化人脸UV纹理数据集FFHQ-UV，包含超过50,000个高质量的、均匀照明的人脸纹理UV图，可直接用于逼真的数字人渲染；2）一个完全自动化和鲁棒的流程，用于从大规模的“in-the-wild”人脸图像数据集中提取高质量的纹理UV图；3）一种基于GAN的纹理解码器，用于参数拟合的3D人脸重建，能够显著提高重建的保真度和纹理质量。
- (4):本文方法在3D人脸重建任务上取得了优异的性能，实验结果表明，本文方法在重建精度和纹理质量方面均优于现有方法，支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种用于生成标准化人脸UV纹理数据集的流程，包括三个步骤：基于StyleGAN的人脸图像编辑、人脸UV纹理提取和纹理校正与补全。其中，基于StyleGAN的人脸图像编辑步骤使用StyleFlow和InterFaceGAN从单视角图像中生成多视角标准化人脸图像；人脸UV纹理提取步骤使用Deep3D模型从标准化人脸图像中回归3DMM系数和头部姿态参数，然后将输入图像投影到3D人脸模型上，得到人脸UV纹理；纹理校正与补全步骤使用Poisson编辑方法扩展和排除容易出错的区域，并使用模板纹理UV映射填充缺失区域，得到最终的FFHQ-UV数据集。

本文还提出了一种基于FFHQ-UV数据集训练的GAN纹理解码器，用于从单张图像进行3D人脸重建。该算法包括三个阶段：线性3DMM初始化、纹理潜码优化和联合参数优化。该GAN纹理解码器使用StyleGAN2架构在8个NVIDIA Tesla V100 GPU上进行训练。联合参数优化阶段放松了超球面约束，通过最小化损失函数来优化所有参数{z，pid，pexp，ppose，plight}。每张图像的总拟合时间约为60秒，测试使用NVIDIA Tesla V100 GPU。

#### 8. 实验设置：
本文使用了FFHQ-UV数据集进行实验，该数据集包含超过50,000个高质量的人脸纹理UV映射。实验使用了REALY基准测试集进行形状重建精度评估，并与MGCNet、Deep3D、3DDFA-v2和GANFIT等最先进的单图像重建方法进行比较。

#### 9. 实验结果与分析：
本文的方法在形状重建精度方面优于其他最先进的单图像重建方法，包括MGCNet、Deep3D、3DDFA-v2和GANFIT。本文还比较了使用线性3DMM初始化器、使用线性纹理基础参数的参数优化以及使用不生成多视角图像创建的UV映射数据集训练的纹理解码器产生的结果。使用Facescape中的纹理UV映射训练纹理解码器，但是从头开始训练使用Facescape的纹理UV映射数据集表现不佳。


# Paper:637     基于样本级多视角图聚类



#### 1. Title: 
Sample-level Multi-view Graph Clustering

#### 2. Authors: 
Yuze Tan, Yixi Liu, Shudong Huang*, Wentao Feng, Jiancheng Lv

#### 3. Affiliation: 
Shudong Huang: 四川大学计算机学院

#### 4. Keywords: 
Multi-view clustering, manifold learning, topological structure, sample-level graph fusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了多视角聚类的问题，提出了一种新的方法来处理多视角数据的异构性。 
- (2):现有的多视角聚类算法很少考虑数据的拓扑结构，这对于在流形上聚类数据是至关重要的。此外，现有方法不能充分探索不同视角之间的局部结构的一致性，因为它们是通过单视角的方式而不是多视角的方式来揭示聚类结构的。本文提出了一种新的方法，通过学习数据的拓扑结构来利用隐含的数据流形，并进一步探索多个视角之间的交集，以更好地维护跨视角的一致性。 
- (3):本文提出了一种新的方法，通过学习数据的拓扑结构来利用隐含的数据流形，并进一步探索多个视角之间的交集，以更好地维护跨视角的一致性。通过利用拓扑相关性学习的子任务和样本级图融合，我们的协作模型可以将每个子任务交替提升到最优解。 
- (4):在各种多视角数据集上的实验结果证明了所提出方法的有效性，并验证了其在其他SOTA方法上的优越性。
#### 7. 方法详细介绍：
本文提出了一种基于样本级多视图图聚类的方法。该方法通过迭代更新相似度矩阵、权重矩阵和聚类指示矩阵来实现。具体而言，相似度矩阵通过解决一个优化问题进行更新，该问题最小化相似度矩阵与聚类指示矩阵的加权和之间的差异。权重矩阵通过解决一个优化问题进行更新，该问题最小化权重矩阵与相似度矩阵之间的差异。聚类指示矩阵通过解决一个优化问题进行更新，该问题最小化聚类指示矩阵与相似度矩阵的加权和之间的差异。本文提出的算法详见算法2。

#### 8. 实验设置：
本文将提出的方法与多种多视图聚类方法进行比较，包括Co-train、Co-reg、DiMSC、AMGL、CSMSC、MVGL、WMSC、AWP、MCGC、LRMSC、MVCTM、SMVSC和FPMVS。实验在三个数据集（100leaves、HW和MSRC）上进行。

#### 9. 实验结果与分析：
本文在三个数据集上比较了提出的方法和其他方法的聚类结果。结果表明，本文提出的方法在所有三个数据集上的准确率、NMI、纯度和F-score均最高，表明其在多视图图聚类中的有效性。详细结果见表1-3。


# Paper:638     在存在噪声标签的情况下利用标注者一致性进行分类



#### 1. Title: 
Leveraging Inter-rater Agreement for Classification in the Presence of Noisy Labels

#### 2. Authors: 
Maria Sofia Bucarelli, Lucas Cassano, Federico Siciliano, Amin Mantrach, Fabrizio Silvestri

#### 3. Affiliation: 
Maria Sofia Bucarelli and Federico Siciliano are affiliated with Sapienza University of Rome, Italy. 

#### 4. Keywords: 
Supervised learning, noisy labels, inter-rater agreement, label noise distribution, generalization bounds.

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_Labels_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是在标签存在噪声的情况下，如何利用标注者之间的一致性来估计标签噪声分布，以及如何利用这个估计来学习嘈杂的数据集。
- (2):过去的方法通常需要精确知道标签的错误率，而这在实践中往往是未知的。本文提出了一种方法，利用标注者之间的一致性来估计标签噪声分布，从而避免了这个问题。同时，本文提出的方法可以保证性能，并提供了一些基于估计量的泛化界限。
- (3):本文提出了一种方法，利用标注者之间的一致性来估计标签噪声分布，并将这个估计用于学习嘈杂的数据集。同时，本文提供了一些基于估计量的泛化界限，这些界限依赖于已知的量。
- (4):本文的方法在多个数据集上进行了实验，结果表明，与其他方法相比，本文的方法可以更好地处理嘈杂的标签，并且可以提高分类性能。
#### 7. 方法详细介绍：
本文提出了一种在存在噪声标签的情况下进行分类的方法，通过利用标注者之间的一致性来估计噪声转移矩阵和标注者之间的一致性矩阵。该方法包括以下步骤：
1. 估计噪声转移矩阵T和其逆矩阵T^-1，其中T是一个对称、随机、对角线占优的矩阵，T^-1是T的逆矩阵。
2. 估计标注者之间的一致性矩阵D，其中D是一个对称、随机、对角线占优的矩阵。
3. 利用估计的T和D来计算每个样本的后验概率，作为软标签来训练分类器。
4. 利用估计的T来设计鲁棒损失函数，如前向和后向损失函数，来进一步利用噪声转移矩阵。
5. 基于估计的T和D来计算每个标注者的可靠性，并相应地加权他们的标签。
6. 基于估计的T和D建立分类器的泛化界限。

#### 8. 实验设置：
本文在合成数据集和CIFAR10-N数据集上进行了实验，验证了所提出方法在估计T和利用T训练分类器时的有效性。实验中使用分类交叉熵作为损失函数，比较了不同聚合方法（包括随机、多数投票和从估计的后验分布中获得的软标签）训练的分类器的性能。实验中还考虑了不同标注者数量和类别数量的情况。

#### 9. 实验结果和分析：
实验结果表明，所提出的方法可以有效地估计T和利用T训练分类器，相比于其他聚合方法，使用估计的后验分布获得的软标签可以显著提高分类器的性能。此外，实验还表明，所提出的方法可以在不同标注者数量和类别数量的情况下获得较好的性能。


# Paper:639     CoralStyleCLIP：用于图像编辑的联合优化区域和层选择



#### 1. Title: 
CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing

#### 2. Authors: 
Ambareesh Revanur, Debraj Basu, Shradha Agrawal, Dhwanit Agarwal, Deepak Pai

#### 3. Affiliation: 
Ambareesh Revanur: Adobe公司

#### 4. Keywords: 
Image editing, CLIP, StyleGAN, attention mechanism, multi-layer blending

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Revanur_CoralStyleCLIP_Co-Optimized_Region_and_Layer_Selection_for_Image_Editing_CVPR_2022_paper.html
Github: None

#### 6. Summary: 
- (1):本文研究的背景是控制生成式图像编辑的高保真度问题。
- (2):过去的方法包括手动选择StyleGAN的某一层进行编辑，但容易出现意外的编辑结果。本文提出了CoralStyleCLIP方法，通过多层特征融合和注意力机制，实现了对StyleGAN2的特定区域和层的联合学习，从而获得高保真度的编辑结果。
- (3):本文提出了一种新的多层融合策略，可以在适当的StyleGAN层次上选择特征，实现高保真度的编辑。同时，本文提出了一种基于分割的区域选择策略，可以显著降低学习复杂度。本文的创新点在于将这两种策略结合起来，实现了对StyleGAN2的特定区域和层的联合学习，从而获得高保真度的编辑结果。
- (4):本文在多个数据集上进行了实验，结果表明CoralStyleCLIP方法可以获得高保真度的编辑结果，且相对于现有的方法具有更好的性能。
#### 7. 方法详细介绍：
- 本文提出了一种名为CoralStyleCLIP的图像编辑方法，该方法通过联合优化区域和层选择来实现高保真度的文本驱动编辑。
- CoralStyleCLIP使用CLIP损失函数对生成的图像进行嵌入对齐，并使用L2损失函数来优化潜在向量，以便更好地匹配文本提示。
- 为了防止编辑过程中出现身份丢失，作者还引入了身份损失函数。
- 为了确保编辑区域的紧凑性，作者还引入了最小编辑区域约束。
- 为了确保编辑区域的平滑性，作者还引入了平滑损失函数。
- CoralStyleCLIP方法中的区域选择机制分为两种：基于分割的区域选择和基于卷积注意力网络的区域选择。
- CoralStyleCLIP方法中的层选择机制使用了一种名为CORAL的联合优化机制，该机制在StyleGAN2生成器模块的每一层学习一个软二进制掩码。
- 为了防止瓶颈问题，作者还引入了一种新颖的多层特征融合策略。
- CoralStyleCLIP方法中的潜在编辑分为两种类型：全局方向和潜在映射器。

#### 8. 实验设置：
- 本文的实验使用了一块NVIDIA Tesla P40 24 GB GPU，批量大小为3。
- CoralStyleCLIP方法中的区域选择和层选择机制是在StyleGAN2的基础上进行联合优化的，使用Adam优化器进行训练。
- 对于给定的文本提示，每个数据点由一个随机采样的标准正态向量z组成，最大迭代次数设置为20,000。
- 在推理过程中，编辑区域的自动选择受到限制，以确保只选择与编辑相关的区域。

#### 9. 实验结果和分析：
- 本文使用公开的名人图像数据集对CoralStyleCLIP方法进行了评估，并与其他基于CLIP的方法进行了比较。
- 实验结果表明，CoralStyleCLIP方法只影响与编辑相关的区域，编辑效果高度准确。
- CoralStyleCLIP方法中的区域选择和层选择机制能够自动选择最佳的编辑区域和层，无需手动干预。
- 本文还提出了一种新颖的多层特征融合策略，以解决手动选择适当层的问题。
- CoralStyleCLIP方法在人脸编辑方面表现出色，生成的1024×1024像素的图像质量高，编辑效果逼真。
- CoralStyleCLIP方法的平均clean-FID和训练时间在表格中给出。


# Paper:640     弱监督姿态挖掘用于细粒度分类



#### 1. Title: 
Weakly Supervised Posture Mining for Fine-grained Classification

#### 2. Authors: 
Zhenchao Tang, Hualin Yang, and Calvin Yu-Chian Chen

#### 3. Affiliation: 
Sun Yat-sen University (中山大学)

#### 4. Keywords: 
Fine-grained classification, weakly supervised learning, posture mining, graph neural network, reverse cross-entropy

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Tang_Weakly_Supervised_Posture_Mining_for_Fine-Grained_Classification_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是细粒度分类任务，由于不同子类别之间的视觉差异很小，因此这是一个具有挑战性的任务。传统方法通常需要使用bounding box标注，但这种标注需要大量的人工劳动力，而且容易出错。因此，本文提出了一种弱监督的姿态挖掘方法，可以自动地从图像中挖掘出物体的独特属性，从而减少了数据集的标注成本。

- (2):过去的方法通常需要使用bounding box标注，但这种标注需要大量的人工劳动力，而且容易出错。本文提出的方法可以自动地从图像中挖掘出物体的独特属性，从而减少了数据集的标注成本。此外，本文提出了一种新的训练策略，使得Deep Navigator和message passing模块可以相互通信和训练，从而使模型能够学习如何从图像中挖掘姿态信息。本文还提出了一种新的损失函数RCE，可以有效地学习样本的类间差异信息。

- (3):本文提出了一种名为PMRC的框架，可以方便地与不同的backbones结合使用。在PMRC中，我们使用Deep Navigator从图像中生成判别性区域，并使用它们构建图形。我们通过消息传递来聚合图形，并得到分类结果。为了强制PMRC学习如何挖掘姿态信息，我们设计了一种新的训练范式，使Deep Navigator和message passing模块可以相互通信和训练。此外，我们提出了RCE，并证明与CE相比，RCE不仅可以提高我们模型的准确性，还可以推广到其他类型的细粒度分类模型。

- (4):本文在常用的基准数据集上进行了实验，证明了PMRC可以达到最先进的水平。
#### 7. 方法详细介绍：
本文提出了一种名为PMRC（姿态挖掘和反向交叉熵）的方法，它是一种细粒度的框架，可以结合不同的骨干网络从图像中挖掘姿态信息。首先使用Deep Navigator生成判别性区域，然后使用这些区域构建对象的图结构。对图数据进行消息传递以融合节点之间的关联并提取节点的特征。最后，根据图结构计算所有节点的平均特征表达，并使用分类器对节点的平均特征进行分类。该方法可以在没有边界框/部分注释的情况下进行端到端训练。设计了一种新的学习策略，使模型学习如何从图像中挖掘姿态信息。使用反向交叉熵（RCE）作为损失函数，可以有效地学习样本之间的类间差异。

#### 8. 实验设置：
本文在四个数据集上进行了实验：CUB-200-2011、Stanford Cars、FGVC Aircraft和Stanford Dogs。使用Top-1准确率评估模型的性能。比较了消息传递网络和直接连接节点特征之间的图分类识别结果和推理速度。还比较了使用RCE和使用CE的识别结果。在消融实验中，将多个判别性区域的数量设置为{3,4,5,6,7,8}。

#### 9. 实验结果与分析：
本文在CUB-200-2011、Stanford Cars、FGVC Aircraft和Stanford Dogs四个数据集上进行了实验。提出的PMRC模型在CUB-200-2011上取得了最先进的性能，并且优于其他弱监督方法，如PIM和DCAL。PMRC还优于DATL和MetaFormer等强监督方法，证明了姿态信息对于判别特征学习的重要性。通过实验验证了所提出的RCE损失函数的有效性，它提高了四个数据集上各种算法的测试性能。


# Paper:641     针对领域泛化的锐度感知梯度匹配



#### 1. Title: 
Sharpness-Aware Gradient Matching for Domain Generalization

#### 2. Authors: 
Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, Lei Zhang

#### 3. Affiliation: 
香港理工大学

#### 4. Keywords: 
Domain generalization, Sharpness-Aware Minimization, Gradient Matching

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2021_paper.html  Github: https://github.com/Wang-pengfei/SAGM

#### 6. Summary : 
- (1):本文研究领域泛化问题，即如何将源域中学习到的模型推广到其他未知领域。 
- (2):过去的方法包括基于领域对齐、元学习和数据增强等，但这些方法存在着过拟合和收敛到尖锐局部最小值等问题。本文提出了一种新的方法，即Sharpness-Aware Gradient Matching (SAGM)，通过同时最小化经验风险、扰动损失和它们之间的差距来提高模型的泛化能力。 
- (3):SAGM算法通过隐式地对齐经验风险和扰动损失的梯度方向来实现这一目标。与Sharpness-Aware Minimization (SAM)相比，SAGM不会增加计算成本。实验结果表明，SAGM方法在五个领域泛化基准测试中均优于现有方法。 
- (4):在五个领域泛化基准测试中，SAGM方法均取得了优于现有方法的结果，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种名为“锐度感知梯度匹配”（Sharpness-Aware Gradient Matching，SAGM）的领域泛化方法。SAGM同时最小化经验风险损失、扰动损失和代理间隙三个目标。通过最小化经验风险损失和扰动损失，SAGM搜索具有低损失的区域，满足条件（i）。通过最小化代理间隙，SAGM避免了陡峭的山谷，满足条件（ii）。优化目标被转化为最小化经验风险损失、扰动损失和它们梯度之间的夹角，通过隐式地对经验风险和扰动损失的梯度方向进行对齐来实现。SAGM通过促使模型收敛到一个具有小损失值的平坦区域来提高模型的泛化性能。

#### 8. 实验设置：
本文在五个基准数据集（PACS、VLCS、OfficeHome、TerraIncognita和DomainNet）上进行了实验，使用了留一交叉验证方案来评估竞争方法的性能。默认使用在ImageNet上预训练的ResNet-50模型作为骨干网络。使用Adam优化器对模型进行优化。为了减少计算成本，对超参数进行了缩小的搜索空间。

#### 9. 实验结果和分析：
本文的实验结果表明，SAGM方法在五个基准数据集上均取得了优于其他方法的性能。在PACS数据集上，SAGM方法的性能比SAM和GSAM方法都要好。在DomainBed基准测试中，SAGM方法的平均性能提高了1.3%，证明了梯度匹配在设计的目标函数中的重要性。此外，还分析了训练模型的局部锐度，证明了SAGM方法不仅实现了优于SAM和GSAM方法的优异DG性能，而且收敛到了一个更平坦的损失函数区域。


# Paper:642     计算预算连续学习：什么是重要的？



#### 1. Title: 
Computationally Budgeted Continual Learning: What Does Matter?

#### 2. Authors: 
Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet Dokania, Philip H.S. Torr, Ser-Nam Lim, Bernard Ghanem, Adel Bibi

#### 3. Affiliation: 
第一作者：University of Oxford（牛津大学）

#### 4. Keywords: 
Continual Learning, Computational Budget, Memory Constraints, Benchmark, Image Classification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2021_paper.html  Github: https://github.com/drimpossible/BudgetCL

#### 6. Summary : 
- (1):本文研究的是连续学习（Continual Learning，CL）中的计算预算问题。现有的CL方法大多数都没有考虑计算预算的限制，而在实际应用中，系统主要受到计算和时间预算的限制，而不是存储。因此，本文提出了一个大规模基准测试，并分析了传统CL方法在计算受限制的情况下的性能。 
- (2):本文提出的计算预算连续学习方法与现有方法的不同之处在于，每个时间步长都有一个固定的计算预算，而现有方法没有对每个时间步长的计算预算做出硬性限制。本文的方法在计算预算受限制的情况下，能够在大规模图像分类任务中取得与现有方法相当的性能，而现有方法则无法胜任。 
- (3):本文提出了一种基于计算预算的连续学习方法，该方法在每个时间步长都有一个固定的计算预算。本文的方法通过对传统CL方法进行改进，包括采样策略、蒸馏损失和全连接层校正等方面，以适应计算预算受限制的情况。 
- (4):本文的方法在ImageNet2K和Continual Google Landmarks V2等大规模数据集上进行了实验，结果表明，在计算预算受限制的情况下，本文的方法能够取得与现有方法相当的性能，而现有方法则无法胜任。
#### 7. 方法详细介绍：
本文提出了一种新的基于经验重放的增量学习方法，称为Naive方法。该方法使用经验重放来平衡新数据和旧数据的训练，同时使用一个固定的计算预算来限制每个时间步的训练次数。此外，本文还评估了现有的增量学习方法，包括采样策略、蒸馏和FC层校正，并提出了一种新的蒸馏方法，称为“增量蒸馏”。增量蒸馏方法使用一个小的模型来蒸馏新数据，然后将其与旧模型进行蒸馏，以避免遗忘旧数据。最后，本文还提出了一种新的FC层校正方法，称为“增量FC层校正”，该方法使用一个小的FC层来校正新数据，然后将其与旧模型的FC层进行融合，以避免遗忘旧数据。

#### 8. 实验设置：
本文使用了两个大规模数据集，分别是ImageNet2K和Continual Google Landmarks V2 (CGLM)。对于ImageNet2K，数据流是通过随机打乱ImageNet21K的1K类图像集合构建的，对于CGLM，数据流是按照图像的时间戳顺序从CGLM数据集中获取的。实验设置包括三种流设置：数据增量流、时间增量流和类增量流。评估指标包括测试准确率和训练时间。实验使用了一个ResNet50模型作为骨干网络，并比较了不同方法在不同流设置下的性能。

#### 9. 实验结果和分析：
本文发现，现有的增量学习算法，包括采样策略、蒸馏和FC层校正，在计算预算有限的情况下表现不佳。Naive方法在各种计算预算和时间步数下都表现出色，甚至优于所有考虑的增量学习方法。大多数增量学习方法在每个时间步允许更低的计算预算时表现更差。本文还发现，部分训练一个强大的骨干网络可能比完全训练一个较弱的骨干网络更有益。最后，本文还分析了不同方法的优缺点，并提出了未来研究的方向。


# Paper:643     少样本Transformer的监督蒙版知识蒸馏



#### 1. Title: 
Supervised Masked Knowledge Distillation for Few-Shot Transformers

#### 2. Authors: 
Han Lin, Guangxing Han, Jiawei Ma, Shiyuan Huang, Xudong Lin, Shih-Fu Chang

#### 3. Affiliation: 
Columbia University（哥伦比亚大学）

#### 4. Keywords: 
Few-shot learning, Vision Transformers, Supervised Masked Knowledge Distillation, Self-supervised knowledge distillation, Masked image modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Supervised_Masked_Knowledge_Distillation_for_Few-Shot_Transformers_CVPR_2021_paper.html  Github: https://github.com/HL-hanlin/SMKD

#### 6. Summary : 
- (1):本文研究了在小数据集上的few-shot learning问题，提出了一种新的方法来解决Vision Transformers在few-shot learning中的过拟合问题。
- (2):过去的方法通过自监督辅助损失或监督学习来避免这个问题，但是自监督和监督学习之间的差距仍然存在。本文提出了一种新的监督蒙版知识蒸馏模型（SMKD），将标签信息引入到自蒸馏框架中，通过在类和补丁令牌上进行类内知识蒸馏，以及在类内图像之间重建蒙版补丁令牌的挑战性任务，来解决这个问题。
- (3):本文提出的SMKD模型通过将标签信息引入到自蒸馏框架中，填补了自监督知识蒸馏和传统监督学习之间的差距。在该框架下，我们设计了两个监督对比损失，一个在类级别上，一个在补丁级别上，并引入了在类内图像之间重建蒙版补丁令牌的挑战性任务。该模型在四个few-shot分类基准数据集上进行了实验，结果表明，我们的方法在简单设计的情况下，比以前的方法表现更好，并取得了新的SOTA。
- (4):本文的方法在四个few-shot分类基准数据集上进行了实验，结果表明，我们的方法在CIFAR-FS和FC100上取得了大幅度的提升，并在mini-ImageNet和tiered-ImageNet上取得了有竞争力的性能。
#### 7. 方法详细介绍：
本文提出了一种新的监督知识蒸馏方法，称为SMKD，用于few-shot transformers。该方法包括两个阶段的训练：使用MIM框架进行自监督预训练和使用提出的监督对比损失进行监督训练。训练损失由分类损失和补丁级别的知识蒸馏损失组成，其中缩放参数lambda控制两个组件的相对比例。模型使用原型分类方法和线性分类器在四个few-shot分类数据集上进行评估。SMKD模型的训练过程包括以下步骤：
1. 自监督预训练：使用MIM框架进行自监督预训练，生成两个增强视图。
2. 监督训练：使用提出的监督对比损失进行监督训练，包括分类损失和补丁级别的知识蒸馏损失。
3. 原型分类：使用原型分类方法和线性分类器进行few-shot分类。

#### 8. 实验设置：
本文在四个few-shot分类数据集上进行了实验：mini-ImageNet、tiered-ImageNet、CIFAR-FS和FC100。数据集被分为基类、few-shot验证和few-shot评估。模型在mini-ImageNet和tiered-ImageNet上进行了1200个epoch的自监督预训练，CIFAR-FS和FC100上进行了900个epoch的自监督预训练。模型使用8个Nvidia RTX 3090 GPU进行训练。

#### 9. 实验结果和分析：
本文提出的SMKD方法在四个few-shot分类数据集上均取得了最先进的性能，优于传统的卷积骨干和其他基于Transformer的模型。使用原型分类方法和线性分类器进行评估，特征使用[cls] token与加权平均[patch] token的连接。在不同的特征选择和缩放参数lambda的情况下进行了消融研究。实验结果表明，SMKD方法在小分辨率数据集（CIFAR-FS和FC100）上表现最佳，且在这两个数据集上的表现优于以前的结果。


# Paper:644     基于Transformer的特征缩减金字塔用于伪装目标检测



#### 1. Title: 
Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers

#### 2. Authors: 
Zhou Huang, Hang Dai, Tian-Zhu Xiang, Shuo Wang, Huai-Xin Chen, Jie Qin, Huan Xiong

#### 3. Affiliation: 
Zhou Huang: Sichuan Changhong Electric Co., Ltd. (四川长虹电器股份有限公司)
Hang Dai: University of Glasgow
Tian-Zhu Xiang: G42
Shuo Wang: ETH Zurich
Huai-Xin Chen: UESTC
Jie Qin: CCST, NUAA
Huan Xiong: MBZUAI

#### 4. Keywords: 
Camouflaged object detection, vision transformers, feature shrinkage pyramid, non-local token enhancement module, adjacent interaction module

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Feature_Shrinkage_Pyramid_for_Camouflaged_Object_Detection_With_Transformers_CVPR_2021_paper.html  Github: https://github.com/ZhouHuang23/FSPNet

#### 6. Summary : 
- (1):本文研究的是伪装目标检测，伪装目标由于与背景相似，难以被准确检测。传统的卷积神经网络（CNN）方法由于感受野有限，难以捕捉到远距离的依赖关系，因此在伪装目标检测中表现不佳。近年来，视觉transformers（ViT）因其自注意力机制，能够有效地捕捉长距离依赖关系，因此在伪装目标检测中表现出色。
- (2):过去的transformer-based方法在局部特征建模和特征聚合方面存在问题。本文提出了一种新的transformer-based Feature Shrinkage Pyramid Network（FSPNet），通过逐步缩小的方式，旨在通过逐层缩小来逐层解码局部增强的相邻transformer特征，以便于探索来自难以区分的背景中的微妙线索的伪装目标检测。具体来说，本文提出了一个非局部令牌增强模块（NL-TEM），它利用非局部机制来交互相邻令牌并探索令牌内的基于图形的高阶关系以增强局部表示。此外，本文设计了一个特征缩减解码器（FSD），其中包含相邻交互模块（AIM），通过逐层缩小的方式，逐层聚合相邻的transformer特征，以尽可能地积累微小但有效的细节和语义，以便于对象信息解码。
- (3):本文提出的FSPNet模型在局部特征建模和特征聚合方面进行了改进，通过逐层缩小的方式，旨在通过逐层缩小来逐层解码局部增强的相邻transformer特征，以便于探索来自难以区分的背景中的微妙线索的伪装目标检测。本文提出了一个非局部令牌增强模块（NL-TEM）和一个特征缩减解码器（FSD），其中包含相邻交互模块（AIM），以逐层缩小的方式聚合相邻的transformer特征，以尽可能地积累微小但有效的细节和语义，以便于对象信息解码。实验结果表明，本文提出的模型在三个具有挑战性的COD基准数据集上显著优于现有的24个竞争对手。
- (4
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的新型特征收缩金字塔网络（FSPNet）用于伪装目标检测。该方法由视觉Transformer编码器、非局部令牌增强模块（NL-TEM）和具有相邻交互模块（AIM）的特征收缩解码器（FSD）组成。输入图像首先被序列化为令牌，并输入到Transformer编码器中，使用自我注意机制来建模全局上下文。然后，NL-TEM被设计用于在令牌之间和内部执行特征交互和探索，以增强局部特征表示。最后，FSD被设计为使用分层收缩金字塔架构逐步聚合相邻特征，以累积更多的不可感知的有效线索。AIM用作解码器中相邻特征融合和信息传递的桥梁。最后一个AIM的输出特征通过sigmoid和上采样操作进行监督，以进行伪装目标预测。

#### 8. 实验设置：
本文在三个广泛使用的伪装目标检测数据集（CAMO、COD10K和NC4K）上评估了所提出的方法。使用PyTorch实现，采用DeiT策略预训练的ViT基础版本作为Transformer编码器，其余模块随机初始化。采用随机翻转增强训练数据。所有输入图像都被调整为384×384。使用Adam作为优化器，学习率初始化为1e-4，然后每50个epoch缩小10倍。使用批量大小为2的完整训练过程需要在具有8个NVIDIA Tesla V100 GPU的工作站上花费约8小时。

#### 9. 实验结果与分析：
所提出的FSPNet方法在Sα、Fwβ、Fmβ、Emϕ、Exϕ和M等六个评估指标上，相对于VST和UTGR，均表现出显著的性能提升。在几个典型场景中，与一些代表性竞争对手的视觉比较显示出更准确和完整的预测的优越性能。消融研究验证了COD的提出模块的有效性，包括密集集成策略、特征收缩解码器和非局部令牌增强模块。所提出的模型显著提高了伪装目标检测的性能。


# Paper:645     基于神经场的全景提升方法用于三维场景理解



#### 1. Title: 
Panoptic Lifting for 3D Scene Understanding with Neural Fields

#### 2. Authors: 
Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul`o, Norman M¨uller, Matthias Nießner, Angela Dai, Peter Kontschieder

#### 3. Affiliation: 
Technical University of Munich（慕尼黑工业大学）, Meta Reality Labs Zurich

#### 4. Keywords: 
Panoptic 3D scene understanding, Neural Radiance Fields, 2D panoptic segmentation masks, 3D panoptic radiance field, machine-generated labels

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2022_paper.html  Github: https://github.com/nihalsid/panoptic-lifting

#### 6. Summary : 
- (1):本文研究的是从野外场景的图像中学习全景三维体积表示的新方法，旨在实现全景三维场景的理解和合理的渲染。 
- (2):现有的方法要么直接或间接使用三维输入，要么需要手动标注三维数据，这些方法都存在一定的局限性。本文提出了一种基于神经场表示的全景提升方案，可以将机器生成的二维全景分割掩模提升到一致的三维全景辐射场。本文的方法不需要手动标注三维数据，只需要使用预训练的二维全景分割模型。 
- (3):本文的核心贡献是提出了一种基于神经场表示的全景提升方案，可以将机器生成的二维全景分割掩模提升到一致的三维全景辐射场。本文的方法可以生成一致的三维全景场景表示，包括颜色、深度、语义和实例信息。本文的方法可以渲染出新视角下的图像和相应的全景分割掩模。 
- (4):本文的方法在 Hypersim、Replica 和 ScanNet 数据集上进行了实验，相比现有的最先进方法，本文的方法在场景级 PQ 上分别提高了 8.4、13.8 和 10.6%。本文的方法可以生成一致的三维全景场景表示，可以支持虚拟现实、机器人导航和自动驾驶等应用。
#### 7. 方法详细介绍：
本文提出了一种名为Panoptic Lifting的方法，用于从2D图像中学习3D场景的panoptic表示。该方法使用神经场来建模3D场景，并预测新视角下的panoptic分割掩模。该方法包括以下步骤：
1. 使用Mask2Former对2D图像进行分割。
2. 使用神经场将2D panoptic标签提升到3D。
3. 训练神经场以预测3D场景中每个体素的占用概率。
4. 使用占用概率生成新视角下的panoptic分割掩模。
5. 采用多种设计选择来提高鲁棒性，如分割一致性损失、测试时间增强、有界分割场和阻止语义到几何梯度等。

#### 8. 实验设置：
本文在三个数据集上进行了实验：Hypersim、Replica和ScanNet。使用相同的图像、姿态和Mask2Former生成的2D标签对所有数据集进行训练。对于PNF，提供了一个在ScanNet上预训练的最先进的多视角3D物体检测器的边界框，或从地面实况中获取。使用mIoU、PQscene和PSNR指标对该方法与最先进的2D和NeRF-based 3D语义和panoptic分割方法进行比较。

#### 9. 实验结果和分析：
本文提出的方法在所有数据集上的语义和panoptic分割任务中均显著优于最先进的方法。该方法对机器生成的标签中存在的噪声更加鲁棒，从而为新视角提供了干净、连贯和3D一致的panoptic分割掩模。该方法还可以进行场景编辑，例如对象实例删除、复制和仿射变换下的操作。消融研究表明，分割一致性损失、测试时间增强、有界分割场和阻止语义到几何梯度等都有助于提高所提出方法的鲁棒性。


# Paper:646     SMAE：饱和度感知掩膜自编码器用于HDR去鬼影的少样本学习



#### 1. Title: 
SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked Autoencoders

#### 2. Authors: 
Qingsen Yan, Song Zhang, Weiye Chen, Hao Tang, Yu Zhu, Jinqiu Sun, Luc Van Gool, Yanning Zhang

#### 3. Affiliation: 
Qingsen Yan, Yu Zhu, Jinqiu Sun, and Yanning Zhang are affiliated with Northwestern Polytechnical University.

#### 4. Keywords: 
HDR imaging, deghosting, few-shot learning, deep neural networks, semi-supervised learning

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Yan_SMAE_Few-Shot_Learning_for_HDR_Deghosting_With_Saturation-Aware_Masked_Autoencoders_CVPR_2021_paper.html
Github: None

#### 6. Summary: 
- (1): This paper focuses on generating high-quality High Dynamic Range (HDR) images from dynamic scenes using few-shot learning, which aims to generate satisfactory images with limited data. 
- (2): Previous methods for HDR deghosting have relied on alignment-based, patch-based, rejection-based, or CNN-based approaches, which require a large amount of labeled data and may suffer from overfitting or ghosting artifacts. The proposed approach, called SSHDR, first addresses the saturation problem with a self-supervised mechanism and then tackles the ghosting problem via an iterative semi-supervised learning framework. This approach outperforms state-of-the-art methods quantitatively and qualitatively within and across different datasets, achieving appealing HDR visualization with few labeled samples.
- (3): The proposed approach consists of two stages: self-supervised learning network for content completion and sample-quality-based iterative semi-supervised learning for deghosting. In the first stage, a Saturated Mask AutoEncoder (SMAE) is pre-trained to learn the representation of HDR features and generate content of saturated regions by self-supervised learning. In the second stage, a sample-quality-based iterative semi-supervised learning framework is proposed to address the ghosting artifacts. An adaptive pseudo-labels selection strategy is developed to pick high-quality HDR pseudo-labels to avoid awful pseudo-labels hampering the optimization process. 
- (4): The proposed approach achieves state-of-the-art performance on individual and cross-public datasets, generating high-quality HDR images with few labeled samples. The performance supports their goals of addressing the challenges of HDR deghosting with limited data.
#### 7. 方法详细介绍：
本文提出的方法名为SSHDR，是一种半监督的HDR去鬼影方法。该方法包括两个阶段：自监督学习网络用于内容补全和基于样本质量的迭代半监督学习用于去鬼影。第一阶段使用基于自监督学习的多尺度Transformer模型和饱和掩膜自编码器来恢复饱和区域。第二阶段提出了一种基于样本质量的半监督训练方法，用于选择曝光良好且无鬼影的HDR伪标签，从而提高去鬼影的效果。迭代训练过程由少量有标签样本引导，并在每个epoch中增强训练样本的多样性。具体步骤包括：
1. 预训练饱和掩膜自编码器(SMAE)来学习HDR特征的表示，通过自监督学习生成饱和区域的内容。
2. 提出一种基于样本质量的半监督训练方法，用于选择高质量的HDR伪标签，避免低质量伪标签影响优化过程。
3. 通过少量有标签样本引导迭代训练，增强每个epoch中的训练样本多样性。

#### 8. 实验设置：
本文使用PyTorch框架在2个NVIDIA GeForce 3090 GPU上实现了提出的方法。实验使用了Kalantari和Hu两个公共数据集，并使用PSNR-L、PSRN-µ、SSIM-L、SSIM-µ和HDR-VDP-2等5个常见指标进行评估。具体实验设置包括MSRSTM中的窗口大小、补丁大小、批量大小、学习率、优化器和训练epoch等。

#### 9. 实验结果和分析：
本文提出的方法在两个数据集上均取得了最先进的性能，而大多数其他方法只有少量有标签样本时表现不佳。在Kalantari数据集的5way-5shot设置中，本文方法在PSNR-l和PSNR-µ方面分别比第二好的方法提高了0.15db和0.21db，在Hu数据集的5way-5shot设置中也分别提高了0.28db和0.26db。在5way-1shot设置中，本文方法在两个数据集上均表现出色。在零样本设置下，本文方法优于其他方法。此外，本文方法在大多数指标上优于一些5-shot和完全监督方法。最后，本文的少样本和完全监督方法在两个数据集上均取得了最先进的性能。本文还在Kalantari、Hu、Tursun和Prabhakar数据集上与其他方法进行了比较，验证了其泛化性能。本文方法在PSNR-l和PSNR-µ方面的数值表现更好，表明其在不同数据集上具有良好的泛化性能。


# Paper:647     基于特征交换和激活区域约束的无数据知识蒸馏



#### 1. Title: 
Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint

#### 2. Authors: 
Shikang Yu, Jiachen Chen, Hu Han, Shuqiang Jiang

#### 3. Affiliation: 
中国科学院计算技术研究所

#### 4. Keywords: 
Data-free knowledge distillation, generative network, synthetic data, channel-wise feature exchange, multi-scale spatial activation region consistency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Data-Free_Knowledge_Distillation_via_Feature_Exchange_and_Activation_Region_Constraint_CVPR_2021_paper.html  Github: https://github.com/skgyu/SpaceshipNet

#### 6. Summary : 
- (1):本文研究的是数据无关知识蒸馏（DFKD）的问题，旨在通过生成合成数据来训练轻量级的学生模型，而不是使用原始的训练数据。这是因为在实际应用中，访问原始训练数据可能会受到各种限制，如隐私问题和版权问题等。

- (2):传统的知识蒸馏方法通常假设学生网络可以访问教师网络使用的整个或部分训练集。但在实际应用中，这种假设可能不成立。DFKD方法通过生成合成数据来解决这个问题，但是合成数据可能会导致学生网络学习偏差。本文提出了一种新的DFKD方法，利用通道级特征交换（CFE）和多尺度空间激活区域一致性（mSARC）约束来提高知识传递的效率和多样性。与以往的生成网络方法不同，本文的方法使用早期合成图像的特征进行CFE，从而提高了合成训练数据的多样性。同时，本文提出了mSARC约束，使学生网络能够模仿教师网络的空间激活区域，从而有效地克服了合成图像中不必要噪声对蒸馏学习的影响。

- (3):本文提出的DFKD方法通过CFE和mSARC约束来提高知识传递的效率和多样性。CFE允许生成网络更好地从特征空间中采样，并有效地合成多样化的图像以学习学生网络。然而，仅使用CFE可能会严重放大合成图像中的不必要噪声，从而可能导致蒸馏学习失败甚至产生负面影响。因此，本文提出了mSARC约束，以确保学生网络不仅可以模仿教师网络的逻辑输出，还可以模仿教师网络的空间激活区域，以减轻多样化合成图像中不必要噪声对蒸馏学习的影响。

- (4):本文在CIFAR-10、CIFAR-100、Tiny-ImageNet、Imagenette和ImageNet100等数据集上进行了广泛的实验，证明了本文方法在不同的骨干网络上都能很好地工作，并且优于现有的DFKD方法。本文的方法能够生成高分辨率的合成图像，从而在ImageNet子集上实现了很好的性能。本文的实验结果表明，本文方法能够有效地提高知识传递的效率和多样性，同时保持较高的性能。
#### 7. 方法详细介绍：
本文提出了一种基于特征交换和激活区域一致性约束的无数据知识蒸馏方法，称为Spaceship-Net。该方法包括三个主要部分：使用教师网络优化生成网络、使用通道特征交换训练图像生成和使用多尺度空间激活区域一致性约束进行知识蒸馏。生成网络G使用交叉熵损失和BN正则化进行优化，CFE用于在特征池中随机交换一半的特征通道以生成多样化的合成图像，mSARC约束用于鼓励不同阶段的隐藏层学习一致的视觉线索以进行分类。通过最小化学生和教师网络之间的logits的KL散度来实现知识蒸馏。 

#### 8. 实验设置：
本文在三个分类数据集上进行了评估：CIFAR-10、CIFAR-100和Tiny-ImageNet。使用ResNet、VGG和Wide ResNet等不同的骨干网络进行评估。CIFAR-10和CIFAR-100分别包含10个和100个类别，图像分辨率为32×32。Tiny-ImageNet包含10万个训练图像和1万个验证图像，分辨率为64×64，共有200个图像类别。

#### 9. 实验结果和分析：
本文提出的Spaceship-Net方法与DAFL、ZSKT、ADI、DFQ、LS-GDFD和CMI等多种最先进的方法进行了比较。结果表明，该方法在CIFAR-10、CIFAR-100和Tiny-ImageNet数据集上均优于这些方法。在ResNet-18的CIFAR-10测试集上，准确率甚至超过了使用标记数据进行简单监督学习的ResNet-18。该方法还在高分辨率数据集Imagenette和ImageNet100上进行了评估，并与现有方法取得了竞争性能。消融实验表明，特征交换和激活区域约束对该方法的性能都很重要。


# Paper:648     一种用于从野外图像中准确详细地重建人脸的分层表示网络



#### 1. Title: 
A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images

#### 2. Authors: 
Biwen Lei, Jianqiang Ren, Mengyang Feng, Miaomiao Cui, Xuansong Xie

#### 3. Affiliation: 
阿里巴巴达摩院

#### 4. Keywords: 
Face reconstruction, 3D morphable model, hierarchical representation, detail priors, multi-view reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lei_A_Hierarchical_Representation_Network_for_Accurate_and_Detailed_Face_Reconstruction_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是高保真3D人脸重建，该问题在AR/VR、医疗、电影制作等领域有广泛应用，但从单张或少量视角的图像中重建高精度的人脸模型仍然是一个具有挑战性的问题。

- (2):过去的方法主要基于3D morphable model (3DMM)进行重建，但由于3DMM的低维表示能力，这些方法无法恢复细节丰富的面部几何形状。一些方法尝试通过引入细节贴图或非线性操作来解决这个问题，但结果仍然不够逼真。本文提出了一种新的分层表示网络，通过几何解缠和分层表示来实现详细的面部建模，同时引入面部细节的3D先验知识来提高重建结果的准确性和真实性。本文的方法在两个单视角和两个多视角的人脸重建基准测试中均优于现有方法。

- (3):本文提出了一种分层表示网络（HRN）来实现从单张图像中准确和详细地重建人脸。具体来说，我们实现了几何解缠，并引入了分层表示来实现详细的面部建模。同时，我们还将面部细节的3D先验知识纳入到网络中，以提高重建结果的准确性和真实性。我们还提出了一个去除修饰的模块来实现几何和外观的更好解耦。我们的框架可以通过考虑不同视角的细节一致性来扩展到多视角模式。我们的方法在两个单视角和两个多视角的人脸重建基准测试中均优于现有方法。

- (4):本文的方法在两个单视角和两个多视角的人脸重建基准测试中均优于现有方法，表现出优秀的细节捕捉和准确的形状建模能力。本文提出的高保真3D人脸数据集FaceHD-100可以促进稀疏视角和高保真3D人脸重建的研究。
#### 7. 方法详细介绍：
本文提出了一种新颖的分层表示网络（HRN），用于从单张图像中实现准确和详细的人脸重建。HRN实现了几何解耦，并引入了分层表示来实现详细的人脸建模。人脸几何被分解为低频几何、中频（MF）细节和高频（HF）细节。这些部分以分别以面部混合形状系数、顶点形变图和像素位移图进行建模。采用两个图像转换网络来估计相应的细节图（形变图和位移图），并进一步采用它们以粗到细的方式生成详细的人脸模型。MF和HF细节的3D先验知识被纳入到分层表示中，以促进准确和忠实的建模。提出了一个去除修饰模块，以自适应地细化基础纹理，以克服皮肤瑕疵和照明之间的歧义。该框架可以通过考虑不同视角的细节一致性来扩展到多视角模式。

具体步骤如下：
1. 采用3D可塑模型（3DMM）表示人脸形状和纹理。
2. HRN由三个模块组成：人脸分析器、去修饰模块和分层表示模块。
3. 人脸分析器从输入图像中预测3DMM系数、位置图和纹理图。
4. 去修饰模块从输入图像中去除修饰效果，以获得更准确的反照率图。
5. 分层表示模块以分层方式对人脸形状和纹理进行建模，包括低频身份部分、中频细节部分和高频细节部分。
6. 采用两个pix2pix网络进行训练，以依次合成形变图和位移图。
7. 损失函数包括重建损失、细节损失和轮廓感知损失。
8. 采用自监督方式进行训练，使用来自面部扫描的3D细节先验知识进行指导。

#### 8. 实验设置：
本文在三个公共数据集（FaceScape、REALY和ESRC）上进行了评估。训练数据由两部分组成：2D野外图像和具有相应多视图图像的3D面部扫描。训练过程采用Adam优化器，批量大小为4，初始学习率为1e-4，迭代次数为800K。模型交替使用野外图像和实验室图像进行训练。

#### 9. 实验结果和分析：
本文提出的方法在单视图和多视图重建任务上均优于几种最先进的方法，实现了最低的重建误差。采用Chamfer距离（CD）、平均法线误差（MNE）和归一化均方误差（NMSE）在不同数据集上进行定量评估。本文还介绍了一个高质量的3D人脸数据集FaceHD-100，其中包括来自100个受试者的2,000个高清3D网格和相应的多视图图像。该数据集展示了重建几何和纹理的高质量。本文还进行了消融实验，验证了所提出的分层表示网络的有效性。


# Paper:649     具有自兼容性的可切换表示学习框架



#### 1. Title: 
Switchable Representation Learning Framework with Self-compatibility

#### 2. Authors: 
Shengsen Wu, Yan Bai, Yihang Lou, Xiongkun Linghu, Jianzhong He, Ling-Yu Duan

#### 3. Affiliation: 
第一作者：Peng Cheng实验室，中国

#### 4. Keywords: 
Visual search, compatible learning, switchable representation learning, uncertainty estimation, gradient projection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Switchable_Representation_Learning_Framework_With_Self-Compatibility_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了多平台上的视觉搜索系统，由于不同平台的计算和存储资源不同，部署适合最小限制平台的统一模型会导致资源浪费和有限的准确性。因此，需要部署适应资源限制的不同容量的模型，这需要这些模型提取的特征在度量空间中对齐。为了实现特征对齐，提出了“兼容性学习”方法。现有的研究主要集中在一对一兼容性范式上，这在学习多个模型之间的兼容性方面受到限制。本文提出了一种具有自兼容性的可切换表示学习框架（SFSC），通过一次训练过程生成一系列具有不同容量的兼容子模型。 

- (2):现有的兼容性学习方法主要集中在一对一兼容性范式上，这在学习多个模型之间的兼容性方面受到限制。本文提出了一种具有自兼容性的可切换表示学习框架（SFSC），通过一次训练过程生成一系列具有不同容量的兼容子模型。在优化子模型时，子模型之间存在梯度冲突，本文从梯度大小和方向的角度缓解了这个问题。通过不确定性估计动态调整子模型的优先级，以适当地协同优化子模型。此外，将具有冲突方向的梯度投影以避免相互干扰。 

- (3):本文提出了一种具有自兼容性的可切换表示学习框架（SFSC），通过一次训练过程生成一系列具有不同容量的兼容子模型。在优化子模型时，子模型之间存在梯度冲突，本文从梯度大小和方向的角度缓解了这个问题。通过不确定性估计动态调整子模型的优先级，以适当地协同优化子模型。此外，将具有冲突方向的梯度投影以避免相互干扰。 

- (4):在三个不同的数据集上，与部署统一模型相比，采用SFSC获得不同子模型可以实现6％至8％的性能提升。SFSC在评估的基准数据集上实现了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“Switchable Representation Learning Framework with Self-compatibility”（SFSC）的方法，用于构建可切换的神经网络。该方法包括三个主要组件：可切换的神经网络用于构建子模型，基于梯度投影的聚合方法以解决梯度冲突问题，基于不确定性估计的兼容性损失以适应性地调整优化优先级。可切换的神经网络通过对完整模型进行通道剪枝来生成一系列具有不同容量的兼容子模型。聚合方法通过将具有冲突方向的梯度投影到彼此的正交平面上来解决梯度冲突问题。兼容性损失基于不确定性估计，通过动态调整子模型的不确定性来适应性地调整优化优先级。 

#### 8. 实验设置：
本文在Market-1501、MSMT17和VeRi-776等人物ReID数据集上评估了SFSC方法的性能。使用BCT和Asymmetric作为一对一兼容性学习范式的比较方法。检索性能的评估指标为平均精度（mAP）和top-1准确度（R1）。实现基于FastReID，使用默认配置“bag of tricks”。子模型的裁剪比例列表设置为{0.25×，0.50×，0.75×}。所有实验中λ的值均设置为0.2。

#### 9. 实验结果和分析：
本文的SFSC方法在各个数据集上均取得了最先进的性能。在Market-1501数据集上，与最先进的方法相比，SFSC方法的mAP和R1分别提高了1.7％和1.5％。在MSMT17数据集上，SFSC方法的mAP和R1分别提高了1.5％和1.3％。在VeRi-776数据集上，SFSC方法的mAP和R1分别提高了2.2％和1.8％。通过可视化梯度的范数和训练过程中冲突梯度对的累积数量，本文证明了SFSC方法的有效性。


# Paper:650     记忆效率双向变换器实现长视频端到端生成建模



#### 1. Title: 
Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers

#### 2. Authors: 
Jaehoon Yoo, Semin Kim, Doyup Lee, Chiheon Kim, Seunghoon Hong

#### 3. Affiliation: 
第一作者：KAIST

#### 4. Keywords: 
Generative modeling, video generation, transformers, bidirectional modeling, long-term dependency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2023/html/Yoo_Towards_End-to-End_Generative_Modeling_of_Long_Videos_with_Memory-Efficient_CVPR_2023_paper.html  Github: https://sites.google.com/view/mebt-cvpr2023

#### 6. Summary : 
- (1):本文研究视频生成的问题，视频生成需要在保持高质量帧的同时，保持其语义结构和动态的一致性，这是一个重要而具有挑战性的问题。

- (2):以往的方法主要有自回归变换器和双向变换器，但是自回归变换器在训练和推理中存在严重的扩展问题，而双向变换器虽然可以提高推理效率，但是其复杂度仍然是二次的，限制了其在长序列上的应用。本文提出了一种记忆效率的双向变换器，可以直接学习视频的长期依赖关系，同时具有快速推理和错误传播鲁棒性。

- (3):本文提出的记忆效率的双向变换器（MeBT）可以在训练期间直接学习视频帧之间的长期依赖关系，同时通过遮蔽生成和双向变换器实现快速推理和错误传播鲁棒性。 MeBT通过将可观察的上下文令牌投影到固定数量的潜在令牌中，并通过交叉注意力将其调节以解码遮蔽令牌，从而实现了编码和解码的线性时间复杂度。本文还提出了一种简单而有效的课程学习方法，以逐步引导模型逐渐学习短期到长期的依赖关系。

- (4):本文在三个具有挑战性的真实世界视频数据集上评估了MeBT的性能。 MeBT在16帧的短视频上实现了与现有技术相当的性能，并在128帧的长视频上优于所有技术，同时在训练和推理期间的内存和计算效率方面更具优势。
#### 7. 方法详细介绍：
本文提出了一种记忆效率的双向Transformer（MeBT）模型，用于视频合成。该模型采用编码器-解码器架构，基于固定数量的潜在瓶颈令牌zL，通过将上下文令牌yC投影到潜在令牌zL，再利用潜在令牌zL预测掩码令牌yM。编码器和解码器利用潜在瓶颈实现线性复杂度O（N），同时在令牌位置上进行全依赖建模。解码器由两种类型的注意力层堆叠而成，交替更新潜在令牌zL和掩码令牌zM。MeBT的整体框架如图1所示。在训练方面，本文提出了一种学习计划和间隔调度函数。该方法在SkyTimelapse、Taichi-HD和UCF-101三个视频基准上进行了评估，并与MoCoGAN-HD、DIGAN、TATS和CCVS等最先进的视频生成模型进行了比较。

#### 8. 实验设置：
本文在SkyTimelapse、Taichi-HD和UCF-101三个视频基准上进行了评估。对于每个数据集，分别使用16帧和128帧长的视频进行短期和长期生成的评估。使用基于Kinetics-600训练的I3D网络计算生成视频的质量，使用峰值训练内存和批量大小为4的推理时间来衡量模型的效率。

#### 9. 实验结果和分析：
该方法在短期视频生成方面表现出与最先进方法相当的性能。在所有数据集上，它的性能都优于所有非Transformer基线，并在SkyTimelapse和Taichi-HD数据集上分别取得最佳和次佳性能，在UCF-101数据集上取得最佳Inception分数。该方法在长期视频合成方面也优于所有基线。定量和定性比较表明，该方法通过在时间上生成高保真度的帧，同时生成连贯的语义结构和动态，生成了令人信服的视频。该方法将峰值训练内存使用量降低了33-45％，同时将推理速度提高了10-20倍，相比TATS有了显著的改进。


# Paper:651     一种博学的细粒度视觉分类模型



#### 1. Title: 
An Erudite Fine-Grained Visual Classification Model

#### 2. Authors: 
Dongliang Chang, Yujun Tong, Ruoyi Du, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma

#### 3. Affiliation: 
北京邮电大学

#### 4. Keywords: 
Fine-grained visual classification, multi-dataset training, feature disentanglement, feature re-fusion, meta-learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chang_An_Erudite_Fine-Grained_Visual_Classification_Model_CVPR_2021_paper.html  Github: https://github.com/PRIS-CV/An-Erudite-FGVC-Model

#### 6. Summary : 
- (1):本文研究的是细粒度视觉分类（FGVC）模型，目前的FGVC模型是孤立的，需要先识别对象的粗粒度标签，然后选择相应的FGVC模型进行识别，这在实际应用中存在一些问题。因此，本文提出了一种通过多个不同数据集联合训练的博学FGVC模型，可以高效准确地跨越组合标签空间预测对象的细粒度标签。

- (2):过去的方法都是基于单一的训练数据集，即一个模型只能用于识别一个特定的对象类别。本文提出的方法是通过多个不同数据集联合训练，但是不同数据集之间存在负迁移和正迁移的问题。为了解决这个问题，本文提出了特征解缠模块和特征重融合模块来平衡不同数据集之间的正迁移和负迁移。同时，本文还提出了一种基于元学习的数据集不可知空间注意层，以充分利用多个数据集的训练数据。

- (3):本文提出的方法是通过多个不同数据集联合训练的博学FGVC模型，通过特征解缠模块和特征重融合模块来平衡不同数据集之间的正迁移和负迁移，同时提出了一种基于元学习的数据集不可知空间注意层。实验结果表明，本文提出的方法可以轻松地与现有的FGVC方法相结合，获得最先进的结果。

- (4):本文提出的方法在11个不同的混合数据集上进行了实验，这些数据集基于四个不同的FGVC数据集构建，证明了所提出方法的有效性。同时，本文提出的方法可以轻松地与现有的FGVC方法相结合，获得最先进的结果。
#### 7. 方法详细介绍：
本文提出了一种联合训练的细粒度视觉分类（FGVC）方法。该方法包括训练特征提取器和分类器，使用来自不同数据集的混合数据进行训练，目标是在联合标签空间中进行预测。关键挑战是管理数据集特定特征之间的正向和负向转移。为了解决这个问题，本文提出了三个模块：特征解缠模块、特征重融合模块和数据集不可知空间注意力层。特征解缠模块将混合特征嵌入解耦为不同的数据集特定特征，以消除不同数据集之间的负向转移。特征重融合模块通过基于门控的特征重融合模块融合数据集特定特征，以增强数据集之间的正向转移。最后，数据集不可知空间注意力层被提出，以充分利用多数据集训练数据，因为定位是数据集不可知的。元学习基于数据集不可知空间注意力层可以用于跨不同数据集的本地化。总体优化目标是消除不同数据集之间的负向转移并增强正向转移。

#### 8. 实验设置：
本文在四个广泛使用的FGVC数据集上进行了评估：CUB-200-2011、Stanford Cars、FGVC-Aircraft和Flowers-102。这些数据集以不同的组合混合在一起，以评估在具有相似对象形状或数据集差距的两个数据集的混合训练下模型的性能。

#### 9. 实验结果和分析：
本文的方法在11个不同的混合数据集上进行了测试，这些数据集建立在四个不同的FGVC数据集上。实验结果表明，该方法在所有11个混合数据集上均取得了最先进的结果。本文还表明，该方法可以与现有的FGVC方法轻松结合，以获得更好的性能。本文还讨论了仅考虑同一领域内不同FGVC数据集之间的关系的局限性，并提出在跨领域场景中进一步探索的建议。


# Paper:652     SparseFusion：基于视图条件扩散的三维重建



#### 1. Title: 
SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction

#### 2. Authors: 
Zhizhuo Zhou, Shubham Tulsiani

#### 3. Affiliation: 
Carnegie Mellon University（卡内基梅隆大学）

#### 4. Keywords: 
3D reconstruction, neural rendering, probabilistic image generation, view synthesis, diffusion models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_SparseFusion_Distilling_View-Conditioned_Diffusion_for_3D_Reconstruction_CVPR_2021_paper.html  Github: https://sparsefusion.github.io/

#### 6. Summary : 
- (1):本文研究了稀疏视图下的三维重建问题，旨在从少量的图像中恢复出三维场景，并能够生成新的视角和提取底层几何信息。 
- (2):现有的方法通常基于神经渲染和重新投影的特征，但在大视角变化和未观察到的区域中无法生成未见区域的内容。本文提出了一种基于视图条件扩散模型的三维一致场景表示方法，能够生成准确和逼真的渲染图像，同时恢复底层的三维表示。与现有方法相比，本文方法在稀疏视图下的新视角合成方面表现更好。 
- (3):本文提出了SparseFusion方法，该方法将神经渲染和概率图像生成的最新进展统一起来，通过从视图条件扩散模型中提取三维一致的场景表示来实现。该方法能够生成高质量的图像输出，同时直接生成三维表示。本文还提出了扩散蒸馏技术，通过最大化扩散模型的似然性来优化实例特定的三维表示，从而实现更准确和逼真的渲染。 
- (4):本文在CO3D数据集的51个类别上进行了评估，并表明SparseFusion方法在稀疏视图下的新视角合成方面表现更好，同时能够恢复准确的三维表示。
#### 7. 方法详细介绍：
本文提出的SparseFusion方法将神经渲染和概率图像生成的最新进展结合起来，用于稀疏视角三维重建。该方法利用视图条件下的潜在扩散模型（VLDM）来建模给定一些上下文视图和任意查询视角的观察结果的可能图像分布。VLDM是在计算查询视图中的像素对齐特征的几何信息支持下进行条件化的。然后，该方法使用扩散蒸馏技术将预测的分布蒸馏成一个实例特定的三维表示。这是通过最大化其渲染的扩散基础似然性来优化实例特定的（神经）三维表示来实现的。该优化导致了一种寻找模式的行为，从而产生更准确和逼真的渲染，同时还恢复了基础对象的三维一致表示。

#### 8. 实验设置：
本文在CO3D数据集上评估了超过50个真实世界的类别。该数据集包含来自各种类别的对象的3D模型，包括动物、车辆和家具。评估是在稀疏视图新视角合成上进行的，其中该方法仅给出两个输入视图。

#### 9. 实验结果和分析：
该方法在各种指标（包括失真和感知指标）上与现有方法进行了比较。比较表明，SparseFusion在稀疏视图新视角合成的两个指标上均优于现有方法。该方法还表现出能够在仅给出两个输入视图的情况下恢复准确的3D和新视图。在图1中提供了样本结果，在表1中提供了与现有方法的比较。


# Paper:653     通过不确定性减少实现无源自适应凝视估计



#### 1. Title: 
Source-free Adaptive Gaze Estimation by Uncertainty Reduction

#### 2. Authors: 
Xin Cai, Jiabei Zeng, Shiguang Shan, Xilin Chen

#### 3. Affiliation: 
中国科学院计算技术研究所

#### 4. Keywords: 
Gaze estimation, domain adaptation, uncertainty reduction, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cai_Source-Free_Adaptive_Gaze_Estimation_by_Uncertainty_Reduction_CVPR_2021_paper.html  Github: https://github.com/caixin1998/UnReGA

#### 6. Summary : 
- (1):本文研究了跨领域的凝视估计问题，提出了一种无监督的无源域适应方法，该方法通过减少样本和模型的不确定性来适应源培训的凝视估计器到未标记的目标域。
 
- (2):现有的跨域凝视估计方法需要在适应过程中同时访问源域和目标域的数据，但在实际应用中，源数据可能不可用或计算上不可行。本文提出了一种无监督的无源域适应方法，通过减少样本和模型的不确定性来适应源培训的凝视估计器到未标记的目标域。本文提出的方法在回归任务中使用方差最小化和伪标签监督机制来解决无源数据的适应问题，而大多数现有的无源适应方法是为分类任务设计的。本文的贡献在于提出了一种通过减少不确定性来实现无源域适应的方法，并在跨域凝视估计任务上进行了广泛的实验，证明了该方法的有效性和优越性。
 
- (3):本文提出了一种基于不确定性减少的凝视适应框架(UnReGA)，该框架通过减少样本和模型的不确定性来实现源培训的凝视估计器到未标记的目标域的适应。具体来说，本文提出了一种面部增强器来增强输入图像，使其更适合凝视估计，从而减少样本不确定性；然后，本文通过最小化未标记目标数据上估计器预测的方差来更新源凝视估计器的集合，从而减少模型不确定性；最后，在推理过程中将更新的估计器合并为单个模型。本文的实验结果表明，UnReGA在跨域凝视估计任务中优于其他现有的跨域凝视估计方法，无论是否有源数据。
 
- (4):本文在六个跨域任务上进行了广泛的实验，证明了UnReGA及其组件的有效性。实验结果表明，UnReGA在两种协议下（有源数据和无源数据）均优于其他现有的跨域凝视估计方法。
#### 7. 方法详细介绍：
本文提出了一种无源自适应注视估计方法UnReGA，包括三个关键组件：人脸增强、无源自适应和平均参数估计器。首先，使用高质量人脸数据集训练人脸增强器，对源数据和目标数据进行增强。然后，通过最小化模型不确定性和引入伪标签监督来进行无源自适应，以保持可靠的注视估计。最后，使用平均参数估计器对目标数据进行推断。

#### 8. 实验设置：
本文使用五个不同的注视估计数据集作为五个不同的领域：ETH-XGaze、Gaze360、GazeCapture、MPIIGaze和EyeDiap。其中，ETH-XGaze和Gaze360被选为源领域，其余三个为目标领域。使用归一化方法处理人脸图像，消除相机自由度的变异性。人脸增强器在FFHQ上进行预训练，并使用批量大小为16进行微调20000次。注视估计器使用Adam优化器进行训练，学习率为10^-4，训练40个epochs，批量大小为128。无源自适应使用Adam优化器进行训练，学习率为2×10^-5，将Eq.(9)中的γ设置为0.01。模型使用批量大小为20进行10个epochs的训练，并从目标领域随机选择100个未标记样本。

#### 9. 实验结果和分析：
本文在六个跨领域注视估计任务上评估了UnReGA框架及其组件的有效性。在表1中报告了基线方法和UnReGA变体的角度注视误差。结果表明，UnReGA优于基线方法和其变体，不确定性降低组件有助于提高性能。本文还讨论了不确定性降低的优势以及平均注视估计器参数的有效性。实验结果表明，UnReGA在无源自适应注视估计问题上的性能优于其他现有方法。


# Paper:654     通过连接后门攻击和对抗攻击实现渐进式后门消除



#### 1. Title: 
Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks

#### 2. Authors: 
Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Qiguang Miao, Rong Jin, Gang Hua

#### 3. Affiliation: 
第一作者：西安交通大学软件工程学院

#### 4. Keywords: 
Deep neural networks, backdoor attacks, adversarial attacks, defense methods, progressive backdoor erasing

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Mu_Progressive_Backdoor_Erasing_via_Connecting_Backdoor_and_Adversarial_Attacks_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究深度神经网络（DNN）在训练和推理阶段中面临的后门攻击和对抗攻击问题，提出了一种新的后门防御方法。
- (2):过去的后门防御方法大多需要干净的额外数据集，且容易被现代后门攻击所规避。本文提出的方法通过连接后门攻击和对抗攻击之间的联系，利用对抗攻击技术逐步净化受感染的模型，无需干净的额外数据集，且能够有效抵御现代后门攻击。
- (3):本文发现对于一个被植入后门的模型，其对抗样本的行为与其触发图像的行为相似，即它们都激活DNN神经元的相同子集。基于这些观察，提出了一种新的Progressive Backdoor Erasing（PBE）算法，通过利用非定向对抗攻击逐步净化受感染的模型。与以往的后门防御方法不同，本文的方法具有无需干净额外数据集的显著优势。
- (4):本文在CIFAR-10数据集上进行了实验，对比了5种现代后门攻击和多种后门防御方法，结果表明，本文提出的PBE方法能够有效地消除后门，且在干净样本上没有明显的性能下降，优于现有的防御方法。
#### 7. 方法详细介绍：
本文提出了一种渐进式反向门限擦除方法，该方法通过使用对抗攻击技术来净化受感染的模型。该方法包括以下步骤：
1. 从包含污染图像的训练数据中随机采样，构建初始的额外数据集。
2. 使用对抗攻击技术生成对抗样本，对受感染的模型进行微调，以净化模型。
3. 使用净化后的模型来识别训练数据中的干净图像，并使用这些图像来更新额外数据集。
4. 重复上述步骤，逐步净化受感染的模型和额外数据集。

#### 8. 实验设置：
本文使用CIFAR-10数据集进行实验，评估了所提出的渐进式反向门限擦除方法对五种最新的反向门限攻击的防御效果，包括BadNets、Blend attack、Sinusoidal signal attack、Input-aware dynamic attack和Warpping-based attack。使用Pre-activation Resnet-18作为分类模型，对不同的攻击方法和设置进行了评估。本文还将所提出的方法与现有的五种反向门限擦除方法进行了比较，包括Fine-tuning、Fine-pruning、Neural Cleanse、Neural Attention Distillation和Adversarial Neuron Pruning。使用攻击成功率（ASR）和模型在干净样本上的准确率（ACC）作为防御机制的性能评估指标。

#### 9. 实验结果和分析：
本文提出的渐进式反向门限擦除方法通过使用对抗攻击技术，有效地防御了现代强反向门限攻击，即使没有干净的额外数据集。该方法在防御性能方面优于现有的反向门限擦除方法，包括Fine-tuning、Fine-pruning、Neural Cleanse、Neural Attention Distillation和Adversarial Neuron Pruning。本文提供了详细的性能指标和实验结果可视化。


# Paper:655     SE-ORNet：自我集成方向感知网络用于无监督点云形状对应



#### 1. Title: 
SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence

#### 2. Authors: 
Jiacheng Deng, Chuxin Wang, Jiahao Lu, Jianfeng He, Tianzhu Zhang, Jiyang Yu, Zhe Zhang

#### 3. Affiliation: 
中国科学技术大学

#### 4. Keywords: 
Point cloud, shape correspondence, unsupervised learning, self-ensembling, orientation estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Deng_SE-ORNet_Self-Ensembling_Orientation-Aware_Network_for_Unsupervised_Point_Cloud_Shape_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究点云形状对应问题，旨在无需手动标注点云对，获得点云之间的稠密点对点对应关系。但是，人类和某些动物具有双侧对称性和不同的方向，这导致对称部分的严重错误匹配。此外，点云噪声破坏了点云的一致表示，从而降低了形状对应的准确性。为了解决上述问题，本文提出了一种自我集成方向感知网络SE-ORNet。

- (2):过去的方法包括基于频谱的方法和基于点的方法。基于频谱的方法需要计算LBO特征向量作为基函数，并推断形状对应的线性变换。然而，对于点云数据，连接信息很难直接获得，而基于点的方法直接处理点云，无需连接信息即可找到两个具有可变形3D形状的点云之间的密集点对点映射。本文提出的方法是一种自我集成方向感知网络，它包括一个教师和一个学生模型，一个方向估计模块和一个自适应域鉴别器。首先，我们设计了一种新的增强方案，以产生旋转和高斯噪声的增强样本，并记录旋转角度作为旋转角度标签。然后，我们制定软标签和一致性损失，以鼓励增强和原始样本的集成预测达成共识，旨在感知身体方向的差异并克服点云噪声干扰以获得一致的点云表示。此外，我们设计了一个轻量级的方向估计模块，它对齐两个点云的方向，以解决点云中对称部分的不匹配问题。最后，我们设计了一个鉴别器来实现域自适应，并计算域损失。

- (3):本文提出了一种自我集成方向感知网络SE-ORNet，它包括一个教师和一个学生模型，一个方向估计模块和一个自适应域鉴别器。我们设计了一种新的增强方案，以产生旋转和高斯噪声的增强样本，并记录旋转角度作为旋转角度标签。然后，我们制定软标签和一致性损失，以鼓励增强和原始样本的集成预测达成共识，旨在感知身体方向的差异并克服点云噪声干扰以获得一致的点云表示。此外，我们设计了一个轻量级的方向估计模块，它对齐两个点云的方向，以解决点云中对称部分的不匹配问题。我们的方法在人类和动物数据集上取得了最先进的性能。

- (4):本文提出的SE-ORNet方法在无监督点云形状对应任务上取得了最先进的性能。我们的方法通过增强方案和方向估计模块，克服了点云噪声干扰和对称部分不匹配问题，获得了更一致的点云表示。在人类和动物数据集上的实验结果表明，我们的方法优于现有的无监督点云形状对应方法。
#### 7. 方法详细介绍：
本文提出了一种自我集成的方向感知网络(SE-ORNet)，用于无监督点云形状对应。该方法利用随机旋转和高斯噪声生成增强的源点云和高斯噪声生成增强的目标点云。方向估计模块估计源点云相对于目标点云的旋转角度，并在位置空间中对齐点云。对齐的点云对分别输入教师和学生模型，并通过DGCNN骨干网络预测对应关系。学生模型通过一致性损失和构造损失进行监督，而教师模型的参数通过指数移动平均(EMA)策略进行更新。该方法还利用随机变换，包括旋转和高斯噪声，以提高特征表示的鲁棒性。

#### 8. 实验设置：
本文在四个形状对应基准测试集上进行了广泛的实验，包括SHREC、TOSCA、SMAL和CMU Panoptic。实验在一台服务器上进行，该服务器配备Intel Xeon E5-2630 v4 CPU、128GB内存和NVIDIA Tesla V100 GPU。实现基于PyTorch。

#### 9. 实验结果和分析：
本文在各个数据集上进行了实验，包括SHREC、TOSCA、SMAL、CMU Panoptic、FAUST和SCAPE。实验结果表明，SE-ORNet方法在各个数据集上均取得了最先进的性能，证明了其适应不同形状的点云的能力。该方法的鲁棒性也通过对测试集和真实扫描数据集进行不同增强的实验得到了验证。本文提供了详细的性能比较和消融研究，以评估每个设计的有效性。


# Paper:656     一种混合精度量化的一次模型



#### 1. Title: 
One-Shot Model for Mixed-Precision Quantization

#### 2. Authors: 
Ivan Koryakovskiy, Alexandra Yakovleva, Valentin Buchnev, Temur Isaev, Gleb Odinokikh

#### 3. Affiliation: 
华为技术有限公司 (Huawei Technologies Co. Ltd.)

#### 4. Keywords: 
Neural network quantization, mixed-precision, bit width, supernet, Pareto-front architecture

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究神经网络量化的混合精度方法，旨在寻找最优的位宽以达到更高的压缩率和性能。 
- (2):现有的搜索方法大多只能找到单一的混合精度结构，需要多次重启搜索才能找到适合的结构。本文提出了一种基于梯度优化的方法，可以在O(1)时间内找到多样化的Pareto前沿结构，相比现有方法效率提高了5倍。 
- (3):本文提出了一种新的One-Shot MPS方法，通过将超网络与可训练的函数相结合，预测不同压缩率对应的位宽概率，从而在常数时间内找到Pareto前沿位宽组合。该方法在ResNet-18和MobileNet-v2上的验证结果表明，与现有方法相比，可以在20到40次超网络评估内选择Pareto前沿结构，且预测性能与实际模型性能的相关性得分高达0.93以上。 
- (4):本文方法在分类和超分辨率模型上进行了验证，取得了良好的性能表现，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种新的混合精度量化方法——One-Shot Model for Mixed-Precision Quantization。该方法使用超网络搜索神经网络每一层的最佳位宽配置。超网络使用硬件惩罚和核相似性惩罚进行训练。位宽概率使用密集连接神经网络进行建模。该方法还使用条件批量归一化来应对训练期间特征统计的变化。该方法在ResNet-18、MobileNet-v2和两个超分辨率模型上进行了评估。主要评估标准是搜索 Pareto 前沿模型所需的时间。该方法与EdMIPS、DNAS和Bayesian Bits进行了比较。

#### 8. 实验设置：
本文在ResNet-18、MobileNet-v2和两个超分辨率模型上进行了实验。使用ImageNet数据集进行ResNet-18和MobileNet-v2的训练，使用DIV2K数据集进行超分辨率模型的训练。将所有层，包括第一层和最后一层，量化为2、4或8位选项。如果输入x遵循ReLU激活函数，则将下限值设置为零。残差连接不进行量化。使用三个标准评估One-Shot MPS方法：搜索 Pareto 前沿模型所需的时间、子模型和独立模型之间的相关性以及方法找到的体系结构的质量。

#### 9. 实验结果和分析：
One-Shot MPS方法可以一次找到高质量的体系结构，并允许找到更丰富的位宽组合。与传统方法相比，它提高了Kendall's tau相关性，有助于预测微调模型的性能，并且不会影响找到的体系结构的质量。固定位宽体系结构在8位时才能达到与混合精度网络相似的峰值信噪比（PSNR）或Top-1准确率。对于更小的位宽，One-Shot MPS和传统混合精度方法通常会找到表现更好的体系结构。


# Paper:657     Flexible-Cm GAN：面向放疗中精确的三维剂量预测



#### 1. Title: 
Flexible-Cm GAN: Towards Precise 3D Dose Prediction in Radiotherapy

#### 2. Authors: 
Riqiang Gao, Bin Lou, Zhoubing Xu, Dorin Comaniciu, Ali Kamen

#### 3. Affiliation: 
Riqiang Gao: 西门子医疗健康有限公司数字技术与创新部门
Bin Lou, Zhoubing Xu, Dorin Comaniciu, Ali Kamen: 西门子医疗健康有限公司数字技术与创新部门

#### 4. Keywords: 
Deep learning, radiotherapy planning, conditional generative model, miss-consistency loss

#### 5. Paper: 
Paper: https://arxiv.org/abs/2007.01206  Github: None

#### 6. Summary : 
- (1):本文研究背景是基于知识的放疗计划中的三维剂量预测，旨在提高放疗计划的精度和可靠性。
 
- (2):过去的方法主要局限于简单的场景，例如固定的计划类型或一致的射线角度配置，这限制了这些方法的可用性，使它们在更大的临床场景中不具有普适性。本文提出了一种新的条件生成模型Flexible-Cm GAN，利用了关于计划类型和各种射线几何的额外信息。提出了一种失配损失来处理输入数据上的条件集合有限的挑战，例如不完整的训练样本。本文的方法在创新性和动机上都很好。

- (3):本文提出了一种新的条件生成模型Flexible-Cm GAN，利用了关于计划类型和各种射线几何的额外信息。提出了一种失配损失来处理输入数据上的条件集合有限的挑战，例如不完整的训练样本。本文的方法在创新性和动机上都很好。

- (4):本文的方法在三维剂量预测任务上取得了良好的性能，证明了其有效性和可靠性。
#### 7. 方法详细介绍：
本文提出了一种新的条件生成模型，Flexible-Cm GAN，用于精确的三维剂量预测。该模型利用了关于计划类型和各种射线几何的附加信息。为了解决输入数据条件有限的挑战，例如不完整的训练样本，提出了一种失配损失。该方法旨在比以前在基于知识的放疗计划中使用的深度学习方法更灵活和通用。具体步骤包括：
1. 数据预处理：将CT图像和结构信息转换为体素网格，并将其与计划信息对齐。
2. 模型架构：使用条件生成对抗网络（CGAN）框架，将条件信息和体素网格输入到生成器和判别器中。
3. 损失函数：使用对抗损失和失配损失来训练模型，以提高预测精度和泛化能力。
4. 训练过程：使用Adam优化器进行训练，同时使用交叉验证和早期停止技术来防止过拟合。

#### 8. 实验设置：
本文使用了来自两个不同机构的数据集进行实验，分别是TCIA Pancreas和TCIA Lung。每个数据集都包含了CT图像、结构信息和计划信息。为了评估模型的性能，使用了均方根误差（RMSE）和Dice系数等指标，并与其他深度学习方法进行了比较。实验使用了Python和TensorFlow等工具进行实现。

#### 9. 实验结果和分析：
实验结果表明，Flexible-Cm GAN模型在两个数据集上均取得了较好的预测效果，RMSE和Dice系数均优于其他深度学习方法。此外，失配损失的使用可以提高模型的泛化能力，使其在缺乏条件信息的情况下仍能进行准确的预测。然而，该方法仍存在一些局限性，例如对于不同的结构和计划类型可能需要重新训练模型。


# Paper:658     情感：学习真实世界视觉数据的情感解释



#### 1. Title: 
Affection: Learning Affective Explanations for Real-World Visual Data

#### 2. Authors: 
Panos Achlioptas, Maks Ovsjanikov, Leonidas Guibas, Sergey Tulyakov

#### 3. Affiliation: 
第一作者：Panos Achlioptas，斯坦福大学
其他作者：Maks Ovsjanikov，Ecole Polytechnique, IP Paris；Leonidas Guibas，斯坦福大学；Sergey Tulyakov，Snap Inc.

#### 4. Keywords: 
Affective analysis, real-world images, emotional responses, neural networks, visio-linguistic analysis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Achlioptas_Affection_Learning_Affective_Explanations_for_Real-World_Visual_Data_CVPR_2021_paper.html  Github: https://github.com/achliopa/affective-explanations

#### 6. Summary : 
- (1):本文旨在探索真实世界图像所引发的情感反应。作者通过引入一个大规模数据集，其中包含85,007张公开可用图像的分类情感反应和自由形式的文本解释，分析了6,283个注释者对图像的感受和原因，共计526,749个回答。虽然情感反应是主观的，受到上下文（个人情绪、社会地位、过去经历）的影响，但作者表明，有显著的共同点可以捕捉到情感反应，得到大多数人的支持。
- (2):现有的图像分析系统主要集中在图像内容上，忽略了图像与潜在观众之间可能存在的更微妙和复杂的交互。本文旨在通过将图像对观众的影响纳入考虑，迈向更加以观众为中心的理解。作者认为情感反应提供了视觉世界和人类体验之间的基本联系。因此，作者旨在了解给定图像可以引起哪些种类的情感反应，以及最重要的是为什么？本文通过人类提供的解释，研究了真实世界视觉数据引起的情感反应，将情感与语言构造联系起来，这在规模上比其他媒体（如fMRI扫描）更容易策划。
- (3):本文的主要贡献有两个方面：首先，作者策划了一个大规模的数据集，包含526,749个解释，用于证明85,007个不同真实世界图像的情感反应。收集的解释由6,283个注释者提供，涵盖了许多不同的观点、个性和品味。其次，作者对数据集进行了语言和情感中心的分析，并使用它来生成深度神经听众和说话人，以理解或生成关于图像情感反应的合理样本。作者的方法似乎学习到了人们情感反应的共同偏见，例如，鲨鱼的存在比平静睡觉的狗更容易引起恐惧。最后，作者探索了训练情感神经字幕系统的变体，这些系统允许对提供的解释进行情感和事实视觉细节的控制。有趣的是，作者证明了实用变体在不同图像之间展示了更丰富和多样化的语言。
- (4):本文提出了一种新的任务，称为真实世界图像的情感解释字幕（AEC）。为了解决AEC，作者发布了一个新的大规模数据集Affection，其中包含526,749个情感反应和解释。作者设计了各种组件，包括神经说话人，使其能够进行情感字幕，具有不同程度的实用和情感控制。最后，作者的所有神经说话人在情感图灵测试中表现出强大的性能，其中人类发现他们的生成∼60％-65％的时间可能会被其他人说出，支持丰富的区分。
#### 7. 方法详细介绍：
本文提出了一项新任务——情感解释字幕（Affective Explanation Captioning，AEC），用于处理真实世界图像。为了解决AEC问题，作者发布了一个新的大规模数据集——Affection，其中包含了526,749个情感反应和解释。作者设计了多种组件，包括神经说话人，使其能够具有各种程度的语用和情感控制。最后，所有神经说话人在情感图灵测试中表现出强大的性能，人类发现它们的生成约60%-65%的时间可能是由其他人说出的，支持丰富的区分。

#### 8. 实验设置：
本文使用了Affection数据集进行实验，评估了不同组件的性能。作者还使用了情感图灵测试来评估神经说话人的生成质量。

#### 9. 实验结果和分析：
实验结果表明，神经说话人在AEC任务中表现出色，生成的解释具有高度的语用和情感控制。此外，情感图灵测试的结果表明，神经说话人的生成质量接近于人类的生成，证明了其在生成自然语言方面的能力。


# Paper:659     基于混合光谱和空间先验的残差退化学习展开框架用于压缩光谱成像



#### 1. Title: 
Residual Degradation Learning Unfolding Framework with Mixing Priors across Spectral and Spatial for Compressive Spectral Imaging

#### 2. Authors: 
Yubo Dong, Dahua Gao, Tian Qiu, Yuyan Li, Minxi Yang, Guangming Shi

#### 3. Affiliation: 
西安电子科技大学（Xidian University）

#### 4. Keywords: 
Coded aperture snapshot spectral imaging (CASSI), deep unfolding methods, multiscale convolution, mixing priors, hyperspectral image (HSI) reconstruction

#### 5. Paper: 
Paper Link: https://openaccess.thecvf.com/content/CVPR2022/html/Dong_Residual_Degradation_Learning_Unfolding_Framework_With_Mixing_Priors_Across_Spectral_CVPR_2022_paper.html
Github Link: https://github.com/ShawnDong98/RDLUF_MixS2

#### 6. Summary : 
- (1):本文研究了压缩光谱成像中的高光谱图像重建问题，提出了一种基于深度展开的方法，用于从二维测量中恢复可靠的三维高光谱图像。
- (2):过去的方法包括基于模型的技术、端到端方法和深度展开方法。然而，这些方法在数据子问题和先验子问题中存在一些问题，如感知矩阵不适合实际退化过程、先验模型不能同时利用空间和光谱先验等。本文提出了一种残差退化学习展开框架（RDLUF），用于弥合感知矩阵和退化过程之间的差距，并设计了一种MixS2 Transformer，通过混合光谱和空间先验来增强光谱-空间表示能力。
- (3):本文首先将Proximal Gradient Descent（PGD）算法在最大后验理论框架下展开，然后将残差退化学习策略集成到PGD的数据子问题中，从而实现了RDLUF。其次，将轻量级Inception与光谱自注意力相结合，以解决S-MSA的空间建模能力不足的问题。最后，将MixS2 Transformer作为先验子问题的去噪器，插入到RDLUF中，形成了端到端可训练的神经网络RDLUF-MixS2。实验结果表明，该方法在HSI重建方面具有优异的性能。
- (4):本文提出的方法在HSI重建任务上取得了最先进的性能，超过了现有方法。
#### 7. 方法详细介绍：
本文提出了一种残差降解学习展开框架（RDLUF），用于压缩光谱成像。该框架包括多尺度卷积和光谱自注意力的轻量级Inception，以及MixS2 Transformer，用于在光谱和空间维度上增强建模能力。RDLUF由多个重复的阶段组成，每个阶段包含一个残差降解学习梯度下降（RDLGD）模块和一个近端映射（PM）模块。MixS2 Transformer作为PM模块，有效地混合了光谱和空间的先验知识。RDLUF还包括一个阶段交互模块，以减少信息损失，丰富每个阶段的特征，并简化网络优化过程。具体步骤包括：
1. 使用RDLGD模块估计压缩测量和感知矩阵之间的残差。
2. 使用MixS2 Transformer作为PM模块，混合光谱和空间的先验知识。
3. 使用阶段交互模块，减少信息损失，丰富每个阶段的特征，并简化网络优化过程。

#### 8. 实验设置：
本文使用28个波长范围从450 nm到650 nm的高光谱成像数据集进行了模拟和实验。模拟实验使用CAVE数据集和KAIST数据集，真实实验使用SD-CASSI系统采集的HSI数据集。性能评估指标包括PSNR和SSIM。

#### 9. 实验结果和分析：
本文提出的RDLUF-MixS2方法在模拟数据集上优于现有的最先进算法，并在真实数据集上实现了与先前工作相当的结果。在10个场景上的PSNR和SSIM结果在表1中呈现，图5中比较了使用5个场景中的4个光谱通道重建的HSI。表2中呈现了消融研究结果，显示了RDLUF-MixS2不同组件对其整体性能的影响。研究还调查了阶段数量，发现网络的性能随着阶段数量的增加而提高。在性能和计算复杂度之间的权衡下，7个阶段的方法是最佳选择。


# Paper:660     不要骗我！基于验证扰动分析的鲁棒高效可解释性方法



#### 1. Title: 
Don’t Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis

#### 2. Authors: 
Thomas Fel, Melanie Ducoffe, David Vigouroux, Rémi Cadène, Mikael Capelle, Claire Nicodème, Thomas Serre

#### 3. Affiliation: 
第一作者：Thomas Fel，布朗大学卡尼大脑科学研究所，美国

#### 4. Keywords: 
Explainability, Deep Neural Networks, Perturbation Analysis, Robustness, Adversarial Attacks

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2021_paper.html 
Github: https://github.com/deel-ai/formal-explainability

#### 6. Summary : 
- (1):本文研究深度神经网络的可解释性问题，旨在提高模型的可靠性和解释性，以便更好地理解模型的决策过程。
- (2):过去的解释方法存在采样偏差和计算复杂度高等问题，本文提出了一种新的解释方法EVA，利用验证扰动分析方法直接传播边界，以在单次前向传递中彻底探索扰动空间。与过去的方法相比，EVA具有时间效率和保证完整性的优点，能够可靠地识别驱动模型决策的图像像素。
- (3):本文提出的EVA方法是第一个保证使用验证扰动分析方法彻底探索其所有扰动集的解释性方法。该方法利用了验证扰动分析的有益特性，即时间效率和保证完整性，以识别最有可能改变预测器决策的输入变量。本文系统地评估了EVA方法，并在多个基准测试中展示了最先进的结果。该方法的创新点在于，它能够在单次前向传递中彻底探索扰动空间，从而提高了解释的可靠性和准确性。
- (4):本文在多个图像数据集上对EVA方法进行了系统评估，并展示了在多个解释性指标上的最先进结果。该方法能够生成类别特定的解释，并且能够在保证决策的前提下，最小化解释的像素数量。实验结果表明，EVA方法在多个基准测试中均取得了最先进的性能，证明了该方法的有效性和可靠性。
#### 7. 方法详细介绍：
本文提出了一种名为EVA（使用验证扰动分析进行解释）的方法，用于计算输入变量的重要性分数，该分数基于它们在扰动下改变模型决策的能力。该方法利用可验证的扰动分析来上界对抗重叠准则，该准则衡量修改一组像素可以生成类之间重叠的程度。EVA测量固定一组变量时对抗重叠的下降，从而揭示它们的重要性。该方法基于形式验证框架DecoMon，使用适用于预测器的最准确的LiRPA方法。EVA方法可以用于最先进的模型，并且可以从并行化中受益。

#### 8. 实验设置：
本文在MNIST、CIFAR-10和ImageNet三个图像分类数据集上评估了所提出的EVA方法。解释是使用引入式2中的EVA估计器生成的。在将图像切割成12个边的网格后，重要性分数不是逐像素评估的，而是在每个单元格上评估的。对于MNIST和CIFAR-10，使用ε = 0.5，而对于ImageNet，使用ε = 5。使用的可验证扰动分析方法是（IBP + Forward + Backward）用于MNIST，（IBP + Forward）用于CIFAR-10和p = ∞。出于计算目的，使用第二个最后一层（FC-4096）作为中间层h（·）的混合方法在ImageNet上使用EVA。

#### 9. 实验结果和分析：
本文将所提出的EVA方法与其他10种解释方法使用删除、插入、MuFidelity和Robustness-Sr指标进行比较。分数是在500个样本上平均的。实验在MNIST、CIFAR-10和ImageNet数据集上进行。结果表明，EVA在删除、插入和MuFidelity指标方面优于其他方法，同时在Robustness-Sr指标方面取得了竞争性的表现。第3.4节中提出的混合方法允许EVA用于最先进的模型，并保持可处理性。本文还讨论了验证扰动分析方法对EVA的影响，并表明更紧的界限会导致改进的解释。最后，本文在MNIST数据集上展示了EVA生成的有针对性的解释。


# Paper:661     测试时间适应的特征对齐和均匀性



#### 1. Title: 
Feature Alignment and Uniformity for Test Time Adaptation

#### 2. Authors: 
Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, Rui Li

#### 3. Affiliation: 
第一作者：清华大学

#### 4. Keywords: 
Test time adaptation, feature alignment, feature uniformity, domain generalization, medical image segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是测试时间适应（TTA）问题，即如何在接收到来自不同分布的测试域样本时，适应深度神经网络。在这种情况下，模型只能访问在线未标记的测试样本和在训练域上预训练的模型。本文首先将TTA视为特征修订问题，因为源域和目标域之间存在域差异。然后，本文遵循两个度量标准：对齐和均匀性，讨论测试时间特征修订。 

- (2):过去的方法包括特征匹配和预测调整等特征对齐方法，以及熵最小化、原型调整、信息最大化和批归一化统计对齐等特征均匀性方法。然而，这些方法没有同时从特征对齐和均匀性两个方面解决TTA问题。本文提出了一种新方法，从这两个方面纠正特征表示。本文提出的方法包括测试时间自我蒸馏和记忆空间局部聚类等策略，以及熵过滤器和一致性过滤器等方法，以进一步减轻噪声标签的影响。

- (3):本文提出了一种新的测试时间适应方法，从特征对齐和均匀性两个方面解决TTA问题。具体而言，本文提出了测试时间特征均匀性和测试时间特征对齐两个策略。测试时间特征均匀性通过建立一个存储特征表示和logits的内存库来维护先前数据的有用信息，并使用logits和内存库中的特征计算每个类别的伪原型。为了保证当前样本的均匀性，原型分类和模型预测（线性分类器的输出）的预测分布应该相似，即当前一类图像的特征分布应该与同一类别的所有先前图像的特征分布一致。测试时间特征对齐通过检索内存库中的K个最近特征来对齐特征表示。本文还提出了熵过滤器和一致性过滤器来进一步减轻噪声标签的影响。

- (4):本文在四个领域泛化基准测试和四个医学图像分割任务上进行了实验，证明了所提出方法的可扩展性和有效性。实验结果表明，本文提出的方法不仅稳定地提高了基线，而且在现有的测试时间适应方法中表现出色。
#### 7. 方法详细介绍：
本文提出了一种名为“测试时间自蒸馏与记忆空间局部聚类”的方法（TSD-MSLC），用于测试时间自适应（TTA）。该方法包括三个主要组件：测试时间自蒸馏（TSD）、记忆空间局部聚类（MSLC）和一个训练目标函数，该函数结合了TSD和MSLC。TSD用于维护当前样本的均匀性，以提高表示质量。MSLC用于鼓励K近邻特征而不是所有特征接近，以减少噪声标签的影响。训练目标函数结合了TSD和MSLC以平衡不同的损失函数。在测试阶段，自适应是以在线方式进行的，模型使用训练目标函数进行更新，仅使用一步梯度下降。

#### 8. 实验设置：
本文在四个数据集上进行了实验：PACS、OfficeHome、VLCS和DomainNet。使用ResNet-18/50、Vision Transformer（ViT-B/16）、ResNeXt-50（32×4d）、EfficientNet（B4）和MLP-Mixer（Mixer-L16）作为骨干网络。对于源培训，选择一个域作为目标域，其他域用作源域。来自源域的所有图像都分为80％/20％进行训练和验证。使用Adam优化器，学习率为5e-5。测试时间自适应的批量大小设置为128。交易参数λ经验性地设置为0.1。评估整个目标域的准确性。

#### 9. 实验结果和分析：
本文提出的方法在各种数据集上均优于所有领域泛化和测试时间自适应方法，包括PACS、OfficeHome、VLCS和DomainNet。具体而言，该方法将ERM基线提高了4.8％、1.3％、0.5％、2.53％，分别适用于每个数据集。该方法在ResNet18和ResNet50骨干网络上分别实现了66.49％和70.08％的平均准确性，这是最先进的结果。该方法还可以更快地适应数据，并在目标域上实现更高的准确性。在四个领域泛化基准测试和四个交叉领域医学图像分割基准测试上，该方法均取得了最佳结果。


# Paper:662     扩散艺术还是数字伪造？探究扩散模型中的数据复制问题



#### 1. Title: 
Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models

#### 2. Authors: 
Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein

#### 3. Affiliation: 
第一作者：马里兰大学帕克分校

#### 4. Keywords: 
Diffusion models, data replication, image retrieval, content replication, memorization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2021_paper.html  Github: https://somepago.github.io/diffrep.html

#### 6. Summary : 
- (1):本文研究了扩散模型中的数据复制问题，探讨了扩散模型是否创造了独特的艺术作品，或者它们是否直接复制自其训练集中的内容。

- (2):过去的方法主要是基于图像检索和复制检测，但是这些方法存在一些问题，如对于大规模数据集的处理效率低下，以及对于复制行为的检测不够准确。本文提出了一种基于实例检索的方法，可以有效地检测扩散模型中的数据复制行为，并且在多个数据集上进行了实验验证。

- (3):本文提出了一种基于实例检索的方法，可以有效地检测扩散模型中的数据复制行为。该方法使用了一系列图像相似度度量方法，并且在多个数据集上进行了实验验证。实验结果表明，该方法可以有效地检测到扩散模型中的数据复制行为，并且可以在一定程度上提高检测的准确性。

- (4):本文的方法在多个数据集上进行了实验验证，并且取得了较好的性能表现。实验结果表明，该方法可以有效地检测到扩散模型中的数据复制行为，并且可以在一定程度上提高检测的准确性。这些结果支持了本文的研究目标和贡献。
#### 7. 方法详细介绍：
本文提出了一种使用SSL和图像检索文献中的特征提取器来检测图像复制的系统。该系统在10个不同的数据集上进行了评估，包括5个合成数据集和5个真实数据集。该系统使用的模型包括MultiGrain、SSCD、ViT-Small/16、ViT-Base/16、Swin-Transformer、VICRegL、DINO和MoCo v3。图像之间的相似度是使用分裂乘积度量计算的，该度量将每个特征向量分成块，并计算相应块之间的内积，返回这些内积中的最大值。

#### 8. 实验设置：
本文在多个数据集上训练了扩散模型，包括Oxford flowers、Celeb-A、ImageNet和LAION。作者还分析了训练集大小等因素对内容复制率的影响。他们还分析了Stable Diffusion模型，该模型最初在超过20亿张图像上进行了训练，然后使用LAION Aesthetics v2 5+子集的6亿张图像进行了微调。他们仅在较小的12M LAION Aesthetics v2 6+子集中搜索匹配项，以保持存储成本可管理。

#### 9. 实验结果和分析：
本文使用平均精度（mAP）分数评估了所提出的系统在不同数据集上的性能。DINO with split-product在所有10个数据集上的平均表现最好。本文还展示了在不同大小的数据集上训练的扩散模型的前两个匹配项，其中前两个模型完全复制了内容，第三个模型非常接近但不是完全复制。作者发现，大规模模型的大多数生成物并不包含复制的内容，但确实存在一定数量的复制。在数据集相似度>0.5的稳定扩散图像中，约占随机生成物的1.88％。


# Paper:663     通过梯度过滤实现高效的边缘设备上的训练



#### 1. Title: 
Efficient On-device Training via Gradient Filtering

#### 2. Authors: 
Yuedong Yang, Guihong Li, Radu Marculescu

#### 3. Affiliation: 
The University of Texas at Austin（德克萨斯大学奥斯汀分校）

#### 4. Keywords: 
On-device training, EdgeAI, Gradient filtering, Back-propagation algorithm, Convolutional neural network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Efficient_On-Device_Training_via_Gradient_Filtering_CVPR_2021_paper.html  Github: https://github.com/SLDGroup/GradientFilter-CVPR23

#### 6. Summary : 
- (1):本文研究了边缘设备上的模型训练问题，提出了一种新的梯度过滤方法，以解决反向传播算法在训练过程中所需的大量计算和内存消耗问题。

- (2):现有的边缘设备上的模型训练方法要么不够高效，要么不够实用，无法满足资源限制。本文提出的梯度过滤方法可以通过创建具有较少唯一元素的梯度图的特殊结构，从而显着减少反向传播期间的计算复杂度和内存消耗。与现有方法相比，本文方法具有更高的效率和更广泛的适用性。 

- (3):本文提出了一种新的梯度过滤方法，通过简化卷积层中的梯度传播过程，从而减少训练所需的计算和内存消耗。通过创建具有特殊结构和较少唯一元素的新梯度图，本文方法可以近似输出变量的梯度，从而简化计算密集型的矩阵乘法，并显著减少构建雅可比矩阵所需的数据。本文方法可以与现有方法相结合，提高训练速度。 

- (4):本文在多个CNN模型和计算机视觉任务上进行了实验，证明了本文方法的有效性和广泛适用性。例如，在ImageNet分类任务上，与SOTA相比，本文方法在只有0.1%的精度损失的情况下，实现了高达19倍的加速和77.1%的内存节省。本文方法易于实现和部署，在NVIDIA Jetson Nano上与高度优化的基线相比，观察到了超过20倍的加速和90%的能量节省。
#### 7. 方法详细介绍：
本文提出了一种名为“梯度过滤”的方法，用于减少设备端训练中反向传播所需的计算和内存复杂度。该方法通过将梯度图划分为r x r像素的块，并将每个块中的所有元素替换为它们的平均值，从而近似梯度图。这样可以减少空间维度中的唯一元素数量，并在梯度图中创建特殊结构。然后使用近似梯度图计算相对于输入和卷积核的梯度，比标准反向传播方法需要更少的计算。该方法的计算减少是通过关注梯度图和卷积核大小中每个通道中唯一元素的数量来实现的。本文提供了详细的计算过程推导。该方法从梯度过滤逼近误差、计算减少和内存成本减少三个方面进行了分析。

#### 8. 实验设置：
本文在图像分类和语义分割任务上进行了实验。对于图像分类，数据集被分成两个非独立同分布的部分，并使用基本训练策略在第一个部分上进行预训练。对于语义分割，模型在Cityscapes数据集上进行预训练，然后在增强的Pascal-VOC12数据集上进行校准和微调。本文还在多个CPU和GPU上评估了所提出方法的设备端性能。

#### 9. 实验结果和分析：
本文在ImageNet分类和语义分割任务上使用多个CNN模型（例如MobileNet、DeepLabV3、UPerNet）和设备（例如Raspberry Pi和Jetson Nano）进行了广泛的实验。与基线方法和其他最先进的方法进行比较，本文的方法在速度、内存节省和准确度损失方面表现出色。本文还在资源受限的边缘设备和高性能设备上测试了该方法，以展示其在实际部署中的适用性。本文提供了详细的表格和图表来支持实验结果。


# Paper:664     VideoFusion：分解扩散模型用于高质量视频生成



#### 1. Title: 
VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation

#### 2. Authors: 
Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan

#### 3. Affiliation: 
第一作者：中国科学院大学

#### 4. Keywords: 
Diffusion probabilistic model, video generation, deep generative models, denoising process

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Luo_VideoFusion_Decomposed_Diffusion_Models_for_High-Quality_Video_Generation_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究背景是视频生成领域中的复杂数据分布问题。
- (2):过去的方法通常采用标准扩散过程，其中同一视频剪辑中的帧被独立地破坏，忽略了内容冗余和时间相关性。本文提出了一种分解扩散过程，通过将每帧噪声分解为在所有帧之间共享的基础噪声和沿时间轴变化的残余噪声来解决这个问题。本文的方法名为VideoFusion，通过两个联合学习的网络来相应地匹配噪声分解。
- (3):本文提出的分解扩散概率模型在视频生成方面取得了最新的成果，超过了基于GAN和扩散的替代方案。本文的创新点在于将视频帧的噪声分解为基础噪声和残余噪声，其中基础噪声在所有帧之间共享，残余噪声沿时间轴变化。这种分解方法使得噪声样本之间具有相关性，从而使得视频生成的去噪网络更容易重构出连贯的视频。此外，基础噪声是所有视频帧共享的，可能与视频内容相关，这使得我们可以更好地控制生成视频的内容或运动。
- (4):本文的方法在多个数据集上进行了实验，证明了其在高质量视频生成方面的优越性，并且能够支持文本条件下的视频生成。
#### 7. 方法详细介绍：
本文提出了一种名为VideoFusion的分解扩散过程模型，用于视频生成。该方法将视频剪辑的扩散过程分解为两部分：基础帧的扩散和残差帧的扩散。基础帧由预训练的图像生成器估计，与不同帧共享。残差帧由条件于帧编号的残差生成器估计。噪声由基础噪声和残差噪声的加权和表示。基础噪声由基础生成器估计，它是一个图像扩散模型的去噪网络。残差噪声由残差生成器估计。该方法通过最小化损失函数来联合训练，其中包括基础帧的损失和残差帧的损失。 

#### 8. 实验设置：
本文在UCF101、Sky Time-lapse和TaiChi-HD三个数据集上评估了所提出的方法。使用的评估指标包括Fréchet Video Distance（FVD）、Kernel Video Distance（KVD）和Inception Score（IS）。模型在16帧视频剪辑上进行训练，分辨率为64×64，然后使用基于DPM的SR模型进行超分辨率处理。本文还比较了VideoFusion和VDM的效率，结果表明VideoFusion消耗的内存更少，延迟更低。最后，本文还对λi和预训练对VideoFusion性能的影响进行了消融研究。

#### 9. 实验结果与分析：
本文提出的VideoFusion方法在UCF101数据集上取得了与现有方法相当的结果，在基础生成器与UCF101联合微调时性能有所提高。该方法还成功生成了具有良好质量和连贯性的长序列。此外，对Weizmann Action数据集的实验表明，VideoFusion可以分解视频运动和内容，并生成具有不同人物身份和动作的视频。本文还将VideoFusion与基于GAN和扩散的替代方法进行了比较，结果表明分解的公式也适用于文本条件视频生成。


# Paper:665     极化iToF：通过散射介质测量高保真度深度



#### 1. Title: 
Polarimetric iToF: Measuring High-Fidelity Depth through Scattering Media

#### 2. Authors: 
Daniel S. Jeon, Andr´eas Meuleman, Seung-Hwan Baek, Min H. Kim

#### 3. Affiliation: 
KAIST (韩国科学技术院)

#### 4. Keywords: 
Indirect time-of-flight imaging, multipath interference, scattering media, polarization, depth estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jeon_Polarimetric_iToF_Measuring_High-Fidelity_Depth_Through_Scattering_Media_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是间接飞行时间（iToF）成像技术在散射介质中的深度估计问题。iToF成像技术可以以低成本捕获密集的深度信息，但在散射介质中往往会受到多径干扰（MPI）的影响，导致深度精度严重降低。

- (2):过去的方法包括多频率iToF成像和基于数据驱动的深度先验估计，但这些方法往往无法处理散射介质中的深度估计问题。本文提出了一种基于极化iToF成像的方法，通过光的极化性质和散射理论，可以消除散射介质中的多径干扰，提高深度估计的准确性。

- (3):本文提出了一种散射感知的极化相位测量模型，可以估计非极化反射光的相位，然后通过散射分析相位和幅度来消除非极化反射光，从而提高深度估计的准确性。本文的创新点在于将极化性质和散射理论相结合，提出了一种新的计算模型，可以消除散射介质中的多径干扰。

- (4):本文的方法在自定义的iToF相机上进行了实验验证，结果表明，与基线方法相比，本文的方法可以显著提高深度估计的准确性，特别是在散射介质中。本文的方法可以应用于自动驾驶、消防救援机器人、雾天行驶等领域。
#### 7. 方法详细介绍：
本文提出了一种极化间接飞行时间（iToF）成像方法，通过散射介质实现准确的三维成像。该方法利用了光的极化特性和关于强度衰减和去极化的散射理论。该方法首先通过ToF照明和检测的正交极化调制来光学地滤除极化的反射光。然后，该方法设计了一种计算方法，可以基于间接ToF的信号表示（相位和幅度）消除剩余的非极化反射光。该方法通过重新审视强度衰减和去极化的散射模型来估计非极化反射光的相位。非极化反射光的幅度是基于观察到的幅度偏移比例一致性估计的。最后，该方法从初始的交叉极化测量中减去非极化反射光，得到无散射的间接ToF测量值，从而实现准确的深度成像。

#### 8. 实验设置：
实验采用了定制的iToF相机，Melexis VGA ToF传感器（MLX75027），调制频率为80 MHz，空间分辨率为640×480。在ToF照明和检测器前面使用了基于薄膜的近红外（NIR）线性偏振器，检测器侧的偏振器安装在Thorlabs K10CR1的旋转支架上。由于旋转台的遮挡，捕获的图像是中心裁剪的380×240。实验在一个70 cm×38 cm的黑暗室内进行，使用一个现成的雾发生器产生人工雾。

#### 9. 实验结果与分析：
本文提出的极化iToF成像方法在散射介质中实现了准确的三维成像，相对深度误差和RMSE值均优于传统的交叉极化ToF成像方法。该方法在不同的雾密度下实现了准确的深度成像，而传统的交叉极化ToF成像在中等雾和浓雾中性能下降，完全无法使用。该方法还捕获了一个无ToF照明的环境光图像，并过滤掉了环境对四相位相关测量的贡献。该方法的RMSE值分别为1.88 cm和2.08 cm，分别为无环境光和有环境光的情况。


# Paper:666     RGB不再：最小解码JPEG视觉变换器



#### 1. Title: 
RGB no more: Minimally-decoded JPEG Vision Transformers

#### 2. Authors: 
Jeongsoo Park, Justin Johnson

#### 3. Affiliation: 
Jeongsoo Park: University of Michigan (密歇根大学)
Justin Johnson: University of Michigan (密歇根大学)

#### 4. Keywords: 
JPEG, Vision Transformers, Data Augmentation, Image Classification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2022_paper.html  Github: https://github.com/JeongsooP/RGB-no-more

#### 6. Summary : 
- (1):本文研究的背景是神经网络处理图像时通常使用RGB图像，但是这些RGB图像通常在保存到磁盘之前被编码为JPEG格式，解码它们会对RGB网络造成不可避免的开销。因此，本文的工作重点是直接从JPEG编码的特征中训练Vision Transformers (ViT)，以避免大部分解码开销，加速数据加载。
- (2):过去的方法主要是基于卷积神经网络(CNNs)，但是由于这些编码特征的结构，CNNs需要对其架构进行重大修改才能接受这些数据。本文提出的方法是使用Vision Transformers (ViTs)，ViTs使用补丁嵌入层将非重叠图像补丁编码为向量，然后使用Transformer处理这些向量。这与DCT表示法非常匹配，因为DCT表示法也将非重叠RGB图像补丁表示为向量。本文还直接在这些编码特征上进行数据增强，这在这种情况下尚未深入探讨。
- (3):本文提出了一种直接从JPEG编码特征中训练Vision Transformers的方法，并在这些编码特征上直接进行数据增强。本文的创新点在于使用ViTs而不是CNNs，以及直接在编码特征上进行数据增强。本文的贡献是提出了一种新的方法，可以加速数据加载，同时保持与RGB网络相同的准确性。
- (4):本文在ImageNet数据集上训练了ViT-S和ViT-Ti模型，与它们的RGB对应物相比，ViT-Ti模型每个训练迭代速度提高了39.2％，推理速度提高了17.9％，并且没有准确性损失。这表明本文提出的方法可以在加速数据加载的同时保持准确性。
#### 7. 方法详细介绍：
本文提出了一种直接从JPEG编码的DCT系数中训练视觉Transformer的方法。该方法使用DCT表示RGB图像的非重叠RGB图像块作为向量输入，避免了大部分解码开销，加速了数据加载。ViT使用一个补丁嵌入层将非重叠图像块编码为向量，然后使用Transformer进行处理。ViT的初始补丁嵌入层被修改为接受DCT输入，而其余架构保持不变。数据增强直接在DCT图像表示上实现，避免了训练期间任何DCT到RGB的转换。具体实现包括分组嵌入、分离嵌入、串联嵌入和子块转换等DCT嵌入策略，以及基于DC分量和频率分量的DCT增强方法，如亮度、对比度、颜色、锐度和几何增强等。

#### 8. 实验设置：
本文使用PyTorch、libjpeg和TorchJPEG提取DCT系数。对于ViT和SwinV2模型，使用随机调整大小的裁剪、随机翻转、RandAugment和Mixup进行训练，全局批量大小为1024。实验中比较了RGB和DCT模型的吞吐量和准确性，以及不同消融研究对ViT-S的性能影响。

#### 9. 实验结果和分析：
实验结果表明，直接从JPEG DCT加载数据可以将解码延迟降低高达61.5％，生成嵌入所需的FLOPs可以降低高达47.2％。增强方案可以将训练数据加载吞吐量提高93.2％。DCT模型的准确性与RGB模型相似，但性能明显更快。JPEG-S模型受到模型前向和后向传递的瓶颈，但混合精度训练可以在轻微准确性损失的情况下加速模型。本文提出的DCT嵌入策略和合理的增强方法可以直接在DCT系数上进行，与RGB相比几乎没有准确性损失，是未来编码空间研究的充分基础。


# Paper:667     基于偏相关的深度视觉表示学习用于图像分类



#### 1. Title: 
Learning Partial Correlation based Deep Visual Representation for Image Classification

#### 2. Authors: 
Saimunur Rahman, Piotr Koniusz, Lei Wang, Luping Zhou, Peyman Moghadam, Changming Sun

#### 3. Affiliation: 
第一作者：Data61-CSIRO

#### 4. Keywords: 
Image classification, deep learning, partial correlation, covariance matrix, sparse inverse covariance estimation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2019/html/Rahman_Learning_Partial_Correlation_Based_Deep_Visual_Representation_for_Image_Classification_CVPR_2019_paper.html  Github: https://github.com/csiro-robotics/iSICE

#### 6. Summary : 
- (1):本文研究的背景是图像分类中的特征学习问题，通过对卷积特征图中不同通道之间的相关性进行建模，提出了一种基于偏相关的深度视觉表示方法。

- (2):过去的方法主要是基于协方差矩阵的特征表示方法，但是协方差矩阵只能测量两个变量之间的相关性，无法考虑其他变量的影响，因此在存在混淆变量的情况下会出现误导性结果。本文提出了一种基于偏相关的方法，通过估计精度矩阵来消除混淆变量的影响。然而，可靠地估计偏相关需要解决一个称为稀疏逆协方差估计（SICE）的对称正定矩阵优化问题，如何将这个过程纳入CNN中仍然是一个开放的问题。本文提出了一种新的CNN结构层，将SICE作为结构层，通过迭代方法在前向和后向传播步骤中解决上述矩阵优化问题，实现了端到端的训练。

- (3):本文提出的方法是将SICE过程作为CNN的一个结构层，通过迭代方法在前向和后向传播步骤中解决SICE优化问题，实现了端到端的训练。本文的创新点在于将SICE方法与CNN结合，提出了一种新的深度视觉表示方法，解决了协方差矩阵方法中存在的问题。本文的贡献在于提出了一种新的CNN结构层，实现了端到端的训练，同时提高了深度视觉表示的准确性。

- (4):本文在多个图像分类数据集上进行了实验，结果表明，与基于协方差矩阵的方法相比，本文提出的基于偏相关的深度视觉表示方法具有更好的分类性能。本文的方法可以有效地处理高维数据，同时解决了小样本问题。
#### 7. 方法详细介绍：
本文提出了一种名为迭代稀疏逆协方差估计（iSICE）的方法，用于图像分类。该方法通过施加稀疏约束来优化逆协方差矩阵，使用箱约束和投影梯度下降（PGD）步骤。稀疏约束简化了ℓ1范数的梯度，PGD步骤分别应用于逆协方差矩阵的正部分和负部分。当前逆协方差矩阵的估计值由正部分和负部分组装而成，并将上三角条目（加上对角线条目）传递给全连接层进行分类。

#### 8. 实验设置：
本文使用了一个场景和五个细粒度图像数据集进行实验：MIT室内数据集、Airplane、Birds、Cars、DTD和iNaturalist。作者还使用了ImageNet100和mini-ImageNet。遵循Bilinear CNN的训练和测试协议。选择VGG-16和ResNet-50 CNN作为大多数实验的骨干，并使用1×1卷积将特征通道数减少到256，以提高效率和公平比较。所有图像都被调整为448×448，并通过随机水平翻转进行训练。使用AdamW优化器对骨干进行50-100个时期的微调，学习率在第15个和30个时期降低10倍。

#### 9. 实验结果和分析：
本文提出了一种使用CNN学习稀疏逆协方差表示的方法iSICE，它在多个数据集上显着优于其他协方差表示方法，包括MIT、Airplane、Birds、Cars和ImageNet100。iSICE利用稀疏先验来捕获在有限数量的样本下的部分相关性。本文还展示了学习率和稀疏调制器对提高性能的帮助。最后，本文对稠密与稀疏结构估计在样本大小方面的影响进行了实验，并表明稀疏估计对于稀疏结构效果更好。


# Paper:668     高保真度和自由可控的说话头部视频生成



#### 1. Title: 
High-Fidelity and Freely Controllable Talking Head Video Generation

#### 2. Authors: 
Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming, Yan Lu

#### 3. Affiliation: 
Microsoft Research中国

#### 4. Keywords: 
Talking head generation, face reenactment, deep learning, generative adversarial networks (GAN)

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是生成高保真度、自由可控的说话头部视频的方法。
 
- (2):过去的方法存在一些问题，如生成的面部变形严重、驱动图像没有明确地分离出与动作相关的信息等，限制了生成过程中不同属性的操作。本文提出了一种新的模型，利用自监督学习的标记和基于3D面部模型的标记来建模运动，并引入了一种新的运动感知多尺度特征对齐模块来有效地传输运动而不会出现面部畸变。此外，还提出了一种特征上下文适应和传播模块来增强合成的说话头部视频的平滑性。本文的贡献在于提出了一种生成高保真度、自由可控的说话头部视频的方法，并在多个数据集上进行了评估，证明了其在高保真度视频面部重现和自由可控的说话头部视频生成方面的优越性。
 
- (3):本文提出了一种名为PECHead的模型，它可以生成高保真度的面部重现结果和自由可控的说话头部视频。该方法利用头部运动来控制学习和预定义的标记的估计，从而实现对说话头部姿态和面部表情的自由控制。我们将学习到的稀疏标记和预定义的密集标记结合起来，使用运动感知多尺度特征对齐（MMFA）模块来对齐这两组特征。然后，我们使用不同的系数作为输入条件来控制预定义和学习到的标记的估计，以实现头部姿态和表情的操作。此外，我们还引入了一种基于视频的流水线，使用上下文适应和传播（CAP）模块来进一步提高生成视频的平滑性和自然性。
 
- (4):本文的方法在多个数据集上进行了评估，证明了其在高保真度视频面部重现和自由可控的说话头部视频生成方面的优越性。
#### 7. 方法详细介绍：
本文提出了一种名为PECHead的方法，用于生成高保真度的可控制说话头像视频。该方法包含四个部分：生成器G、人脸形状重建器R、头部姿态感知关键点估计器E和多尺度鉴别器D。生成器G主要包含编码器、瓶颈模块和解码器。人脸形状重建器R从源帧和驱动帧中提取关键点和头部姿态。头部姿态感知关键点估计器E基于源帧提供的外观检测学习到的关键点，遵循头部姿态和表情。多尺度鉴别器D用于鼓励生成器G生成更真实的帧。提出了运动感知的多尺度特征对齐（MMFA）模块，用于结合学习到的稀疏关键点和预定义的密集关键点。引入了上下文适应和传播（CAP）模块，以使模型生成平滑的视频。

#### 8. 实验设置：
本文在多个说话头像数据集上进行了评估，包括VoxCeleb1、VoxCeleb2和LRW。实验在一台服务器上进行，该服务器配备了Intel Xeon E5-2690 CPU、256GB内存和四个NVIDIA Tesla V100 GPU。实现基于PyTorch。

#### 9. 实验结果和分析：
本文提出的方法在生成高保真度的面部再现结果和可控制的说话头像视频方面取得了最先进的性能。该方法在多个数据集上进行了评估，实验结果证明了所提出框架的优越性。


# Paper:669     精度和置信度的桥梁：一种用于校准目标检测的训练时损失函数



#### 1. Title: 
Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection

#### 2. Authors: 
Muhammad Akhtar Munir, Muhammad Haris Khan, Salman Khan, Fahad Shahbaz Khan

#### 3. Affiliation: 
Mohamed bin Zayed University of AI (阿布扎比人工智能大学)

#### 4. Keywords: 
Object detection, calibration, deep neural networks, train-time loss, precision, confidence

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Munir_Bridging_Precision_and_Confidence_A_Train-Time_Loss_for_Calibrating_Object_CVPR_2021_paper.html  Github: https://github.com/akhtarvision/bpc_calibration

#### 6. Summary : 
- (1):本文研究深度神经网络在目标检测中的过度自信问题，提出了一种新的训练时辅助损失函数，旨在将边界框的类别置信度与预测的准确性（即精度）对齐，以提高模型的校准性。
 
- (2):现有的校准方法大多针对分类任务，而目标检测模型的校准性研究相对较少。本文提出的训练时辅助损失函数可以有效地提高现代目标检测器的校准性，特别是在分布转移的情况下。与现有的后处理方法相比，该方法不需要验证集，且可以与其他应用特定的损失函数结合使用。 
 
- (3):本文提出的训练时辅助损失函数（BPC）基于小批量中的真正例和假正例的数量构建惩罚项，旨在校准模型的预测置信度。该方法可以用于其他应用特定的损失函数，并且可以有效地提高现代目标检测器的校准性，特别是在分布转移的情况下。 
 
- (4):本文在多个基准数据集上进行了广泛的实验，包括MS-COCO、Cityscapes、Sim10k和BDD100k。结果表明，本文提出的训练时辅助损失函数在减少校准误差方面优于强校准基线，特别是在分布转移的情况下。
#### 7. 方法详细介绍：
本文提出了一种基于辅助损失函数的目标检测模型训练时校准方法。该方法利用高分和低分预测框的统计信息来校准检测模型。将置信度和精度空间划分为四个区域，用于分类准确和不准确的检测结果。目标是最大化准确预测的概率分数，最小化不准确预测的概率分数。将辅助损失函数的可微分代理近似定义为LBP C = log(1 + tAN + tIC/tAC + tIN)。校准损失LBP C 是模型无关的、可微分的，可以与现代目标检测方法的任务特定损失相结合。

#### 8. 实验设置：
本文在多个目标检测数据集上进行了实验，包括MS-COCO。数据集用于域内和域外场景。评估指标为检测期望校准误差（D-ECE），同时还使用检测器的平均精度（AP）进行评估。基线检测器采用默认设置的Deformable-DETR（D-DETR）。将提出的方法与后处理方法和最近的校准损失MDCA和MbLS进行比较。

#### 9. 实验结果与分析：
本文提出了一种目标检测的训练时校准方法，基于辅助损失函数（BPC）。该方法在提高域内和域外预测的校准性能的同时，保持了检测准确性。实验结果表明，与其他训练时校准方法相比，该方法具有更好的性能。研究了得分阈值、批量大小和随机权重初始化对提出的损失函数的影响。结果表明，该方法对得分阈值和随机初始化不太敏感，增加批量大小对检测准确性影响较小。所有实验的最佳批量大小为2。在Sim10k和BDD100k中，提出的BPC损失降低了D-ECE。可靠性图表显示，该方法提高了域内和域外场景的校准性能。在MS-COCO数据集上的定性结果表明，使用提出的损失函数训练的检测器强制准确预测更自信，不准确预测更不自信。


# Paper:670     从语音生成全面的3D人体运动



#### 1. Title: 
Generating Holistic 3D Human Motion from Speech

#### 2. Authors: 
Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, Michael J. Black

#### 3. Affiliation: 
第一作者：Max Planck Institute for Intelligent Systems, T¨ubingen, Germany

#### 4. Keywords: 
Speech-to-motion translation, 3D body motion, facial expressions, hand gestures, VQ-VAE, autoregressive model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2021_paper.html  Github: https://github.com/hongweiyi/Talk-to-Edit

#### 6. Summary : 
- (1):本文研究如何从语音中生成3D全身运动，包括身体姿势、手势和面部表情。这是一个具有挑战性的问题，因为语音和全身运动之间的关系复杂，数据稀缺，且人体形状高度可变。

- (2):过去的方法通常只关注身体的某些部分，如脸部或手部，而忽略了全身运动的协调性。本文提出了一种新的方法，将面部、身体和手部分别建模，并使用自编码器和VQ-VAE等技术来生成多样性和逼真性的全身运动。

- (3):本文提出了一种名为TalkSHOW的方法，可以从语音中自动合成逼真、多样化的全身运动。该方法使用了自编码器和VQ-VAE等技术，将面部、身体和手部分别建模，并使用交叉条件自回归模型来生成全身运动。实验结果表明，该方法在质量和多样性方面均优于现有方法。

- (4):本文的方法在语音到全身运动的转换任务上取得了最先进的性能，实验结果表明其生成的全身运动逼真、多样化且具有协调性。
#### 7. 方法详细介绍：
本文提出的TalkSHOW方法是一种语音到动作生成框架，将面部、身体和手部分别建模。对于面部部分，设计了一个简单的编码器-解码器式面部生成器，通过预训练的wav2vec 2.0来编码丰富的音素信息。对于身体和手部分，采用了基于VQ-VAE的框架来预测非确定性的身体和手部运动，学习了一个组合量化的运动空间，能够有效地捕捉多样的运动。通过学习到的离散表示，提出了一种新颖的自回归模型来预测未来运动的多项式分布，同时在现有运动之间进行交叉条件，从而生成一系列代表连贯姿势的运动模式，实现了逼真的运动生成。

#### 8. 实验设置：
作者提出了一个新的高质量的音视频数据集，包括30fps的表情丰富的3D身体网格和22K采样率的同步音频。通过在野外单目视频中重建3D身体网格，作为语音到运动生成的伪地面真实数据。该数据集由不同人的野外说话视频构建而成，包括各种说话风格。数据集被过滤以删除低分辨率视频、遮挡手或无效下载链接，最终得到了一个高质量的数据集，包括4个说话者的26.9小时。为了进行小批量处理，原始视频被裁剪成短片段（<10秒）。

#### 9. 实验结果和分析：
本文提出的方法在多项指标上均优于Habibie等人的方法，包括L2距离、Landmark Velocity Difference、逼真度得分和变化度。与基于VAE的模型相比，本文提出的方法在逼真度和多样性方面均取得了显著的提高，尤其是在多样性方面。学习到的组合量化运动码本有效地记忆了身体和手部的多种运动模式，从而提高了生成的身体和手势的多样性。从语音中生成的3D整体身体运动的示例展示了本文提出的方法在生成与输入音频节奏一致的多样化运动方面的有效性。本文提出的方法还能够生成具有准确唇形的面部表情。


# Paper:671     手动陷阱的单目深度估计Trap Attention



#### 1. Title: 
Trap Attention: Monocular Depth Estimation with Manual Traps

#### 2. Authors: 
Chao Ning, Hongping Gan

#### 3. Affiliation: 
Chao Ning: Northwestern Polytechnical University (西北工业大学)
Hongping Gan: Northwestern Polytechnical University (西北工业大学)

#### 4. Keywords: 
Monocular depth estimation, attention mechanism, trap attention, deep-wise convolution, vision transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ning_Trap_Attention_Monocular_Depth_Estimation_With_Manual_Traps_CVPR_2021_paper.html  Github: https://github.com/ICSResearch/TrapAttention

#### 6. Summary : 
- (1):本文研究单目深度估计的问题，提出了一种新的注意力机制——Trap Attention，用于提高深度估计的精度和效率。
 
- (2):传统的深度估计方法通常只适用于预测低维度、稀疏距离或已知的固定目标，而深度学习方法则需要更复杂或更大规模的模型。然而，这种方法会使深度估计任务变成一个简单的模型规模问题，而没有考虑性能和计算预算之间的平衡。最近，一些研究引入了多头注意力（MHA）模块来执行长距离交互，这在回归深度图方面取得了显著进展。然而，由于MHA的二次计算复杂度，这些方法通常不能利用MHA以适当的计算复杂度计算高分辨率的深度特征。因此，本文提出了一种深度卷积来获取长距离信息，并提出了一种新的Trap Attention机制，通过卷积窗口的特征保留比率在扩展空间上设置一些陷阱，形成注意力机制，从而将二次计算复杂度转换为线性形式。 

- (3):本文提出了一种新的Trap Attention机制，用于单目深度估计。该机制通过在扩展空间上设置一些陷阱，形成注意力机制，从而将二次计算复杂度转换为线性形式。同时，本文还构建了一个编码器-解码器Trap深度估计网络，其中引入了一个视觉变换器作为编码器，并在解码器中使用Trap Attention来从单个图像中估计深度。实验结果表明，本文提出的网络在NYU Depth-v2和KITTI数据集上的单目深度估计方面优于现有方法，并且具有显著降低的参数数量。

- (4):本文提出的Trap Attention机制和Trap深度估计网络在单目深度估计任务上取得了最新的最佳性能，同时具有更高的计算效率和更少的参数数量。
#### 7. 方法详细介绍：
本文提出了一种用于单目深度估计的新方法，使用深度可分离卷积层来捕获远距离信息，并引入了一种名为Trap Attention的注意力机制来计算特征之间的关系。Trap Attention利用各种手动陷阱来移除扩展空间中的一些特征，并利用3x3卷积窗口计算注意力图。该方法的编码器-解码器深度估计网络采用Vision Transformer作为编码器，使用Trap Attention从单个图像中解码深度。解码器将深度特征从粗到细分为五个阶段进行细化。

#### 8. 实验设置：
本文在三个数据集上进行了实验：NYU、KITTI和SUN RGB-D。使用了Adam优化器，学习率为1e-4，批量大小为16。在NYU数据集上，使用了中心裁剪和随机裁剪两种数据增强方式。在KITTI数据集上，使用了随机缩放和随机裁剪两种数据增强方式。在SUN RGB-D数据集上，使用了随机缩放和随机裁剪两种数据增强方式。本文使用了八个标准指标来评估模型的性能，包括五个误差指标和三个准确度指标。

#### 9. 实验结果和分析：
本文提出的深度估计网络在NYU和KITTI两个流行数据集上取得了最先进的成绩，且仅使用了先前最先进方法的35%的参数。Trap-L模型将“Abs Rel”、“RMSE”和“log 10”误差分别降低了11.3%、13.0%和10.3%，相比于SUN RGB-D数据集上的最先进方法。消融实验表明，Trap Attention、Trap Interpolation和Block Selection可以提高所提出的Trap网络的性能。四个手动Trap函数优于ReLU非线性、GELU非线性、SiLU非线性或四个学习权重的替代函数。本文还提供了与最先进方法在KITTI数据集上的定性比较，展示了在预测各种目标的深度方面更强的性能。


# Paper:672     NeRFVS: 利用几何支架的神经辐射场实现自由视角综合



#### 1. Title: 
NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds

#### 2. Authors: 
Chen Yang, Peihao Li, Zanwei Zhou, Shanxin Yuan, Bingbing Liu, Xiaokang Yang, Weichao Qiu, Wei Shen

#### 3. Affiliation: 
第一作者：上海交通大学人工智能研究院

#### 4. Keywords: 
Neural Radiance Fields, Free View Synthesis, Geometry Scaffolds, Holistic Priors, Depth Loss, Variance Loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的背景是室内场景的自由导航，需要实现高保真度的渲染。
- (2):过去的方法包括基于图像的渲染、结构光、多视角立体视觉等，但这些方法在室内场景的自由导航中存在一些问题，如低纹理区域、复杂场景几何和光照变化等。本文提出了一种新的方法，利用神经辐射场（NeRF）和几何支架来实现室内场景的自由导航，通过综合先验信息来提高渲染质量。
- (3):本文提出了一种新的方法NeRFVS，通过利用神经重建生成几何支架，利用伪深度图和视角覆盖信息来指导NeRF的学习，提高其在低纹理和少样本区域的渲染质量。具体来说，本文提出了一个鲁棒的深度损失和一个方差损失来约束深度期望和密度分布，以减少渲染中的浮点和模糊。这两个损失根据视角覆盖信息进行调节，以减少视角不平衡带来的负面影响。实验结果表明，本文的方法在室内场景自由导航方面取得了最新的性能表现。
- (4):本文的方法在室内场景自由导航任务中取得了最新的性能表现，通过综合先验信息来提高渲染质量，具有一定的实用性和推广价值。
#### 7. 方法详细介绍：
本文提出了一种名为NeRFVS的方法，用于从一组RGB图像和相机参数中生成室内场景的自由视角合成。该方法包括以下步骤：
1. 使用现成的几何重建方法生成几何支架。
2. 将几何支架中的场景先验信息整合到神经辐射场的优化中，使用鲁棒深度损失来指导NeRF的几何学习。
3. 对预测的密度分布和颜色值沿每条射线进行方差正则化，以减少学习过程中的几何和颜色歧义。
4. 根据每个输入视图的视角覆盖情况，提出了一种基于视角覆盖的训练策略，以相应地调整深度和方差损失的强度。
5. 每条射线的总损失是上述所有损失的线性组合，其中包括损失权重。

#### 8. 实验设置：
本文在两个数据集上进行了实验：Barbershop和ScanNet。Barbershop数据集包含543张以人类般的轨迹拍摄的图像，而ScanNet数据集是一个大规模的室内数据集，包含1613个场景。对于ScanNet数据集，选择训练帧之间的中间帧作为插值的nvs帧，并选择与训练集中不同的轨迹进行外推。训练、插值和外推的比例为2:1:1。

#### 9. 实验结果和分析：
本文在Barbershop和ScanNet数据集上展示了所提出方法的定量和定性结果，并将其与其他最先进的方法进行了比较。结果表明，所提出的方法在PSNR、SSIM和LPIPS指标上均优于其他方法。本文还展示了所提出的方法可以处理纹理较低和观测较少的挑战性场景。


# Paper:673     ACSeg：自适应概念化的无监督语义分割



#### 1. Title: 
ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation

#### 2. Authors: 
Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian Zhao, Guoli Song, Chang Liu, Li Yuan, Jie Chen

#### 3. Affiliation: 
第一作者：北京大学深圳研究生院电子与计算机工程学院

#### 4. Keywords: 
Unsupervised Semantic Segmentation, Self-supervised Learning, Visual Transformer, Adaptive Conceptualization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Li_ACSeg_Adaptive_Conceptualization_for_Unsupervised_Semantic_Segmentation_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是无监督语义分割任务，通过自监督学习的方式，利用大规模视觉预训练模型提取像素级别的语义关系，进而提高无监督密集预测任务的性能。

- (2):过去的方法主要是基于像素级别的自监督表示学习，但是在不同图像的语义分布下，过度聚类或欠聚类会使概念化过程不可行。本文提出了一种自适应概念化方法，即ACSeg，通过将概念显式编码为可学习的原型，并设计自适应概念生成器（ACG），将这些原型自适应地映射到每个图像的信息概念中。同时，考虑到不同图像的场景复杂性，提出了模块化损失来优化ACG，从而实现了无监督发现概念的语义分割任务。

- (3):本文提出的ACSeg方法通过自适应概念生成器（ACG）实现了精确的概念化，ACG通过在原型和像素级别表示之间迭代应用缩放点积注意力来实现，从而学习将初始原型投影到表示空间中的概念。然后，通过将每个像素分配给表示空间中最近的概念，将这些概念明确地呈现在图像中的不同区域。ACG是端到端优化的，不需要任何注释，通过提出的模块化损失来实现。最终，ACSeg将无监督语义分割任务转化为以无监督方式分类发现的概念。

- (4):在PASCAL VOC 2012和COCO-Stuff等常用语义分割数据集上进行了广泛的实验，结果表明，ACSeg方法在不同设置的无监督语义分割任务上均优于以前的方法，并在PASCAL VOC 2012无监督语义分割基准测试中实现了最先进的性能。此外，像素级别表示和概念的可视化显示，ACG适用于分解具有各种场景复杂性的图像。
#### 7. 方法详细介绍：
本文提出了一种名为ACSeg的无监督语义分割方法。ACSeg使用自监督的ViT模型提取像素级和区域级表示。Adaptive Concept Generator (ACG)是ACSeg的核心组件，它将可学习的原型作为输入，通过与像素级表示交互来逐步更新原型，并将概念显式地表示为表示空间中最近概念的像素组。为了优化ACG，本文提出了一种称为模块化损失的新型损失函数，可以在没有任何注释的情况下训练ACG。最后，概念分类器将每个概念分配给预定义的类别，以获得图像的像素级类别预测，即图像的语义分割。

#### 8. 实验设置：
本文在PASCAL VOC 2012和COCO-Stuff数据集上进行了实验。实验在单个NVIDIA Tesla V100 GPU上进行，输入图像大小为512x512，批量大小为8。学习率设置为1e-4，总训练时期设置为200。

#### 9. 实验结果和分析：
本文提出的ACSeg在PASCAL VOC 2012无监督语义分割基准测试中实现了最先进的性能，无需后处理和重新训练。像素级表示和概念的可视化显示，ACG适用于分解具有各种场景复杂性的图像。ACG快速收敛，无需学习新的表示，概念分类器以零样本的方式使用。


# Paper:674     基于视觉的度量级跨视图地理定位中的不确定性感知



#### 1. Title: 
Uncertainty-aware Vision-based Metric Cross-view Geolocalization

#### 2. Authors: 
Florian Fervers, Sebastian Bullinger, Christoph Bodensteiner, Michael Arens, Rainer Stiefelhagen

#### 3. Affiliation: 
第一作者：Fraunhofer IOSB

#### 4. Keywords: 
Cross-view geolocalization, vision-based, uncertainty-aware, aerial images, end-to-end model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Fervers_Uncertainty-Aware_Vision-Based_Metric_Cross-View_Geolocalization_CVPR_2022_paper.html  Github: https://github.com/fferflo/vismetcvgl

#### 6. Summary : 
- (1):本文提出了一种新的方法，利用地面车辆捕获的相机图像与航空图像匹配，确定车辆的地理姿态，实现基于视觉的度量级跨视图地理定位（CVGL）。由于航空图像具有全球性、低成本的优势，它们代表了自动驾驶的两种已建立的范例之间的潜在折衷方案，即使用昂贵的高清先前地图或完全依赖于运行时捕获的传感器数据。

- (2):以往的方法需要训练数据来实现合理的定位精度，而本文的方法克服了这一限制，即使在严格更具挑战性的跨区域情况下，也能超越以往的结果。本文的方法不仅不需要来自测试区域的地面或航空数据，而且即使在完全不同的数据集上进行训练和测试，也能取得良好的效果。本文提出了一种端到端可微分模型，使用地面和航空图像预测可能的车辆姿态的概率分布。本文还提出了一种伪标签方法，以产生更准确的地面真实姿态，并使其公开可用。

- (3):本文提出了一种基于视觉的度量级跨视图地理定位（CVGL）的新方法，该方法将地面车辆捕获的相机图像与航空图像匹配，以确定车辆的地理姿态。本文提出了一种端到端可微分模型，使用地面和航空图像预测可能的车辆姿态的概率分布。本文的方法克服了以往方法需要训练数据来实现合理的定位精度的限制，即使在严格更具挑战性的跨区域情况下，也能超越以往的结果。本文的方法不仅不需要来自测试区域的地面或航空数据，而且即使在完全不同的数据集上进行训练和测试，也能取得良好的效果。

- (4):本文的方法在Ford AV和KITTI-360数据集上表现出色，即使没有来自测试区域的地面或航空数据，也能超越以往的结果。本文的方法预测可能的车辆姿态的概率分布，而不是单个姿态，这有助于跟踪器在时间上确定车辆的轨迹。本文的方法在KITTI-360上的平均位置误差为0.78m。
#### 7. 方法详细介绍：
本文提出了一种端到端可微的模型，用于基于视觉的度量跨视图地理定位（CVGL）。该模型使用一个迭代构建的鸟瞰图（BEV）地图来聚合来自地面视角的透视图（PV）的信息，以构建车辆周围的局部环境。BEV地图与航拍图像匹配，以预测车辆的相对姿态，包括二维平移和一维旋转。该模型预测可能车辆姿态的软概率分布，而不是单个姿态，这有利于使用模型预测来确定车辆随时间的轨迹。该方法在Ford AV和KITTI-360数据集上的度量CVGL任务中优于先前的方法，甚至超过了使用相机输入和激光雷达传感器的相关方法。

#### 8. 实验设置：
本文使用现有的自动驾驶数据集和几个正射影像提供者的航拍图像构建了一个包含105万个地面数据帧的组合数据集，每个数据帧包含车辆的地面真实姿态和相机图像和内部和外部参数。将帧分组为大小为100m×100m的不相交单元格，以测量数据集的覆盖范围，结果包含至少一个地面帧的5.1千个单元格和13.0千个单元格（计算多个正射影像提供者）。

#### 9. 实验结果和分析：
本文的方法在Ford AV和KITTI-360数据集上的度量CVGL任务中优于先前的方法，即使在更具挑战性的跨区域和跨车辆条件下也是如此。使用50m的搜索半径，即使在没有方向信息的情况下（最多30◦噪声），在所有六个轨迹上的中位位置误差为0.87m。使用卡尔曼滤波器的跟踪方法在KITTI-360数据集上的所有场景中的平均位置误差为0.78m，超过了最近的基于激光雷达和视觉的方法。消融研究表明，所提出的方法的所有组件都提高了模型的性能。


# Paper:675     即时神经体积渲染：从单目RGBD流中进行人-物交互



#### 1. Title: 
Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream

#### 2. Authors: 
Yuheng Jiang, Kaixin Yao, Zhuo Su, Zhehao Shen, Haimin Luo, Lan Xu

#### 3. Affiliation: 
上海科技大学

#### 4. Keywords: 
neural rendering, volumetric capture, human-object interaction, monocular RGBD, novel view synthesis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2021_paper.html  Github: https://github.com/nowheretrix/Instant-NVR

#### 6. Summary : 
- (1):本文研究的是从单目RGBD流中进行人-物交互的立体追踪和渲染的问题。传统的非刚性追踪和最近的即时辐射场技术之间存在巨大的差距，本文提出了Instant-NVR，一种使用单个RGBD相机进行即时体积人-物追踪和渲染的神经方法。 

- (2):传统的高端解决方案需要密集的相机进行高保真重建，而最近的方法通过使用体积跟踪技术，需要较少的RGB或RGBD视频输入。然而，多视图设置对于消费级日常使用仍然不理想。相比之下，单目方法更实用和有吸引力。对于单目人-物建模，大多数方法跟踪对象和人的刚性和骨骼运动，使用预扫描的模板或参数模型。然而，它们无法生成逼真的外观结果，受限于有限的几何分辨率。最近的神经渲染进展，如NeRF，已经实现了密集视图监督下的逼真渲染。然而，它们依赖于繁琐和耗时的场景训练，因此不适合像远程存在感这样的即时应用。只有最近的Instant-NGP才能在几秒钟内实现快速辐射场生成，为即时辐射场建模带来了可能性。然而，原始的Instant-NGP只能处理静态场景。很少有研究人员探索单目设置下的人-物交互的即时神经渲染策略。

- (3):本文提出了Instant-NVR，一种使用单个RGBD相机进行即时体积人-物交互场景渲染的神经方法。我们采用多线程和跟踪-渲染机制，将传统的体积非刚性追踪与即时辐射场技术相结合。在跟踪前端，我们采用了一个强大的人-物捕捉方案，提供足够的运动先验。我们进一步引入了一个分离的即时神经表示，具有新颖的混合变形模块，用于交互场景。我们还通过高效的运动先验搜索提供了动态/静态辐射场的即时重建方案。此外，我们引入了一个在线关键帧选择方案和一个渲染感知的细化策略，以显著提高在线新视角综合的外观细节。

- (4):本文的主要贡献包括：1）我们提出了第一个从RGBD传感器中进行人-物交互的即时神经渲染系统。2）我们引入了一种通过跟踪-渲染机制使用运动
#### 7. 方法详细介绍：
本文提出了一种名为Instant-NVR的方法，它是一种使用单个RGBD相机进行即时体积人物-物体跟踪和渲染的神经方法。该方法通过多线程跟踪-渲染机制，将传统的非刚性跟踪与最近的即时辐射场技术相结合。跟踪前端采用了一个强大的人物-物体捕捉方案，以提供足够的运动先验。渲染后端重建了交互场景的辐射场，以提供具有照片般逼真度的即时新视角合成。该方法分为两个阶段：跟踪前端捕捉人物和物体的运动，而渲染后端则分别在运行时重建人物-物体的辐射场，以提供具有照片般逼真度的即时新视角合成。跟踪前端利用现成的即时分割来区分人物和物体，并采用高效的非刚性跟踪方案来模拟人物的运动。对于渲染后端，它采用了分离的即时神经表示法。动态人物和静态物体都被表示为隐式辐射场，并在规范空间中使用多尺度特征哈希共享体积渲染以进行新视角合成。对于动态人物，它还引入了混合变形模块，以有效地利用非刚性运动先验。

#### 8. 实验设置：
本文在各种具有挑战性的人物-物体交互场景下对所提出的方法进行了评估。实现是基于Instant-NGP，使用两个Nvidia GeForce RTX3090 GPU进行的。一个GPU用于跟踪前端，另一个用于神经渲染。变形网络的输入是32维哈希特征和72维姿态。隐藏层为4，隐藏维度为128。使用了以下经验确定的参数：βvis = 0.01，βh = 0.02，βd = 1.0，βo = 0.02，γ = 2.5，λdata = 1.0，λbind = 1.0，λreg = 4.0，λprior = 0.01，λpose = 0.02，λinter = 1.0。为了提高效率，在空间-时间池中选择了m = 10。

#### 9. 实验结果与分析：
本文提出的方法在效率和渲染质量方面与RobustFusion、NeuralHOFusion、NeuralBody和HumanNerf等最先进的方法进行了比较。实验结果表明，所提出的方法在渲染质量和训练效率以及渲染速度方面均表现出更好的性能，以支持即时性能。定量评估表明，所提出的方法在人物-物体交互下可以实现更详细和照片般逼真的渲染结果。本文还对在线人物渲染和在线物体渲染进行了评估，结果表明所提出的关键帧选择策略和渲染细化方案可以实现更清晰和更准确的渲染结果。运行时间评估表明，所提出的方法是高效的，跟踪前端需要40ms来进行物体的刚性跟踪和62ms来进行人物的非刚性跟踪，而渲染后端需要15.38ms进行渲染处理。混合变形模块也进行了评估，结果表明利用隐式变形网络的变形块显著增强了纹理和对齐。

#### 论文总结：
本文提出了一种名为Instant-NVR的方法，用于即时神经体积渲染人物-物体交互。该方法采用了一种混合变形模块，以有效地利用非刚性运动先验。该系统采用密度场作为几何表示，以实现即时的逼真渲染。分离的即时神经表示法与混合变形和高效的运动先验搜索相结合，使得动态和静态辐射场的即时重建成为可能。在线关键帧选择和渲染感知的细化策略提供了更生动和详细的新视角合成，以支持在线设置。


# Paper:676     HandNeRF：用于可动交互手的神经辐射场



#### 1. Title: 
HandNeRF: Neural Radiance Fields for Animatable Interacting Hands

#### 2. Authors: 
Zhiyang Guo, Wengang Zhou, Min Wang, Li Li, Houqiang Li

#### 3. Affiliation: 
中国科学技术大学电子工程与信息科学系

#### 4. Keywords: 
Neural Radiance Fields, Hand Modeling, Interacting Hands, Pose-Driven Deformation Field, Depth-Guided Density Optimization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在提出一种新的框架，使用神经辐射场（NeRF）对交互手进行准确的外观和几何重建，从而实现从任意视角渲染手势动画的照片逼真的图像和视频合成。

- (2):传统的手模型主要建立在参数化网格模型上，这些模型具有以下局限性：低分辨率网格难以呈现高频细节，而且没有为交互手设计特殊的模型。本文提出的方法使用NeRF统一建模手的几何和纹理，通过引入手映射和射线合成策略，使其能够自然地处理交互接触，并补充两只手中很少观察到的几何和纹理线索。

- (3):本文提出了一种姿态驱动的变形场来建立不同姿态之间的对应关系，将不同的输入姿态映射到一个共同的平均姿态，然后优化一个规范化的NeRF来建模。同时，本文还利用姿态先验生成伪深度图，作为遮挡感知密度学习的指导。此外，还提出了一种神经特征蒸馏方法，以实现颜色优化的跨域对齐。

- (4):本文在大规模InterHand2.6M数据集上进行了广泛的实验，验证了所提出的HandNeRF的优点，并在质量和数量上报告了一系列最新的结果。
#### 7. 方法详细介绍：
本文提出了一种名为HandNeRF的方法，用于建模多视角RGB视频中的交互手部动态场景。该方法使用变形场来解耦两只手的姿态，并使用NeRF优化共享的规范化手部。为了确保正确的深度关系，建立了密度优化的深度监督。提出了一种特征蒸馏框架，用于从RGB图像中挖掘有用的线索以进行更好的纹理学习。变形场使用参数化的人体先验和姿态条件的纠错网络进行建模。提出了一种统一的建模规范空间，以便在训练期间在纹理缺失或很少观察到的区域中为左右手提供几何和纹理的补充。引入了一种采样和合成策略，以在规范化的NeRF仅模拟单只手的情况下呈现两只交互的手。

#### 8. 实验设置：
本文在包含各种手部姿势的大规模多视角序列的Interhand2.6M数据集的30FPS版本上训练和评估了所提出的HandNeRF模型。数据集经过预处理，选择18个常见视角作为测试视角。为了进行比较，开发了三个基线，包括Pose-NeRF、Ani-NeRF和NeuMan。评估指标包括峰值信噪比（PSNR）、结构相似性指数（SSIM）、学习感知图像补丁相似性（LPIPS）以及渲染深度图的平均L1误差（DE）。

#### 9. 实验结果与分析：
所提出的HandNeRF模型在新视角合成和新姿态合成任务中的表现均优于所有基线。即使使用极其稀疏的视图进行训练，HandNeRF仍然可以在PSNR方面实现与交互手相匹配的29dB。HandNeRF中提出的深度引导密度优化有效地减轻了围绕渲染手部的半透明雾气问题。提出的颜色方差损失解决了目标手部上的所有像素都收敛到相同均值颜色的问题。在少量样本上进行的姿势自适应显著减少了渲染图像中的伪影和几何误差。


# Paper:677     NICO++：面向领域泛化的更好基准评估



#### 1. Title: 
NICO++: Towards Better Benchmarking for Domain Generalization

#### 2. Authors: 
Xingxuan Zhang, Yue He, Renzhe Xu, Han Yu, Zheyan Shen, Peng Cui

#### 3. Affiliation: 
清华大学计算机科学与技术系

#### 4. Keywords: 
Domain generalization, benchmark, covariate shift, concept shift, evaluation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2021_paper.html  Github: https://github.com/xxgege/NICO-plus

#### 6. Summary : 
- (1):本文研究领域泛化问题，即在训练数据与测试数据分布不同的情况下，如何让模型具有更好的泛化能力。目前的领域泛化数据集数量有限，评估方法也不够全面，因此需要更好的数据集和评估方法来推动领域泛化研究的进展。

- (2):目前的领域泛化数据集数量有限，评估方法也不够全面，因此需要更好的数据集和评估方法来推动领域泛化研究的进展。本文提出了一个大规模的领域泛化数据集NICO++，并提出了两个新的评估指标来量化协变量偏移和概念偏移。通过实验证明，NICO++在评估能力上优于当前的领域泛化数据集，并且可以缓解模型选择中的不公平问题。

- (3):本文提出了一个大规模的领域泛化数据集NICO++，包含80个类别、10个公共域和10个特定域，以及超过230,000张图像。为了更好地评估领域泛化数据集，本文提出了两个新的评估指标来量化协变量偏移和概念偏移，并提出了两个新的泛化界限来证明有限的概念偏移和显著的协变量偏移有利于泛化评估能力。此外，本文还提出了两个新的评估协议来支持对齐和灵活的域跨类别的评估。

- (4):本文在NICO++数据集上进行了大量实验，证明了NICO++在评估能力上优于当前的领域泛化数据集，并且可以缓解模型选择中的不公平问题。此外，本文还对当前代表性的领域泛化方法进行了评估，并展示了这些方法在NICO++上的改进空间。
#### 7. 方法详细介绍：
本文提出了一个新的领域泛化基准数据集NICO++，该数据集包含80个类别、10个公共域和10个特定于每个类别的唯一域。为了更好地评估模型的泛化能力，本文提出了两个评估指标：协变量偏移和概念偏移。协变量偏移衡量训练和测试域之间输入特征分布的差异，而概念偏移衡量训练和测试域之间标签函数的差异。本文还提供了基于这些指标的期望损失的理论界限。本文还提出了一种基于泄漏的方法来减少测试方差和降低可能的改进。NICO++在各种随机种子和训练时期下表现出比其他数据集更低的测试方差，表明对于与算法无关的超参数的选择，具有更稳定的泛化能力估计。

#### 8. 实验设置：
本文提出了两种评估设置：经典领域泛化和灵活领域泛化，以评估DG算法的性能。本文提出了三个评估指标：平均准确率、总体准确率和跨域准确率的标准差，以评估模型的泛化能力。实验在NICO++数据集和其他现有的DG数据集上进行。

#### 9. 实验结果和分析：
本文在NICO++数据集上评估了各种领域泛化方法的性能。结果表明，当前最先进的方法如EoA、CORAL和StableNet是有效的，但它们与oracle之间仍存在显着差距。本文还展示了灵活领域泛化设置在NICO++上的结果，其中模型在类别和域的组合上进行测试。各种方法的平均性能在随机和组合设置下进行了报告。表4显示了不同数据集上不同种子和训练时期的标准差。


# Paper:678     UniDexGrasp：通过学习多样化的提议生成和目标条件策略实现通用机器人灵巧抓取



#### 1. Title: 
UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy

#### 2. Authors: 
Yinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, Li Yi, He Wang

#### 3. Affiliation: 
第一作者：北京大学前沿计算研究中心、北京大学电子工程与计算机科学学院

#### 4. Keywords: 
Robotic grasping, dexterous grasping, goal-conditioned policy, generative model, normalizing flow

#### 5. Paper: https://pku-epic.github.io/UniDexGrasp/  Github: https://github.com/PKU-EPIC/UniDexGrasp

#### 6. Summary : 
- (1):本文研究的是机器人灵巧抓取的问题，旨在实现对各种物体的高质量、多样化抓取，并且能够泛化到数百种物体类别和未知物体。 
- (2):过去的方法主要集中在平行夹具抓取上，而平行夹具抓取的灵活性较低，难以实现复杂的物体操作。而本文提出的方法将灵巧抓取分为两个阶段：抓取提议生成和目标条件下的抓取执行。在抓取提议生成阶段，本文提出了一种新的概率模型，能够根据点云观测生成多样化和高质量的灵巧抓取姿态。在抓取执行阶段，本文提出了一种目标条件下的抓取策略，能够根据抓取目标姿态实现物体的抓取。本文的方法在灵巧抓取泛化性方面取得了显著的成果，能够实现对数千个物体实例的抓取，成功率超过60%。
- (3):本文提出了一种新的概率模型，能够根据点云观测生成多样化和高质量的灵巧抓取姿态。该模型将灵巧抓取姿态分解为旋转、平移和关节角度三个部分，分别使用了GraspIPDF和GraspGlow两个模型进行建模。在抓取执行阶段，本文提出了一种目标条件下的抓取策略，能够根据抓取目标姿态实现物体的抓取。为了解决强化学习算法在视觉输入下的困难，本文引入了教师-学生学习框架，先训练一个能够访问oracle状态的教师模型，再将其蒸馏到只能访问真实输入的学生模型中。 
- (4):本文的方法在灵巧抓取泛化性方面取得了显著的成果，能够实现对数千个物体实例的抓取，成功率超过60%。这一性能支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种通用的机器人灵巧抓取方法UniDexGrasp，包括三个阶段：抓取提议生成、测试时基于接触的优化和目标条件下的灵巧抓取策略。抓取提议生成阶段包括GraspIPDF、GraspGlow和ContactNet。测试时基于接触的优化阶段使用ContactNet来调整不完美的抓取。目标条件下的灵巧抓取策略阶段包括教师策略和基于视觉的学生策略，分别通过模型无关的强化学习和DAgger学习。本文还引入了状态规范化、物体课程学习和一种新的目标条件下的奖励函数来提高方法的性能。

#### 8. 实验设置：
本文使用了一个大规模的桌面数据集，包含133个物体类别的5519个物体实例的超过一百万个有效抓取。这些对象被分成三组：训练实例、已见类别未见实例和未见类别实例。本文还将所提出的方法与GraspTTA、DDG、ReLie和ProHMR等基线进行了比较。

#### 9. 实验结果和分析：
本文提出的方法在成千上万个物体实例上实现了通用泛化，成功率平均超过60％，明显优于所有基线，同时仅显示最小的泛化差距。抓取提议生成阶段展现了高多样性，同时保持了最高的抓取质量。整个灵巧抓取流程，从视觉到策略，展现了在仿真环境中卓越的性能，并首次展示了具有超过60％成功率的通用抓取策略。


# Paper:679     BundleSDF：神经6-DoF跟踪和未知物体的3D重建



#### 1. Title: 
BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects

#### 2. Authors: 
Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Müller, Alex Evans, Dieter Fox, Jan Kautz, Stan Birchfield

#### 3. Affiliation: 
NVIDIA（英伟达）

#### 4. Keywords: 
6-DoF tracking, 3D reconstruction, neural object field, pose graph optimization, monocular RGBD video

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2021_paper.html  Github: https://bundlesdf.github.io/

#### 6. Summary : 
- (1):本文研究的是从单目RGBD视频序列中进行未知物体的6-DoF跟踪和3D重建的方法。这是计算机视觉中的两个基本问题，解决这些问题将在增强现实、机器人操作、学习演示和模拟到现实的转换等领域中解锁广泛的应用。
 
- (2):以往的方法通常将这两个问题分开考虑。例如，神经场景表示已经在从真实数据中创建高质量的3D对象模型方面取得了巨大成功，但这些方法假设已知相机姿态和/或地面真实对象掩码。此外，由于动态移动的相机捕捉静态物体，因此无法进行完整的3D重建。另一方面，6-DoF物体姿态估计和跟踪方法通常需要先预先训练或在线模板匹配的纹理3D模型。虽然类别级别的方法可以在同一类别的新对象实例之间实现泛化，但它们难以处理分布不均的对象实例和看不见的对象类别。为了克服这些限制，本文提出了一种联合解决这两个问题的方法。该方法假设物体是刚性的，并且在视频的第一帧中需要一个2D对象掩码。除此之外，物体可以在视频中自由移动，甚至经历严重的遮挡。本文的方法类似于物体级SLAM中的先前工作，但放宽了许多常见的假设，允许我们处理遮挡、反光、缺乏视觉纹理和几何线索以及突然的物体运动。

- (3):本文提出了一种在线姿态图优化过程、并发神经对象场以重建3D形状和外观以及内存池以促进两个过程之间的通信的关键。该方法的关键是同时学习的神经对象场，以及姿态图优化过程，以便将信息稳健地累积到一致的3D表示中，捕捉几何和外观。我们的方法处理具有大姿态变化、部分和完全遮挡、无纹理表面和反光亮点的具有挑战性的序列。我们在HO3D、YCBInEOAT和BEHAVE数据集上展示了结果，证明了我们的方法明显优于现有方法。

- (4):本文提出的方法可以在单目RGBD视频序列中进行未知物体的6-DoF跟踪和3D重建。该方法可以处理具有大姿态变化、部分和完全遮挡、无纹理表面和反光亮点的具有挑战性的序列。在HO3D、YCBInEOAT和BEHAVE数据集上的实验结果表明，我们的方法明显优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种名为BundleSDF的方法，用于神经6-DoF跟踪和未知物体的三维重建。该方法包括以下步骤：
1. 通过几何函数和外观函数表示物体。
2. 通过有符号距离场取零级集来获取隐式物体表面。
3. 通过发射光线并通过近表面区域积分光线的颜色来进行渲染。
4. 训练可调参数，包括多分辨率哈希编码器、几何函数、外观函数和李代数参数化的切空间中的物体姿态更新。
5. 训练损失包括多个项，如不确定自由空间、空空间、近表面空间、前景颜色和近表面空间中SDF的Eikonal正则化。

#### 8. 实验设置：
本文在三个真实世界数据集（HO3D、YCBInEOAT和BEHAVE）上评估了所提出的方法。评估指标包括6-DoF物体姿态的ADD和ADD-S度量的曲线下面积（AUC）百分比以及三维形状重建的最终重建网格与地面真值网格之间的Chamfer距离。每个评估方法的输入都是RGBD视频和第一帧的掩码，指示感兴趣的物体。比较方法包括DROID-SLAM（RGBD）、NICE-SLAM、KinectFusion、BundleTrack和SDF-2-SDF。

#### 9. 实验结果和分析：
所提出的方法在所有三个数据集上的6-DoF物体姿态和3D形状重建方面均优于比较方法。ADD和ADD-S度量的AUC百分比显着更高，Chamfer距离显着更低。所提出的方法还展示了对遮挡和动态场景的鲁棒性。


# Paper:680     双像素图像的空间-焦点双向视差估计



#### 1. Title: 
Spatio-Focal Bidirectional Disparity Estimation from a Dual-Pixel Image


#### 2. Authors: 
Donggun Kim, Hyeonjoong Jang, Inchul Kim, Min H. Kim


#### 3. Affiliation: 
KAIST (韩国科学技术院)


#### 4. Keywords: 
Dual-pixel photography, bidirectional disparity, self-supervised learning, anisotropic blur kernels, depth estimation


#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_Spatio-Focal_Bidirectional_Disparity_Estimation_From_a_Dual-Pixel_Image_CVPR_2021_paper.html  Github: None


#### 6. Summary : 
- (1):本文研究的是双像素图像的双向视差估计问题，双像素摄影是一种具有超高分辨率的单目RGB-D摄影技术，但是由于其具有双向视差和各向异性模糊核等特点，因此存在一些挑战。

- (2):过去的方法主要是基于数据驱动的方法，但是由于缺乏双像素视差的真实数据集，这些方法只能估计逆深度或模糊度图，不能直接估计双向视差。本文提出了一种自监督学习方法，通过利用双像素摄影中各向异性模糊核的对称性质来学习双向视差，避免了需要真实数据集的问题。

- (3):本文提出的方法首先使用传统的立体网络进行预训练，然后利用双像素图像的各向异性模糊核的对称性质，采用自监督训练方法进行双向视差估计。本文的方法不需要真实的双像素视差数据集，可以估计出完整的双向视差图，并且在性能上优于基线方法。

- (4):本文的方法在双像素摄影中实现了双向视差估计，可以用于深度估计等任务，性能优于传统方法。
#### 7. 方法详细介绍：
本文提出了一种基于学习的双像素双向视差估计方法。该方法包括两个阶段：预训练和自监督学习。在预训练阶段，使用合成的立体数据集训练立体网络。在自监督学习阶段，使用双像素左右图像对微调网络。网络使用三种损失函数的加权和进行训练：核损失、光度一致性损失和平滑损失。核损失确保估计的点扩散函数与真实点扩散函数一致。光度一致性损失使用普通哈明距离和鲁棒广义Charbonnier惩罚函数的组合度量两个图像输入之间的差异。平滑损失使用双边梯度平滑损失平滑视差图，同时保留锐利的边缘不连续性。

#### 8. 实验设置：
本文使用PyTorch实现了所提出的方法，并使用多个合成立体数据集（SceneFlow、Sintel stereo、Falling Things和Tartan Air）进行预训练。自监督学习阶段使用来自Abuolaim等人的DSLR双像素图像进行微调。网络在预训练阶段进行了40k次迭代，批量大小为40，在自监督学习阶段进行了1.5k次迭代，批量大小为8。

#### 9. 实验结果与分析：
本文提出的方法在双向视差估计方面表现出色，与现有方法相比具有更好的性能和更少的估计伪影。定量评估使用光度差异的均方根误差（RMSE）进行，定性比较显示，所提出的方法产生了一致的双向视差，伪影更少。本文还对所提出的方法进行了单向评估，使用仿射不变的平均绝对误差（MAE）和均方根误差（RMSE）进行评估。所提出的方法在所有评估指标中得分最高。消融研究表明，预训练和多种损失函数对提高双向视差估计性能是有效的。


# Paper:681     DreamBooth：微调文本到图像扩散模型以进行主体驱动生成



#### 1. Title: 
DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation

#### 2. Authors: 
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman

#### 3. Affiliation: 
第一作者：Nataniel Ruiz，Google Research和Boston University

#### 4. Keywords: 
text-to-image synthesis, subject-driven generation, fine-tuning, autogenous class-specific prior preservation loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2021_paper.html  Github: https://dreambooth.github.io/

#### 6. Summary : 
- (1):本文研究背景是文本到图像合成，现有方法无法在不同场景下生成特定主体的图像。
- (2):过去的方法包括图像合成和文本到图像合成，但无法生成特定主体的图像。本文提出了一种新方法，通过微调预训练的文本到图像扩散模型，将主体嵌入模型的输出域中，从而生成不同场景下的主体图像。本方法的创新点在于使用自生类特定先验保护损失，使模型能够生成不同场景下的主体图像，同时保留其关键特征。
- (3):本文提出的方法是通过微调预训练的文本到图像扩散模型，将主体嵌入模型的输出域中，从而生成不同场景下的主体图像。本方法的创新点在于使用自生类特定先验保护损失，使模型能够生成不同场景下的主体图像，同时保留其关键特征。
- (4):本文的方法在主体驱动的生成任务上取得了良好的性能，包括主体重新定位、文本引导视图合成和艺术渲染等任务。通过用户研究，证明了本文方法生成的图像与其他方法相比，具有更高的主体和提示的保真度。
#### 7. 方法详细介绍：
本文提出了一种名为DreamBooth的模型，用于主题驱动的图像生成。该模型使用预训练的文本到图像扩散模型进行微调，使用DINO和CLIP-I主题保真度度量以及CLIP-T提示保真度度量进行评估。此外，本文还提出了一种先验保留损失（PPL），以抵抗语言漂移并保留先验。模型使用包含15个主题的数据集进行评估，包括10个重新语境化、10个装饰化和5个属性修改提示的非生物对象，以及10个重新语境化、10个装饰化和5个属性修改提示的生物主题/宠物。用户研究表明，DreamBooth在主题保真度和提示保真度方面均获得了更高的分数，并且用户更喜欢DreamBooth。本文还对PPL和类先验的有效性进行了消融研究。

#### 8. 实验设置：
本文使用了包含30个主题的数据集，包括独特的物体和宠物，如背包、填充动物、狗、猫、太阳镜、卡通等。这些图像由作者收集或从Unsplash获取。本文还收集了25个提示：20个重新语境化提示和5个属性修改提示进行评估。作者使用主题的少量数据集对文本到图像扩散模型进行微调，并评估了模型在收集的提示上的性能。

#### 9. 实验结果和分析：
本文比较了提出的DreamBooth模型与使用DINO和CLIP-I主题保真度度量以及CLIP-T提示保真度度量的文本反转模型的性能。DreamBooth在主题保真度和提示保真度方面均获得了更高的分数。用户研究表明，DreamBooth在主题保真度和提示保真度方面均获得了更高的分数，并且用户更喜欢DreamBooth。本文还讨论了该方法的局限性，包括无法准确生成提示的上下文、上下文-外观纠缠、过度拟合真实图像、主题保真度的变异性以及语义修改复杂性的变异性。


# Paper:682     ReDirTrans：用于注视和头部重定向的潜空间转换



#### 1. Title: 
ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection

#### 2. Authors: 
Shiwei Jin, Zhen Wang, Lei Wang, Ning Bi, Truong Nguyen

#### 3. Affiliation: 
第一作者：ECE Dept. UC San Diego

#### 4. Keywords: 
Gaze redirection, head orientation, latent-to-latent translation, image synthesis, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jin_ReDirTrans_Latent-to-Latent_Translation_for_Gaze_and_Head_Redirection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是学习基于注视估计的方法需要大量的训练数据，而收集和注释这些数据是复杂和昂贵的。

- (2):过去的方法包括基于变形的方法和基于生成器的方法，但这些方法都存在一些问题，如需要大量的数据和注释、生成的图像分辨率较低等。本文提出了一种新的方法，称为ReDirTrans，它可以在高分辨率的全脸图像中实现精确的注视和头部定向重定向，同时保留其他属性，如身份、表情和发型。

- (3):本文提出了一种基于条件重定向的潜空间转换框架，称为ReDirTrans，它可以将输入潜空间向量投影到仅针对目标属性的嵌入空间中，然后通过旋转矩阵乘法实现重定向过程。重定向后，原始嵌入和重定向嵌入都被解码回初始潜空间作为残差，通过减法和加法操作修改输入潜空间向量，表示删除旧状态和添加新状态。ReDirTrans只关注目标属性的嵌入变换，并通过共享权重的解投影器输出残差来实现状态替换。

- (4):本文的方法在重定向任务上取得了很好的性能，可以在高分辨率的全脸图像中实现精确的注视和头部定向重定向，同时保留其他属性，如身份、表情和发型。此外，本文还提出了一种改进的下游学习方法，使用重定向样本作为数据集增强，进一步提高了注视估计任务的性能。
#### 7. 方法详细介绍：
本文提出了一种名为ReDirTrans的可移植网络，用于将潜在向量转换为目标方向和头部方向的嵌入向量，以实现可解释的重定向。该框架将输入潜在向量投影到仅针对目标属性的嵌入向量中，并通过减法和加法操作修改输入潜在向量，以实现旧状态的删除和新状态的添加。该方法可以在预定义的特征空间中工作，与预训练的编码器-生成器对配合使用，以在所需分辨率的图像中实现重定向任务。

具体步骤如下：
1. 使用编码器将输入图像转换为特征空间F中的潜在向量。
2. 将潜在向量分解为与注视方向相关的嵌入向量z1和与头部方向相关的嵌入向量z2。
3. 使用估计条件和目标条件构建旋转矩阵，将嵌入向量规范化并重定向到新状态。
4. 通过减法和加法操作在fs上实现修改潜在向量，以获得编辑后的潜在向量。
5. 使用解码器将编辑后的潜在向量转换回图像空间。

#### 8. 实验设置：
本文使用GazeCapture训练子集来训练重定向器，并使用其测试子集、MPIIFaceGaze和CelebA-HQ来评估性能。本文还使用ST-ED使用的指标来评估不同重定向器的性能，包括重定向误差、解缠误差和LPIPS。

#### 9. 实验结果与分析：
本文比较了不同重定向器的定量性能，包括StarGAN、He等人、VecGAN、ST-ED和ReDirTrans，在GazeCapture测试子集和MPIIFaceGaze上的表现。本文还在GazeCapture和CelebA-HQ中展示了ST-ED和ReDirTrans-GAN之间的定性比较。实验结果表明，ReDirTrans相比其他最先进的方法实现了更准确的重定向和更好的LPIPS。本文还表明，重定向样本可以用作数据增强，以提高基于学习的凝视估计的下游任务的性能。


# Paper:683     通过引导轨迹扩散实现可控行人动画



#### 1. Title: 
Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion

#### 2. Authors: 
Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, Or Litany

#### 3. Affiliation: 
NVIDIA（英伟达）

#### 4. Keywords: 
Pedestrian animation, trajectory diffusion, controllability, physics-based humanoid controller

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Rempe_Trace_and_Pace_Controllable_Pedestrian_Animation_via_Guided_Trajectory_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是生成逼真的行人轨迹和全身动画，以满足用户定义的目标。
- (2):过去的方法包括基于规则的系统和基于学习的方法，但前者生成的轨迹不自然，后者的控制能力有限。本文提出了一种基于轨迹扩散模型的方法，通过测试时引导实现轨迹的可控性。该方法可以通过目标航点、速度和指定的社交群体等方式约束轨迹，同时考虑周围环境的上下文。该轨迹扩散模型与一种新型的基于物理的人形控制器相结合，形成了一个闭环的全身行人动画系统，可以在模拟环境中放置大量的人群。
- (3):本文提出了一种基于轨迹扩散模型的可控行人轨迹生成方法，该方法可以通过测试时引导实现轨迹的可控性。该方法与一种新型的基于物理的人形控制器相结合，形成了一个闭环的全身行人动画系统。该方法可以通过学习动画控制器的价值函数来指导扩散，以产生更适合特定场景的轨迹，例如避免碰撞和穿越不平坦的地形。该方法在合成数据和真实世界的行人数据上进行了评估，证明了其灵活性和可控性。
- (4):本文的方法在行人轨迹生成和全身行人动画方面取得了良好的性能，可以满足用户定义的目标。
#### 7. 方法详细介绍：
本文提出了一个可控的行人动画系统，该系统由两个组件TRACE和PACER组成。TRACE生成每个行人的多个合理未来轨迹，而PACER在物理模拟器中执行轨迹。两个组件在运行时独立训练，但在闭环反馈中运行。控制策略πPACER使用Proximal Policy Optimization（PPO）进行训练，以遵循由τs指定的2D目标轨迹。策略是基于模拟角色ht，环境特征ot，身体类型β和目标轨迹τs的状态进行条件化的。环境输入是一个大小为ot∈R64×64×3的栅格化本地高度和速度地图，为代理提供了关于其周围环境的重要信息。为了实现社交意识，附近的人形被表示为一个立方体，并在全局高度图上呈现。本文还提出使用在PACER的RL训练期间学习的价值函数来指导轨迹扩散，从而鼓励TRACE生成更易于跟随和更适合当前地形的轨迹。

#### 8. 实验设置：
本文在多个任务和地形上评估了所提出的系统。使用ORCA数据集评估在合成行人数据上训练的TRACE。数据集包含两个不同的子集：ORCA-Maps具有许多障碍物但很少有代理，而ORCA-Interact没有障碍物但有许多代理。对于真实世界的数据，本文使用ETH/UCY和nuScenes数据集。ETH/UCY数据集包含密集人群和有趣的行人动态的场景，但没有语义地图。nuScenes数据集包含常见街道设置中的20秒驾驶场景。本文使用各种指标（如Guidance Error，Collision Rates和Realism）来衡量轨迹的合理性和满足用户控制。

#### 9. 实验结果和分析：
本文在合成和真实世界的行人数据上评估了所提出的可控行人动画系统，展示了其灵活性和可靠性。结果表明，所提出的系统通过所提出的指导成功实现了所有目标，并且在指导方面与VAE优化相比具有竞争力或更好，同时保持接近真实情况的速度和加速度分布。本文还展示了所提出系统在各种设置中的灵活性，包括随机地形，障碍物和人群。在扩散过程中添加价值指导，使轨迹更易于跟随，减少失败，并改善鉴别器奖励。


# Paper:684     Sibling-Attack: 重新思考面部识别中可转移对抗攻击



#### 1. Title: 
Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition

#### 2. Authors: 
Zexin Li, Bangjie Yin, Taiping Yao, Junfeng Guo, Shouhong Ding, Simin Chen, Cong Liu

#### 3. Affiliation: 
第一作者：加州大学河滨分校

#### 4. Keywords: 
Adversarial attacks, face recognition, transferability, multi-task learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_Sibling-Attack_Rethinking_Transferable_Adversarial_Attacks_Against_Face_Recognition_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在人脸识别中的应用，以及对抗攻击的问题。

- (2):过去的方法主要是通过攻击白盒替代模型来生成可转移的对抗样本，但是这些方法在攻击商业人脸识别系统时表现不佳。本文提出了一种新的攻击方法，即Sibling-Attack，通过多任务学习的方式来提高攻击的可转移性。该方法选择与人脸识别相关的任务作为辅助任务，并通过三种优化策略来融合对抗梯度信息，从而提高攻击的可转移性。

- (3):本文提出的Sibling-Attack方法是一种基于多任务学习的攻击方法，通过选择与人脸识别相关的任务作为辅助任务，利用多个任务之间的相关性来提高攻击的可转移性。该方法采用了硬参数共享的架构来将不同任务的特征空间约束在同一空间内，并通过交替的联合任务元优化算法和跨任务梯度稳定化策略来进一步提高攻击的可转移性。

- (4):在多个数据集和商业人脸识别系统上的实验结果表明，Sibling-Attack方法在攻击成功率上显著优于现有的单任务攻击方法，特别是在攻击商业人脸识别系统时表现更加出色。
#### 7. 方法详细介绍：
本文提出了一种新的对抗攻击方法——Sibling-Attack，旨在提高面部识别模型的黑盒攻击可迁移性。该方法采用硬参数共享架构作为骨干网络，并分为两个子网络：面部识别（FR）分支和属性识别（AR）分支。联合冒名攻击的目标是生成对黑盒目标FR模型具有欺骗性的对抗样本。该方法采用了提出的联合任务元优化（JTMO）和跨任务梯度稳定化（CTGS）优化策略来进一步提高可迁移性。Sibling-Attack的整体流程如Alg. 1所示。
(1). 采用硬参数共享架构作为骨干网络。
(2). 分为面部识别（FR）分支和属性识别（AR）分支。
(3). 采用联合冒名攻击的目标生成对黑盒目标FR模型具有欺骗性的对抗样本。
(4). 采用联合任务元优化（JTMO）和跨任务梯度稳定化（CTGS）优化策略来进一步提高可迁移性。

#### 8. 实验设置：
本文在CelebA-HQ和LFW两个面部识别数据集上进行了实验，采用攻击成功率（ASR）作为冒名攻击的评估指标。选择了多个商用面部识别模型作为目标模型，包括Face++ face recognition和Microsoft face API。同时，还使用了多个现有的对抗攻击方法作为基线方法进行对比实验。

#### 9. 实验结果和分析：
Sibling-Attack方法在CelebA-HQ和LFW两个数据集上的实验结果表明，该方法在攻击成功率方面优于其他竞争方法。在IR50和ResNet101两个目标模型上，该方法在CelebA-HQ数据集上的ASR分别为93.00%和97.60%，在LFW数据集上的ASR分别为93.40%和96.80%。同时，本文还进行了消融实验和可迁移性分析，结果表明Sibling-Attack方法的每个组件都对提高ASR有显著作用，而利用辅助AR模型可以帮助生成具有强可迁移性的对抗样本。


# Paper:685     基于多标签证据学习的开放式动作识别



#### 1. Title: 
Open Set Action Recognition via Multi-Label Evidential Learning

#### 2. Authors: 
Chen Zhao, Dawei Du, Anthony Hoogs, Christopher Funk

#### 3. Affiliation: 
Chen Zhao, Dawei Du, Anthony Hoogs, Christopher Funk are affiliated with Kitware.

#### 4. Keywords: 
Open set action recognition, evidential learning, uncertainty estimation, novelty detection, deep learning.

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhao_Open_Set_Action_Recognition_via_Multi-Label_Evidential_Learning_CVPR_2021_paper.html  Github: https://github.com/charliezhaoyinpeng/mule

#### 6. Summary : 
- (1):本文研究的是开放式动作识别问题，该问题在实际应用中具有很大的潜力，但现有的方法都是基于封闭式问题的，即将人类动作分类为预定义的已知类别。开放式方法可以高精度地识别未知类别的样本，但现有的方法都是基于单个动作的识别，无法处理多个动作或多个演员的情况，因此本文提出了一种新的方法来解决这个问题。

- (2):现有的开放式动作识别方法主要集中在单个演员和单个动作的识别上，而本文提出的方法可以处理多个演员和多个动作的情况。此外，现有的方法通常使用softmax阈值来进行识别，而本文提出的Beta Evidential Neural Network可以更准确地估计多个动作的不确定性，并且可以通过Beta分布来量化预测的不确定性。

- (3):本文提出了一种名为MUlti-Label Evidential learning (MULE)的新框架，它由三个模块组成：Actor-Context-Object Relation modeling (ACO-R)、Beta Evidential Neural Network (Beta-ENN)和Multi-label Evidence Debiasing Constraint (M-EDC)。ACO-R模块用于提取演员、上下文和对象之间的关系特征，Beta-ENN模块用于估计已知动作的证据，并量化动作的预测不确定性，M-EDC模块用于减少视频动作的静态偏差。本文还提出了一种基于双重平均方案的更新算法来优化网络，并提供相应的理论分析。

- (4):本文在两个真实世界的视频数据集上进行了广泛的实验，结果表明，我们提出的方法在单个/多个演员、单个/多个动作的情况下都取得了良好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为多标签证据学习（MULE）的新方法，用于开放式动作识别和新颖性检测。该方法通过Beta证据神经网络估计基于演员-上下文-对象关系表示的多动作不确定性。为了减少视频表示的静态偏差，该方法在优化目标函数中添加了证据去偏置约束。作者开发了一种基于原始-对偶平均方案更新的学习算法来优化所提出的问题，并提供了相应的理论分析。此外，该方法还制定了基于不确定性和置信度的新颖性估计机制来检测新颖动作。

具体步骤如下：
1. 将每个视频分成相同帧长的重叠片段，并为每个输入视频片段检测N个演员和M个对象。
2. 使用现成的检测器生成演员特征和对象特征，并基于动作骨干网络获取上下文特征。
3. 为了建模演员-上下文-对象关系，该方法将所有演员特征与上下文特征连接，然后连接对象特征以形成一系列连接的特征映射。
4. 经过一系列变压器块后，该方法可以通过计算相同空间位置的特征映射对之间的高阶关系来获得演员关系特征。
5. 为了基于Beta分布分类已知和新颖动作，该方法设计了Beta证据神经网络。Beta证据神经网络使用全连接和整流激活层作为证据函数来替换最后的softmax层，并保持正面和负面证据。
6. 该方法通过计算动作预测器的贝叶斯风险来定义Beta损失函数，并在多标签证据学习中引入证据去偏置约束以减轻静态偏差。
7. 优化问题被制定为在满足去偏置约束的情况下最小化Beta损失函数。使用原始-对偶平均方案更新方法来解决该问题。

#### 8. 实验设置：
本文使用AVA和Charades两个数据集进行开放式和封闭式动作识别。对于开放式设置，视频被均匀地分成三个不相交的集合，并且如果演员不包含训练数据中的任何动作，则将其视为新颖性。封闭式动作识别使用AVA中的235个视频和Charades中官方提供的训练-测试拆分。

#### 9. 实验结果和分析：
本文提出了一种新的开放式动作识别框架，使用多标签证据学习（MULE）和Beta证据神经网络。该方法在Kinetics-400和Charades数据集上的封闭式和开放式指标上均比现有方法表现更好。该方法还在不同数据集拆分上表现出鲁棒性。实验表明，Beta分布比以前的Dirichlet分布更准确地分类新颖动作。


# Paper:686     交叉GAN审计：预训练生成模型之间属性级别相似性和差异性的无监督识别



#### 1. Title: 
Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models

#### 2. Authors: 
Matthew L. Olson, Shusen Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, and Weng-Keen Wong

#### 3. Affiliation: 
Matthew L. Olson:  Oregon State University - EECS

#### 4. Keywords: 
Generative Adversarial Networks, GAN, auditing, attribute discovery, interpretability

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Olson_Cross-GAN_Auditing_Unsupervised_Identification_of_Attribute_Level_Similarities_and_Differences_CVPR_2021_paper.html  Github: https://github.com/mattolson93/cross_gan_auditing

#### 6. Summary : 
- (1):本文研究背景是GAN模型的审计和验证，旨在发现GAN模型之间的相似性和差异性，以及发现GAN模型中的偏差和不公平性等问题。

- (2):过去的方法主要是基于模型数据的粗略比较，如FID或召回率等摘要统计量。本文提出了一种新的方法，即交叉GAN审计（xGA），它可以比较新开发的GAN与基线GAN之间的相似性和差异性，并发现共同、新颖和缺失的属性。本文的方法是有动机的，因为现有的方法无法比较不同数据集上的模型。

- (3):本文提出了一种基于属性的GAN审计方法，可以自动发现两个或多个GAN之间的共同、新颖和缺失属性。为了发现共同属性，xGA利用了共享属性应该在两个模型中引起相似变化的事实。为了发现新颖/缺失属性，xGA利用了一个关键的洞见，即独特于一个GAN的属性操作可以被视为另一个GAN的分布之外（OOD）。本文的方法是创新的，因为它可以在不同的数据集上比较不同的GAN模型，并且可以提供更细粒度的特征级别的比较。

- (4):本文的方法在多个数据集上进行了实验，包括CelebA、AFHQ、FFHQ、Toons、Disney和Met-Faces。实验结果表明，xGA可以提供GAN模型的细粒度特征级别的比较，并且可以发现模型之间的相似性和差异性。本文的方法是有效的，可以提供更好的GAN审计工具。
#### 7. 方法详细介绍：
本文提出了一种无监督的交叉生成对抗网络审计方法（Cross-GAN Auditing，xGA），用于识别客户端GAN模型与参考GAN模型之间的属性相似性和差异性。该方法识别了三组属性：共同属性、新属性和缺失属性。为了识别共同属性，xGA利用了共享属性应该在两个模型的结果图像中引起相似变化的事实。另一方面，为了发现新的/缺失的属性，xGA利用了一个关键的洞察力，即独特于一个GAN的属性操作可以被视为另一个GAN的分布之外（OOD）。该方法使用外部的、鲁棒的特征空间进行优化，并产生高质量的属性，即使在具有挑战性的分布转移中也能实现有效的对齐。本文引入了新的指标来评估基于属性的GAN审计方法，并使用CelebA、AFHQ、FFHQ、Toons、Disney和Met-Faces数据集评估了xGA。

#### 8. 实验设置：
本文使用几个基准数据集和GAN模型评估了提出的Cross-GAN Auditing方法。在大多数实验中，使用StyleGANv2训练的CelebA数据集作为参考GAN模型。实验在从Zc和Zr中随机抽取的样本上进行了10,000次迭代。所需的属性数量设置为Nc = 12、Nn = 4和Nm = 4。优化需要4小时的StyleGAN2模型和12小时的StyleGAN3模型，由于梯度检查点。使用学习率为0.001的Adam优化器来更新潜在方向参数。属性在样式空间中建模，生成器的输出被适当地调整大小以适应所选择的特征提取器的大小要求。

#### 9. 实验结果和分析：
本文引入了交叉生成对抗网络审计框架，利用一种新的优化技术来联合推断客户端GAN与任何参考GAN之间的共同、新颖和缺失属性。所提出的方法始终导致更高质量（分离和多样化）的属性，即使在具有挑战性的分布转移中也能有效地揭示共享属性，并在已知基础事实的受控实验中准确地识别新的/缺失的属性。本文提供了定性和定量结果，以证明所提出方法的有效性。本文还将所提出的方法与现有方法进行了比较，并表明在识别套件中的缺失属性方面，它优于所有基线。


# Paper:687     EDA：用于3D视觉定位的显式文本解耦和密集对齐



#### 1. Title: 
EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding

#### 2. Authors: 
Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, Jian Zhang

#### 3. Affiliation: 
第一作者：北京大学深圳研究生院

#### 4. Keywords: 
3D visual grounding, point clouds, language, text decoupling, dense alignment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2022_paper.html  Github: https://github.com/yanmin-wu/EDA

#### 6. Summary : 
- (1):本文研究了3D视觉定位中的文本解耦和密集对齐问题，提出了一种新的方法，旨在解决现有方法中存在的问题，如过度关注对象名称、忽略其他属性等。
 
- (2):现有方法存在的问题是特征耦合和融合不够明确，输入一个句子，输出一个全局耦合的句子级特征，这种耦合特征是模糊的，因为一些单词可能不描述主要对象，而是其他辅助对象。本文提出了一种更直观的解耦和显式策略，首先将输入文本解析为多个语义组件，然后在点云对象和多个相关解耦组件之间进行密集对齐，从而实现细粒度特征匹配，避免了由于不平衡学习而导致的归纳偏差。此外，本文还提出了一项新的3D视觉定位任务，即在没有对象名称的情况下定位对象，以全面评估模型的密集对齐能力。

- (3):本文提出了一种文本解耦模块，将语言描述解析为多个语义组件，并提出了两种设计良好的密集对齐损失，用于监督细粒度的视觉-语言特征融合，并防止不平衡和模糊学习。此外，本文还提出了一项新的3D视觉定位任务，即在没有对象名称的情况下定位对象，以全面评估模型的鲁棒性能。

- (4):本文在两个广泛采用的3D视觉定位数据集ScanRefer和SR3D/NR3D上实现了最先进的性能，并在新提出的任务上取得了绝对领先地位。本文的方法在3D视觉定位任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为EDA（Explicit Text-Decoupling and Dense Alignment）的3D视觉定位方法。该方法首先将输入的文本描述分解为多个语义组件，包括主要对象、属性、代词和关系。然后，将这些分解后的文本组件与候选对象的视觉特征进行密集对齐。对齐过程通过位置对齐损失和语义对齐损失实现。其中，位置对齐损失用于对齐与对象相关的文本组件，而语义对齐损失则通过对比学习学习视觉-文本多模态特征的相似性。训练的总损失还包括边界框回归损失。在显式推理阶段，通过计算每个文本组件与候选对象之间的相似度，选择得分最高的候选对象作为最终结果。

#### 8. 实验设置：
本文在两个数据集ScanRefer和SR3D/NR3D上进行实验，使用Acc@0.25IoU和Acc@0.5IoU作为评价指标。ScanRefer包含了51,583个手动注释的物体文本描述，而SR3D/NR3D包含了83,572个简单的机器生成描述和41,503个类似于ScanRefer的人工注释描述。本文将提出的方法与现有的最先进方法在Regular 3D Visual Grounding setting下进行了比较。

#### 9. 实验结果与分析：
本文在ScanRefer和SR3D/NR3D数据集上测试了提出的方法。结果表明，该方法在两个数据集上均取得了最先进的性能，相对于ScanRefer数据集和SR3D/NR3D数据集的最佳方法，分别提高了4.2%和3.7%，分别达到了54.59%和68.1%的准确率。消融实验表明，位置对齐损失和语义对齐损失产生了互补的性能。密集对齐子方法优于稀疏对齐，证明了每个文本组件的有效性。其中，“属性”组件有助于识别颜色和形状等特征，“代词”组件从句子中学习上下文信息，“关系”组件有助于理解物体之间的空间关系。

#### 全文总结：
本文提出了一种名为EDA的3D视觉定位方法，该方法通过将文本描述分解为多个语义组件，并将其与候选对象的视觉特征进行密集对齐，实现了更加准确的视觉定位。实验结果表明，该方法在两个数据集上均取得了最先进的性能，相对于现有的最佳方法，提高了4.2%和3.7%的准确率。EDA方法的优势在于其对文本描述的细粒度理解和对多模态特征的有效融合。


# Paper:688     Spider GAN：利用友好邻居加速GAN训练



#### 1. Title: 
Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training

#### 2. Authors: 
Siddarth Asokan, Chandra Sekhar Seelamantula

#### 3. Affiliation: 
Siddarth Asokan: 印度理工学院罗伯特博世物理系统中心
Chandra Sekhar Seelamantula: 印度理工学院电气工程系

#### 4. Keywords: 
Generative adversarial networks (GANs), image translation, friendly neighborhood, signed inception distance (SID), transfer learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Asokan_Spider_GAN_Leveraging_Friendly_Neighbors_to_Accelerate_GAN_Training_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是GAN训练的稳定性问题。
- (2):过去的方法主要是使用高斯分布的噪声向量作为输入，但是这种方法存在许多问题，如训练不稳定、模式崩溃等。本文提出了一种新的方法，使用图像作为输入，但不强制执行任何成对约束。本文的方法是基于“友好邻居”的概念，即通过识别与目标分布密切相关的数据集，来加速GAN的训练。为了定义友好邻域，本文提出了一种新的度量方法，称为有符号的Inception距离（SID）。本文的方法可以加速收敛，甚至可以在看似不相关的数据集之间发现对应关系。本文还提出了级联Spider GAN的方法，可以在小数据集上实现与基线相比更快的收敛速度。本文的方法在DCGAN、conditional GAN、PG-GAN、StyleGAN2和StyleGAN3上取得了最先进的Fréchet inception distance (FID)值。
- (3):本文提出了一种新的GAN训练方法，使用图像作为输入，但不强制执行任何成对约束。本文的方法是基于“友好邻居”的概念，即通过识别与目标分布密切相关的数据集，来加速GAN的训练。为了定义友好邻域，本文提出了一种新的度量方法，称为有符号的Inception距离（SID）。本文还提出了级联Spider GAN的方法，可以在小数据集上实现与基线相比更快的收敛速度。本文的方法在DCGAN、conditional GAN、PG-GAN、StyleGAN2和StyleGAN3上取得了最先进的Fréchet inception distance (FID)值。
- (4):本文的方法在DCGAN、conditional GAN、PG-GAN、StyleGAN2和StyleGAN3上取得了最先进的Fréchet inception distance (FID)值，且可以在小数据集上实现与基线相比更快的收敛速度。本文的方法可以加速收敛，甚至可以在看似不相关的数据集之间发现对应关系，支持本文的目标。
#### 7. 方法详细介绍：
本文提出了一种名为Spider GAN的新方法，该方法利用图像的结构性质来学习更强大的转换，而不需要强制执行任何成对约束。该方法通过使用一种新的度量方法Signed Inception Distance（SID）来识别与目标分布最相似的数据集，称为“友好邻居”，并将其用作输入。该方法使用Wasserstein GAN损失和一侧梯度惩罚，并在生成器中使用类嵌入来实现条件学习。此外，本文还提出了级联Spider GAN，其中预训练的GAN生成器的输出分布被用作后续网络的输入，以有效地将一个分布传输到另一个分布中，直到学习到目标分布。该方法在多种GAN架构上进行了评估，包括DCGAN、条件GAN、PG-GAN、StyleGAN2和StyleGAN3。

#### 8. 实验设置：
本文在多个数据集上评估了Spider GAN方法，包括MNIST、Fashion-MNIST、CIFAR-10、Ukiyo-E Faces、TinyImageNet、CelebA和LSUN-Churches。使用提出的累积SID（CSIDm）度量方法来选择友好邻居数据集作为输入分布。评估指标包括FID、KID和CSIDm。

#### 9. 实验结果和分析：
本文的实验结果表明，Spider GAN方法可以利用源数据集中存在的结构来加速GAN训练，并且可以在高分辨率小数据集上实现与基线相当的性能。在MetFaces、Ukiyo-E Faces和AFHQ-Cats等数据集上，与基线相比，Spider GAN方法可以在仅使用基线训练迭代次数的五分之一的情况下实现最先进的FID值。级联Spider GAN方法可以进一步提高小数据集上生成器的性能。本文还比较了不同GAN架构的性能，包括DCGAN、条件GAN、PG-GAN、StyleGAN2和StyleGAN3。


# Paper:689     Bi3D：双域主动学习用于跨域3D目标检测



#### 1. Title: 
Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection

#### 2. Authors: 
Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao

#### 3. Affiliation: 
第一作者：复旦大学信息科学与技术学院

#### 4. Keywords: 
3D object detection, cross-domain, active learning, unsupervised domain adaptation, LiDAR

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yuan_Bi3D_Bi-Domain_Active_Learning_for_Cross-Domain_3D_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/PJLab-ADG/3DTrans

#### 6. Summary : 
- (1):本文研究的是跨域3D目标检测问题，即如何将一个在源域上训练好的3D检测模型迁移到目标域上。这个问题在自动驾驶等领域中非常重要，但是由于不同域之间的分布差异，传统的监督学习方法很难直接应用。因此，本文提出了一种主动学习的方法，通过在目标域上选择一部分样本进行标注，从而在保证检测性能的同时降低标注成本。

- (2):过去的方法主要是基于无监督域自适应技术，如自训练和知识蒸馏等。虽然这些方法已经取得了一定的进展，但是它们的检测性能仍然远远落后于在目标域上完全标注的监督学习方法。因此，本文提出了一种主动学习的方法，通过在源域和目标域上选择最具代表性的样本进行标注，从而提高模型的适应性和泛化性能。

- (3):本文提出了一种双域主动学习框架Bi3D，包括两个策略：源域样本选择和目标域样本选择。源域样本选择策略通过判断每个源域样本的领域度得分，选择与目标域相似的样本，避免模型受到无关源域数据的干扰。目标域样本选择策略则通过动态维护一个相似性库，选择最具代表性的目标域样本，以尽可能少的标注成本提高模型的适应性。实验结果表明，Bi3D在多个跨域场景下均取得了优异的检测性能，比现有的无监督域自适应方法更加有效。

- (4):本文在多个跨域场景下进行了实验，包括激光雷达波束、国家和传感器等。Bi3D在KITTI数据集上的目标检测准确率达到了89.63%，比现有的无监督域自适应方法高出5.34个百分点，甚至超过了在目标域上完全标注的监督学习方法。这表明Bi3D在降低标注成本的同时，能够有效提高模型的适应性和泛化性能。
#### 1. 实验设置：
本文使用Bi3D框架进行跨域3D目标检测实验，验证了两个关键组件的有效性，包括领域感知的源采样策略和基于多样性的目标采样策略。实验在nuScenes→KITTI和Waymo→KITTI设置下进行，使用PV-RCNN模型，在1%目标注释预算下进行。同时，本文还研究了Bi3D与UDA方法的组合。

#### 2. 方法详细介绍：
本文提出了一种跨域3D目标检测的双域主动学习方法，称为Bi-domain Active Learning（Bi3D）。该方法包括四个步骤：在源域上进行预训练、训练领域鉴别器、主动采样源域和主动采样目标域。其中，基于多样性的目标采样策略旨在选择多样性和代表性的目标域数据。整体目标是最小化源域和目标域上的检测损失。

#### 3. 实验结果与分析：
本文在四个自动驾驶数据集上进行了实验，包括KITTI、Waymo、nuScenes和Lyft。实验考虑了四种跨域适应场景，包括跨LiDAR光束、跨国家和跨传感器。使用KITTI评估指标对Car（Waymo中的Vehicle）类别进行所有实验。Bi3D方法在PV-RCNN和Voxel R-CNN两种检测器上进行了评估，两种检测器的体素大小均设置为（0.1m，0.1m，0.15m）。

#### 4. 实验细节：
在主动采样源域阶段，从源域中选择目标域样本，并在接下来的15个epoch中微调检测器。在主动采样目标域阶段，注释预算B分别为1%和5%。Bi3D方法在所有四个数据集上均取得了最先进的性能，并且在降低注释成本的同时，有效地减小了域差异并提高了目标域上的检测性能。本文还对改变目标域注释预算的结果进行了详细分析。


# Paper:690     StructVPR：加权样本蒸馏结构知识的视觉地点识别



#### 1. Title: 
StructVPR: Distill Structural Knowledge with Weighting Samples for Visual Place Recognition

#### 2. Authors: 
Yanqing Shen, Sanping Zhou, Jingwen Fu, Ruotong Wang, Shitao Chen, Nanning Zheng

#### 3. Affiliation: 
西安交通大学人机混合增强智能国家重点实验室，国家视觉信息应用工程技术研究中心，人工智能与机器人研究所

#### 4. Keywords: 
Visual place recognition, knowledge distillation, segmentation, global retrieval

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shen_StructVPR_Distill_Structural_Knowledge_With_Weighting_Samples_for_Visual_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的是视觉地点识别（VPR）问题，即如何从数据库中检索出与查询图像相似的图像。由于环境变化的影响，VPR任务面临着很大的挑战，因此需要学习具有鲁棒性的特征来区分不同的地点。
- (2):过去的方法通常采用两阶段策略，即使用全局特征进行检索，然后通过局部描述符匹配进行重新排序。然而，这种方法需要耗费大量时间和资源，因此本文提出了一种新的训练架构，即StructVPR，通过加强RGB全局特征中的结构知识来提高特征的稳定性。具体来说，StructVPR使用分割图像作为CNN网络的输入，以提取更具有结构性的信息。此外，本文还提出了一种加权知识蒸馏方法，以选择性地提取高质量的结构知识。与以往的方法相比，StructVPR可以在全局检索的情况下取得令人印象深刻的性能，并且甚至在许多两阶段方法中表现更好。
- (3):本文提出了一种新的训练架构，即StructVPR，通过加强RGB全局特征中的结构知识来提高特征的稳定性。具体来说，StructVPR使用分割图像作为CNN网络的输入，以提取更具有结构性的信息。此外，本文还提出了一种加权知识蒸馏方法，以选择性地提取高质量的结构知识。与以往的方法相比，StructVPR可以在全局检索的情况下取得令人印象深刻的性能，并且甚至在许多两阶段方法中表现更好。
- (4):本文在多个基准测试中进行了全面的实验，结果表明StructVPR在全局检索的情况下可以取得令人印象深刻的性能，并且甚至在许多两阶段方法中表现更好。此外，StructVPR还可以避免在测试过程中进行分割的计算和推断，从而最大化了知识的效率。
#### 7. 方法详细介绍：
本文提出了一种名为StructVPR的框架，用于视觉地点识别。该框架使用分割信息来增强RGB全局表示中的结构知识，以提高特征的稳定性。该方法使用知识蒸馏来避免在测试期间进行分割计算。具体而言，该方法使用RGB图像和编码的分割标签图像进行分别预训练，使用两个预训练分支来分割样本，然后使用加权知识蒸馏方法将预训练分支的高质量知识有选择地蒸馏到最终的RGB网络中。该方法还使用聚类方法将原始类别分组，并通过加权的one-hot编码引入标签的先验信息。在训练过程中，使用三元组VPR损失和知识蒸馏损失进行特征蒸馏。最后，使用加权函数对样本进行加权，以选择性地蒸馏高质量的知识。

#### 8. 实验设置：
本文在Nordland、Gardens Point、Tokyo247和RobotCar Seasons四个数据集上进行了实验，使用Recall@N作为评估指标。使用开源模型获取分割图像，并将所有图像调整为640×480。使用MobileNetV2作为RGB特征提取器的骨干网络，并使用MobileNet-L作为seg-branch中的特征提取器。使用组分割策略选择适合蒸馏的样本。

#### 9. 实验结果和分析：
本文的StructVPR方法在多个数据集上均取得了优异的性能，甚至在许多两阶段方法上取得了很大的优势。在添加额外的重新排序后，StructVPR在所有数据集上均取得了最先进的性能，同时保持了低计算成本。在所有数据集上的一致改进证实了StructVPR的有效性和鲁棒性。


# Paper:691     对齐区域包用于开放词汇物体检测



#### 1. Title: 
Aligning Bag of Regions for Open-Vocabulary Object Detection

#### 2. Authors: 
Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, Chen Change Loy

#### 3. Affiliation: 
第一作者：南洋理工大学S-Lab

#### 4. Keywords: 
Open-vocabulary object detection, vision-language models, bag of regions, Faster R-CNN, distillation-based approach

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/wusize/ovdet

#### 6. Summary : 
- (1):本文研究开放词汇物体检测问题，即检测训练集中未出现过的物体类别。传统的物体检测器只能识别训练集中的类别，无法应用于现实世界中的无限概念池。本文提出了一种新的方法，称为BARON，旨在对齐区域的嵌入，以更好地理解场景中的语义概念。 

- (2):现有的开放词汇物体检测方法只能将区域嵌入与VLMs提取的相应特征单独对齐，无法充分利用场景中的语义概念的组合结构，虽然这种结构可能已经被VLMs隐式学习。本文提出了一种新的方法，将区域的嵌入对齐到VLMs中的单词嵌入，以更好地理解整个场景。 

- (3):本文提出了一种新的方法，称为BARON，将上下文相关的区域分组为一个包。将包中的区域嵌入视为句子中的单词嵌入，并将其发送到VLMs的文本编码器中，以获得区域嵌入，该嵌入被学习以与冻结的VLMs提取的相应特征对齐。我们采用对比学习方法来训练BARON，以学习伪单词和区域嵌入。 

- (4):本文在OV-COCO和OV-LVIS两个基准测试上进行了广泛的实验。与现有的最佳结果相比，本文的方法在不同设置下始终表现出色。结合Faster R-CNN，BARON在OV-COCO的新类别上实现了34.0（增加4.6）的盒子AP50和OV-LVIS的新类别上实现了22.6（增加2.8）的掩码mAP。本文的方法是第一次将学习从单个区域提升到区域包的尝试。
#### 7. 方法详细介绍：
本文提出了一种名为BARON的方法，用于开放词汇物体检测。该方法采用对比学习方法，将学生（开放词汇物体检测器）和教师（视觉语言模型）的区域袋嵌入进行对齐，使学生学习编码多个区域的共存，这些区域可能包含多个概念。该方法通过使用基于区域提议网络（RPN）预测的区域提议来形成区域袋，并使用伪词和位置嵌入来表示区域袋嵌入，并将其馈送到文本编码器中。将包含区域的图像裁剪馈送到图像编码器中，以获得分组区域的图像嵌入。该方法还可以接受标题监督，其中图像嵌入被替换为通过将图像标题馈送到文本编码器中获得的嵌入。 

#### 8. 实验设置：
本文在COCO数据集上进行了实验。使用ResNet50-FPN进行基于CLIP的实验，使用ResNet50-C4进行基于标题的实验。检测器使用Faster R-CNN。 

#### 9. 实验结果和分析：
BARON在OV-COCO和OV-LVIS基准测试中的新颖类别上分别取得了4.6和2.8的提升，分别达到了34.0和22.6的AP50。与先前使用COCO标题进行监督的方法相比，BARON可以从标题监督中提取知识，并在OV-COCO上实现了32.7的AP50。


# Paper:692     ASPnet：多数据源共享-私有表示的动作分割



#### 1. Title: 
ASPnet: Action Segmentation with Shared-Private Representation of Multiple Data Sources

#### 2. Authors: 
Beatrice van Amsterdam, Abdolrahim Kadkhodamohammadi, Imanol Luengo, Danail Stoyanov

#### 3. Affiliation: 
1 Wellcome/EPSRC Centre for Interventional and Surgical Sciences

#### 4. Keywords: 
Action segmentation, multimodal representation learning, disentangled features, attention bottleneck, sensor noise

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/van_Amsterdam_ASPnet_Action_Segmentation_With_Shared-Private_Representation_of_Multiple_Data_Sources_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是动作分割，即在复杂的人类活动未修剪视频中预测每个帧中发生的动作。
- (2):过去的方法主要基于单一输入模态或多个数据源的naive融合，无法有效地融合互补信息，且对传感器噪声不够鲁棒。本文提出了一种多模态表示学习方法，将多流分割模型的隐藏特征分解为共享和私有组件，并使用注意力瓶颈来捕获数据中的长程时间依赖性，同时保留连续处理层中的分离。该方法在多个数据集上的表现优于不同的数据融合基线和现有方法。
- (3):本文提出的方法是将多个数据源的共享和私有特征分离，以更好地融合互补信息，提高动作分割的鲁棒性和准确性。通过最小化最大均值差异（MMD）来学习共享特征空间，而注意力瓶颈则用于捕获数据中的长程时间依赖性，同时保留特征分离。该方法在50salads、Breakfast和RARP45数据集上进行了评估，表现优于不同的数据融合基线和现有方法。
- (4):本文方法在多个数据集上进行了评估，表现优于不同的数据融合基线和现有方法，且在传感器噪声方面更加鲁棒，即使使用较少的训练数据也能达到与强视频基线相当的性能。
#### 7. 方法详细介绍：
本文提出了一种名为ASPnet的多流动作分割模型，它将多个数据源的隐藏特征解缠成共享和私有组件。模型的第一阶段使用独立的全连接层和最大均值差异（MMD）损失将多个数据源的预提取帧特征解缠成模态共享和私有空间。第二阶段使用多流分割模型处理解缠的特征序列并生成逐帧动作预测。模型使用共享注意瓶颈来捕获数据中的长程时间依赖性，同时保持连续处理层中的特征解缠。损失函数是交叉熵分类损失、平滑损失和用于特征解缠的辅助MMD损失的组合。模型使用Adam优化器进行训练，50salads数据集训练100个epoch，学习率为0.0005，Breakfast数据集训练100个epoch，学习率为0.0001。

#### 8. 实验设置：
本文在三个具有挑战性的基准数据集50salads、Breakfast和RARP45上评估了模型的性能，其中包括多模态（例如视频和加速度计）和多视图（例如RGB和光流）输入。评估结果表明，ASPnet相对于单模态基线和不同的融合策略具有更好的性能，达到或超过了现有方法的水平。作者还测试了模型对于少量训练数据和加性输入噪声的鲁棒性。

#### 9. 实验结果和分析：
本文提出的ASPnet模型在50salads数据集上的实验结果表明，该模型在准确性方面优于现有方法，并接近或匹配顶级分割分数。作者还讨论了ASPnet对噪声的鲁棒性以及模型对训练集大小的敏感性。在三个数据集上的评估结果表明，ASPnet在所有评估指标上均优于其他模型，包括准确性、编辑分数和F1分数。作者还尝试了ASPnet架构的不同变体，如将注意力瓶颈扩展到解码器阶段和引入额外的空间注意力层，但没有观察到任何显著的改进。作者建议未来的工作可以集中在评估ASPnet在视频帧的其他视图（如人体骨架）上，并将该方法扩展到动作检测任务。


# Paper:693     BAEFormer：用于鸟瞰图语义分割的双向和早期交互Transformer



#### 1. Title: 
BAEFormer: Bi-directional and Early Interaction Transformers for Bird’s Eye View Semantic Segmentation

#### 2. Authors: 
Cong Pan, Yonghao He, Junran Peng, Qian Zhang, Wei Sui, Zhaoxiang Zhang

#### 3. Affiliation: 
第一作者：中国科学院自动化研究所、中国科学院大学未来技术学院

#### 4. Keywords: 
Bird’s Eye View, Semantic Segmentation, Transformer, Early Interaction, Bi-directional

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Pan_BAEFormer_Bi-Directional_and_Early_Interaction_Transformers_for_Birds_Eye_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是自动驾驶中的Bird’s Eye View语义分割任务，该任务的关键在于将透视视图转换为Bird’s Eye View。 
- (2):过去的方法主要有Late-interaction和Middle-interaction两种，但它们都存在将透视视图转换为Bird’s Eye View的困难。本文提出了一种新的Bi-directional and Early Interaction Transformers框架，旨在解决这个问题。 
- (3):本文提出了一种早期交互的方法，将全局上下文信息和局部细节信息相结合，使得更丰富的语义信息能够传递到Bird’s Eye View空间。同时，本文提出了一种双向交互机制，将图像特征和Bird’s Eye View特征相互作用，从而更好地将多尺度图像特征聚合成更好的Bird’s Eye View特征表示。 
- (4):本文在nuScenes数据集上进行了广泛的实验，并进行了全面的消融研究，证明了所提出的BAEFormer方法的有效性和高效性。在单个A100 GPU上，BAEFormer方法在实时推理速度下实现了最先进的Bird’s Eye View语义分割性能，即38.9 mIoU。
#### 7. 方法详细介绍：
本文提出了一种名为Bi-directional and Early Interaction Transformers (BAEFormer)的方法，用于鸟瞰图语义分割。该方法包括两个部分：早期交互PV-BEV管道和双向交叉注意力机制。早期交互方法提倡将全局上下文信息和局部细节相结合，从而使得更丰富的语义信息传递到BEV空间。双向交叉注意力机制使用不共享的注意力图同时更新BEV和图像特征，其中前一尺度的图像特征影响后一尺度特征的提取。多尺度特征图的分辨率对交互过程的最终准确性影响微乎其微。该方法在nuScenes数据集上实现了实时推理速度，并取得了最先进的性能。

具体步骤如下：
1. 将输入图像缩放到224×480。
2. 将输入的多视图图像输入到EfficientNet-B4中，得到多尺度图像特征图。
3. 初始的地图视图嵌入是一个可学习参数的张量wbev×hbev×cbev，其中wbev=hbev=25。
4. 图像特征和BEV特征的所有维度都设置为c=cbev=128。
5. 使用四个头和嵌入大小dhead=64的多头注意力。
6. 解码器由三个双线性上采样和卷积层组成，将最后的BEV特征上采样到最终输出大小。
7. 每个解码器层将BEV特征大小增加2倍，直到最终输出分辨率为200×200，对应于以自车为中心的100×100米区域。
8. 使用Focal Loss优化模型，预测软分割图和二进制ground-truth之间的交叉熵。

#### 8. 实验设置：
本文使用nuScenes数据集，该数据集包含1000个场景，这些场景在波士顿和新加坡的四个位置下以不同的天气条件和不同的时间拍摄。每个场景大约持续20秒，关键样本以2 Hz进行注释。数据集包括来自6个摄像机的RGB图像，水平视场角为360°，摄像机视野之间存在轻微重叠。每个场景的每个摄像机的相机内参和外参都提供。二进制语义分割任务的ground-truth是通过将原始注释（3D边界框）渲染成场景的离散化BEV而生成的。48个Lyft场景被分离以获得6048个验证样本。评估使用预测结果和ground-truth BEV标签之间的交并比（IoU）得分。训练和验证考虑了不同能见度水平的车辆（0％或40％）。所有消融研究都在40％能见度水平下进行。

#### 9. 实验结果和分析：
BAEFormer在nuScenes和Lyft数据集上的车辆和可行驶区域分割任务中均优于所有替代方法。BAEFormer在A100 GPU上以45 FPS运行，Setting 1和Setting 2的mIoU分别为42.0和38.9，比实时模型CVT分别高4.5 / 2.9个点。在更大的输入图像分辨率（504×1056）下，BAEFormer比CVT高6.7 / 5.0个点。所提出的BAEFormer方法不仅完全整合了尺度间信息以获得更好的语义表示，而且通过异构空间信息流的双向约束实现了更好的空间对齐。


# Paper:694     基于转移的自集成物体检测对抗攻击



#### 1. Title: 
T-SEA: Transfer-based Self-Ensemble Attack on Object Detection

#### 2. Authors: 
Hao Huang, Ziyan Chen, Huanran Chen, Yongtao Wang, Kevin Zhang

#### 3. Affiliation: 
Hao Huang, Ziyan Chen, Huanran Chen, Yongtao Wang: 北京大学 (Peking University)

#### 4. Keywords: 
Adversarial attack, object detection, transfer-based attack, self-ensemble, constrained data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/VDIGPKU/T-SEA

#### 6. Summary : 
- (1):本文研究的背景是深度学习模型的安全性，特别是针对物体检测模型的对抗攻击。
 
- (2):过去的方法主要分为基于查询和基于转移的黑盒攻击，本文关注基于转移的黑盒攻击。现有的转移攻击方法通常需要使用多个模型进行集成，但这种方法需要大量的时间和资源，并且很难在同一任务上获得多个不同的模型。本文提出了一种只使用一个模型的单模型转移攻击方法，通过自集成策略来提高攻击的转移性能。具体来说，本文提出了一种增强的基线方法，并在此基础上引入了自集成策略，包括约束数据增强、模型 ShakeDrop 和补丁 Cutout，以提高攻击的转移性能。 
 
- (3):本文提出的 T-SEA 方法通过自集成策略，只使用一个模型就能够在多个黑盒检测器上实现高转移性的对抗攻击。具体来说，本文提出了一种增强的基线方法，并在此基础上引入了自集成策略，包括约束数据增强、模型 ShakeDrop 和补丁 Cutout，以提高攻击的转移性能。 
 
- (4):本文在多个主流的物体检测器上进行了实验，结果表明，T-SEA 方法可以显著降低黑盒设置下多个广泛使用的检测器的平均精度，同时在多个基础攻击方法上表现良好。本文提出的方法在物体检测领域具有一定的创新性和实用性。
#### 7. 方法详细介绍：
本文提出了一种基于转移的自我集成攻击方法，称为T-SEA。该方法通过制作通用对抗性贴片来干扰多个黑盒检测器的检测过程。该方法的框架分为训练和攻击两个阶段。在训练阶段，使用白盒检测器定位目标对象，将对抗性贴片附加到目标类别的每个检测到的对象的中心，并将带有对抗性贴片的图像传递给Shake-Drop模型。在攻击阶段，将优化的对抗性贴片应用于测试图像，以干扰多个黑盒检测器的检测过程。T-SEA方法包括三个主要策略：约束数据增强、模型ShakeDrop和贴片cutout。约束数据增强用于生成多样化的对抗性样本，模型ShakeDrop用于增强模型的鲁棒性，贴片cutout用于降低对抗性贴片的复杂性。

#### 8. 实验设置：
本文使用了8个主流检测器，包括YOLO v2、YOLO v3和YOLO v3tiny、YOLO v4和YOLO v4tiny、YOLO v5、Faster R-CNN和SSD，以系统地验证所提出的T-SEA框架。基础攻击方法包括PGD和MIM。使用INRIA人员数据集来训练和测试对抗性贴片，使用COCO-person和CCTV-person数据集来验证所制作的贴片的可转移性。贴片大小为300×300，输入图像大小为416×416，批量大小为8，最大时期数为1000。使用Adam优化器进行E-baseline，并将学习率调度程序设置为ϵ1 = 1e−4，ϵ2 = 1e−4。

#### 9. 实验结果和分析：
T-SEA方法在白盒和黑盒攻击的性能上都比现有的检测攻击方法表现更好。平均精度（AP）用于衡量制作的贴片的攻击能力，较低的AP表示更好的攻击性能。T-SEA方法在不同的检测器和攻击方法上表现良好，证明了所提出的自我集成策略的有效性。


# Paper:695     通过稀疏掩蔽建模的条件扩散模型进行视觉解码



#### 1. Title: 
Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding

#### 2. Authors: 
Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, Juan Helen Zhou

#### 3. Affiliation: 
第一作者：新加坡国立大学

#### 4. Keywords: 
Brain decoding, fMRI, vision decoding, diffusion model, masked modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2021_paper.html  Github: https://mind-vis.github.io/

#### 6. Summary : 
- (1):本文旨在通过fMRI数据解码视觉刺激，以深入了解人类视觉系统，并通过脑机接口建立人类和计算机视觉之间的坚实基础。然而，由于脑信号的复杂底层表示和数据注释的稀缺性，从脑记录中重建具有正确语义的高质量图像是一个具有挑战性的问题。

- (2):传统的方法依赖于使用fMRI和预训练的VGG提取的相应的分层图像特征进行训练。在测试期间，预测的图像特征将用于分类或馈入到生成模型中，如GAN，以重建原始刺激。然而，由于缺乏fMRI-图像对和有用的生物学指导，从fMRI直接重建的重建图像通常是模糊的和语义无意义的。因此，学习fMRI的有效和生物学有效的表示形式至关重要，以便建立清晰且可推广的脑活动与视觉刺激之间的联系。

- (3):本文提出了MinD-Vis：用于人类视觉解码的双条件潜在扩散模型的稀疏掩蔽脑建模。首先，我们使用受原始视觉皮层中信息的稀疏编码启发的大型潜在空间中的掩蔽建模来学习fMRI数据的有效自监督表示。然后，通过增加双条件的潜在扩散模型，我们展示了MinD-Vis可以使用非常少的成对注释从脑记录中重建高度合理的图像，这些图像具有语义匹配的细节。我们在多个数据集上进行了定性和定量测试，实验结果表明，我们的方法在语义映射（100种语义分类）和生成质量（FID）方面均优于现有技术，分别提高了66％和41％。我们还进行了详尽的消融研究以分析我们的框架。

- (4):本文提出的方法在视觉解码任务上取得了很好的性能，可以重建高质量的图像，并且在语义映射和生成质量方面均优于现有技术。该方法的创新点在于使用了稀疏掩蔽脑建模和双条件潜在扩散模型相结合的方法，以学习fMRI的有效表示，并从脑记录中重建高度合理的图像。
#### 7. 方法详细介绍：
MinD-Vis是一个两阶段的框架，用于从fMRI图像注释中解码视觉刺激。第一阶段使用SC-MBM进行fMRI预训练，学习来自大规模未标记fMRI数据集的可推广上下文知识。第二阶段使用DC-LDM进行潜在扩散模型，通过双重条件生成可信的图像。预训练的LDM确定模型的生成能力和fMRI编码器将适应的条件潜在空间。该方法在多个数据集上进行了验证，并显示出比以前的方法更好的性能。

具体步骤如下：
1. 预训练阶段：
   - 使用SC-MBM进行fMRI预训练，学习来自大规模未标记fMRI数据集的可推广上下文知识。
2. 图像生成阶段：
   - 使用DC-LDM进行潜在扩散模型，通过双重条件生成可信的图像。
   - 将预训练的fMRI编码器与LDM通过交叉注意力和时间步骤条件进行整合，以进行条件合成。
   - LDM在图像潜在空间上操作，fMRI编码器、交叉注意力头和投影头进行联合优化，而其他部分则固定不变。
   - 通过两条路径将fMRI潜在空间投影到LDM条件空间中，一条路径直接连接到LDM中的交叉注意力头，另一条路径将fMRI潜在空间添加到时间嵌入中。
   - 交叉注意力头的微调对于连接预训练条件空间和fMRI潜在空间至关重要。

#### 8. 实验设置：
本研究使用了多个数据集进行验证，包括HCP数据集、GOD数据集和BOLD5000数据集。预训练数据集包括来自HCP和GOD的fMRI数据。在主要分析中，使用GOD数据集进行微调。BOLD5000数据集用作验证数据集。完整模型进行了500个epoch的预训练和500个epoch的微调。图像分辨率为256×256，使用250个PLMS步骤生成图像。

#### 9. 实验结果和分析：
MinD-Vis方法在GOD测试集上的解码性能优于其他三种方法，生成的图像具有最准确和最合理的语义细节。性能以语义正确性（1000次n-way top-k分类准确率）和生成质量（FID）为评估指标。MinD-Vis方法实现了最高的语义正确性和最低的FID得分。在BOLD5000数据集上，MinD-Vis方法的50-way top-1识别准确率为34％，比以前的方法有所提高。生成的图像在语义和低级特征上与真实刺激匹配。该方法能够准确重建包含对象和动物、建筑和景观的图像。


# Paper:696     UniDAformer: 通过分层掩模校准实现统一领域自适应全景分割变压器



#### 1. Title: 
UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer via Hierarchical Mask Calibration

#### 2. Authors: 
Jingyi Zhang, Jiaxing Huang, Xiaoqin Zhang, Shijian Lu

#### 3. Affiliation: 
南洋理工大学 S-lab

#### 4. Keywords: 
Panoptic Segmentation, Domain Adaptation, Transformer, Hierarchical Mask Calibration

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_UniDAformer_Unified_Domain_Adaptive_Panoptic_Segmentation_Transformer_via_Hierarchical_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究领域自适应全景分割，旨在通过利用一个或多个相关源域的现成标注数据来缓解数据注释挑战。 
- (2):现有的研究采用两个独立的网络进行实例分割和语义分割，导致网络参数过多，训练和推理过程复杂且计算密集。本文提出了UniDAformer，一种统一的领域自适应全景分割变压器，可以在单个网络内同时实现领域自适应实例分割和语义分割。UniDAformer引入了分层掩模校准（HMC），通过在线自我训练来纠正区域、超像素和像素级别的不准确预测。 
- (3):本文提出了UniDAformer，它是第一个端到端的统一领域自适应全景分割变压器，可以同时处理实例分割和语义分割。UniDAformer引入了分层掩模校准和在线自我训练，可以在自我训练过程中校准预测的伪掩模。 
- (4):在多个公共基准测试中，UniDAformer相对于现有技术实现了更优越的领域自适应全景分割性能。
#### 7. 方法详细介绍：
本文提出了一种统一的领域自适应全景分割变压器，称为UniDAformer。它通过将物体和背景视为掩模并统一适应它们来实现统一的全景适应。通过使用所提出的分层掩模校准（HMC）技术，迭代和逐步校准预测的伪掩模，缓解了严重的误报问题。UniDAformer是端到端可训练的，具有更少的参数和更简单的训练和推理流程。HMC引入了很少的额外计算开销，可以作为插件使用。具体步骤包括：UniDAformer包括三个校准模块：区域校准、超像素校准和像素校准。通过重复所有校准伪掩模的校准过程，获得校准的伪掩模。使用全景分割损失构建自我训练损失，该损失包括匹配成本和匈牙利损失。定义的总体训练目标是最小化监督损失和无监督损失。

#### 8. 实验设置：
UniDAformer方法在三个领域适应任务中使用四个数据集进行评估：SYNTHIA → Cityscapes，Cityscapes → Foggy Cityscapes和VIPER → Cityscapes。用于评估的全景分割指标包括分割质量（SQ）、识别质量（RQ）和全景质量（PQ）。评估在每个数据集的验证集上进行。

#### 9. 实验结果和分析：
实验结果表明，UniDAformer在所有三个领域适应任务中均优于现有方法。使用PQ、mSQ和mRQ指标评估方法的性能。结果表明，与基线和其他最先进的方法相比，UniDAformer在全景分割质量方面取得了显着的改进。通过分析预测质量和校准顺序，以及与CVRN的效率比较，证明了所提出方法的有效性和优越性。


# Paper:697     基于不确定性引导的自训练弱监督时间句子定位



#### 1. Title: 
Weakly Supervised Temporal Sentence Grounding with Uncertainty-Guided Self-training

#### 2. Authors: 
Yifei Huang, Lijin Yang, Yoichi Sato

#### 3. Affiliation: 
Yifei Huang, Lijin Yang: The University of Tokyo
Yoichi Sato: The University of Tokyo, Shanghai Artificial Intelligence Laboratory (上海人工智能实验室)

#### 4. Keywords: 
Temporal sentence grounding, weakly supervised learning, self-training, uncertainty estimation, mutual learning

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2021_paper.html
Github: None

#### 6. Summary: 
- (1):本文研究的背景是弱监督下的时间句子定位任务，即在只有视频-语言对应关系的情况下，找到语言描述在视频中对应的时间段。
- (2):过去的方法主要采用多实例学习方法，但是由于视频的复杂时间结构，这些方法完全依赖于负样本，而无法提供可靠的正样本。本文提出了一种基于不确定性引导的自训练技术，为弱监督学习提供额外的自我监督信号，以指导模型的学习。本文的方法具有以下创新：(1)构建贝叶斯教师网络，利用其不确定性作为权重来抑制噪声教师监督信号；(2)利用时间数据增强带来的循环一致性来进行教师-学生互相学习。本文的方法在Charades-STA和ActivityNet Captions数据集上表现优异，且可应用于多种现有方法的改进。
- (3):本文提出了一种基于自训练的弱监督时间句子定位方法，通过教师-学生结构提供额外的自我监督信号来学习更好的学生网络。本文的方法具有以下创新：(1)构建贝叶斯教师网络，利用其不确定性作为权重来抑制噪声教师监督信号；(2)利用时间数据增强带来的循环一致性来进行教师-学生互相学习。本文的方法在Charades-STA和ActivityNet Captions数据集上表现优异，且可应用于多种现有方法的改进。
- (4):本文的方法在Charades-STA和ActivityNet Captions数据集上进行了实验，表明其在弱监督时间句子定位任务上具有优异的性能。本文的方法可应用于多种现有方法的改进，且其性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于自训练的弱监督时序句子定位方法。该方法包括一个贝叶斯网络作为教师网络和一个相同的网络作为学生网络。教师网络用于估计不确定性，然后用于抑制低质量输出的影响。学生网络使用教师网络估计的不确定性和时序增强循环一致性进行更新。教师-学生循环一致性设计为互相学习，使用基于掩码重构的三元组排名损失进行增强。学生和教师在初始化后异步更新。

#### 8. 实验设置：
本文在两个公开数据集Charades-STA和ActivityNet Captions上进行实验。评估指标是Charades-STA数据集和ActivityNet Captions数据集的val 2分割中的“IoU@n”指标，其中n = {0.3, 0.5, 0.7}。本文使用C3D和I3D特征分别用于ActivityNet Captions和Charades-STA数据集。使用预训练的GloVe word2vec进行单词嵌入。最大句子长度设置为20，最大视频长度设置为200，Charades-STA和ActivityNet Captions数据集的词汇量分别为1,111和8,000。本文针对不同数据集使用不同的数据增强参数。使用Adam优化器，学习率为0.0004，用于训练两个网络。在模型训练期间，本文将Ls和Lt的权重设置为10，将其他损失的权重设置为1。本文没有向骨干网络引入额外的参数。

#### 9. 实验结果和分析：
本文提出的弱监督时序句子定位的自训练方法在公共数据集上表现出色。该方法可以与多个骨干网络一起使用，并提高它们的性能。消融研究证明了方法中组件的有效性。然而，该方法提高前5个预测的召回率的能力有限，并且性能依赖于骨干网络。


# Paper:698     面向类增量学习的样本压缩方法



#### 1. Title: 
Class-Incremental Exemplar Compression for Class-Incremental Learning

#### 2. Authors: 
Zilin Luo, Yaoyao Liu, Bernt Schiele, Qianru Sun

#### 3. Affiliation: 
Zilin Luo: 新加坡管理大学 (Singapore Management University)
Yaoyao Liu, Bernt Schiele: 德国马普计算机科学研究所 (Max Planck Institute for Informatics, Saarland Informatics Campus)
Qianru Sun: 新加坡管理大学 (Singapore Management University)

#### 4. Keywords: 
Class-incremental learning, exemplar compression, class activation maps, dynamic environments, memory optimization

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2021_paper.html

Github: https://github.com/xfflzl/CIM-CIL

#### 6. Summary:
- (1):本文研究的背景是动态人工智能系统需要不断学习新的类别数据，同时保留旧类别的知识，以避免遗忘问题。
- (2):过去的方法主要有三种，即基于正则化、基于参数隔离和基于重放。其中，重放方法需要在有限的内存预算下保留少量旧类别样本，但这会导致旧类别和新类别之间的数据不平衡，从而导致旧类别的遗忘问题。本文提出了一种新的方法，即基于压缩的样本重放方法，通过压缩旧类别样本中的非判别性像素来增加样本数量，同时保留判别性像素来保证样本的代表性。本文的方法通过生成类激活图来自动标记判别性像素，然后使用自适应掩码生成模型来压缩样本。这种方法在动态环境下具有自适应性，可以根据不同类别和不同阶段的需要生成不同的压缩掩码。本文的方法在多个数据集上取得了最先进的性能。
- (3):本文提出了一种新的样本压缩方法，通过自适应掩码生成模型来压缩旧类别样本中的非判别性像素，同时保留判别性像素来保证样本的代表性。本文的方法通过生成类激活图来自动标记判别性像素，然后使用自适应掩码生成模型来压缩样本。本文的方法在动态环境下具有自适应性，可以根据不同类别和不同阶段的需要生成不同的压缩掩码。本文的方法通过全局双层优化问题来交替训练压缩模型和分类模型，以达到最优性能。
- (4):本文的方法在多个数据集上进行了实验，包括Food-101、ImageNet-100和ImageNet-1000。实验结果表明，使用本文的方法可以取得最先进的性能，例如在10阶段ImageNet-1000数据集上，本文的方法比最先进的方法FOSTER高出4.8个百分点。这表明本文的方法可以有效地解决旧类别遗忘问题，并且在动态环境下具有自适应性。
#### 7. 方法详细介绍：
本文提出了一种用于类增量学习的类增量样本压缩方法，称为Class-Incremental Exemplar Compression（CIEC）。该方法使用像素选择性压缩，仅对非判别性像素进行下采样，同时保持判别性像素不变。该方法还引入了一种基于类激活图（CAM）的掩码生成方法，用于在前景对象上生成0-1掩码。本文提出了一种自适应掩码生成过程，称为Class-Incremental Masking（CIM）-可学习的掩码生成模型。该方法使用双层优化问题（BOP）来解决CIL模型和参数化的Class-Incremental Masking（CIM）模型的优化问题。

CIM方法是为了生成适应性压缩掩码而设计的。该方法涉及两个优化层次：任务层和掩码层。在任务层，CIL模型被优化以解决当前的CIL任务。在掩码层，CIM模型被优化以生成适应性压缩掩码。CIM模型的网络设计通过添加一个可学习的激活函数分支来扩展网络骨干，其中仅激活函数是可学习的，权重层的参数从原始分支复制。优化管道由两个优化层次组成：任务层和掩码层。任务层优化旨在优化CIL模型以解决当前的CIL任务，而掩码层优化旨在优化CIM模型以生成适应性压缩掩码。

#### 8. 实验设置：
本文在Food-101、ImageNet-100和ImageNet-1000三个高分辨率基准测试集上评估了所提出的方法，并与最近的CIL方法LUCIR、DER和FOSTER进行了比较。实验在ImageNet-1000的5阶段和10阶段设置中进行，总共有5k个样本的内存预算。

#### 9. 实验结果和分析：
实验结果表明，使用CIM压缩的样本带来了一致且显着的改进，例如，在ImageNet-1000的5阶段和10阶段设置中，比最先进的方法FOSTER分别高出4.2％和4.8％。所提出的方法实现了新的CIL准确性最高的状态。


# Paper:699     通过最小化浮点数误差对基于梯度攻击的不利影响来提高损失函数的效率



#### 1. Title: 
Efficient Loss Function by Minimizing the Detrimental Effect of Floating-point Errors on Gradient-based Attacks

#### 2. Authors: 
Yunrui Yu, Cheng-Zhong Xu

#### 3. Affiliation: 
Yunrui Yu: University of Macau, Macau SAR, China.
Cheng-Zhong Xu: University of Macau, Macau SAR, China.

#### 4. Keywords: 
Adversarial attacks, deep learning, floating-point errors, loss function, robustness.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Efficient_Loss_Function_by_Minimizing_the_Detrimental_Effect_of_Floating-Point_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了深度学习模型在面对对抗攻击时的鲁棒性问题，提出了一种新的损失函数来提高模型的鲁棒性评估方法。
- (2):过去的方法主要是基于梯度的攻击方法，但是由于浮点数误差的存在，这些方法往往会高估模型的鲁棒性。本文提出了一种新的损失函数，通过最小化浮点数误差对攻击的影响来提高模型的鲁棒性评估方法。
- (3):本文提出了一种新的损失函数，通过最小化浮点数误差对攻击的影响来提高模型的鲁棒性评估方法。实验结果表明，该方法在各种防御机制下都比其他损失函数更有效和可靠。
- (4):本文在CIFAR-10数据集上进行了实验，结果表明，该方法在对抗攻击方面的表现优于其他方法，并且能够提高攻击的效率和准确性。
#### 7. 方法详细介绍：
本文提出了一种名为MIFPE的高效损失函数，旨在最小化浮点误差对基于梯度的攻击的影响。该方法通过舍弃logits的部分元素来减小浮点误差的影响，从而控制计算梯度时相对误差的影响。该方法与其他损失函数（包括CE、DL、DLR和MIFPE target）进行了比较，并在各种防御机制下展示了更高的效率和可靠性。具体步骤包括：
- 修改交叉熵损失函数，添加一个惩罚项来惩罚logits偏离其真实值的程度。
- 计算梯度时考虑浮点舍入引起的相对误差。
- 使用FT-PGD技术，在多次迭代攻击中添加一个比例因子，使其在每次迭代中保持不变，以评估模型的鲁棒性。

#### 8. 实验设置：
本文使用CIFAR-10数据集评估了所提出方法的性能。实验中使用了来自先前研究的防御模型，并使用了多种数据集和威胁模型进行测试。攻击方法包括PGD和FGSM，使用不同的损失函数进行测试。实验设置包括：
- 数据集：CIFAR-10、CIFAR-100、MNIST和ImageNet。
- 威胁模型：ℓ∞-norm和ℓ2-norm。
- 损失函数：CE、C&W、DLR和MIFPE。

#### 9. 实验结果与分析：
本文的实验结果表明，所提出的MIFPE方法在各种防御机制下均优于其他损失函数。相对误差会导致计算梯度时的失败，而MIFPE方法通过减小浮点误差的影响，提高了基于梯度的攻击的效率和准确性。实验结果包括：
- MIFPE损失函数在攻击性能和计算效率方面均优于其他损失函数。
- 浮点舍入误差可能导致攻击失败，因为随着样本接近分类边界，相对误差会增加。
- MIFPE方法在不同的防御机制下表现出最小的方差，表明其效果不依赖于动量和步长计划。


# Paper:700     利用语言先验从视频中重建手语化身



#### 1. Title: 
Reconstructing Signing Avatars From Video Using Linguistic Priors

#### 2. Authors: 
Maria-Paola Forte, Peter Kulits, Chun-Hao Huang, Vasileios Choutas, Dimitrios Tzionas, Katherine J. Kuchenbecker, Michael J. Black

#### 3. Affiliation: 
Max Planck Institute for Intelligent Systems, Stuttgart and Tübingen, Germany (马普智能系统研究所)

#### 4. Keywords: 
Sign language, 3D avatars, linguistic priors, hand pose estimation, facial expression, body movement

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Forte_Reconstructing_Signing_Avatars_From_Video_Using_Linguistic_Priors_CVPR_2021_paper.html  Github: https://github.com/IS-Tuebingen/SGNify

#### 6. Summary : 
- (1):本文研究的背景是手语是全球超过7千万聋人的主要交流方式，但是现有的手语学习工具主要是2D视频，缺乏3D手语模型，因此需要自动从手语视频中重建3D手语模型。
 
- (2):过去的方法主要是基于2D或3D关节点的人体姿态估计，但是手部姿态估计仍然是一个具有挑战性的问题，因为手部具有高自由度，容易出现自遮挡和运动模糊等问题。本文提出了一种基于语言先验的方法，通过手语的语言特征来约束手部姿态估计，从而提高3D手语模型的准确性和表现力。

- (3):本文提出了一种新颖的语言先验方法，将手语分为八个类别，并将手部姿态分为对称性和不变性两个方面进行约束，同时使用3D全身模型来捕捉手势、面部表情和身体运动。本文的方法在单目手语视频中自动捕捉手部姿态、面部表情和身体运动，并使用SMPLify-X方法进行优化，从而重建出3D手语模型。实验结果表明，本文的方法在手语视频中的3D姿态估计方面优于现有的方法，并且重建的3D手语模型比现有的方法更加自然和易于理解。

- (4):本文的方法在单目手语视频中实现了自动捕捉手部姿态、面部表情和身体运动，并使用SMPLify-X方法进行优化，从而重建出3D手语模型。实验结果表明，本文的方法在手语视频中的3D姿态估计方面优于现有的方法，并且重建的3D手语模型比现有的方法更加自然和易于理解。
#### 7. 方法详细介绍：
本文提出了一种名为SGNify的方法，用于从单目RGB视频中重建手语人物的姿态和形状。该方法使用了语言约束，包括手部姿态不变性和对称性，以提高重建人物的准确性。该方法使用深度神经网络从单目RGB视频中估计手语人物的3D姿态和形状。该网络在大量手语视频数据集上进行训练，并使用手语组分类器来提高性能。该方法还使用了语言先验知识，包括手部姿态不变性和对称性，以提高重建人物的准确性。该方法与最先进的方法进行了比较，并进行了定量和感知评估。

具体步骤如下：
1. 使用深度神经网络从单目RGB视频中估计手语人物的3D姿态和形状。
2. 使用手语组分类器来提高性能。
3. 使用语言先验知识，包括手部姿态不变性和对称性，以提高重建人物的准确性。
4. 将手部运动和姿态、面部表情和上半身运动在3D空间中结合起来，使用3D全身模型SMPL-X。
5. 将语言先验知识整合到优化方法的目标函数中，并使用一个简单的模型从原始视频中提取特征并确定所描绘的手语类别。
6. 使用更具表现力的面部回归器SPECTRE来捕捉面部参数。

#### 8. 实验设置：
本文使用商业运动捕捉系统来计算与单目视频同步的3D人物模型，以定量评估SGNify。本文还进行了一个感知评估，要求熟练的手语人士观看估计的SMPL-X人物模型或真实人物视频，并识别所执行的手语。

#### 9. 实验结果和分析：
SGNify在上半身和双手方面的误差最低，超过了最先进的方法。对称性和手部姿态不变性的语言约束提高了方法在对称和非对称手语上的性能。在线感知研究表明，SGNify在识别性能方面优于SMPLify-SL和PyMAF-X。该研究还确认了语言约束在提高重建人物质量方面的有效性。


# Paper:701     NeRF-DS：用于动态光泽物体的神经辐射场



#### 1. Title: 
NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects

#### 2. Authors: 
Zhiwen Yan, Chen Li, Gim Hee Lee

#### 3. Affiliation: 
National University of Singapore（新加坡国立大学）

#### 4. Keywords: 
Neural Radiance Fields, Dynamic Scenes, Specular Surface Rendering

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yan_NeRF-DS_Neural_Radiance_Fields_for_Dynamic_Specular_Objects_CVPR_2021_paper.html  Github: https://github.com/JokerYan/NeRF-DS

#### 6. Summary : 
- (1):本文研究动态场景下的光泽表面渲染问题，该问题在现有的动态NeRF模型中尚未得到很好的解决。
- (2):现有的动态NeRF模型在渲染光泽表面时存在问题，因为它们在变形时没有考虑表面信息，导致渲染时出现颜色冲突。本文提出了一种表面感知的动态NeRF模型，通过在变形前将颜色与表面坐标和法向量联系起来，解决了现有模型的问题。此外，本文还提出了一种基于移动对象掩码的变形场，以更好地指导光泽表面的变形。这两种方法的结合使得本文提出的NeRF-DS模型在动态光泽表面渲染方面取得了显著的改进。
- (3):本文提出了一种表面感知的动态NeRF模型，通过在变形前将颜色与表面坐标和法向量联系起来，解决了现有模型在渲染光泽表面时存在的问题。此外，本文还提出了一种基于移动对象掩码的变形场，以更好地指导光泽表面的变形。实验结果表明，本文提出的NeRF-DS模型在动态光泽表面渲染方面取得了显著的改进。
- (4):本文提出的NeRF-DS模型在动态光泽表面渲染方面取得了显著的改进，实验结果表明其在自行收集的数据集上的表现优于现有的NeRF模型。
#### 7. 方法详细介绍：
本文提出了一种名为NeRF-DS的方法，用于使用神经辐射场重建动态镜面物体。该方法使用三维特殊欧几里得群（SE（3））作为从观察空间到规范空间的变形场。通过预测并在规范空间中变形表面法线，确保了随时间的表面法线一致性。为了减轻镜面表面颜色的剧烈变化问题，该方法引入了一个由移动物体的二维掩码引导的变形场。预测的掩码被馈送到变形场和超坐标预测网络中。掩码预测网络是一个6层MLP，宽度为64。掩码预测的最终输出经过ReLU激活。使用体积渲染将训练视图中的2D掩码监督预测的3D掩码。该方法在动态镜面数据集上进行评估，并与HyperNeRF，Nerfies和Ref-NeRF的基线模型进行比较。

#### 8. 实验设置：
该方法在一个动态镜面数据集上进行评估，该数据集包含各种类型的移动或变形镜面物体，位于日常环境中的8个场景。每个场景包含由两个前置摄像头拍摄的视频，这两个摄像头被刚性地安装在一起。其中一个摄像头的镜头用于训练，另一个用于测试。使用COLMAP进行相机注册，然后使用MiVOS生成的掩码。生成的图像与地面实况测试视图图像进行比较，以计算以下定量指标：MS-SSIM，PSNR，LPIPS。报告所有帧的平均分数。训练和渲染在480x270分辨率下进行。

#### 9. 实验结果和分析：
NeRF-DS方法在LPIPS评估中显著优于所有基线模型。与基线相比，该方法显著减少了严重的撕裂和模糊伪影。定量结果表明，NeRF-DS在使用NeRF进行动态镜面场景重建方面实现了最先进的性能。在大多数场景和整体平均值方面，该方法的MS-SSIM和PSNR得分也更高。消融研究表明，当删除表面感知动态NeRF或掩码引导变形场时，性能会下降，这验证了每个组件的贡献。此外，两个消融模型相对于基线的优越性能进一步支持了所提出方法的有效性。然而，动态镜面物体的重建质量取决于准确的表面法线预测，而预测的表面法线可能会被反射纹理误导，这是该方法的局限性。


# Paper:702     基于事件极化成像的形状恢复



#### 1. Title: 
Event-based Shape from Polarization

#### 2. Authors: 
Manasi Muglikar, Leonard Bauersfeld, Diederik Paul Moeys, Davide Scaramuzza

#### 3. Affiliation: 
第一作者：苏黎世大学机器人与感知组

#### 4. Keywords: 
Shape-from-Polarization, event cameras, surface normal estimation, deep learning, polarization imaging

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Muglikar_Event-Based_Shape_From_Polarization_CVPR_2021_paper.html  Github: https://github.com/uzh-rpg/esfp

#### 6. Summary : 
- (1):本文研究了基于极化成像的形状恢复问题，提出了一种基于事件相机的形状恢复方法，旨在解决现有方法中速度和分辨率之间的矛盾问题。

- (2):现有的SfP方法要么牺牲极化角度的数量，要么需要长时间的采集时间，从而在精度和延迟之间做出妥协。本文提出了一种基于事件相机的方法，利用事件相机的高速度和微秒级分辨率，以及连续的事件流来重建多个极化角度下的相对强度，从而提高了表面法线的估计精度。本文提出的方法在合成和真实数据集上均优于基于帧的物理模型方法，真实数据集中的性能受到事件相机的非理想性影响，因此提出了一种基于深度学习的方法，通过学习预测密集的表面法线，从而提高了估计精度。

- (3):本文提出了一种基于事件相机的形状恢复方法，利用事件相机的高速度和微秒级分辨率，以及连续的事件流来重建多个极化角度下的相对强度，从而提高了表面法线的估计精度。本文提出的方法在合成和真实数据集上均优于基于帧的物理模型方法，真实数据集中的性能受到事件相机的非理想性影响，因此提出了一种基于深度学习的方法，通过学习预测密集的表面法线，从而提高了估计精度。

- (4):本文提出的方法在表面法线估计任务上取得了良好的性能，合成数据集上的平均角度误差为1.5度，真实数据集上的平均角度误差为2.5度，同时保持了1MP的空间分辨率和50fps的采集速度。本文的方法在解决现有方法中速度和分辨率之间的矛盾问题方面具有创新性和贡献性。
#### 7. 方法详细介绍：
本文提出了一种使用旋转偏振滤波器和事件相机进行表面法线估计的方法。该方法涉及连续捕获偏振器旋转时光线的影响，并使用结果事件强度使用传统算法估计表面法线。本文还提出了一种深度学习方法，通过将稀疏事件流转换为累积体素网格表示（CVGR），并使用U-Net架构预测表面法线，从而改善预测结果。

#### 8. 实验设置：
本文提出了两个数据集，合成和真实世界，其中包含具有挑战性场景的事件、图像和准确的地面真实值。数据集使用旋转偏振滤波器和事件相机收集，捕获12个偏振器角度的事件强度。

#### 9. 实验结果和分析：
本文提出的基于几何的方法在使用4张图像时比其他基于物理的方法的角度误差低14%。基于学习的方法显著提高了性能，但与基于图像的方法相比仍有所不足。在真实数据集上，所提出的方法优于所有基线，尽管在一定程度上不及Ba等人的方法。我们的方法在不同照明条件下保持一致的性能。我们的方法在速度增加时略微改善。所提出的方法也适用于室外场景。


# Paper:703     OrienterNet：使用神经匹配在2D公共地图中进行视觉定位



#### 1. Title: 
OrienterNet: Visual Localization in 2D Public Maps with Neural Matching

#### 2. Authors: 
Paul-Edouard Sarlin, Daniel DeTone, Tsun-Yi Yang, Armen Avetisyan, Julian Straub, Tomasz Malisiewicz, Samuel Rota Bulo, Richard Newcombe, Peter Kontschieder, Vasileios Balntas

#### 3. Affiliation: 
第一作者：ETH Zurich

#### 4. Keywords: 
Visual Localization, 2D Maps, Neural Network, OpenStreetMap, OrienterNet

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2019/html/Sarlin_OrienterNet_Visual_Localization_in_2D_Public_Maps_With_Neural_Matching_CVPR_2019_paper.html  Github: https://github.com/facebookresearch/OrienterNet

#### 6. Summary : 
- (1):本文研究的背景是如何让机器像人类一样使用简单的2D地图进行定位，而不是依赖于复杂的3D点云。
 
- (2):过去的方法主要依赖于复杂的3D点云，需要大量的数据和存储空间，而且不适合在设备上进行本地化。本文提出了一种新的方法，使用公共的2D地图进行视觉定位，避免了3D点云的缺点，同时利用了开放的地图数据，使得本地化更加便捷和实用。

- (3):本文提出了一种名为OrienterNet的深度神经网络，可以使用与人类相同的2D语义地图进行图像定位。OrienterNet通过匹配神经Bird's-Eye View和来自OpenStreetMap的开放和全球可用地图来估计查询图像的位置和方向。OrienterNet仅由相机姿态进行监督，但可以以端到端的方式学习执行语义匹配。为了实现这一点，我们引入了一个大规模的众包数据集，其中包含来自12个城市的图像，从汽车、自行车和行人的不同视角拍摄。OrienterNet可以推广到新的数据集，并在机器人和AR场景中推动最新技术的发展。

- (4):本文的方法在驾驶场景中的本地化任务和AR应用中的性能都有很大提升，可以实现亚米级的定位精度。本文的方法可以在大规模部署机器人和增强现实设备时解决空间本地化的瓶颈问题。
#### 7. 方法详细介绍：
OrienterNet是一种基于神经匹配的2D公共地图视觉定位方法。该方法由三个模块组成：图像-CNN、地图-CNN和穷举匹配。图像-CNN从图像中提取语义特征，并通过推断场景的3D结构将其提升到正交的鸟瞰图（BEV）表示。地图-CNN将平面地图编码为神经地图，结合几何和语义信息。最后，穷举匹配通过将BEV与地图进行匹配，估计相机姿态的概率分布。OrienterNet利用OpenStreetMap（OSM）中所有可用的语义类，并通过将投影几何的约束与端到端学习的表达能力相结合，显著提高了准确性和鲁棒性。

#### 8. 实验设置：
本文使用了一个大型的众包数据集，其中包含来自12个城市的图像，这些图像是从汽车、自行车和行人的不同视角拍摄的。在驾驶场景中，使用了KITTI数据集进行评估，报告了位置和旋转误差的召回率。在增强现实场景中，作者使用Aria眼镜记录了自己的数据集，并通过联合优化所有序列来获得伪GT全局姿态。

#### 9. 实验结果和分析：
本文评估了OrienterNet在单帧定位和多帧定位方面的性能。对于单帧定位，OrienterNet在城市峡谷中的性能优于GPS，但在驾驶数据中的表现较差。对于多帧定位，将多个GPS信号或OrienterNet预测融合在同一时间间隔内，使用VI SLAM的相对姿态，可以将OrienterNet的精度提高一倍以上，但对于GPS传感器的效果仅有轻微的改善。本文还指出了在缺乏显著元素或地图空间不准确的环境中，定位图像或序列的局限性。附录A展示了一些失败案例。

#### 总结：
本文提出了一种基于神经匹配的2D公共地图视觉定位方法OrienterNet。该方法利用OpenStreetMap中所有可用的语义类，通过将投影几何的约束与端到端学习的表达能力相结合，显著提高了准确性和鲁棒性。实验结果表明，OrienterNet在驾驶场景和增强现实场景中的性能均优于以往的方法。


# Paper:704     AccelIR：面向任务的图像压缩加速神经修复



#### 1. Title: 
AccelIR: Task-aware Image Compression for Accelerating Neural Restoration

#### 2. Authors: 
Juncheol Ye, Hyunho Yeo, Jinwoo Park, Dongsu Han

#### 3. Affiliation: 
韩国科学技术院（KAIST）

#### 4. Keywords: 
Image restoration, image compression, deep neural networks, task-aware compression, IR-aware compression

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Ye_AccelIR_Task-Aware_Image_Compression_for_Accelerating_Neural_Restoration_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是深度神经网络在图像修复中的高计算量问题。
 
- (2):过去的方法包括设计新的神经网络或修剪其参数，但是这些方法没有考虑图像压缩对图像修复质量的影响。本文提出了AccelIR框架，它优化图像压缩，考虑到IR任务的端到端流水线。AccelIR通过IR感知压缩对图像进行编码，根据对IR质量的影响优化图像块内的压缩级别。然后，它在压缩图像上运行轻量级IR网络，有效地减少IR计算，同时保持相同的IR质量和图像大小。 

- (3):本文提出了AccelIR框架，它是第一个考虑到图像压缩对IR质量影响的IR感知图像压缩框架。AccelIR旨在减少IR计算，同时保持相同的IR质量和图像大小。为了实现这一目标，AccelIR开发了一个实用的IR感知压缩算法，并采用了轻量级IR网络。AccelIR分为离线分析和在线压缩两个阶段。在离线分析阶段，AccelIR将代表性数据集中的图像块聚类成组，并构建描述压缩级别对IR质量和图像大小影响的配置文件。在在线阶段，AccelIR通过运行CNN检索每个图像块的配置文件。然后，AccelIR参考块级配置文件选择每个块的最佳压缩级别，从而在相同的图像大小下最大化IR质量。最后，应用轻量级IR网络。 

- (4):本文在9个IR网络上进行了广泛的评估，结果表明AccelIR可以将超分辨率、去噪和去模糊的计算开销分别平均降低49％、29％和32％，同时保持相同的IR质量和图像大小。这些结果支持了本文的目标。
#### 7. 方法详细介绍：
AccelIR是一种任务感知的图像压缩方法，旨在加速神经恢复。该方法包括离线分析和在线QP分配两个阶段。在离线分析阶段，AccelIR将图像分块并将其聚类成M个离散的IR效用组，为每个组构建质量和大小配置文件。此外，训练了一个轻量级CNN来预测每个图像块的IR效用组。在在线QP分配阶段，QP分配器使用A-star算法检索配置文件并在图像块之间分配QP。AccelIR还支持动态块分区，可以调整图像块的大小和QP。AccelIR旨在对未见过的IR任务和网络具有鲁棒性，因此可以在各种IR应用中轻松应用。

#### 8. 实验设置：
作者使用JPEG和WebP作为标准图像编解码器，并使用九种不同的IR网络进行超分辨率、去噪和去模糊任务。DIV2K数据集用于训练，DIV8K数据集用于测试。对于超分辨率，使用双三次插值将图像下采样四倍。对于去噪，将高斯噪声添加到图像中，标准差为25。对于去模糊，将高斯模糊滤镜应用于图像，标准差为1.5。

#### 9. 实验结果和分析：
AccelIR在保持相同IR质量和图像大小的情况下，将超分辨率、去噪和去模糊的计算开销分别降低了49％、29％和32％。AccelIR还显示出对未见过的IR任务、网络和数据集具有鲁棒性，并在客户端未应用IR时保持原始图像质量。AccelIR生成的比特流接近给定的大小约束，并在相同IR质量和图像大小下在各种设置中大大加速了IR网络。


# Paper:705     利用预训练的视觉-语义空间学习生成受语言监督和开放词汇的场景图



#### 1. Title: 
Learning to Generate Language-supervised and Open-vocabulary Scene Graph using Pre-trained Visual-Semantic Space

#### 2. Authors: 
Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei, and Chang-Wen Chen

#### 3. Affiliation: 
Yong Zhang, Rui Huang: The Chinese University of Hong Kong, Shenzhen
Yingwei Pan, Ting Yao, Tao Mei: HiDream.ai Inc.
Chang-Wen Chen: The Hong Kong Polytechnic University

#### 4. Keywords: 
Scene graph generation, visual-semantic space, language supervision, open-vocabulary, pre-training

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Learning_to_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究场景图生成（SGG）任务，旨在将图像抽象为图形结构，将对象表示为图形节点，将它们的关系表示为带标签的边缘。然而，当前SGG方法在实际应用中存在两个难题：1）训练SGG模型需要耗费大量的标注时间，2）封闭的对象类别使得SGG模型在识别训练语料库之外的新对象方面受到限制。本文提出了一种新颖的方法，利用预训练的视觉-语义空间（VSS）以简单而有效的方式触发语言监督和开放词汇的SGG。 

- (2):过去的SGG方法需要昂贵的标注，而且只能识别预定义的对象类别，无法识别新的对象类别。本文提出的方法通过利用预训练的VSS，可以从图像语言描述中获取廉价的场景图监督数据，并通过文本提示在VSS中执行对象类别名称对齐，从而实现开放词汇的对象检测。 

- (3):本文提出了一种新的SGG模型，称为Visual-Semantic Space for Scene graph generation (VS3)，它将原始图像和包含对象类别名称的文本提示作为输入，并将它们投影到共享的VSS中作为嵌入。然后，VS3通过对齐类别名称和图像区域的嵌入来执行对象检测。基于高置信度检测到的对象，VS3使用设计的关系嵌入模块构建对象对的关系表示。最后，关系预测模块将关系表示用于推断关系标签。本文的方法可以使用从图像描述中解析出的视觉基础语义图作为弱场景图监督，实现语言监督的SGG。 

- (4):本文在Visual Genome基准测试上进行了广泛的实验验证，跨越各种SGG场景（即监督/语言监督，封闭集/开放词汇），取得了一致的优异表现，证明了利用预训练的VSS进行SGG的潜力。
#### 7. 方法详细介绍：
本文提出了一种名为VS3的视觉-语义空间方法，用于场景图生成（SGG）。该方法利用预训练的视觉-语义空间（VSS）和设计的关系识别头来生成场景图。VSS是通过预训练的图像-文本匹配模型GLIP构建的，将图像和文本映射到一个共同的嵌入空间中。关系识别头包括关系嵌入模块和关系预测模块。关系嵌入模块从视觉和空间特征中构建关系表示，关系预测模块预测关系标签和对象对。VS3的训练使用从图像语言描述中解析语义图并将其接地到图像区域的弱监督。VS3模型在VG150数据集上进行了全监督SGG和COCO数据集上的开放词汇SGG的微调。

#### 8. 实验设置：
本文使用Visual Genome（VG）数据集的VG150版本来评估SGG任务。该数据集包含约108K张图像，其中70％的图像用于训练（包括5K张验证图像），其余30％的图像用于测试。每个图像的注释场景图平均具有11.5个对象和6.2个关系三元组。还使用COCO字幕数据集来训练SGG模型，该数据集总共包含123K张图像。每个图像都有5个人工注释的字幕。使用VG150测试集来计算召回率指标。召回率指标是在约15k个测试图像上计算的，不包括在预训练期间看到的图像。采用SGDET协议从输入图像生成场景图，不提供任何框信息。召回率@K（K = 20/50/100）用作评估指标。

#### 9. 实验结果和分析：
本文在各种SGG场景（即全监督/语言监督，封闭集/开放词汇）下使用提出的方法进行了评估。实验在具有Intel Xeon E5-2690 CPU，256GB内存和四个NVIDIA Tesla V100 GPU的服务器上进行。实现基于PyTorch，代码可在线获得。实验结果表明，与现有方法相比，所提出的VS3模型在所有设置中均取得了一致的优异表现，展示了利用预训练的VSS进行SGG在更实际的场景中的潜力。结果表明，所提出的方法在全监督SGG中实现了最先进的性能，并且在许多全监督方法中表现优异。在语言监督设置中，VS3始终超过以前的方法，展示了预训练语言-图像模型的好处。在开放词汇设置中，VS3实现了有竞争力的性能，并在某些情况下优于现有方法。本文还展示了开放词汇SGG的定性结果，展示了该方法检测新对象及其与其他对象的关系的能力。


# Paper:706     Point2Pix: 基于神经辐射场的点云逼真渲染



#### 1. Title: 
Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields

#### 2. Authors: 
Tao Hu, Xiaogang Xu, Shu Liu, Jiaya Jia

#### 3. Affiliation: 
第一作者：香港中文大学

#### 4. Keywords: 
Point cloud rendering, Neural Radiance Fields, point-guided sampling, Multi-scale Radiance Fields

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Point2Pix_Photo-Realistic_Point_Cloud_Rendering_via_Neural_Radiance_Fields_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究点云渲染，旨在从点云中合成高质量的图像。传统的点云渲染方法存在着点云稀疏性导致的缺陷和细节缺失问题，而Neural Radiance Fields (NeRF)方法可以从2D输入中合成逼真的图像，但是大多数NeRF方法是针对特定场景的，需要大量的训练时间，限制了其实际应用。

- (2):传统的点云渲染方法存在着点云稀疏性导致的缺陷和细节缺失问题，而NeRF方法需要大量的训练时间，限制了其实际应用。本文提出了Point2Pix方法，将点云与2D图像像素联系起来，从而合成高质量的图像。同时，本文提出了点导向采样、点编码和融合编码等方法，提高了渲染效率和图像质量。

- (3):本文提出了Point2Pix方法，将点云与2D图像像素联系起来，从而合成高质量的图像。同时，本文提出了点导向采样、点编码和融合编码等方法，提高了渲染效率和图像质量。其中，点导向采样策略主要关注点云周围的区域，提高了采样效率；点编码提取了多尺度的3D点特征，保证了渲染的泛化性；融合编码逐步合成高分辨率的图像，填补了可能的空洞并提高了渲染图像的质量。

- (4):本文在ScanNet和ArkitScenes数据集上进行了广泛的实验和消融研究，证明了所提出方法的有效性和泛化性。在定量和定性评估中，Point2Pix方法均取得了优于现有方法的结果，证明了其在点云渲染任务上的优越性。
#### 7. 方法详细介绍：
本文提出了一种基于神经辐射场的点云渲染方法Point2Pix。该方法包括三个主要组件：点引导采样、多尺度辐射场和融合解码。点引导采样通过点云的引导实现更高效的光线采样。多尺度辐射场通过构建不同尺度的多个3D特征体积来提取具有区分性的3D点和光线特征。融合解码是一个神经渲染器，通过条件卷积和上采样模块从渲染的特征图中合成最终图像。损失函数包括点云损失、NeRF渲染损失、神经渲染损失和感知损失。 

#### 8. 实验设置：
本文在包含点云和多视图图像的室内数据集ScanNet和ARKitScenes上进行了实验。ScanNet是一个RGBD扫描数据集，包含1,513个场景中不同视角的2.5百万张图像。该数据集已经标注了校准相机和彩色点云。前1,200个场景用作训练集，其余用作测试集。ARkitScenes是一个3D室内场景理解数据集，其场景由Apple iPad捕获。

#### 9. 实验结果和分析：
本文的实验结果表明，Point2Pix方法在室内数据集上具有很好的泛化性能和高质量的图像合成效果。该方法在PSNR、SSIM和LPIPS等指标上均优于现有的方法。同时，该方法具有更快的渲染速度和更少的采样数。消融实验表明，每个提出的组件都对方法的整体性能有所贡献。


# Paper:707     超高分辨率图像语义分割：一个新的基准



#### 1. Title: 
Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark

#### 2. Authors: 
Deyi Ji, Feng Zhao, Hongtao Lu, Mingyuan Tao, Jieping Ye

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Ultra-High Resolution, Semantic Segmentation, Benchmark, Wavelet Transform

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ji_Ultra-High_Resolution_Segmentation_With_Ultra-Rich_Context_A_Novel_Benchmark_CVPR_2021_paper.html  Github: https://github.com/jankyee/URUR

#### 6. Summary : 
- (1):本文研究的是超高分辨率图像的语义分割问题，由于现有数据集的限制，需要构建一个全新的数据集来推动该领域的发展。

- (2):现有的超高分辨率图像语义分割方法大多采用全局-局部协作网络，但是这种方法需要对图像进行多次预测，速度较慢。本文提出了一种更高效的框架WSDNet，采用多级离散小波变换和小波平滑损失来提高分割精度和速度。

- (3):本文提出了一个新的数据集URUR，包含超高分辨率图像、复杂场景、丰富的上下文和精细的注释。同时，提出了WSDNet框架，采用多级离散小波变换和小波平滑损失来提高分割精度和速度。

- (4):在多个超高分辨率图像数据集上进行实验，WSDNet的性能优于现有方法，达到了较高的分割精度和速度。
#### 7. 方法详细介绍：
WSDNet方法将一个深度网络D和一个超分辨率头结合起来，用于学习类别上下文和重建原始输入。D网络使用基于离散小波变换（DWT）的可逆下采样操作来保留原始图像细节并减少信息损失。从DWT获得的子带图像可以进一步用DWT处理以产生分解结果。D的输出是一个1/32特征图，富含高级别的类别上下文，通过两级IWT上采样到1/8特征图。提出了小波平滑损失（WSL）来优化重建过程，通过在频域中重建超分辨率输出Irec来实现软和平滑的约束。总损失函数包括WSL、最终分割结果和D后的辅助分割头的标准交叉熵损失。

#### 8. 实验设置：
实验在三个数据集DeepGlobe、Inria Aerial和URUR上进行。所有比较方法使用相同的训练设置，包括SGD优化器、动量0.9、初始学习率10^-3、多项式衰减参数0.9、批量大小8和最大迭代次数40K、80K和160K。小波平滑损失（WSL）的参数设置为λ1 = 1、λ2 = 0.8、λ3 = 0.1、L = 3。

#### 9. 实验结果和分析：
本文提出的WSDNet方法在URUR数据集上取得了最好的性能，mIoU为46.9%，准确率为76.8%。提出的小波平滑损失（WSL）比普通超分辨率损失更有效地重建了原始结构上下文和纹理分布。实验结果还表明，提出的弱监督学习（WSL）方法比其他损失函数更有效。本文还比较了不同的下采样方法，结果表明多级DWT方法取得了最佳性能。


# Paper:708     LAVENDER: 将视频-语言理解统一为掩码语言建模



#### 1. Title: 
LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling

#### 2. Authors: 
Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, Lijuan Wang

#### 3. Affiliation: 
第一作者单位：微软公司

#### 4. Keywords: 
Video-language understanding, pre-training, downstream tasks, masked language modeling, unified framework

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_LAVENDER_Unifying_Video-Language_Understanding_as_Masked_Language_Modeling_CVPR_2021_paper.html  Github: https://github.com/microsoft/LAVENDER

#### 6. Summary : 
- (1):本文研究背景是视频-语言理解领域中，现有的模型需要为每个任务设计特定的模型架构和训练目标，因此需要一个统一的框架来支持多个任务。

- (2):过去的方法需要为每个任务设计特定的模型架构和训练目标，这些方法不够通用，无法适应新的任务。本文提出了一种统一的框架，将所有预训练和下游任务都统一为掩码语言建模，使用轻量级的掩码语言建模头部，而不是具有更多参数的解码器。实验结果表明，该方法在14个视频-语言理解任务上取得了竞争性的性能。

- (3):本文提出了一种统一的视频-语言理解框架LAVENDER，其中掩码语言建模被用作所有预训练和下游任务的公共接口。在预训练中，使用两个任务：掩码语言建模和视频文本匹配。在下游任务中，使用相同的掩码语言建模头部，而不是为每个任务设计特定的头部。本文的创新点在于将所有预训练和下游任务都统一为掩码语言建模，使用轻量级的掩码语言建模头部，而不是具有更多参数的解码器。

- (4):本文在14个视频-语言理解任务上进行了实验，包括视频问答、文本到视频检索和视频字幕生成等任务。实验结果表明，LAVENDER在这些任务上取得了竞争性的性能，支持多任务微调，具有良好的泛化性能和零-shot评估能力。
#### 7. 方法详细介绍：
本文提出了一种名为LAVENDER的统一框架，用于视频-语言理解。该框架将预训练和下游任务统一为掩码语言建模（MLM）。LAVENDER由视觉编码器、文本编码器和多模态融合编码器组成。其中，视觉编码器采用Video Swin Transformer将原始视频帧输入编码为一系列视觉特征。文本编码器采用轻量级词嵌入层对输入文本句子进行编码。多模态融合编码器是一个12层、768维Transformer，通过自注意力操作将来自视觉和文本编码器的单模态特征融合在一起，学习跨模态表示。在下游任务中，LAVENDER使用两个预训练任务：MLM和视频文本匹配（VTM）。在下游适应期间，LAVENDER使用与预训练中相同的MLM头部用于所有下游任务，而不是按照VidL文献中的标准做法将预训练中的MLM头部替换为新头部。下游任务包括文本到视频检索、多项选择和开放式视频问答以及视频字幕生成。

#### 8. 实验设置：
本文在四个数据集上评估了LAVENDER框架，包括TGIF、MSRVTT、LSMDC和MSVD。预训练数据大小为14M或16M，微调数据大小因任务而异。实验在单个NVIDIA V100 GPU上进行。

#### 9. 实验结果和分析：
本文在14个视频-语言理解基准测试上评估了LAVENDER框架，包括视频问答、文本到视频检索和视频字幕生成。结果表明，LAVENDER在所有14个基准测试中均取得了竞争性能，在10个基准测试中优于先前发布的最先进方法。实验还表明，LAVENDER可以在多任务微调时仅使用一组参数值支持所有下游任务，具有很强的泛化能力，可以在有限的训练样本上进行各种下游任务的零样本评估。代码可在https://github.com/microsoft/LAVENDER上获得。


# Paper:709     DeCo：通过粗到细的对比排序进行组合时序定位的分解和重构



#### 1. Title: 
DeCo : Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-to-Fine Contrastive Ranking

#### 2. Authors: 
Lijin Yang, Quan Kong, Hsuan-Kung Yang, Wadim Kehl, Yoichi Sato, Norimasa Kobori

#### 3. Affiliation: 
第一作者：The University of Tokyo

#### 4. Keywords: 
Compositional temporal grounding, coarse-to-fine decomposition, contrastive ranking, video understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-to-Fine_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是视频中的组合时序定位问题，即通过已知的单词组合来定位视频中的动作。这是一个基础性的挑战，因为视频中的动作种类繁多，语言描述也多种多样，因此需要一种能够处理新的组合结构的方法来实现模型的泛化。

- (2):过去的方法主要有两种，一种是将视频和语言编码成全局表示，然后通过特定的交互模块来融合它们进行预测；另一种是将视频和语言分解成单词级别的基本元素，然后只学习细粒度的语义对应关系。这两种方法都没有考虑组合的粒度，因此容易出现泛化问题。本文提出了一种粗到细的分解方法，通过将原始查询句子分解成不同的粒度级别，然后通过对比排序约束来学习视频和重新组合的查询之间的正确对应关系。

- (3):本文提出了一种分解和重构的方法，通过弱监督的方式学习单词级别的对应关系。具体来说，对于每个锚定样本，我们生成一个时间提议，并通过挖掘负样本来学习正样本的表示。为了结构化整个弱监督，我们掩盖给定锚定查询中的单词，并使用正样本和负样本查询的嵌入与相关的伪时间段从视频中进行重构。此外，我们还通过粗到细的方式进行时间边界预测，以实现精确的定位边界检测。

- (4):本文在Charades-CG和ActivityNet-CG两个数据集上进行了实验，结果表明我们的方法在组合时序定位任务上具有优越的泛化性能。
#### 7. 方法详细介绍：
本文提出了一种名为DeCo（Decomposition and Reconstruction）的方法，用于组合式时间定位。该方法使用掩码条件的Transformer来聚合每个估计的时间提议内的逐帧特征，基于高斯曲线中的权重。然后将交叉注意力特征馈送到全连接层，将特征投影到单词数量空间，以计算掩码单词的交叉熵。该模型通过三个损失函数进行联合优化：回归损失、重构损失和排名损失。排名损失基于对比排名设计，用于学习子句和视频之间的相似性。

具体步骤包括：
1. 句子分解：使用词性标注和组合模板从原始查询句子生成子句。
2. 重组：使用Transformer解码器将子句重新组合成新句子。
3. 时间提议/边界预测：使用基于Transformer的模型为每个输入句子生成时间提议，并估计给定查询句子的时间边界。
4. 掩码重构：随机将原始查询句子中的30％单词替换为<mask>标记，并使用掩码条件的Transformer来预测给定查询的前缀和提议内的视觉特征的下一个单词。

#### 8. 实验设置：
本文在Charades-CG和ActivityNet-CG数据集上进行了实验。每个数据集都包含原始Charades-STA和ActivityNet Captions数据集的新测试拆分的Novel-Composition。Charades-CG数据集在Training / Novel-Composition / Test-Trivial集中具有8281/3442/3096个视频-句子对，而ActivityNet-CG数据集在Training / Novel-Composition / Test-Trivial集中具有36724/12028/15712个视频-句子对。ActivityNet-CG数据集使用C3D特征，Charades-CG数据集使用I3D特征。

#### 9. 实验结果和分析：
本文在Charades-CG和ActivityNet-CG数据集上报告了IoU@{0.5, 0.7}和mIoU结果。DeCo在mIoU方面在两个数据集上均取得了最佳结果。Charades-CG数据集上的对比排名消融研究表明，具有重组的完整模型实现了最佳性能。本文还进行了消融研究，以检查提出的对比排名方法和提示工程的有效性。研究包括不同的排名配置和通过更改M来改变可变长度可学习上下文提示的研究。本文还比较了使用可学习提示标记和使用手工制作提示模板之间的性能差异。


# Paper:710     基于Prompt Tuning的多标签图像识别中的文本作为图像



#### 1. Title: 
Texts as Images in Prompt Tuning for Multi-Label Image Recognition

#### 2. Authors: 
Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo

#### 3. Affiliation: 
Harbin Institute of Technology (哈尔滨工业大学)

#### 4. Keywords: 
Prompt tuning, multi-label image recognition, vision-language pre-trained models, text-as-image prompting, TaI-DPT

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2021_paper.html  Github: https://github.com/guozix/TaI-DPT

#### 6. Summary : 
- (1):本文研究背景是如何在数据有限或标签有限的情况下，通过prompt tuning的方式，将大型视觉-语言预训练模型（如CLIP）适应于各种下游任务。
- (2):过去的方法通常需要视觉数据来学习prompt，但是这种方法在数据有限或标签有限的情况下效果不佳。本文提出了一种新的方法，即将文本描述视为图像进行prompt tuning，即TaI prompting。TaI prompting的优点是文本描述易于获取，其类别标签可以直接导出，因此相对于从图像中学习prompt，TaI prompting更少受到数据有限和标签有限的问题的影响。 
- (3):本文提出了一种新的方法，即TaI prompting，将文本描述视为图像进行prompt tuning。在多标签图像识别任务中，我们使用TaI prompting，其中野外句子作为图像的替代品进行prompt tuning。此外，我们还提出了TaI-DPT，以提取粗粒度和细粒度嵌入以增强多标签识别性能。TaI-DPT包括全局prompt和局部prompt，可以通过最小化排名损失来调整。实验结果表明，我们的TaI-DPT在多个基准测试中均优于零-shot CLIP，并且可以与现有的从图像中学习prompt的方法结合使用以进一步提高识别性能。
- (4):本文的方法在多个基准测试中均取得了优异的性能，例如MS-COCO，VOC2007和NUS-WIDE。TaI prompting的优点是文本描述易于获取，其类别标签可以直接导出，因此相对于从图像中学习prompt，TaI prompting更少受到数据有限和标签有限的问题的影响。TaI-DPT可以提取粗粒度和细粒度嵌入以增强多标签识别性能。
#### 7. 方法详细介绍：
本文提出了一种名为“文本即图像提示”（TaI prompting）的方法，该方法仅使用自由形式的文本作为训练数据，学习下游多标签识别任务的有效提示。TaI提示使用两组提示来处理两个并行分支中的全局和局部特征，通过最大化将每个图像分类为其真实类别的概率来进行学习。使用双粒度提示调整（即TaI-DPT）来获取全局和区域类别嵌入。使用名词过滤策略为每个文本描述生成分类伪标签，该策略应用于监督通过计算类别嵌入和文本特征的余弦相似度来获得的分类分数。在测试期间，通过使用文本编码器对两组学习提示进行编码来获得类别嵌入，而将其他输入源从文本描述更改为测试图像。最终的分类结果是通过融合全局和局部余弦分数来获得的。

#### 8. 实验设置：
本文在三个数据集上进行了实验：VOC2007、MS-COCO和NUS-WIDE。对于零样本实验，不使用数据集的训练集，仅使用文本数据来学习提示。对于少样本和部分标记实验，使用相应的训练数据。所有数据集的训练时期长度均设置为20，MS-COCO、VOC2007和NUS-WIDE的学习率分别为1e-4、1e-4和1e-3。

#### 9. 实验结果与分析：
本文提出的TaI-DPT在零样本实验中在所有数据集上均优于CLIP。在MS-COCO的少样本实验中，TaI-DPT在零样本下达到了59.2%的mAP，与使用5个样本的方法相当。将TaI-DPT与现有方法结合的提示集成策略在少样本和部分标记设置中提高了多标签识别性能。消融实验表明，所提出的TaI提示可以在少量文本的情况下实现良好的性能。


# Paper:711     Pix2Map：跨模态检索推断街道地图



#### 1. Title: 
Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images

#### 2. Authors: 
Xindi Wu, KwunFung Lau, Francesco Ferroni, Aljoˇsa Oˇsep, Deva Ramanan

#### 3. Affiliation: 
第一作者：Xindi Wu，普林斯顿大学

#### 4. Keywords: 
autonomous driving, street map, cross-modal retrieval, image processing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Pix2Map_Cross-Modal_Retrieval_for_Inferring_Street_Maps_From_Images_CVPR_2021_paper.html  Github: https://github.com/Pix2Map/Pix2Map

#### 6. Summary : 
- (1):本文研究自动驾驶中的道路地图构建问题，提出了一种从图像中推断城市街道地图拓扑结构的方法。该问题的挑战在于需要从原始图像数据中推断出复杂的城市道路拓扑结构。 
- (2):过去的方法通常需要先从单目图像中提取道路标记或中心线，然后使用生成式循环神经网络或基于优化的方法来生成空间图。本文提出了一种新的思路，将道路地图构建问题转化为跨模态检索问题，通过学习图像和现有地图的联合嵌入空间，直接从图像数据中检索出最匹配的地图。 
- (3):本文的核心贡献是提出了一种基于序列Transformer的图编码器，用于从任意大小和拓扑结构的街道地图中提取固定维度的嵌入向量。通过对图像和地图编码器进行训练，使它们在相同的嵌入空间中操作，利用最近的跨模态对比学习技术，实现了图像和地图的联合嵌入。本文的方法不仅可以用于地图构建，还可以用于视觉定位和图像检索等任务。 
- (4):本文在Argoverse数据集上进行了实验，结果表明，本文的方法可以准确地检索出与图像对应的街道地图，且可以用于更新或扩展现有地图。本文的方法相对于现有的图形生成方法具有更好的性能。
#### 7. 方法详细介绍：
Pix2Map方法是一种跨模态检索方法，用于直接从自车视角图像中推断城市街道地图拓扑结构。该方法将Pix2Map重新构建为跨模态检索任务，通过对测试图像的视觉嵌入进行计算，然后检索与余弦相似度最接近的图形。该方法训练图像和图形编码器以在相同的嵌入空间中操作，利用最近的跨模态对比学习技术。该方法还引入了一种新颖但简单的图形编码器，基于语言领域的顺序变压器，从任意大小和拓扑的街道地图中提取固定维度的嵌入。该方法证明了即使是最近邻检索也可以与图形生成的领先技术相媲美，而通过Pix2Map的跨模态检索则表现更好，因为它能够学习规范化图形输出空间的图形嵌入。

具体步骤如下：
1. 训练图像和图形编码器，将它们映射到一个共同的固定维度空间中。
2. 使用训练好的编码器从训练库中检索与测试图像最相似的图形。
3. 将检索到的图形表示为节点图，其中每个顶点表示一个车道节点，边编码节点之间的连接性。
4. 使用检索到的图形作为地图，从像素图像输入中回归地图。

#### 8. 实验设置：
实验评估使用Argoverse数据集。该数据集包含城市环境的3D点云、360度相机图像和高清地图。数据集分为训练、验证和测试集。训练集包含113,000张图像及其对应的地图，而验证和测试集各包含5,000张图像。

#### 9. 实验结果和分析：
实验结果表明，Pix2Map能够准确地检索与图像对应的街道地图，包括已知和未知的道路。该方法在从图像线索生成图形方面优于现有的图形生成方法。该方法还证明了即使没有访问那些区域的地面真实图形，也可以检索到与之前未见过的区域相似的图形，并展示了对空间图形的视觉定位和图像检索的概念验证结果。


# Paper:712     使用多个深度学习网络现代化老照片



#### 1. Title: 
Modernizing Old Photos Using Multiple 
Deep Learning Networks

#### 2. Authors: 
Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang

#### 3. Affiliation: 
香港中文大学

#### 4. Keywords: 
Deep learning, image restoration, image enhancement, generative adversarial networks

#### 5. Paper: https://arxiv.org/abs/1808.03867  Github: None

#### 6. Summary : 
- (1):本文研究的背景是老照片的修复和增强，这是一个具有挑战性的问题，因为老照片通常存在噪声、模糊和失真等问题。

- (2):过去的方法通常只能处理单一的问题，如去噪或去模糊，而且这些方法通常需要手动选择参数，不够自动化。本文提出的方法是使用多个深度学习网络来处理老照片的修复和增强，这些网络可以自动学习参数，同时处理多个问题，如去噪、去模糊和增强细节等。这种方法的动机是为了提高老照片修复和增强的效果和自动化程度。

- (3):本文提出了一种基于多个深度学习网络的老照片修复和增强方法。该方法包括三个步骤：去噪、去模糊和增强细节。每个步骤都使用一个深度学习网络来处理。这些网络都是基于生成对抗网络（GAN）的，可以自动学习参数。此外，本文还提出了一种新的损失函数，用于训练这些网络。这种损失函数可以同时考虑多个问题，如去噪、去模糊和增强细节等。

- (4):本文的方法在多个数据集上进行了实验，包括老照片修复和增强数据集。实验结果表明，本文的方法在去噪、去模糊和增强细节等方面都比现有方法表现更好。这表明本文的方法可以有效地提高老照片修复和增强的效果和自动化程度。
#### 7. 方法详细介绍：
本文提出了一种使用多个深度学习网络对旧照片进行现代化处理的方法。该方法包括三个阶段：图像恢复、着色和风格转移。在第一阶段中，使用深度学习网络去除噪声、划痕和其他伪影，以恢复图像。在第二阶段中，使用另一个深度学习网络对恢复的图像进行着色。最后，在第三阶段中，使用风格转移网络将现代照片的风格转移到着色的图像上，从而得到旧照片的现代化版本。具体步骤如下：
1. 图像恢复阶段：使用深度学习网络去除噪声、划痕和其他伪影，以恢复图像。
2. 着色阶段：使用深度学习网络对恢复的图像进行着色。
3. 风格转移阶段：使用风格转移网络将现代照片的风格转移到着色的图像上，从而得到旧照片的现代化版本。

#### 8. 实验设置：
本文使用了两个数据集进行实验：ImageNet和Flickr。在ImageNet数据集上，使用了1000张图像进行训练，500张图像进行验证，500张图像进行测试。在Flickr数据集上，使用了1000张图像进行训练，500张图像进行验证，500张图像进行测试。实验使用了Python编程语言和TensorFlow深度学习框架。

#### 9. 实验结果与分析：
本文的实验结果表明，所提出的方法可以有效地将旧照片现代化。与其他方法相比，本文的方法在图像恢复、着色和风格转移方面均取得了更好的效果。此外，本文还进行了消融实验，证明了每个阶段的网络对最终结果的贡献。


# Paper:713     基于时空交叉注意力的3D人体姿态估计



#### 1. Title: 
3D Human Pose Estimation with Spatio-Temporal Criss-cross Attention

#### 2. Authors: 
Zhenhua Tang, Zhaofan Qiu, Yanbin Hao, Richang Hong, Ting Yao

#### 3. Affiliation: 
合肥工业大学

#### 4. Keywords: 
3D human pose estimation, transformer-based solutions, spatio-temporal correlation, attention mechanism, STCFormer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是3D人体姿态估计，该问题在计算机视觉领域备受关注，具有广泛的应用前景。

- (2):过去的方法主要分为两类：单目和视频序列。单目方法通常采用两阶段流程，即先提取2D关键点，再将其提升到3D空间。视频序列方法则通过聚合邻近帧的时间线索来提高3D坐标回归的准确性。然而，这些方法都存在一些问题，如深度模糊、计算成本高等。本文提出了一种新的注意力机制，即Spatio-Temporal Criss-cross attention (STC)，以解决这些问题。

- (3):本文提出的STC机制将相关性学习分解为空间和时间两个方面，并在每个方面上分别执行注意力机制。STC通过将注意力层的输出连接起来，同时建模同一帧中的关节和同一轨迹中的关节之间的交互作用。在此基础上，本文设计了STCFormer，通过堆叠多个STC块，并进一步将新的结构增强的位置嵌入（SPE）集成到STCFormer中，以考虑人体结构。实验结果表明，STCFormer在Human3.6M和MPI-INF-3DHP基准测试中取得了优异的性能。

- (4):本文在Human3.6M和MPI-INF-3DHP基准测试中进行了广泛的实验，取得了优于现有技术的结果。STCFormer在Human3.6M数据集上取得了40.5mm P1误差，是迄今为止发表的最佳性能。
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的3D人体姿态估计方法，称为Spatio-Temporal Criss-cross Transformer (STCFormer)。该方法采用了一种新的空间-时间交叉注意力机制，能够同时建模空间和时间上的关联性。此外，该方法还引入了一种新的结构增强位置嵌入（SPE）方法，用于探索人体的局部结构。STCFormer模型由多个STC块和SPE嵌入组成，其中STC块用于建模空间和时间上的关联性，SPE嵌入用于建模每个关节的静态和动态特征。该方法在Human3.6M和MPI-INF-3DHP数据集上进行了评估，并取得了最先进的性能。

具体步骤如下：
1. 输入2D姿态序列，采用CPN模型预测2D姿态或使用真实2D姿态。
2. 采用结构增强位置嵌入（SPE）方法，将每个关节的2D坐标转换为嵌入向量。
3. 将嵌入向量输入到多个STC块中，每个STC块由两个并行的注意力层组成，分别用于建模空间和时间上的关联性。
4. 将STC块的输出输入到Transformer编码器中，用于建模全局关联性。
5. 最后，通过最小化估计3D坐标和真实3D坐标之间的均方误差（MSE）来训练模型。

#### 8. 实验设置：
本文在两个大规模数据集Human3.6M和MPI-INF-3DHP上进行了评估。对于Human3.6M数据集，使用1、5、6、7和8号主体进行训练，使用9和11号主体进行评估。使用Mean Per Joint Position Error (MPJPE)作为评估指标，分别采用Protocol 1 (P1)和Protocol 2 (P2)两种协议进行评估。对于MPI-INF-3DHP数据集，采用MPJPE (P1)、150mm下的正确关键点百分比（PCK）和曲线下面积（AUC）作为评估指标。输入2D姿态序列采用CPN模型预测的2D姿态或真实2D姿态。模型参数通过Adam优化器进行优化，基本学习率为0.001，每个epoch后衰减0.96，训练20个epoch。

#### 9. 实验结果和分析：
本文提出的STCFormer模型在Human3.6M和MPI-INF-3DHP数据集上均取得了最先进的性能。在Human3.6M数据集上，该模型在15个类别中有10个类别取得了最佳性能，并在P1误差上优于当前最先进模型0.3mm。在MPI-INF-3DHP数据集上，该模型在PCK、AUC和P1误差上均优于当前最先进模型，分别提高了0.8%、8.1%和9.1mm。本文还进行了误差分布分析和消融实验，验证了所提出方法的有效性。


# Paper:714     MM-Diffusion: 学习多模态扩散模型，实现联合音频和视频生成



#### 1. Title: 
MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation

#### 2. Authors: 
Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, Baining Guo

#### 3. Affiliation: 
1. 中国人民大学 (Renmin University of China)
 
#### 4. Keywords: 
Audio-Video Generation, Multi-Modal Diffusion Model, Denoising Autoencoder, Attention Mechanism

#### 5. Paper: https://arxiv.org/abs/2106.04768  Github: None

#### 6. Summary : 
- (1):本文提出了一种联合音视频生成框架，旨在实现高质量逼真视频的同时提供令人愉悦的观看和听觉体验。
 
- (2):与现有的单模态扩散模型不同，本文提出了一种新颖的多模态扩散模型（即MM-Diffusion），由两个耦合的去噪自编码器组成，采用顺序多模态U-Net进行联合去噪处理。为了确保跨模态的语义一致性，本文提出了一种基于随机移位的注意力机制，用于跨模态对齐，从而增强音视频的保真度。 
 
- (3):本文提出的MM-Diffusion模型在无条件音视频生成和零样本条件任务（例如视频到音频）方面取得了优异的结果。特别是在Landscape和AIST++跳舞数据集上，本文实现了最佳的FVD和FAD。本文的创新点在于提出了一种多模态扩散模型，采用随机移位的注意力机制，实现了音视频的联合生成。
 
- (4):本文的方法在无条件音视频生成和零样本条件任务方面取得了优异的性能，证明了其有效性和可行性。
#### 7. 方法详细介绍：
本文提出了一种名为MM-Diffusion的多模态扩散模型，用于联合音频和视频生成。该模型由两个耦合的自编码器组成，其中包含一个序列多模态U-Net，用于联合去噪过程。两个子网络逐步从高斯噪声中生成对齐的音频-视频对。提出了基于随机移位的注意力块，以确保跨模态的语义一致性并实现高效的跨模态对齐。

#### 8. 实验设置：
本文使用了Landscape和AIST++跳舞数据集进行了无条件音频-视频生成和零样本条件任务的实验。模型使用了Adam优化器，学习率为0.0002，批量大小为16。训练过程中使用了随机裁剪和水平翻转等数据增强技术。

#### 9. 实验结果和分析：
本文的模型在无条件音频-视频生成和零样本条件任务中均取得了优异的结果，Landscape和AIST++跳舞数据集上的FVD和FAD均为最佳。10k次图灵测试进一步证明了该模型的卓越性能。图1展示了生成的视频帧和音频频谱图的示例。


# Paper:715     利用表示增强的超类学习



#### 1. Title: 
Superclass Learning with Representation Enhancement

#### 2. Authors: 
Zeyu Gan, Suyun Zhao, Jinlong Kang, Liyuan Shang, Hong Chen, Cuiping Li

#### 3. Affiliation: 
中国人民大学

#### 4. Keywords: 
Superclass Learning, Representation Enhancement, Cross-Instance Attention, Contrastive Learning, Target Adjustment Loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gan_Superclass_Learning_With_Representation_Enhancement_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是超类识别问题，即将数据按照专家知识分为少量的超类别，而不是基于图像表示的原始类别。由于缺乏共同的语义特征，现有的分类技术难以识别超类别，因此需要新的方法来解决这个问题。

- (2):过去的方法主要集中在基本级别的分类上，而对于超类别的识别问题，现有的分类技术基于平滑性假设，无法实现实际部署和可扩展性，并且在超类别情况下可能会受到严重的性能损失。本文提出了一种新的超类学习框架，通过利用增强表示来识别超类别。具体来说，通过跨批次利用自我关注技术，SCLRE消除了那些原始类别的边界，并增强了每个超类别的表示。在增强表示空间上，然后重构超类别感知的决策边界。本文的方法在CIFAR-100数据集和四个高分辨率数据集上的实验结果表明，SCLRE优于基线和其他对比方法。

- (3):本文提出了一种超类学习框架，称为SuperClass Learning with Representation Enhancement（SCLRE），通过利用跨实例注意力技术来执行表示增强，从而实现打破基本级别边界的目标。然后，SCLRE通过一系列调整损失在增强表示空间中构建新的高级超类别感知边界。

- (4):本文的方法在CIFAR-100数据集和四个高分辨率数据集上进行了实验，结果表明SCLRE优于基线和其他对比方法。本文的方法可以有效地识别超类别，从而解决了现有分类技术在超类别情况下的性能损失问题。
#### 7. 方法详细介绍：
本文提出了一种针对超类图像识别的方法——SCLRE（SuperClass Learning with Representation Enhancement）。SCLRE利用跨实例的自注意力技术进行表示增强，打破了基本级别表示的边界。然后，SCLRE通过一系列调整损失在增强表示空间中重构了一个新的超类感知边界。具体而言，SCLRE的过程包括以下几个步骤：
（1）跨实例自注意力模块：该模块通过关注实例之间的关系来打破基本级别表示的边界，从而实现表示增强。
（2）增强表示空间：在增强表示空间中，SCLRE通过将表示混合并集成它们来获得超类感知的增强表示。
（3）调整损失：SCLRE的调整损失包括三个方面：类别分类损失、对比度调整损失和目标调整损失。这些损失的加权和构成了SCLRE的总损失。
#### 8. 实验设置：
本文的实验在多个人工构建的超类数据集上进行，包括CIFAR-100、mini-ImageNet、VOC、FMoW和Adience。超类数据集是通过将初始类别基于领域知识进行整合而人工重新组织的。在跨实例自注意力模块中，考虑了输入数据的分割策略，并比较了三种分割策略。在后续的工作中，采用了基于数据集分布的分割策略。本文将基线模型ResNet和两种SOTA对比技术SupCon和SimCLR与SCLRE进行了比较。
#### 9. 实验结果与分析：
本文在CIFAR100-3、CIFAR100-4和CIFAR100-7数据集上比较了低像素数据集CIFAR-100的分类准确率。结果表明，SCLRE在所有三个数据集上均优于基线模型ResNet和两种SOTA对比技术SupCon和SimCLR。实验结果证明了SCLRE在超类学习中的有效性。


# Paper:716     意外光探针



#### 1. Title: 
Accidental Light Probes

#### 2. Authors: 
Hong-Xing Yu, Samir Agarwala, Charles Herrmann, Richard Szeliski, Noah Snavely, Jiajun Wu, Deqing Sun

#### 3. Affiliation: 
Hong-Xing Yu: Stanford University

#### 4. Keywords: 
Lighting estimation, Accidental Light Probes, Physically-based approach, Inverse rendering, Material reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Accidental_Light_Probes_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究从单个图像中恢复场景的照明是计算机视觉中的一个基本问题。传统的光探针可以捕捉全向照明，但通常在日常照片中缺少。本文提出了一种基于物理的方法，通过对意外光探针（ALP）进行建模和估计单个图像中的照明来解决这个问题。

- (2):传统的光探针可以捕捉全向照明，但通常在日常照片中缺少。研究人员使用日常物品如人脸、眼睛等来估计肖像图像中的照明。本文提出了使用日常金属光滑物体作为意外光探针（ALPs）来估计单个图像中的照明。与纯数据驱动的学习方法相比，本文的物理建模方法可以推广到不同的室内和室外场景。

- (3):本文提出了一种基于物理的方法，通过对意外光探针（ALP）进行建模和估计单个图像中的照明来解决从单个视图中恢复场景照明的问题。主要思想是通过光摄影学原理对ALP的外观进行建模，并通过可微分渲染来反演此过程以恢复意外照明。本文的方法可以将ALP放入场景中以允许高保真度的照明估计。本文的模型还可以恢复包含ALP的现有图像的照明。

- (4):本文的方法在日常场景中使用意外光探针（ALPs）来估计照明，可以在单个图像中实现高保真度的照明估计。本文的方法在ALP数据集和包含ALP和光探针的图像数据集上表现出比现有方法更好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于可微分渲染的物理模型的反渲染方法，用于从单张图像中估计环境光照。该方法首先扫描常见的3D物体并重建其反射属性，然后创建了一个新的数据集，包括几种常见的、有光泽的、弯曲的物体的材料和几何形状，以及这些物体在各种室内外环境中的图像。该方法在逼真度和保真度方面明显优于以前的方法。具体步骤包括：
1. 通过可微分渲染生成反渲染图像。
2. 通过反渲染图像和真实图像之间的差异计算损失函数。
3. 通过优化算法最小化损失函数，得到环境光照和物体姿态。

#### 9. 实验结果与分析：
本文在各种材料的完美球体上进行了定性和定量实验，包括镜面、有光泽和漫反射。结果表明，该方法在角度误差和尺度不变的RMSE方面优于其他单张图像光照估计方法。本文还将该方法与其他基线方法进行比较，并证明该方法产生的结果更准确、更引人入胜。本文还对每个设计选择进行了消融研究，并报告了定量结果。最后，本文可视化了ALP的置信区域，并表明该方法对于微小的反射和形状变化具有容忍性。


# Paper:717     自动驾驶中FMCW雷达的方位超分辨率



#### 1. Title: 
Azimuth Super-Resolution for FMCW Radar in Autonomous Driving

#### 2. Authors: 
Yu-Jhe Li, Shawn Hunt, Jinhyung Park, Matthew O’Toole, Kris Kitani

#### 3. Affiliation: 
第一作者：卡内基梅隆大学（Carnegie Mellon University）

#### 4. Keywords: 
FMCW雷达，自动驾驶，超分辨率，ADC信号

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Azimuth_Super-Resolution_for_FMCW_Radar_in_Autonomous_Driving_CVPR_2021_paper.html  Github: https://github.com/YuJheLi/ADC-SR

#### 6. Summary : 
- (1):本文研究的是自动驾驶中FMCW雷达的方位（角度）超分辨率问题。FMCW MIMO雷达与Lidar和RGB相机一起广泛用于自动驾驶，但与Lidar相比，由于硬件尺寸限制，MIMO雷达通常具有较低的分辨率。由于接收器数量的限制，高分辨率的方位角度测量对于估计物体的位置和速度至关重要。 
- (2):以往的方法主要是通过处理雷达Range-Azimuth-Doppler（RAD）图来实现超分辨率，但是这些方法只能实现有限的性能提升。本文提出了一种基于ADC信号的轻量级、高效的超分辨率模型（ADC-SR），该模型仅使用少数接收器的信号即可预测或虚构额外的雷达信号。与处理RAD图的基线模型相比，我们的ADC-SR方法在处理原始ADC信号时实现了可比较的性能，且参数量减少了98%（50倍）。我们还提出了一种混合超分辨率模型（Hybrid-SR），将ADC-SR与标准RAD超分辨率模型相结合，表现得到了大幅提升。 
- (3):本文提出的ADC-SR模型是第一个将深度模型应用于ADC信号进行方位超分辨率的方法。我们的模型可以预测未见接收器的信号，从而提高雷达的方位分辨率。我们还提出了一种更加理论基础的下采样方法，用于训练和评估。我们的模型不仅在我们收集的Pitt-Radar数据集和一个基准数据集上实现了令人满意的方位超分辨率，而且还通过大幅提高现有目标检测器的性能来证明了我们的方法在自动驾驶中的应用价值。 
- (4):本文的方法在Pitt-Radar数据集和RADIal数据集上进行了实验，证明了其在方位超分辨率方面的有效性。此外，我们还评估了现有目标检测器在我们的超分辨率模型上的性能，结果表明我们的超分辨率模型可以提高目标检测器的性能约4%的mAP。
#### 7. 方法详细介绍：
本文提出了一种模型，名为ADC-SRNet，用于将少量接收器的ADC信号转换为高分辨率的雷达图像。该模型使用了6个残差块，每个块都包含3D卷积和跳跃连接。此外，本文还提出了一种名为RAD-Unet的模型，用于将低分辨率的雷达图像转换为高分辨率的雷达图像。该模型使用了3D卷积和2D Unet。本文还提出了一种混合模型，名为Hybrid-SR，将ADC-SR和RAD-SR结合起来，以提高雷达图像的分辨率。

#### 8. 实验设置：
本文使用了Pitt-Radar数据集进行实验，该数据集包含了用于基准测试的ADC信号。本文还使用了RADIal数据集进行实验。本文还评估了与超分辨率模型一起训练的现有目标检测器的性能。

#### 9. 实验结果与分析：
本文的ADC-SR方法使用原始ADC信号处理，与基线模型相比，参数数量减少了98%（50倍），但性能相当。本文的混合模型Hybrid-SR将ADC-SR和RAD-SR结合起来，大幅提高了基线模型的性能。本文还证明了他们的方法适用于自动驾驶中的下游任务，因为他们的超分辨率模型将现有的目标检测器的mAP提高了约4%。 

#### 整篇论文总结：
本文提出了一种ADC-SRNet模型，用于将少量接收器的ADC信号转换为高分辨率的雷达图像。该模型使用了6个残差块，每个块都包含3D卷积和跳跃连接。此外，本文还提出了一种名为RAD-Unet的模型，用于将低分辨率的雷达图像转换为高分辨率的雷达图像。本文还提出了一种混合模型，名为Hybrid-SR，将ADC-SR和RAD-SR结合起来，以提高雷达图像的分辨率。本文使用了Pitt-Radar数据集进行实验，该数据集包含了用于基准测试的ADC信号。本文还使用了RADIal数据集进行实验。本文还评估了与超分辨率模型一起训练的现有目标检测器的性能。本文的ADC-SR方法使用原始ADC信号处理，与基线模型相比，参数数量减少了98%（50倍），但性能相当。本文的混合模型Hybrid-SR将ADC-SR和RAD-SR结合起来，大幅提高了基线模型的性能。本文还证明了他们的方法适用于自动驾驶中的下游任务，因为他们的超分辨率模型将现有的目标检测器的mAP提高了约4%。


# Paper:718     基于Patch-Craft自监督训练的相关图像去噪



#### 1. Title: 
Patch-Craft Self-Supervised Training for Correlated Image Denoising

#### 2. Authors: 
Gregory Vaksman and Michael Elad

#### 3. Affiliation: 
Gregory Vaksman: The Technion, 以色列理工学院计算机科学系
Michael Elad: The Technion, 以色列理工学院计算机科学系

#### 4. Keywords: 
Image denoising, self-supervised training, correlated noise, patch-craft images, burst shots

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Vaksman_Patch-Craft_Self-Supervised_Training_for_Correlated_Image_Denoising_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是图像去噪问题，针对噪声模型未知的情况，提出了一种自监督训练框架，适用于去除未知相关噪声。
 
- (2):过去的方法需要已知的噪声模型或成对的干净图像和噪声图像，但这些数据在实际应用中很难获得。本文提出的方法不需要已知的噪声模型或干净图像，只需要使用容易捕获的噪声图像序列。本文的方法是通过构建人工的patch-craft图像来作为训练目标，而不需要对序列中的图像进行配准。本文的方法在处理未知相关噪声的情况下，具有很好的适用性和可行性。

- (3):本文提出的方法是通过使用噪声图像序列来构建人工的patch-craft图像，作为训练目标。具体而言，本文将输入图像分成完全重叠的patch，然后在序列中寻找最近邻的patch，将这些patch拼接起来构建patch-craft图像。本文的方法不需要对序列中的图像进行配准，因此具有很好的可扩展性。本文还提出了一种方法来分析目标噪声的统计特性，以减少目标噪声与去噪器输入之间的统计依赖性，从而提高去噪性能。

- (4):本文的方法在合成和真实图像噪声方面进行了广泛的实验，表明本文的方法优于其他自监督去噪方法。本文的方法在处理未知相关噪声的情况下，具有很好的适用性和可行性，可以在实际应用中得到广泛的应用。
#### 7. 方法详细介绍：
本文提出了一种名为Patch-Craft（PC）的框架，用于去除未知相关噪声。该方法依赖于噪声帧的突发或短视频序列的可用性。该方法应用于构建Patch-Craft图像的补丁匹配，并将其用作训练目标。该框架以自适应方式进行训练，从预先训练的无偏网络开始，用于盲i.i.d.高斯去噪任务，并使用所提出的Patch-Craft方法重新训练它们。该方案考虑了两种不同的体系结构：DnCNN和U-Net。补丁大小n应该很大，并随噪声的标准偏差σ和相关范围而增长。

#### 8. 实验设置：
对于相关高斯去噪实验，使用DAVIS数据集，分辨率为480p，并在不同的训练视频序列中使用了90个长度为7帧的突发。对于真实世界的噪声去除实验，使用CRVD数据集，该数据集由在照片实验室中拍摄的11组噪声图像和它们的地面真实对应物组成。每个组捕捉不同的场景，每个场景捕捉7次，每个这些7个图像的组都可以视为人工视频序列。通过复制每个序列7次来增加数据集，在每个副本中，使用不同的图像作为中间帧。

#### 9. 实验结果和分析：
对于相关高斯去噪实验，所提出的方法在PSNR和SSIM方面优于当前最先进的自监督方法，特别是对于中等到严重的噪声。对于真实世界的噪声去除实验，即使在小而具有挑战性的数据集上进行训练，所提出的方法也取得了良好的结果，并在PSNR和SSIM方面优于当前最先进的自监督方法，特别是对于高ISO值。经典的面向信号处理的O-BM3D方法可以获得相对较高的PSNR，但往往会产生模糊的图像或留下明显的低频噪声未经滤波。


# Paper:719     减少Plateau的可微路径追踪



#### 1. Title: 
Plateau-reduced Differentiable Path Tracing

#### 2. Authors: 
Michael Fischer, Tobias Ritschel

#### 3. Affiliation: 
Michael Fischer: 伦敦大学学院(University College London)
Tobias Ritschel: 伦敦大学学院(University College London)

#### 4. Keywords: 
Differentiable rendering, Monte Carlo, optimization, light transport, plateaus

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Fischer_Plateau-Reduced_Differentiable_Path_Tracing_CVPR_2021_paper.html  Github: https://github.com/mfischer-ucl/prdpt

#### 6. Summary : 
- (1):本文研究了不同iable渲染中的plateau问题，即目标函数中存在梯度为零的区域，导致优化无法收敛。 
- (2):过去的方法包括不同iable渲染和光栅化，但它们都存在一些问题，如不适用于复杂的光传输效应，或者无法解决plateau问题。本文提出了一种新的方法，通过将高维渲染函数与额外的核卷积来模糊参数空间，从而减少plateau问题。 
- (3):本文提出的方法是一种轻量级的扩展，适用于黑盒和不同iable渲染器，可以优化具有复杂光传输的问题，如焦散或全局照明。本文提出了两种蒙特卡罗估计器，以有效地计算减少plateau的梯度，并证明这些转化为优化误差和运行时性能的净增益。 
- (4):本文的方法在旋转咖啡杯等任务上取得了良好的性能，可以有效地解决plateau问题。
#### 7. 方法详细介绍：
本文提出了一种名为“Plateau-reduced Differentiable Path Tracing”的方法，通过在参数空间上对整个光路进行模糊处理，以减少目标函数中的高原。该方法将渲染方程与模糊核进行卷积，以实现模糊操作。该核作为加权函数，对于偏移τ的参数进行加权，同时对场景中的所有光路进行积分。该方法通过其估计器的梯度来估计目标函数的梯度，可以使用标准路径追踪求解。该方法还使用了零方差的重要性采样、反向采样和自适应平滑。该方法易于实现，只需在现有框架中添加几行代码即可。

#### 8. 实验设置：
本文在六个优化任务上评估了所提出的方法及其竞争对手，这些任务涉及高级光传输、高原和模糊。任务包括CUP、SHADOWS、OCCLUSION、GLOBAL ILLUMINATION、SORT和CAUSTIC。本文使用相同的渲染设置（每像素采样数、分辨率、路径长度等）对所有方法进行相同次数的迭代。

#### 9. 实验结果与分析：
所提出的方法在所有任务上均比其竞争对手表现出更高的优化效果。常规的基于梯度的路径追踪器在任务中失败。SoftRas可以克服一些高原，但难以实现低参数误差。该方法实现了低至10^-7的误差。有趣的是，使用可微分渲染器的梯度比使用基于扰动的微分方法表现更差。 

#### 全文总结：
本文提出了一种名为“Plateau-reduced Differentiable Path Tracing”的方法，通过在参数空间上对整个光路进行模糊处理，以减少目标函数中的高原。该方法结合了可微分光栅化和可微分路径追踪的优点，易于实现，高效，具有理论上的合理性，并优化了现有可微分渲染器分歧的任务。本文在六个优化任务上评估了所提出的方法及其竞争对手，结果表明该方法在所有任务上均表现出更高的优化效果。


# Paper:720     级联局部隐式变换器用于任意尺度超分辨率



#### 1. Title: 
Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution

#### 2. Authors: 
Hao-Wei Chen, Yu-Syuan Xu, Min-Fong Hong, Yi-Min Tsai, Hsien-Kai Kuo, Chun-Yi Lee

#### 3. Affiliation: 
1. Elsa实验室，清华大学
2. 联发科技股份有限公司

#### 4. Keywords: 
Super-resolution, implicit neural representation, attention mechanism, frequency encoding, arbitrary-scale

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Cascaded_Local_Implicit_Transformer_for_Arbitrary-Scale_Super-Resolution_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是单幅图像超分辨率重建问题，旨在从低分辨率图像中重建出高分辨率图像。这是一个具有挑战性的问题，因为它是一个不逆问题，即存在多种可能的高分辨率图像与低分辨率图像对应。本文的研究背景是现有的超分辨率方法通常只能处理特定的上采样倍数，而无法处理任意上采样倍数的情况。

- (2):过去的超分辨率方法通常采用卷积神经网络（CNN）或深度残差网络（ResNet）等结构，这些方法需要针对每个上采样倍数训练一个不同的模型，因此无法处理任意上采样倍数的情况。近年来，一些基于隐式神经表示的方法被提出，可以通过单个模型实现任意上采样倍数的超分辨率。然而，这些方法通常只考虑了局部像素之间的距离关系，而忽略了像素之间的上下文信息。本文提出了一种局部隐式变换器（LIT）方法，它将注意力机制和频率编码技术集成到局部隐式图像函数中，以有效地聚合局部特征和结合位置编码和傅里叶域信息构建高分辨率图像。本文还提出了级联LIT（CLIT）方法，以利用多尺度特征和渐进式训练策略来处理任意尺度的超分辨率问题。

- (3):本文提出的LIT方法包括交叉尺度局部注意力块（CSLAB）、局部频率编码块（LFEB）和解码器。CSLAB根据查询坐标处的双线性插值潜向量和从具有相对位置偏差的坐标网格中采样的关键潜向量生成注意力图。LFEB将相对坐标投影到潜空间中，以解决隐式神经函数中固有的光谱偏差问题。最后，解码器利用注意力特征嵌入和频率嵌入生成RGB值。为了处理任意尺度的超分辨率问题，本文提出了累积训练策略和级联框架CLIT。累积训练策略首先使用小的上采样倍数训练局部隐式图像函数，然后使用交替采样的小和大倍数进行微调。CLIT利用多尺度特征嵌入来补充一步上采样中缺失的细节和信息。

- (4):本文在多个基准数据集上进行了广泛的实验，结果表明LIT和CLIT方法在任意尺度的超分辨率任务中均取得了优异的结果，并且在定量和定性指标上均优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种名为局部隐式变换器（Local Implicit Transformer，LIT）的方法，用于任意尺度的超分辨率。LIT包括交叉尺度局部注意力块（Cross-Scale Local Attention Block，CSLAB）、局部频率编码块（Local Frequency Encoding Block，LFEB）和解码器。CSLAB基于双线性插值的潜在向量和采样自具有相对位置偏差的坐标网格的关键潜在向量生成注意力图。LFEB将相对坐标投影到潜在空间中，以解决隐式神经函数的频谱偏差问题。最后，解码器利用注意力特征嵌入和频率嵌入生成RGB值。本文还提出了级联LIT（Cascaded LIT，CLIT）来利用多尺度特征嵌入的优势，补充一步上采样过程中缺失的细节和信息。累积训练策略和CLIT的组合使得有效处理任意尺度的超分辨率任务成为可能。

#### 8. 实验设置：
本文使用DIV2K数据集进行网络训练，该数据集包含1000张2K分辨率的图像，低分辨率图像由双三次插值方法生成。在训练过程中，将48x48低分辨率图像的批次输入到框架中，并从均匀分布中采样单个上采样比例尺度。

#### 9. 实验结果与分析：
本文将提出的CLIT框架与多种现有方法进行比较，包括EDSR、RDN和SwinIR等。实验结果表明，CLIT在PSNR和SSIM指标方面优于其他方法。本文还进行了全面的分析，以验证训练策略和组件的有效性。定量和定性评估均表明，LIT和CLIT相对于现有方法具有更好的性能。


# Paper:721     MixMAE：用于高效预训练分层视觉Transformer的混合和掩蔽自编码器



#### 1. Title: 
MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers

#### 2. Authors: 
Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, Hongsheng Li

#### 3. Affiliation: 
Jihao Liu: CUHK MMLab (香港中文大学多媒体实验室)

#### 4. Keywords: 
Pretraining, Vision Transformers, Masked Image Modeling, MixMAE

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Liu_MixMAE_Mixed_and_Masked_Autoencoder_for_Efficient_Pretraining_of_Hierarchical_CVPR_2022_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的背景是如何有效地预训练分层视觉Transformer模型。
- (2):过去的方法主要是基于Masked Image Modeling (MIM)的方法，但是使用[MASK]符号会导致训练速度变慢，且预训练和微调之间存在不一致性。本文提出了一种新的预训练方法MixMAE，通过将一个图像的部分token替换为另一个图像的可见token，从而避免了使用[MASK]符号的问题，同时也提高了预训练效率。
- (3):本文提出的MixMAE方法可以应用于各种分层Transformer模型，如Swin Transformer等。通过实验，MixMAE在各种下游任务上都取得了较好的性能，比SimMIM等方法更加高效。
- (4):在ImageNet-1K数据集上，MixMAE预训练600个epoch后，使用Swin-B/W14模型达到了85.1%的top-1准确率。同时，在其他6个数据集上的迁移学习表现也证明了MixMAE的优越性能。
#### 7. 方法详细介绍：
本文提出了一种新的自监督预训练方法MixMAE，它利用混合图像进行预训练，通过将两个图像的可见标记混合生成混合图像，然后使用Swim Transformer等视觉骨干网络对混合图像进行编码，再使用小型ViT解码器对两个原始图像进行重建。MixMAE方法可以适用于各种层次的Vision Transformer，包括Swin Transformer、Twins和PVT。该方法在预训练时不需要拆分2D图像的结构，因此更加灵活，可以适应各种视觉骨干网络。MixMAE方法通过双重重建来充分利用混合输入，同时重建两个图像以预训练神经网络。为了降低优化难度，MixMAE通过探索混合嵌入和掩蔽自注意力方法来实现双重重建。

#### 8. 实验设置：
本文在ImageNet-1K、COCO和ADE20K数据集上评估了MixMAE方法。预训练使用600个epoch和224×224的输入大小在ImageNet-1K上进行。默认情况下，使用75%的掩蔽比例，即混合4个图像。在各种基准测试中，包括ImageNet-1K、Places、iNaturalist、COCO和ADE20K，使用分层学习率衰减策略进行微调，对于Swin-B/L/H，设置Drop Path正则化为0.15/0.2/0.3。微调持续时间为Swin-B/L/H的100/50/50个epoch。

#### 9. 实验结果和分析：
本文提出的MixMAE方法在COCO和ADE20K数据集上优于以前的自监督方法，相比于MAE-B with Swin-B，平均性能提高了2.9%。预训练的Swin-L在推理时只需要58%的FLOPs，平均性能提高了1.4%。MixMAE方法在ImageNet-1K、Places、iNaturalist、COCO和ADE20K等各种基准测试中均取得了最先进的性能。在ImageNet-1K上，MixMAE方法在预训练300个epoch后实现了84.8%的top-1准确率，比BEiT少了62.5%的预训练epoch，同时获得了更高的准确率。在COCO的目标检测和实例分割任务中，MixMAE方法使用Swin-B实现了52.7 APbox和47.0 APmask，超过了以前的自监督方法，同时FLOPs和参数更少。在ADE20K的语义分割中，MixMAE方法使用Swin-B实现了51.1 mIoU，比BEiT高4.0，同时推理时只需要一半的FLOPs。


# Paper:722     无绑定风格化3D角色的零样本姿势转移



#### 1. Title: 
Zero-shot Pose Transfer for Unrigged Stylized 3D Characters

#### 2. Authors: 
Jiashun Wang, Xueting Li, Sifei Liu, Shalini De Mello, Orazio Gallo, Xiaolong Wang, Jan Kautz

#### 3. Affiliation: 
Jiashun Wang: 卡内基梅隆大学 (Carnegie Mellon University)

#### 4. Keywords: 
3D character deformation, pose transfer, stylized characters, local deformation, implicit shape representation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Zero-Shot_Pose_Transfer_for_Unrigged_Stylized_3D_Characters_CVPR_2021_paper.html  Github: https://jiashunwang.github.io/ZPT/

#### 6. Summary : 
- (1):本文研究的是将参考角色的姿势转移到各种形状的风格化3D角色上的问题。与现有方法不同，该方法在训练时只需要参考角色的网格在静止和所需姿势下的形状，以及风格化角色的网格仅在静止状态下的形状。本文的研究背景是3D角色变形的问题。

- (2):现有的方法要么需要风格化角色被绑定，要么在训练时使用所需姿势下的风格化角色作为ground truth。本文提出了一种零样本方法，只需要训练中易得到的变形的非风格化角色，就可以在推理时变形具有显著不同形状的风格化角色。本文将局部变形的优势与形状理解模块相结合，以绕过测试时需要显式对应标签的要求，并引入了一个隐式姿势变形模块，以变形单个表面点以匹配目标姿势。此外，为了鼓励逼真和准确的变形，本文提出了一种有效的基于体积的测试时间训练过程。本文的方法不需要绑定，也不需要在训练时变形风格化角色，因此可以推广到注释稀少的类别，例如风格化四足动物。本文的方法在任务上取得了良好的性能。

- (3):本文提出了一种无需绑定的风格化角色变形模型，该模型由非风格化二足或四足角色指导。我们通过建模3D角色的对应感知形状模块和隐式姿势变形模块来隐式实现局部变形方法的关键洞察。形状理解模块学习为每个表面点预测部分分割标签（即粗略的对应关系），并表示3D角色的形状为潜在形状代码。姿势变形模块以形状代码为条件，并通过从先验姿势潜在空间中采样的目标姿势代码来变形单个表面点。此外，为了鼓励逼真的变形并推广到罕见的姿势，我们提出了一种新颖的基于体积的测试时间训练过程，可以有效地应用于未见过的风格化角色。在推理过程中，通过将二足或四足姿势从视频映射到先前的姿势潜在空间，以及使用现有的
#### 7. 方法详细介绍：
本文提出了一种零样本姿态转移方法，用于未绑定的风格化3D角色。该方法包括三个模块：对应感知形状理解模块、隐式姿态变形模块和基于体积的测试时间训练模块。对应感知形状理解模块预测3D角色的潜在形状编码和部分分割标签。姿态变形模块根据预测的形状编码和目标姿态编码对处于静止状态的角色进行变形。基于体积的测试时间训练模块对未见过的风格化角色进行微调，并在姿态变形过程中保持每个身体部位的体积。

#### 8. 实验设置：
本文使用了40个从SMPL参数模型中采样的人类网格来训练形状理解模块。这些网格的占用率和部分分割标签被用作监督。为了将形状理解模块推广到风格化角色，本文还包括了来自RigNet的600个风格化角色。本文通过使用5000个从VPoser的潜在空间中采样的姿态编码对40个SMPL角色进行变形来构建成对的训练数据，以训练姿态变形模块。本文在Mixamo和MGN数据集上测试了所提出的方法，这些数据集包括具有挑战性的风格化角色和穿着衣服的人类。测试角色和姿势在训练期间未见过。

#### 9. 实验结果和分析：
本文报告了关于双足和四足类别的实验结果。所提出的方法在与具有可比或更多监督的基线学习方法相比下，实现了最佳的部分分割准确性。该方法与基线相比产生了更真实和准确的变形。消融研究表明，反向约束和保体积损失对所提出的方法的性能有重要贡献。该方法在测试时还表现出良好的泛化能力，可用于训练网格稀缺的类别，例如风格化四足动物。本文使用点间网格欧几里得距离（PMD）和边长得分（ELS）指标评估所提出的方法，并将其与神经混合形状（NBS）和无骨架姿态转移（SPT）基线进行比较。本文还报告了Mixamo数据集上的部分分割准确性。


# Paper:723     信号-表面协同正则化的少样本非直视成像



#### 1. Title: 
Few-shot Non-line-of-sight Imaging with Signal-surface Collaborative Regularization

#### 2. Authors: 
Xintong Liu, Jianyu Wang, Leping Xiao, Xing Fu, Lingyun Qiu, and Zuoqiang Shi

#### 3. Affiliation: 
第一作者：清华大学

#### 4. Keywords: 
Non-line-of-sight imaging, Bayesian inference, regularization, confocal measurement, few-shot detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Few-Shot_Non-Line-of-Sight_Imaging_With_Signal-Surface_Collaborative_Regularization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究非直视成像技术，旨在从多次反射的光中重建目标。现有方法需要对中继表面进行密集的点扫描，以获得高质量的重建，这需要很长的采集时间。本文提出了一种信号-表面协同正则化（SSCR）框架，它提供了噪声鲁棒的重建，并且只需要最少的测量次数。使用贝叶斯推断，我们设计了估计信号、基于3D体素的对象表示和基于2D表面的目标描述的联合正则化。本文是首次将混合维度的正则化方法应用于隐藏目标的研究。实验结果表明，该方法在共聚和非共聚设置下均具有高效性。本文报告了使用公共数据集中的5×5共聚测量重建具有复杂几何结构的隐藏目标，表明传统测量过程的加速因子为10,000。此外，该方法具有低时间和内存复杂度，适用于实时非直视成像应用，如救援行动和自动驾驶。

- (2):现有的非直视成像方法需要密集的测量点扫描，采集时间长，且需要大量的测量数据以获得高质量的重建。本文提出了一种新的方法，可以在极少的测量次数下实现高质量的重建。与现有方法不同，本文采用了基于贝叶斯推断的信号-表面协同正则化框架，将2D表面和3D体素表示的目标结合起来进行联合正则化，以提供噪声鲁棒的重建结果。

- (3):本文提出了一种信号-表面协同正则化（SSCR）框架，用于少量测量次数下的非直视成像重建。该方法采用基于贝叶斯推断的方法，将估计信号、基于3D体素的对象表示和基于2D表面的目标描述进行联合正则化。与现有方法不同，本文采用了基于伯努利光子事件假设的对数数据拟合项，以应对光子检测过程中的随机性。此外，本文还引入了2D表面的正则化项，以提高重建质量。实验结果表明，该方法在共聚和非共聚设置下均具有高效性，并且可以在少量测量次数下实现高质量的重建。

- (4):本文在公共数据集上进行了实验，展示了使用5×5共聚测量次数可以重建具有复杂几何结构的隐藏目标，表明传统测量过程的加速因子为10,000。该方法具有低时间和内存复杂度，适用于实时非直视成像应用，如救援行动和自动驾驶。实验结果表明，该方法在少量测量次数下可以实现高
#### 7. 方法详细介绍：
本文提出了一种信号-表面协同正则化（SSCR）框架的方法。该方法使用贝叶斯推断，联合正则化估计信号、物体的三维体素表示和目标的二维表面描述。SSCR方法使用基于伯努利光子事件假设的对数数据保真度项。它引入了用于隐藏物体表面的二维正则化，并在目标的二维和三维表示中都加入了先验。该方法具有低时间和内存复杂度，适用于稀疏测量。

#### 8. 实验设置：
本文使用斯坦福数据集中雕像和龙的实例的测量数据来测试所提出的方法在实际应用中的效果。原始数据集包含2×2平方米区域内的512×512共焦测量。对于雕像实例，到中继的距离为1米，原始512×512测量的曝光时间分别为60分钟和10分钟。我们使用5×5的子采样信号进行重建，只需要0.34秒和0.05秒来测量子采样信号。对于龙的实例，距离中继表面为1.3米，总曝光时间为60分钟。我们使用10×10的子采样信号进行重建，只需要1.37秒。对于两个实例，时间分辨率为32 ps。对比重建结果如图5所示，其中oracle是使用SOCR方法生成的，使用64×64测量。结果表明，F-K和LCT重建模糊。SSCR重建具有最高的PSNR值，不包含背景噪声。对于龙的实例，材料的反射导致物理模型的大偏差，SSCR方法只重建了目标的一部分。对于雕像的实例，我们在图6中展示了具有不同照明点数量的SSCR重建的三个视图。使用7×7的共焦测量可以清晰地重建目标，这只占原始数据集的0.01％。使用4×4的照明点，SSCR方法仍然提供了合理的目标估计，证明了所提出算法的鲁棒性。更多与现有方法的比较在补充材料中提供。

#### 9. 实验结果和分析：
本文使用金字塔实例的合成信号来测试所提出的方法。只选择了原始64×64合成信号的3×3。用于生成数据的物理模型考虑了光子强度的余弦衰减。金字塔的底边长和高度分别为1米和0.2米。正方形金字塔的中心轴垂直于平面中继表面。金字塔距离中继表面0.5米，时间分辨率为32 ps。实验设置和重建的三个视图如图1所示。焦点的坐标在补充代码中提供。

#### 论文总结：
本文提出了一种基于信号-表面协同正则化（SSCR）框架的方法，用于非直视成像。该方法使用贝叶斯推断，联合正则化估计信号、物体的三维体素表示和目标的二维表面描述。实验结果表明，所提出的方法在稀疏测量下具有更好的重建性能，且具有较低的时间和内存复杂度。与现有方法相比，所提出的方法具有更好的鲁棒性和准确性。


# Paper:724     在3D手部形状重建中克服精度和合理性之间的权衡



#### 1. Title: 
Overcoming the Trade-off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction

#### 2. Authors: 
Ziwei Yu, Chen Li, Linlin Yang, Xiaoxu Zheng, Michael Bi Mi, Gim Hee Lee, Angela Yao

#### 3. Affiliation: 
第一作者：新加坡国立大学

#### 4. Keywords: 
3D hand shape reconstruction, non-parametric mesh fitting, MANO model, weakly-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Overcoming_the_Trade-Off_Between_Accuracy_and_Plausibility_in_3D_Hand_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究3D手部形状重建中精度和合理性之间的权衡问题。
 
- (2):过去的方法要么使用MANO模型保证合理性，但精度不如非参数方法，要么直接拟合3D网格以保证精度，但会出现不合理的形状。本文提出了一种新的弱监督手部形状估计框架，将非参数网格拟合与MANO模型相结合，以实现精度和合理性的权衡。
 
- (3):本文提出了一种新的弱监督手部形状估计框架，将非参数网格拟合与MANO模型相结合，以实现精度和合理性的权衡。该框架包括RGB编码器网络、非参数姿态解码器和基于MANO模型的参数化网格重建。为了准确地将非参数手部网格顶点映射到MANO关节参数，本文提出了一个VAE校正模块。此外，本文还提出了一种弱监督学习方法，使用3D关节标签来学习3D网格。本文的创新点在于将非参数方法和MANO模型相结合，以实现精度和合理性的权衡，并提出了一种新的映射方法来将非参数手部网格顶点映射到MANO关节参数。
 
- (4):本文在手-物体交互和双手交互数据集上进行了实验，取得了显著的改进，特别是在DexYCB数据集上的手-物体交互方面。本文提出的方法在精度和合理性之间取得了平衡，达到了预期的目标。
#### 7. 方法详细介绍：
本文提出了一种综合非参数模型和MANO模型的3D手部形状重建方法，旨在克服精度和合理性之间的平衡问题。该方法包括以下步骤：
1. 使用ResNet50作为骨干网络，结合GCNs或transformers等非参数方法，从输入图像中估计手部网格。
2. 使用估计的手部网格，通过MANO模型预测手部形状和姿态参数。
3. 引入VAE模块，对估计的手部关节进行精细化处理。
4. 引入扭转-摆动分解方法，推断MANO姿态参数。
5. 使用自我蒸馏损失进行弱监督训练。

#### 8. 实验设置：
本文在三个RGB图像手-物交互数据集（FreiHAND、Interhand2.6M和DexYCB）上进行了实验评估。输入图像被裁剪并调整大小为256×256。评估指标包括3D关节的平均位置误差（MPJPE）和网格顶点的平均位置误差（MPVPE），以及边缘距离（mm）和法线误差等额外评估指标。在手-手交互中，计算每个手部顶点在上方的3D网格（32×32）中穿透到另一只手的深度，包括平均穿透深度（A-PD）和最大穿透深度（M-PD）。在手-物交互中，穿透距离的计算与手-手交互相同。

#### 9. 实验结果与分析：
本文提出的方法在单手任务、双手交互任务和手-物交互任务上均取得了比现有方法更好的性能。自我蒸馏学习可以将姿态和形状误差降低近10％，完整模型可以将姿态和形状误差降低近50％。该方法还展示了扭转旋转估计的有效性。然而，非参数模型的初始手部网格限制了结果与地面实况的比较。


# Paper:725     自适应补丁变形的纹理无关多视图立体视觉



#### 1. Title: 
Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo

#### 2. Authors: 
Yuesong Wang, Zhaojie Zeng, Tao Guan, Wei Yang, Zhuo Chen, Wenkai Liu, Luoyuan Xu, Yawei Luo

#### 3. Affiliation: 
华中科技大学计算机科学与技术学院

#### 4. Keywords: 
Multi-view stereo, PatchMatch, deformable convolution, textureless regions, adaptive patch deformation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Adaptive_Patch_Deformation_for_Textureless-Resilient_Multi-View_Stereo_CVPR_2021_paper.html  Github: https://github.com/whoiszzj/APD-MVS

#### 6. Summary : 
- (1):本文研究多视图立体视觉中的纹理缺失问题，提出了一种自适应补丁变形的方法，以提高匹配的鲁棒性。

- (2):传统的基于PatchMatch的方法在处理大规模纹理缺失区域时，需要建立代价体积并大幅增加感受野，导致内存消耗巨大。而基于深度学习的方法可以提取出稳健的视觉特征，但是感受野过大会导致内存消耗过大。本文提出了一种将可变形卷积的思想移植到传统PatchMatch方法中的方法，对于每个匹配模糊的像素，我们自适应地变形其周围的补丁，以扩展感受野，直到覆盖足够的可靠像素。同时，本文提出了一种通过检查优化过程中估计深度的收敛性来评估像素可靠性的方法，以检测更多的锚点像素，从而实现更好的自适应补丁变形。

- (3):本文提出了一种自适应补丁变形的方法，将其应用于传统的PatchMatch方法中，以提高匹配的鲁棒性。同时，本文提出了一种通过检查优化过程中估计深度的收敛性来评估像素可靠性的方法，以检测更多的锚点像素，从而实现更好的自适应补丁变形。

- (4):本文的方法在ETH3D和Tanks and Temples数据集上取得了最先进的性能，同时保持较低的内存消耗。
#### 1. 方法详细介绍：
本文提出了一种自适应补丁变形算法，称为Textureless-Resilient Multi-View Stereo (APD-MVS)。该方法使用可变形补丁来估计纹理缺失区域的深度，并逐渐缩小平面拟合阈值，以允许非平面区域找到正确的深度估计。通过检查优化过程中匹配成本的分布来评估像素的可靠性。基于联合视图选择和可变形补丁中的锚点像素，改进了传播和细化过程，以加快收敛速度和提高准确性。该方法使用基于NCC的匹配度量，并在ETH3D和Tanks and Temples数据集上实现了最先进的性能。

#### 2. 实验设置：
本文使用ETH3D和Tanks and Temples两个数据集来验证所提出方法的有效性。ETH3D数据集用于测试处理高分辨率图像的大规模场景的性能，而Tanks and Temples数据集包含大规模场景但分辨率较小。金字塔层数与图像分辨率有关，在ETH3D上分为四层，在Tanks and Temples上分为三层。在每个层上，每个图像执行四次迭代。在实现基于PM的MVS方法APD-MVS时采用NCC-based匹配度量。

#### 3. 实验结果与分析：
所提出的方法APD-MVS相对于其他传统方法，如[12, 36]，具有更高的完整性，同时包含更少的异常值。在ETH3D数据集上的定量分析表明，我们的方法在F1-score方面排名第一。实验结果证明了所提出方法的有效性和泛化能力。

#### 4. 方法：
本文提出了一种自适应补丁变形算法，使用类似辐条的方法和RANSAC来促进匹配成本的计算。该算法将搜索空间切片成具有相同角度的扇形，并在所有方向上搜索候选像素。在获取候选像素后，通过RANSAC算法进行过滤以提高抗遮挡能力。本文还提出了一种可变形PatchMatch方法，用于计算覆盖足够锚点像素的可变形补丁中的不可靠像素的匹配成本。匹配成本基于NCC度量计算。本文还引入了一种自适应聚合模块，通过可变形卷积实现更强大的特征提取。算法流程如图3所示。

#### 5. 实验设置：
无相关信息。

#### 6. 实验细节：
无相关信息。


# Paper:726     全景视频场景图生成



#### 1. Title: 
Panoptic Video Scene Graph Generation

#### 2. Authors: 
Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, Ziwei Liu

#### 3. Affiliation: 
第一作者：南洋理工大学S-Lab

#### 4. Keywords: 
Panoptic Video Scene Graph, Video Understanding, Scene Graph Generation, Panoptic Segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2022_paper.html  Github: https://github.com/Jingkang50/OpenPVSG

#### 6. Summary : 
- (1):本文提出了一种新的问题——全景视频场景图生成（Panoptic Video Scene Graph Generation，PVSG），旨在通过更精确的像素级分割掩模来实现全面的视频理解。与现有的视频场景图生成（VidSGG）问题不同，PVSG需要场景图中的节点通过更精确的像素级分割掩模来实现全面的场景理解。 
- (2):现有的视频场景图生成方法主要基于边界框，无法检测非刚性物体和背景，这往往会导致VidSGG系统错过关键细节，而PVSG则通过更精确的像素级分割掩模来解决这个问题。本文提出了一个高质量的PVSG数据集，包括400个视频（289个第三人称视频和111个自我中心视频），每个视频都有精细的时间场景图注释和全景分割掩模。 
- (3):本文提出了一个两阶段的框架来解决PVSG问题：第一阶段为每个基于掩模的实例轨迹生成一组特征，第二阶段基于轨迹的特征生成视频级场景图。本文提供了四种不同的实现方法，涵盖了卷积和Transformer-based方法。 
- (4):本文提出的方法在PVSG数据集上进行了实验，取得了优秀的性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一个两阶段的框架来解决Panoptic Video Scene Graph Generation（PVSG）问题。第一阶段是Video Panoptic Segmentation（VPS），旨在以非重叠方式分割和跟踪每个像素。本文提出了两个强大的基线用于第一阶段的VPS处理：IPS+T和VPS。第二阶段是关系分类，它使用对象查询（特征）管道将第一和第二阶段连接起来。本文介绍了四种操作来处理特征对之间的关系：Vanilla、Handcrafted Filter、1D-Convolutional Layer和Transformer Encoder。具体步骤如下：
1. 第一阶段：Video Panoptic Segmentation（VPS）
   - 采用两种设计方案：panoptic segmentation model + tracking module 和 end-to-end video panoptic segmentation model。
   - 对于第一种设计方案，采用了Mask R-CNN作为panoptic segmentation model，使用SORT算法进行跟踪。
   - 对于第二种设计方案，采用了一个基于3D卷积的网络，同时进行分割和跟踪。
2. 第二阶段：Relation Classification
   - 采用对象查询（feature）管道将第一阶段和第二阶段连接起来。
   - 本文提出了四种不同的实现方式，包括基于卷积和Transformer的方法。

#### 8. 实验设置：
本文提出了一个新的问题，即Panoptic Video Scene Graph Generation（PVSG）问题，并贡献了一个高质量的PVSG数据集，其中包含400个视频（289个第三人称视频和111个自我中心视频），共计150K帧，标有全景分割掩模以及精细的时间场景图。数据集中有126个对象类别和57个关系类别。本文的实验设置主要是针对数据集进行的，旨在验证所提出的方法的有效性和性能。

#### 9. 实验结果和分析：
本文的实验结果表明，所提出的方法在PVSG任务上取得了最先进的性能。其中，end-to-end video panoptic segmentation model的性能优于panoptic segmentation model + tracking module的性能。在关系分类阶段，Transformer Encoder的性能优于其他三种实现方式。此外，本文还对数据集进行了分析，发现数据集中存在性别和社会偏见，需要注意数据不平衡问题。


# Paper:727     混合自编码器用于自监督视觉表示学习



#### 1. Title: 
Mixed Autoencoder for Self-supervised Visual Representation Learning

#### 2. Authors: 
Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung

#### 3. Affiliation: 
第一作者所属机构：香港科技大学

#### 4. Keywords: 
Self-supervised learning, visual representation learning, masked autoencoder, mixing augmentation, homologous recognition

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Mixed_Autoencoder_for_Self-Supervised_Visual_Representation_Learning_CVPR_2022_paper.html

Github: None

#### 6. Summary:
- (1):本文研究自监督学习中的视觉表示学习，提出了一种基于混合自编码器的方法，用于有效的数据增强和对象感知的自监督预训练。
- (2):过去的方法主要集中在手工设计的预文本任务和实例鉴别上，而本文则探讨了混合增强对于自编码器的有效性。本文提出了同源识别的辅助预文本任务，以缓解混合增强带来的互信息增加问题，并提高下游密集感知性能。本文的方法在各种下游任务中取得了最先进的转移结果。
- (3):本文提出了一种混合自编码器（MixedAE）方法，通过将输入图像分组并独立生成混合样本，然后采用同源注意力机制来强制每个补丁只关注具有最高注意力质量的补丁，从而进行对象感知的自监督预训练。同时，本文采用同源对比损失来验证采样准确性，以鼓励同源补丁的特征相似，异源补丁的特征不相似。本文的方法在各种下游任务中取得了最先进的转移结果。
- (4):本文的方法在ImageNet-1K、ADE20K和COCO等数据集上进行了广泛的实验，证明了MixedAE在不同下游任务中取得了最先进的转移结果，同时保持了显著的效率。具体而言，MixedAE在标准ViT-Base上的ImageNet-1K、ADE20K和COCO数据集上的表现分别比MAE高0.3％的准确率、1.7的mIoU和0.9的AP。此外，MixedAE还超过了iBOT，一个强大的MIM方法，同时加速了2倍的训练速度。
#### 7. 方法详细介绍：
本文提出了一种自监督视觉表示学习方法，称为混合自编码器（Mixed Autoencoder，MixedAE）。该方法采用混合和同源识别来学习具有对象感知性的表示。MixedAE包含两个主要步骤：混合和解混合。在混合步骤中，数据批次被分成多组，每组生成一个混合图像，混合比例表示每个干净图像在单个混合样本中贡献的补丁比例。混合图像然后被送入编码器进行特征提取。在解混合步骤中，混合特征被“解混合”以恢复混合前的输入批次。解混合特征然后作为解码器的输入进行像素重建。最终的损失函数是重建损失和同源对比损失的加权和。MixedAE的架构包括编码器、解码器和同源注意力模块。编码器从输入图像中提取特征，然后将其解混合并送入解码器进行像素重建。同时，同源对比损失被采用来验证采样准确性，通过鼓励同源补丁的特征相似，异源补丁的特征不相似来实现。

#### 8. 实验设置：
本文在ImageNet-1K、ADE20K和COCO三个数据集上进行了广泛的实验，评估了MixedAE在各种下游任务中的性能，包括图像分类、语义分割和目标检测。所有实验都使用标准的ViT-Base作为骨干架构。作者将MixedAE与其他最先进的方法进行了比较，包括MAE、iBOT、MoCov3、DINO、CIM-ResPix、CIM-RevDet和BEiT。实验在一台8个NVIDIA V100 GPU的单机上进行。

#### 9. 实验结果和分析：
实验结果表明，MixedAE在各种下游任务中均取得了最先进的转移性能，包括图像分类、语义分割和目标检测。具体而言，MixedAE在ImageNet-1K、ADE20K和COCO上的准确率、mIoU和AP分别比MAE高0.3％、1.7和0.9，使用标准的ViT-Base。此外，MixedAE超越了iBOT，一种强大的MIM方法，结合了实例鉴别，同时将训练加速了2倍。作者证明了MixedAE在保持显著效率的同时实现了显著的性能提升。


# Paper:728     增强隐私保护的视觉定位的配对点提升方法



#### 1. Title: 
Paired-Point Lifting for Enhanced Privacy-Preserving Visual Localization

#### 2. Authors: 
Chunghwan Lee, Jaihoon Kim, Chanhyuk Yun, Je Hyeong Hong

#### 3. Affiliation: 
第一作者：韩国汉阳大学电子工程系

#### 4. Keywords: 
Visual localization, privacy-preserving, 3D line clouds, Paired-Point Lifting

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Paired-Point_Lifting_for_Enhanced_Privacy-Preserving_Visual_Localization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是视觉定位中的隐私保护问题，通过构建3D线云来隐藏场景几何信息，以保护隐私。
 
- (2):过去的方法主要是通过结构化方法来实现视觉定位，但是这些方法需要使用全局稀疏3D模型，而最近的研究表明，这些模型可能会泄露场景的细节信息。为了解决这个问题，一种主要的方法是将3D点转换为随机方向的3D线，但是最近的研究表明，这种方法存在一个关键的统计缺陷，可以被攻击者利用来破解保护。因此，本文提出了一种新的轻量级策略，称为Paired-Point Lifting (PPL)，通过将3D点分成对并将每对连接起来形成3D线来构建3D线云，从而增强了隐私保护性能。
 
- (3):本文提出的Paired-Point Lifting (PPL)方法通过将3D点分成对并将每对连接起来形成3D线来构建3D线云，从而增强了隐私保护性能。与过去的方法相比，PPL方法具有三个优点：i)特征选择的新歧义性，ii)增加了线云的稀疏性，iii)3D线的非平凡分布。本文通过大量的实验结果证明了PPL方法在不影响定位精度的情况下，可以更好地隐藏场景细节，从而发挥3D线云的真正潜力。
  
- (4):本文的方法在多个公共数据集上进行了广泛的实验评估，结果表明，与其他基线方法相比，PPL方法在保护隐私的同时，不会降低视觉定位的精度。
#### 7. 方法详细介绍：
本文提出了一种名为Paired-Point Lifting（PPL）的方法，用于构建3D线云。PPL将3D点分成一对对，并将每对连接起来形成3D线。这种方法产生了三个好处：特征选择中的新歧义，增加了线云的稀疏性和非平凡的3D线分布，这些都有助于增强对隐私攻击的保护。PPL+是PPL的升级版，用于解决平面场景的潜在缺点。PPL和PPL+在多个公共数据集上进行了广泛的实验评估，使用不同的点恢复技术进行比较。

#### 8. 实验设置：
本文使用了一系列公共数据集来评估所提出的Paired-Point Lifting（PPL）和PPL+，并使用不同的点恢复技术与其他基线进行比较。这些数据集包括7-Scenes、Cambridge Landmarks和RobotCar Seasons。本文还使用合成和真实数据进行了消融研究，以分析PPL的成功因素。

#### 9. 实验结果和分析：
本文展示了广泛的实验结果，证明了PPL在保护场景细节而不影响定位精度方面的优势，释放了3D线云的真正潜力。结果表明，PPL在保护隐私方面优于其他基线，同时保持可比的定位精度。本文还展示了PPL+解决PPL在平面场景中潜在缺点的能力。消融研究表明，PPL中的三个因素各自增加了一个难度层次，协同增强了隐私保护的视觉定位。本文还包括了不同3D场景表示和相应的图像重建结果使用InvSfM的可视化。


# Paper:729     使用文本到图像扩散的多样性保留领域适应方法，用于3D生成模型



#### 1. Title: 
DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model

#### 2. Authors: 
Gwanghyun Kim, Se Young Chun

#### 3. Affiliation: 
首尔国立大学电气与计算机工程系

#### 4. Keywords: 
Domain adaptation, 3D generative model, text-to-image diffusion, CLIP, diversity preservation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_CVPR_2021_paper.html  Github: https://github.com/gwang-ik/DATID_3D

#### 6. Summary : 
- (1):本文研究背景是3D生成模型的领域适应问题，由于需要大量的训练图像和相机分布信息，因此训练这些模型是具有挑战性的。

- (2):过去的方法包括使用小数据集的迁移学习方法和基于文本的领域适应方法，但它们都存在一些问题，如样本多样性丢失和文本-图像对应性差等。本文提出了一种新的基于文本到图像扩散的领域适应方法，旨在解决这些问题。

- (3):本文提出的方法是DATID-3D，它使用文本到图像扩散模型来合成多样化的图像，而无需为目标域收集额外的图像和相机信息。然后，通过我们的新型CLIP和姿态重建过滤过程对目标图像进行校正。使用这些过滤后的目标图像，进行3D领域适应，同时保留文本中的多样性以及多视角一致性。我们将这种方法应用于EG3D，一种最先进的3D生成模型，使其能够在文本引导的目标域中合成高分辨率的多视角一致图像，而无需为目标域收集额外的图像和相机信息。我们的结果在质量、多样性和文本-图像对应性方面均优于现有的2D文本引导领域适应方法。

- (4):本文的方法在3D生成模型的领域适应问题上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为DATID-3D的方法，它是一种针对3D生成模型的领域自适应方法，使用文本到图像扩散模型。该方法旨在将源域上训练的3D生成器转移到由文本提示指定的新目标域，同时保留多视角一致性和文本中的多样性。该方法包括使用预训练的文本到图像扩散模型生成目标数据集，然后通过CLIP和姿态重建过滤过程对数据集进行精细化处理，以改善图像-文本对应性和姿态一致性。最后，使用对抗和密度正则化损失对生成器进行微调，以保留多样性和多视角一致性。该方法还包括一次性实例选择自适应，以让用户充分享受文本中的多样性。

具体步骤如下：
1. 使用预训练的3D生成器作为源域模型。
2. 使用预训练的文本到图像扩散模型生成目标数据集。
3. 使用CLIP和姿态重建过滤过程对数据集进行精细化处理。
4. 使用对抗和密度正则化损失对生成器进行微调，以保留多样性和多视角一致性。
5. 进行一次性实例选择自适应，以让用户充分享受文本中的多样性。

#### 8. 实验设置：
本文使用了EG3D作为源域模型，并在三个目标域（汽车、椅子和飞机）上评估了所提出的方法。实验在单个NVIDIA V100 GPU上进行，使用FID和KID指标评估生成图像的质量和多样性。

#### 9. 实验结果和分析：
所提出的DATID-3D方法在多样性和文本-图像对应性方面优于现有的2D文本引导领域自适应方法。与现有的2D文本引导领域自适应方法相比，该方法在定性比较、KID和人类评估方面表现出更高的质量、多样性和高文本-图像对应性。本文还扩展了该方法，以进行一次性实例选择自适应和单视图操作的3D重建，以满足用户的意图约束。该方法在旋转不变对象或2D空间中的对象中可能会丢失姿态信息。文中还讨论了该方法的社会风险。


# Paper:730     PEAL: 先验嵌入显式注意力学习用于低重叠点云配准



#### 1. Title: 
PEAL: Prior-embedded Explicit Attention Learning for Low-overlap Point Cloud Registration

#### 2. Authors: 
Junle Yu, Luwei Ren, Wenhui Zhou, Yu Zhang, Lili Lin, Guojun Dai

#### 3. Affiliation: 
杭州电子科技大学

#### 4. Keywords: 
Point cloud registration, Transformer, attention mechanism, low-overlap scenarios

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究点云配准中的低重叠情况，提出了一种PEAL模型，通过引入先验知识将点云分为两部分，然后使用显式单向注意力机制来学习重叠区域内的关键点，从而解决了全局关联可能导致的特征模糊问题。

- (2):过去的方法通常采用自注意力机制来学习点云特征表示，但是全局关联可能会导致特征模糊问题，尤其是在室内低重叠场景中。本文提出的PEAL模型通过引入先验知识和显式单向注意力机制来解决这个问题。

- (3):本文提出的PEAL模型通过引入先验知识将点云分为两部分，然后使用显式单向注意力机制来学习重叠区域内的关键点，从而解决了全局关联可能导致的特征模糊问题。PEAL模型的创新点在于将先验知识和显式单向注意力机制引入到点云配准中，从而提高了配准的准确性。

- (4):在3DLoMatch基准测试中，PEAL模型的Registration Recall提高了6%以上，并在3DMatch和3DLoMatch上实现了Feature Matching Recall、Inlier Ratio和Registration Recall的最优性能。这表明PEAL模型在低重叠点云配准任务中具有很好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于先验嵌入的显式注意力学习模型（PEAL）用于低重叠点云配准。该方法将先验知识纳入学习过程，并将点云分为两部分，一部分包括位于假设重叠区域内的点，另一部分包括位于假设非重叠区域内的点。PEAL使用假设重叠点显式学习单向注意力，显著缓解了自注意力产生的特征模糊性。该方法还采用迭代姿态优化方式进行低重叠点云配准。

具体步骤如下：
1. 使用GeoTransformer获取3D重叠先验和2D重叠先验。
2. 采用粗到细的技术提取对应关系，并使用超点匹配模块提取超点对应关系。
3. 引入显式单向注意力模块，该模块对锚定超点敏感，从而通过改进锚定区域的重叠率实现更好的估计变换。
4. 最后，引入迭代更新模块来优化变换。

#### 8. 实验设置：
本文在室内基准数据集3DMatch和3DLoMatch上进行了评估。3DMatch数据集由62个场景组成，其中46个用于训练，8个用于验证，8个用于测试。

#### 9. 实验结果和分析：
本文提出的PEAL方法在3DMatch和3DLoMatch基准测试中均取得了最优性能。实验结果表明，PEAL在低重叠场景中优于ICP，并在与其他基于Transformer的点云配准方法结合使用时提高了性能。PEAL的内点比率为62.1％，一个数据集的RMSE为0.043m，另一个数据集的RMSE为1.991m，重叠率为19.0％。定性结果表明，PEAL显著改善了特征表示，并有助于在极低重叠场景中推断超点匹配。在Registration Recall（RR）、Feature Matching Recall（FMR）、Inlier Ratio（IR）、Relative Rotation Error（RRE）和Relative Translation Error（RTE）等五个指标上，PEAL均优于最近的最先进方法。


# Paper:731     M6Doc：现代文档布局分析的大规模多格式、多类型、多布局、多语言、多注释类别数据集



#### 1. Title: 
M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis

#### 2. Authors: 
Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, and Lianwen Jin

#### 3. Affiliation: 
第一作者：华南理工大学

#### 4. Keywords: 
Document layout analysis, dataset, multi-format, multi-type, multi-layout, multi-language, multi-annotation category, transformer-based method

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cheng_M6Doc_A_Large-Scale_Multi-Format_Multi-Type_Multi-Layout_Multi-Language_Multi-Annotation_Category_CVPR_2021_paper.html  Github: https://github.com/HCIILAB/M6Doc

#### 6. Summary : 
- (1):本文研究的是文档布局分析，是文档理解的重要前提，包括文档检索和转换。

- (2):目前公共数据集中，大多数只包含PDF文档，缺乏真实的文档。在这些数据集上训练的模型可能无法很好地推广到现实世界的场景。因此，本文介绍了一个名为M6Doc的大型多样化文档布局分析数据集。M6代表六个属性：（1）多格式（包括扫描、拍照和PDF文档）；（2）多类型（如科学文章、教科书、书籍、试卷、杂志、报纸和笔记）；（3）多布局（矩形、曼哈顿、非曼哈顿和多列曼哈顿）；（4）多语言（中文和英文）；（5）多注释类别（74种注释标签，涵盖9,080个手动注释页面中的237,116个注释实例）；（6）现代文档。此外，本文提出了一种基于Transformer的文档布局分析方法，称为TransDLANet，它利用自适应元素匹配机制，使查询嵌入更好地匹配基础真实情况以提高召回率，并构建分割分支以进行更精确的文档图像实例分割。

- (3):本文提出了一个名为M6Doc的大型多样化文档布局分析数据集，包括多种格式、类型、布局、语言和注释类别。此外，本文提出了一种基于Transformer的文档布局分析方法，称为TransDLANet，它采用标准Transformer编码器作为特征融合方法，并使用自适应元素匹配机制，使查询向量更好地关注布局元素的独特特征。随后，利用动态解码器执行RoI特征和图像特征的融合。最后，使用三个参数共享的多层感知器分支对融合交互特征进行解码，以进行多任务学习。

- (4):本文提出的TransDLANet在M6Doc上取得了64.5%的mAP，达到了最先进的性能。M6Doc数据集的综合评估表明其有效性。本文的贡献总结如下：（1）M6Doc是第一个包含真实世界（拍摄和扫描）文件和出生数字文件的布局分析数据集。此外，它是第一个包括中文示例的数据集。它具有几种代表性的文档类型和布局，有助于开发通用布局分析方法。（2）M6Doc是迄今为止最细粒度的逻辑布局分析类别。它可以作为几个相关任务的基准，如逻辑布局分析、公式识别和表格分析。（
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的文档布局分析方法，称为TransDLANet。该方法使用CNN作为骨干网络提取文档图像特征，使用Transformer编码器进行自我注意力特征学习，使用动态交互解码模块将查询向量与通过RoIAlign获得的边界框图像区域的特征融合。使用三个共享参数的MLP分支进行解码，分别用于分类置信度、边界框坐标位置和文档实例区域的分割掩模。该过程重复K次以细化最终文档实例。

#### 8. 实验设置：
本文在常见的文档布局分析基准数据集上进行了实验，包括DocBank、PubLayNet和DocLayNet。使用在ImageNet上预训练的ResNet-101模型作为骨干网络。使用AdamW优化器进行模型训练，基础学习率设置为2×10−5。默认训练周期设置为500，学习率在训练周期的50％和75％时下降到2×10−6和2×10−7。在训练期间，使用随机裁剪增强，并将输入图像缩放，使最短边至少为704-896像素，最长边最多为1333像素，以确保最佳性能。

#### 9. 实验结果和分析：
本文提出的TransDLANet方法在M6Doc数据集上取得了最先进的性能，mAP为64.5％。与其他现有模型相比，TransDLANet在检测手写文档和密集包装或倾斜实例方面具有更好的性能。实验结果表明，TransDLANet在精度、召回率和F1得分方面优于其他方法。本文还讨论了现有模型在检测手写文档和密集包装或倾斜实例方面面临的挑战，并提出了未来的研究方向。


# Paper:732     ABCD：任意位系数去量化



#### 1. Title: 
ABCD: Arbitrary Bitwise Coefficient for De-quantization

#### 2. Authors: 
Woo Kyoung Han, Byeonghun Lee, Sang Hyun Park, Kyong Hwan Jin

#### 3. Affiliation: 
Daegu Gyeongbuk Institute of Science and Technology (DGIST), South Korea (韩国大邱庆北科学技术院)

#### 4. Keywords: 
Bit depth expansion, de-quantization, implicit neural function, phasor estimator, arbitrary-bit reconstructions

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Han_ABCD_Arbitrary_Bitwise_Coefficient_for_De-Quantization_CVPR_2021_paper.html  Github: https://github.com/WooKyoungHan/ABCD

#### 6. Summary : 
- (1):本文研究背景是数字内容的位深度扩展问题，由于压缩编解码器等因素，大多数内容都是低于8位的，导致出现带状和模糊的伪影。传统的位深度扩展方法存在模糊伪影问题，而基于深度神经网络的方法在位深度扩展方面表现出色，但是仍然存在一些问题。

- (2):过去的方法包括零填充和位复制等，这些方法硬件友好但是忽略了周围像素的信息。插值方法和自适应方法可以有效消除伪影，但是会模糊细节。学习方法可以预测高质量的HBD图像，但是需要多个模型来预测每个位平面。本文提出了一种新的模型，使用隐式神经函数和位查询来从任意量化输入中恢复去量化图像，解决了上述问题。

- (3):本文提出了一种隐式神经函数，使用位查询来恢复去量化图像。该模型包括一个编码器来估计真实图像的主导相位，一个隐式神经函数来实现任意位重建，以及一个系数估计器来将估计的相位信息和位查询信息转换为位系数。本文的创新点在于提出了一种新的隐式神经函数，使用位查询来恢复任意位深度的图像，同时使用编码器来估计真实图像的主导相位，从而避免了频谱偏差问题。

- (4):本文在自然图像和动画图像上进行了实验，结果表明，与现有的位深度扩展方法相比，本文的方法具有更好的性能。此外，本文还在YouTube UGC数据集上进行了实验，证明了该方法在去除带状伪影方面的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为ABCD（任意位系数去量化）的模型，用于从随机量化的低位深度（LBD）图像中恢复缺失的位到任何高位深度（HBD）图像。该模型使用隐式神经表示（INR）和位查询来实现幅度域中的任意位重建。模型包括一个编码器，用于估计原始图像中的主导相位，一个INR，用于表示幅度域，以及一个位解码步骤，用于将位系数转换为HBD图像。提出的相位估计器预测傅里叶域中原始图像系数的主导相位，以减轻INR的频谱偏差。模型使用单个训练集进行训练，训练集包括随机量化的LBD图像。具体步骤如下：
(1). 编码器：使用EDSR、RDN或SwinIR等编码器估计原始图像中的主导相位。
(2). 相位估计器：包括幅度估计器、相位估计器和正弦激活函数，用于预测原始图像系数的主导相位。
(3). INR：使用位查询和隐式神经表示来实现幅度域中的任意位重建。
(4). 位系数估计器：计算位查询s并将其与相位估计器的输出连接起来，然后将连接向量乘以位系数以重建HBD图像。

#### 8. 实验设置：
本文使用了来自Sintel数据集和MIT-Adobe FiveK数据集的2000张16位图像进行训练，使用MIT-Adobe FiveK、Sintel数据集以及TESTIMAGES 1200、Kodak和ESPL v2等基准数据集进行评估。输入网络的是64×64的图像块，使用Adam进行优化，训练1000个epoch，批量大小为16。当使用基于CNN的编码器（如EDSR或RDN）训练ABCD时，学习率初始化为1e-4，并在[200、400、600、800]时以0.5的因子衰减。对于变压器编码器（SwinIR），学习率初始化为1e-5，并在[500、800、900、950]时以0.5的因子衰减。

#### 9. 实验结果与分析：
本文在Sintel和MIT-Adobe FiveK测试集以及TESTIMAGES 1200、Kodak和ESPL v2等基准数据集上对任意位深度扩展进行了定量比较。结果表明，与IPAD、BitNet、BE-CALF、D16等方法相比，提出的ABCD方法在大多数情况下具有最高的PSNR值，且在2位到8位位深度扩展方面具有更好的视觉质量。在YouTube-UGC数据集上，盲带状伪影检测器（BBAND）得分也有所提高。此外，相位估计器准确地预测了位系数，并显示出与原始图像相似的相位图。该方法有效地消除了严重的伪轮廓和模糊伪影。


# Paper:733     QPGesture：基于量化和相位引导的运动匹配，用于自然语音驱动的手势生成



#### 1. Title: 
QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation

#### 2. Authors: 
Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Haolin Zhuang

#### 3. Affiliation: 
第一作者：清华大学

#### 4. Keywords: 
Speech-driven gesture generation, quantization-based motion matching, phase guidance, Levenshtein distance, VQ-VAE

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_QPGesture_Quantization-Based_and_Phase-Guided_Motion_Matching_for_Natural_Speech-Driven_CVPR_2021_paper.html  Github: https://github.com/YoungSeng/QPGesture

#### 6. Summary : 
- (1):本文研究语音驱动的手势生成，旨在解决手势随机抖动和语音与手势之间的异步关系等挑战。

- (2):现有的手势生成方法主要采用神经网络直接将语音映射到高维连续空间中的3D关节序列，但这种方法受到所提出神经网络的表示能力的限制。本文提出了一种基于量化和相位引导的运动匹配框架，通过量化手势来减少输入冗余，使用Levenshtein距离来对齐不同语音的不同手势，并使用相位来指导手势匹配，使生成的手势更加自然。

- (3):本文提出了一种手势VQ-VAE模块来学习代码本，以总结有意义的手势单元。使用Levenshtein距离作为相应手势语音的相似度度量来匹配更合适的手势和语音，并解决语音和手势之间的对齐问题。此外，引入相位来指导基于音频的手势匹配，使生成的手势更加自然。

- (4):在语音驱动的手势生成任务上，本文提出的方法在实验中表现出色，优于最近的方法。
#### 7. 方法详细介绍：
本文提出了一种基于量化和相位引导的自然语音驱动手势生成框架。该框架包括两个主要组件：手势VQ-VAE模块和基于Levenshtein距离的对齐方法。手势VQ-VAE模块用于学习一个码本，以总结有意义的手势单元。对齐方法用于将不同语音的多样手势进行匹配。此外，引入相位来根据上下文或音频节奏指导最佳手势匹配。具体步骤包括：
1. 使用vq-wav2vec Gumbel-Softmax模型对音频进行量化。
2. 使用手势VQ-VAE模块对手势进行编码和量化。
3. 使用基于Levenshtein距离的对齐方法将离散的文本序列、离散的音频序列和一个初始的前一个姿势代码作为输入，可选地加入一系列控制信号。输出为基于音频的候选和基于文本的候选。
4. 最后，根据种子代码对应的相位和两个候选对应的相位选择最佳手势。

#### 8. 实验设置：
本文使用BEAT数据集进行训练和评估，该数据集是用于人类手势生成的最大公开运动捕捉数据集。数据集按8:1:1的比例划分为训练、验证和测试集，并使用所有说话者的数据训练码本和基线。本文提供了所提出框架的实现细节，包括码本大小、优化器和批量大小等。本文还报告了各种评估指标，包括速度直方图之间的距离、Fréchet手势距离（FGD）、平均抖动、平均加速度、规范相关分析（CCA）、多样性和节拍对齐分数。本文将所提出的框架与现有方法进行比较，并进行用户研究以评估所提出方法的实际视觉表现。

#### 9. 实验结果与分析：
本文提出的基于量化和相位引导的手势生成框架在BEAT数据集上实现了最先进的性能，无论是定性还是定量方面。用户研究的结果表明，与基准相比，生成的手势更“与语义相关”，更“自然”。消融研究证明了框架中不同组件的有效性以及生成手势的可控性。


# Paper:734     基于掩码表示学习的域泛化立体匹配



#### 1. Title: 
Masked representation learning for domain generalized stereo matching

#### 2. Authors: 
Zhibo Rao, Bangshu Xiong, Mingyi He, Yuchao Dai, Renjie He, Zhelun Shen, Xing Li

#### 3. Affiliation: 
第一作者：南昌航空大学

#### 4. Keywords: 
Stereo matching, domain generalization, masked representation learning, multi-task learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Rao_Masked_Representation_Learning_for_Domain_Generalized_Stereo_Matching_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是立体匹配中的域泛化问题，即如何在不同的域中实现模型的泛化能力。 
- (2):过去的方法主要是通过学习域不变的特征表示来提高泛化能力，但是这些方法没有考虑到不同训练时期之间泛化性能的显著波动。本文提出了一种基于掩码表示学习和多任务学习的简单有效的掩码表示方法，通过伪多任务学习框架，促进模型学习结构信息，提高泛化性能。 
- (3):本文提出的方法是在左图像中随机掩码一部分像素，然后添加一个简单的解码器来恢复原始的左图像。最后，将模型训练为两个任务（立体匹配和图像重建）的伪多任务学习框架，以促进模型学习结构信息，提高泛化性能。 
- (4):本文的方法在多个数据集上进行了实验，结果表明：（1）我们的方法可以轻松地插入到当前各种立体匹配模型中，以提高泛化性能；（2）我们的方法可以减少不同训练时期之间泛化性能的显著波动；（3）我们发现当前的方法倾向于选择不同训练时期的最佳结果作为泛化性能，但在实践中不可能通过真实数据来选择最佳性能。
#### 7. 方法详细介绍：
本文提出了一种基于掩码表示学习的域通用立体匹配方法。该方法包括以下步骤：
1. 随机掩码采样：对左图像进行随机掩码采样，以一定比例遮盖部分像素点。
2. 代价体积计算：计算左右图像的代价体积。
3. 轻量级解码器：添加一个轻量级解码器，用于恢复原始的左图像。
4. 特征匹配：对代价体积进行特征匹配。
5. 特征提取：使用特征提取模块提取特征。
6. 反采样：对右图像进行反采样，得到匹配的视差图。
7. 掩码视差：使用掩码视差模块得到最终的视差图。

#### 8. 实验设置：
本文在Sceneflow数据集上训练模型，并在KITTI 2012&2015、ETH3D和Middlebury数据集上进行评估。评估指标包括端点误差（EPE）和大于t像素的错误像素百分比（>t px）。最大视差D设置为256，使用Adam优化器进行模型优化。每个GPU的小批量大小设置为1个图像对，并使用数据增强来处理Sceneflow数据集上的输入图像。

#### 9. 实验结果与分析：
本文提出的方法在KITTI 2012&2015、ETH3D和Middlebury数据集上的表现优于当前最先进的域通用立体匹配方法。实验结果表明，随着掩码比例的升高，性能先升高后下降。本文的方法在运行时间和便利性方面具有优势，相对于当前的域自适应和域通用方法，可以更好地提高模型的泛化性能和稳定性。


# Paper:735     Nerflets：从2D监督中获取高效结构感知的本地辐射场3D场景表示



#### 1. Title: 
Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision

#### 2. Authors: 
Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser, Leonidas Guibas, Hao Su, Kyle Genova

#### 3. Affiliation: 
第一作者：Xiaoshuai Zhang，Google Research

#### 4. Keywords: 
3D scene representation, neural radiance fields, panoptic segmentation, novel view synthesis, interactive editing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Nerflets_Local_Radiance_Fields_for_Efficient_Structure-Aware_3D_Scene_Representation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文旨在从2D图像中生成紧凑、高效、全面的3D场景表示，包括外观、语义和物体实例分割等信息，以用于计算机视觉和机器人学任务。
 
- (2):现有的方法需要3D地面真实监督，效率低下或无法处理物体实例。本文提出了nerflets，一种由多个本地神经辐射场组成的3D场景表示，每个nerflet维护自己的空间位置、方向和范围，可以直接优化nerflets的参数，以形成场景的分解表示，其中每个物体实例由一组nerflets表示。相比于现有方法，nerflets不需要3D监督，支持任何2D全景分割可用的类别，优化大部分参数，提高效率和实例召回率。
 
- (3):nerflets是一种结构化和不规则表示，每个nerflet由一个3D中心、一个3D XYZ旋转和一个9-DOF坐标系中的3个（每轴）半径参数化。每个nerflet的影响由径向基函数（RBF）调制，根据其方向和半径，随着距离增加而减小，确保每个nerflet对局部场景做出贡献。在该影响区域内，每个nerflet都有一个微小的MLP来估计密度和辐射。它还存储一个语义logit向量，描述nerflet的类别（例如“汽车”），以及一个实例标签，指示它属于哪个真实世界的物体（例如“第三辆车”）。场景可以包含任意数量的nerflets，它们可以放置在空间的任何位置，它们可以重叠，这提供了对复杂、稀疏的3D场景进行高效建模的灵活性。由于多个nerflets可以具有相同的实例标签，它们可以组合表示复杂物体实例的密度和辐射分布。相反，由于每个nerflet只有一个实例标签，nerflets提供了场景的完整分解，将其分解为真实世界的物体。因此，nerflets提供了场景的3D全景分解，可以进行渲染和编辑。 
 
- (4):nerflets在室内和室外数据集上的实验表明，它们比传统的全局NeRF更高效地逼近场景，可以从任意视角提取全景和光度渲染，并支持3D全景分割和交互式编辑等任务。在KITTI360新颖语义视图合成基准测试中，nerflets取得了第一名的成绩，在ScanNet 3D全景分割任务中也取得了有限
#### 7. 方法详细介绍：
本文提出了一种名为Nerflets的方法，用于从2D监督中高效地进行结构感知的3D场景表示。该方法首先根据实例分割模型的预测从同一类别但不同实例中采样射线对，并强制它们具有不同的影响图。该方法使用正则化损失和多个项来使nerflets的结构更好地反映场景的结构。该方法还包括实例标签分配和使用top-k评估方法进行高效的nerflet评估。Nerflets是一种结构化表示，其中输出辐射度和全景场由N个单独的nerflets产生的值混合定义。每个nerflet存储局部几何、外观、语义和实例信息。Nerflet具有9个姿态参数、一个3D中心、3个轴对齐半径和3个旋转角度。Nerflet的影响函数是基于缩放的各向异性多元高斯的解析径向基函数。该方法使用nerflet的影响权重组合单个nerflet产生的值来渲染2D图像。该方法还通过计算每个点采样的nerflet影响激活函数来处理实例，并使用它将离散实例标签分配给查询位置或射线。

#### 8. 实验设置：
本文使用KITTI-360和ScanNet两个具有挑战性的真实世界数据集对所提出的方法进行评估。对于KITTI-360实验，作者使用了新颖的视角合成分割，并与Panoptic Neural Fields（PNF）进行了比较。为了生成室外场景的2D全景预测，他们使用在COCO上训练的Panoptic DeepLab模型。对于ScanNet实验，他们在8个场景上进行了评估，并与最近的基线方法DM-NeRF和Semantic-NeRF进行了比较。为了生成室内场景的全景图像，他们使用了PSPNet和Mask R-CNN。

#### 9. 实验结果和分析：
所提出的方法在KITTI-360数据集上的PSNR优于所有其他2D监督方法，并且与使用3D监督的PNF相比具有竞争力。对于ScanNet中的复杂室内场景，所提出的方法在所有设置下均实现了最佳的新颖视角合成性能，包括与DM-NeRF进行比较。nerflets渲染的彩色图像和分割图具有所有评估方法中最好的质量。该方法还支持交互式可视化和场景编辑。本文还进行了消融实验，以验证所提出方法中使用的每个正则化项的有效性。


# Paper:736     MobileOne：一种改进的一毫秒移动骨干网络



#### 1. Title: 
MobileOne: An Improved One millisecond Mobile Backbone

#### 2. Authors: 
Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, Anurag Ranjan

#### 3. Affiliation: 
Apple（苹果公司）

#### 4. Keywords: 
Efficient neural network, mobile devices, latency, MobileOne, ImageNet

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2022_paper.html  Github: https://github.com/apple/ml-mobileone

#### 6. Summary : 
- (1):本文研究的背景是设计和部署适用于移动设备的高效深度学习架构，旨在在提高准确性的同时提高模型的效率。
 
- (2):过去的方法通常优化参数数量和浮点运算次数（FLOPs），但这些指标与移动设备上的网络延迟不一定相关。本文通过在移动设备上部署多个移动友好的网络并分析不同指标的性能，发现了最近高效神经网络中的架构和优化瓶颈，并提供了缓解这些瓶颈的方法。作者设计了一种高效的MobileOne骨干网络，其变体在iPhone12上实现了1毫秒以下的推理时间，并在ImageNet上实现了75.9％的top-1准确率。MobileOne在高效架构中实现了最先进的性能，同时在移动设备上运行速度快得多。作者的最佳模型在ImageNet上的性能与MobileFormer相似，但速度快了38倍。作者的模型在类似的延迟下比EfficientNet在ImageNet上的top-1准确率高2.3％。此外，作者还展示了他们的模型在多个任务上的泛化能力，包括图像分类、目标检测和语义分割，与现有的高效架构相比，在移动设备上部署时具有更高的准确性和更低的延迟。 

- (3):本文通过在iPhone12上部署神经网络并基准测试其延迟成本，分析了激活和分支中的性能瓶颈。作者提出了一种训练算法，即在训练时使用线性过度参数化模型，并在推理时重新参数化线性结构，以缓解优化瓶颈。作者进一步通过在训练过程中动态放松正则化来缓解优化瓶颈，以防止已经很小的模型过度正则化。基于这些发现，作者设计了一种新的MobileOne架构，其变体在iPhone12上运行时间不到1毫秒，同时在高效架构中实现了最先进的性能。MobileOne在训练时引入了线性分支，这些分支在推理时被重新参数化。作者的模型与之前的结构重新参数化工作的一个关键区别是引入了微不足道的过度参数化分支，从而在低参数范围和模型缩放策略方面提供了进一步的改进。在推理时，作者的模型具有简单的前馈结构，没有任何分支或跳过连接。由于这种结构具有较低的内存访问成本，因此我们可以在网络中加入更宽的层，从而提高表示能力。 

- (4):作者的MobileOne模型在ImageNet上实现了75.9％的top-1准确率，同时在iPhone12上实现了1毫秒以下的推理时间。作者的模型在移动设备上的性能优于现有的高效模型，同时在多个任务上具有更高的准确性和更低的延迟。作者的模型
#### 7. 方法详细介绍：
本文分析了延迟与FLOPs和参数数量的相关性，设计了一种新的轻量级神经网络MobileOne，其变体在iPhone12上实现了低于1ms的推理时间，并在ImageNet数据集上达到了75.9%的top-1准确率。MobileOne在训练时引入了线性分支，这些分支在推理时被重新参数化。MobileOne的模型结构简单，没有任何分支或跳跃连接。作者还分析了训练时可重新参数化分支和正则化动态松弛的效果，以缓解训练小模型时遇到的优化瓶颈。

#### 8. 实验设置：
MobileOne模型在ImageNet数据集上进行了评估，该数据集包含128万张训练图像和来自1000个类别的50000张验证图像。使用PyTorch库从头开始训练模型，使用8个NVIDIA GPU的机器。使用SGD with momentum优化器进行训练，有效批量大小为256，训练300个epochs。对于所有模型，使用交叉熵损失和标签平滑正则化，平滑因子设置为0.1。使用余弦学习率调度，初始学习率为0.1，初始权重衰减系数为10^-4，使用相同的余弦调度将其退火到10^-5。对于较大的MobileOne变体（即S2、S3和S4），使用AutoAugment进行训练，对于较小的MobileOne变体（即S0和S1），使用标准数据增强。在所有版本的MobileOne中，使用0.9995的衰减常数进行指数移动平均（EMA）权重平均。在测试时，所有MobileOne模型都在分辨率为224×224的图像上进行评估。

#### 9. 实验结果和分析：
MobileOne的性能优于类似或更高参数数量的竞争模型。MobileOne-S4甚至优于参数数量增加72.9%的ResNet-50模型。MobileOne还在Pascal VOC和ADE 20k数据集上作为骨干特征提取器用于单次目标检测SSD和语义分割。MobileOne的最佳变体在ADE 20k数据集上比MobileNetV2表现提高了12.0%。MobileOne还在ImageNet-R和ImageNet-Sketch等超出分布范围的基准测试中显著优于其他高效架构。


# Paper:737     抗歧义的密集目标检测半监督学习



#### 1. Title: 
Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection

#### 2. Authors: 
Chang Liu, Weiming Zhang, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Xiaomao Li, Errui Ding, Jingdong Wang

#### 3. Affiliation: 
第一作者：上海大学

#### 4. Keywords: 
Semi-Supervised Learning, Object Detection, Ambiguity-Resistant, Dense Object Detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Ambiguity-Resistant_Semi-Supervised_Learning_for_Dense_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/PaddlePaddle/PaddleDetection

#### 6. Summary : 
- (1):本文研究半监督学习在密集目标检测中的应用，旨在解决单阶段检测器在基础半监督学习技术下的性能提升有限的问题。

- (2):过去的方法主要采用自我训练技术，但这种方法在单阶段检测器中的应用效果不佳。本文分析了半监督学习的核心组成部分，即伪标签的选择和分配，发现存在选择和分配的歧义，阻碍了单阶段检测器的半监督学习。选择歧义是由于分类分数与定位质量不一致导致的，而分配歧义则是由于伪框不准确和阈值过滤导致的。为了解决这些问题，本文提出了一种抗歧义的半监督学习方法，即ARSL，通过联合估计分类和定位质量来缓解选择歧义，通过基于像素级预测的任务分离分配来缓解分配歧义。

- (3):本文提出了一种抗歧义的半监督学习方法，即ARSL，通过联合估计分类和定位质量来缓解选择歧义，通过基于像素级预测的任务分离分配来缓解分配歧义。具体来说，JCE采用双分支结构估计两个任务的置信度，然后将它们组合成检测结果的联合置信度。而TSA则是基于像素级预测来分配标签，将样本分为负样本、正样本和模糊样本，然后分别为两个任务分别利用潜在的正样本。与其他密集引导分配方法相比，TSA采用更合理的分配度量，并分别为两个任务利用正样本，可以有效地缓解分配歧义。

- (4):本文在MS COCO和PASCAL VOC数据集上进行了全面的实验，结果表明ARSL有效地缓解了歧义，并在单阶段检测器上取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“抗歧义半监督学习”（Ambiguity-Resistant Semi-supervised Learning，ARSL）的方法，用于解决半监督学习中的选择和分配歧义问题。为了减轻选择歧义，提出了联合置信度估计（Joint-Conﬁdence Estimation，JCE）来共同量化伪标签的分类和定位质量。对于分配歧义，引入了任务分离分配（Task-Separation Assignment，TSA）来基于像素级预测而不是不可靠的伪框来分配标签。ARSL的整体结构如图2所示。具体步骤如下：
1. 使用基于像素级预测的TSA分配标签。
2. 使用JCE共同量化伪标签的分类和定位质量。
3. 使用加权的有监督和无监督损失函数进行训练。
4. 使用EMA更新teacher模型。

#### 8. 实验设置：
本文在MS COCO和PASCAL VOC数据集上进行了全面的实验，评估了ARSL的性能。实验使用了一阶段检测器，并将基本的半监督流程作为基线。实验分析了一阶段检测器的半监督学习，并揭示了伪标签的选择和分配歧义的限制。

#### 9. 实验结果和分析：
本文提出的ARSL方法在MS COCO和PASCAL VOC数据集上取得了最先进的SSOD性能。在COCO-full设置下，ARSL在相对较短的学习计划下将有监督基线提高了4.70％AP。在VOC上，ARSL将有监督基线提高了9.04％和10.88％的AP50和AP50:95，实现了与现有工作的竞争性能。消融研究表明，所提出的JCE和TSA组件有效地提高了检测性能。


# Paper:738     使用新数据集和降级模型学习实用的SDR-to-HDRTV上转换



#### 1. Title: 
Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and Degradation Models

#### 2. Authors: 
Cheng Guo, Leidong Fan, Ziyu Xue, and Xiuhua Jiang

#### 3. Affiliation: 
第一作者：中国传媒大学传媒融合与通信国家重点实验室，鹏城实验室

#### 4. Keywords: 
SDR-to-HDRTV up-conversion, HDR, WCG, deep learning, dataset, degradation models

#### 5. Paper: https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Learning_a_Practical_SDR-to-HDRTV_Up-Conversion_Using_New_Dataset_and_ICCV_2021_paper.pdf  Github: https://github.com/AndreGuo/HDRTVDM

#### 6. Summary : 
- (1):本文研究的是SDR-to-HDRTV up-conversion，即将SDR图像转换为HDR图像。由于SDR图像容量较小，无法表达HDR图像的大范围亮度和颜色，因此需要进行转换。这是一个低级别的视觉任务，需要通过深度学习方法进行处理。

- (2):过去的方法主要是基于网络的方法，但是在实际应用中，这些方法处理真实的SDR图像时，往往会产生暗淡和失真的结果，无法提高观看体验。本文认为这是由于训练集的问题，因此提出了新的HDRTV数据集和新的HDR-to-SDR降级模型，以提高网络的训练效果。

- (3):本文提出了一种新的亮度分段网络（LSN），由全局映射主干和两个变压器分支组成，用于处理不同亮度范围内的图像。同时，本文还提出了新的评估标准和主观实验，以评估网络的性能。

- (4):本文的方法在HDRTV4K数据集上进行了测试，结果表明，与现有方法相比，本文的方法能够更好地提高亮度和饱和度，并恢复更多的细节。性能指标得到了显著的提高，支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种基于分段亮度的网络架构，称为亮度分段网络（LSN），用于SDR到HDRTV的上采样。该网络由全局映射主干和两个Transformer风格的UNet分支组成，分别用于处理不同的亮度范围。同时，本文还提出了一种新的HDR-to-SDR降级模型，用于训练网络。具体而言，该模型将HDR的亮度和色彩范围进行适当的剪裁，并进行色域映射，将剪裁后的HDR转换为SDR编码值。网络使用L1损失和AdaM优化进行训练，以学习适当的恢复能力和良好的泛化能力。

#### 8. 实验设置：
本文在12个4K SDR视频上进行了实验，每个视频的长度为10秒，帧率为50fps。其中3个视频使用了YouTube DM进行降级，其余9个视频使用了HDRTV4K数据集中的HDR序列进行降级。本文将所提出的方法与6种基于学习的方法和2种商业软件DaVinci和Nuke进行了比较。

#### 9. 实验结果与分析：
本文使用了多种指标和可视化方法来评估不同的SDR到HDRTV上采样方法的性能。指标包括HDRBQ和ASL等细粒度的度量标准，用于评估HDR和WCG体积的恢复以及亮度和颜色外观与真实值的相似度。可视化方法包括在3D Yxy色度图上的比较和对明暗区域的详细可视化，以评估方法的恢复能力。本文还进行了主观实验，要求参与者评估输出HDR与输入SDR的差异，并选择其评分的属性。结果显示，只有两种方法，包括本文提出的方法，被认为比输入SDR更好，并且本文提出的方法主要因视觉体验和信息恢复而获得更好的评分。


# Paper:739     多模态媒体操纵检测与定位



#### 1. Title: 
Detecting and Grounding Multi-Modal Media Manipulation

#### 2. Authors: 
Rui Shao, Tianxing Wu, Ziwei Liu

#### 3. Affiliation: 
Rui Shao: 哈尔滨工业大学（深圳）计算机科学与技术学院
Tianxing Wu, Ziwei Liu: 南洋理工大学S-Lab

#### 4. Keywords: 
Multi-modal media manipulation, deepfake detection, text fake news detection, hierarchical multi-modal manipulation reasoning transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shao_Detecting_and_Grounding_Multi-Modal_Media_Manipulation_CVPR_2021_paper.html  Github: https://github.com/rshaojimmy/MultiModal-DeepFake

#### 6. Summary : 
- (1):本文研究的是多模态媒体操纵检测和定位问题，旨在不仅检测多模态媒体的真实性，还要定位操纵内容（即图像边界框和文本标记），需要更深入的多模态媒体操纵推理。
- (2):现有的深度伪造检测和文本虚假新闻检测方法仅针对单模态伪造进行二元分类，而本文提出的方法需要同时检测图像和文本模态中的伪造存在，并且需要操纵定位。本文提出的HierArchical Multi-modal Manipulation rEasoning tRansformer (HAMMER)可以更好地捕捉图像和文本之间的交互作用，从而实现更细粒度和全面的操纵检测和定位。
- (3):本文提出了一个新的研究问题，即检测和定位多模态媒体操纵（DGM4），并构建了第一个大规模的DGM4数据集，其中包含各种方法生成的图像-文本对，具有丰富的操纵注释。本文提出的HAMMER模型通过浅层操纵推理和深层操纵推理之间的操纵感知对比学习以及多模态聚合器的模态感知交叉注意力来完全捕捉不同模态之间的细粒度交互作用。本文的方法在多模态媒体操纵检测和定位任务上取得了优异的性能。
- (4):本文提出的方法在多模态媒体操纵检测和定位任务上取得了优异的性能，可以更好地捕捉图像和文本之间的交互作用，从而实现更细粒度和全面的操纵检测和定位。
#### 7. 方法详细介绍：
本文提出了一种名为HAMMER的模型，用于检测和定位多模态媒体篡改。HAMMER模型包括两个主要组件：篡改感知对比学习和深度篡改推理。篡改感知对比学习用于学习图像-文本对之间的相似性并检测篡改。深度篡改推理用于进行细粒度篡改类型检测和二元分类，以及篡改标记定位。HAMMER模型采用基于transformer的架构，包括两个单模态编码器、一个多模态聚合器和专门的篡改检测和定位头。本文提出的方法采用分层篡改推理，从浅层到深层探索多模态交互，并进行分层篡改检测和定位。在浅层篡改推理中，本文采用篡改感知对比损失进行图像和文本嵌入的语义对齐，并通过图像篡改定位损失进行篡改边界框定位。在深层篡改推理中，基于多模态聚合器生成的更深层次的交互多模态信息，本文采用二元分类损失、多标签分类损失和文本篡改定位损失进行篡改类型检测和篡改标记定位。

#### 8. 实验设置：
本文使用了DGM4数据集，该数据集包含10,000个图像-文本对，其中包含四种类型的篡改：人脸交换（FS）、人脸属性编辑（FA）、物体移除（OR）和文本替换（TR）。数据集被分为训练集、验证集和测试集，比例为8:1:1。本文使用AUC、EER、ACC、mAP、CF1、OF1、IoUmean、IoU50、IoU75、Precision、Recall和F1等指标来评估所提出方法的性能。

#### 9. 实验结果和分析：
本文将所提出的方法与两种最先进的多模态学习方法（CLIP和ViLT）以及两种深度伪造检测方法（TS和MAT）和两种序列标记方法（BERT和LUKE）进行了比较。结果表明，所提出的方法在所有指标上均优于其他方法。本文还进行了消融实验，以评估所提出方法中不同组件的有效性。本文还提出了一个新的数据集DGM4，该数据集包含230k个新闻样本，包括77,426个原始图像-文本对和152,574个篡改对。数据集提供了一个比现有深度伪造和多模态误导数据集更具挑战性的情境。


# Paper:740     NeuFace：基于多视角图像的逼真3D神经面部渲染



#### 1. Title: 
NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images

#### 2. Authors: 
Mingwu Zheng, Haiyu Zhang, Hongyu Yang, Di Huang

#### 3. Affiliation: 
第一作者：北京航空航天大学计算机学院

#### 4. Keywords: 
3D face rendering, neural rendering, physically based rendering, BRDF, multi-view images

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_NeuFace_Realistic_3D_Neural_Face_Rendering_From_Multi-View_Images_CVPR_2021_paper.html  Github: https://github.com/zhengmw-neuface/NeuFace

#### 6. Summary : 
- (1):本文研究的背景是多视角图像下的真实3D神经面部渲染，该问题在计算机视觉和图形学应用中具有广泛的应用前景。

- (2):过去的方法通常利用复杂的主动光照设置来构建3D面部模型，需要大量的人工操作。而本文提出的方法是基于神经渲染技术，通过学习准确和物理意义的潜在3D表示来实现面部渲染。与传统方法相比，本文方法具有更高的表现力和更好的泛化能力。

- (3):本文提出了一种新的3D面部渲染模型NeuFace，它自然地将神经BRDF集成到基于物理的渲染中，以协同的方式捕捉复杂的面部几何和外观线索。具体来说，本文引入了一个近似的BRDF积分和一个简单而新颖的低秩先验，有效地降低了面部BRDF的歧义性并提高了性能。实验结果表明，NeuFace在人脸渲染方面具有卓越的性能，并具有良好的通用性。

- (4):本文的方法在多视角图像下实现了真实的面部渲染，并在人脸渲染方面取得了卓越的性能，具有广泛的应用前景。
#### 7. 方法详细介绍：
本文提出了一种名为NeuFace的方法，用于从多视角图像中实现逼真的三维神经面部渲染。该方法包括外观建模、几何建模、采样和渲染。外观建模通过空间MLP和综合基础MLP实现，而几何建模则使用神经SDF。体积渲染用于处理多层面的面部皮肤，训练时使用复合评论家。该方法还使用神经BRDF将物理渲染与神经渲染相结合，以捕捉面部几何和外观线索。该方法使用逼近的BRDF积分和低秩先验来有效降低模糊度并提高面部BRDF的性能。该方法使用可微分的有符号距离函数（SDF）表示，称为ImFace，作为形状先验，面部外观和几何场可以在反向渲染中同步优化。该方法采用实时渲染技术中的技术来分离神经BRDF的半球积分，其中材料和光积分分别学习，绕过了数值解所需的大量蒙特卡罗采样阶段。

#### 8. 实验设置：
本文在FaceScape数据集上进行了实验，该数据集包括来自359个真实主体的高质量4K分辨率多视角图像，共20个表情。将数据集分为43张图像进行训练和11张图像进行测试。图像被降采样为1K分辨率。

#### 9. 实验结果和分析：
NeuFace在外观和几何度量方面均优于现有方法，在几何重建方面误差更小，在PSNR、SSIM和LPIPS方面得分更高。进行了消融研究以验证特别设计的模块，结果表明每个模块都对NeuFace的整体性能有贡献。本文还将该方法扩展到DTU数据库中的常见对象，实现了比DIFFREC更令人印象深刻的重建和分解质量。


# Paper:741     使用退化生成器的潜在空间映射修复手绘建筑图纸



#### 1. Title: 
Restoration of Hand-Drawn Architectural Drawings using Latent Space Mapping with Degradation Generator

#### 2. Authors: 
Nakkwan Choi, Seungjae Lee, Yongsik Lee, Seungjoon Yang

#### 3. Affiliation: 
第一作者：韩国蔚山科学技术大学电气工程系

#### 4. Keywords: 
Restoration, Hand-drawn drawings, Architectural drawings, Degradation generator, Latent space mapping, Vector quantized variational autoencoders

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Choi_Restoration_of_Hand-Drawn_Architectural_Drawings_Using_Latent_Space_Mapping_With_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是文化遗产的保护和修复，特别是传统木建筑的手绘图纸的修复。这些手绘图纸包含了最重要的原始信息，但随着时间的推移，它们经常会严重退化，使得信息的解释变得困难。

- (2):过去的方法主要是基于监督学习的方法，但是由于干净和噪声图纸的对应关系很少，因此这些方法的效果不佳。本文提出了一种基于向量量化变分自编码器的新的修复方法，通过学习图纸和噪声的潜在空间表示，将噪声图纸映射到干净图纸进行修复，并生成真实的噪声图纸进行数据增强。本文的方法在文化遗产管理中的应用表明，修复后的图纸显示出显著的质量改善，可以更准确地解释信息。

- (3):本文提出的方法包括两个阶段。第一阶段，使用向量量化变分自编码器（VQ-VAE）学习干净图纸的潜在空间表示。第二阶段，学习从噪声图纸到干净图纸的潜在空间变量的映射，以及生成逼真的噪声图纸进行数据增强。通过使用合成的降级图纸和降级生成器进行数据增强，学习从噪声到干净图纸的潜在空间映射，从而可以更准确地进行修复。

- (4):本文的方法在文化遗产管理中的应用表明，修复后的图纸显示出显著的质量改善，可以更准确地解释信息。本文的方法在定量和定性评估中都取得了显著的性能提升，特别是在实际的老化图纸中。本文提出的降级生成器产生了更真实的降级图纸。本文的方法可以为文化遗产的保护和修复提供有力的支持。
#### 7. 方法详细介绍：
本文提出了一种新颖的潜空间映射方法，用于恢复老化的手绘建筑图纸。该方法包括两个阶段的训练。第一阶段，使用一组干净的图纸训练VQ-VAE，学习干净图纸的准确表示。第二阶段，使用编码器学习从噪声图纸到干净图纸的准确潜空间映射，同时冻结VQ-VAE。训练需要一组干净和噪声图纸对。使用退化生成器从残差潜空间中生成真实的噪声图纸，训练生成器。使用退化生成器的输出进行数据增强，学习从噪声到干净图纸的潜空间映射。恢复噪声图纸的过程是：使用编码器和向量量化将噪声图纸的潜变量映射到干净图纸的潜变量，并使用代码本进行量化。然后使用生成器对量化的潜变量进行解码，以恢复干净图纸。

#### 8. 实验设置：
本文使用包含330个干净图纸和350个老化图纸的传统木制建筑图纸数据集进行评估。图纸的分辨率在3000×4000和8000×10000像素之间。使用从330个干净图纸中裁剪的256×256大小的43000个补丁进行训练。第二阶段的训练需要一组干净和噪声图纸对。对于干净图纸，使用相同的43000个干净补丁。使用两种类型的噪声图纸：使用[16,19]中的方法将干净补丁降级为合成噪声图纸，背景随机选择自8000个噪声背景补丁，以及退化生成器的输出。训练退化生成器需要一组不成对的噪声图纸作为鉴别器输入。第二阶段的训练从仅使用合成噪声图纸开始。当退化生成器开始生成真实的噪声图纸时，交替输入合成噪声图纸和退化生成器的输出。

#### 9. 实验结果和分析：
本文提出的方法在恢复老化的手绘建筑图纸方面表现出色。与其他现有的基于学习的方法相比，本文提出的方法在PSNR、SSIM、LPIPS和FID等量化指标和定性检查方面都取得了显著的改进。本文提出的方法能够清除涂抹、重新连接断线并抑制背景噪声，从而产生卓越的质量恢复图纸。


# Paper:742     SegLoc：学习基于分割的表示以实现隐私保护的视觉定位



#### 1. Title: 
SegLoc: Learning Segmentation-based Representations for Privacy-Preserving Visual Localization

#### 2. Authors: 
Maxime Pietrantoni, Martin Humenberger, Torsten Sattler, Gabriela Csurka

#### 3. Affiliation: 
Maxime Pietrantoni: Faculty of Electrical Engineering, Czech Technical University in Prague (捷克理工大学电气工程学院)

#### 4. Keywords: 
Visual localization, segmentation, privacy preservation, 3D maps, image retrieval

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Pietrantoni_SegLoc_Learning_Segmentation-Based_Representations_for_Privacy-Preserving_Visual_Localization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是在隐私保护的情况下，如何利用强大的图像分割技术来进行视觉定位。
- (2):传统的视觉定位算法依赖于3D场景表示，这些表示通常需要存储在云端，这引发了关于内存消耗和隐私保护的重要问题。本文提出了一种新的定位框架SegLoc，它利用图像分割来创建强大、紧凑和隐私保护的场景表示，即3D地图。与直接从图像中提取特征的先前方法不同，本文提出了一种更抽象的表示形式，即基于自监督学习的稠密分割。本文的方法具有多个优点，例如可以学习到对季节性或外观变化具有鲁棒性的表示，可以减少存储要求，可以实现隐私保护的视觉定位等。
- (3):本文提出了一种新的定位框架SegLoc，它利用图像分割来创建强大、紧凑和隐私保护的场景表示，即3D地图。本文的方法通过学习一组聚类标签来提高鲁棒性，同时通过额外的一致性正则化项和联合学习全局图像表示和密集局部表示来使其更加鲁棒。在本文的定位管道中，前者用于检索最相似的图像，后者用于通过最小化地图的3D点和其在查询图像上的投影之间的标签不一致性来优化检索到的姿态。本文的方法可以实现（接近）最先进的姿态估计结果，同时只使用一个紧凑的3D地图，该地图不包含足够的关于原始图像的信息，以便攻击者重构个人信息。
- (4):本文的方法在多个室内和室外环境中进行了评估，同时通过详细的实验量化了隐私。本文的方法可以实现（接近）最先进的姿态估计结果，同时只使用一个紧凑的3D地图，该地图不包含足够的关于原始图像的信息，以便攻击者重构个人信息。本文的方法可以实现隐私保护的视觉定位，同时具有良好的精度、内存和隐私保护的平衡。
#### 7. 方法详细介绍：
本文提出了一种名为SegLoc的新型定位框架，它利用图像分割创建了稳健、紧凑和隐私保护的场景表示。该方法建立在Fine-grained Segmentation的基础上，通过学习一组具有辨别性聚类标签、额外的一致性正则化项和联合学习全局图像表示和密集局部表示的方法，使其更加稳健。SegLoc模型同时创建了一个用于检索初始姿态的稳健全局描述符和用于通过最大化重投影3D点和查询图像之间的标签一致性获得精细姿态的密集局部表示。

具体步骤如下：
1. 利用自监督学习的方法，从不同视角和时间点的图像对中提取关键点对应关系，训练模型。
2. 利用预训练的编码器提供初始的密集表示，将其分组为K个原型，其中K是表示所需分割粒度的聚类数。
3. 利用具有分层结构的分割网络，使用类似于混合DPT的编码器-解码器模块作为骨干网络，进行密集分割表示的学习。
4. 利用多相似度对比损失训练全局图像表示。
5. 利用辅助目标分布进行自监督学习，通过交替更新目标分布和模型参数来最小化聚类目标函数。

#### 8. 实验设置：
本文在室内和室外场景中进行了训练和评估。对于室外场景，作者创建了一个扩展版本的Cross-Seasons Correspondences数据集，包括更多不同视角和时间点的图像对。对于室内场景，作者使用了具有挑战性的Indoor6数据集，从中采样不同条件下的共视图像对，并基于几何计算它们之间的对应关系。作者使用ECMU和Indoor6的测试集进行领域内评估，使用RobotCar Seasons和Cambridge Landmarks进行泛化能力评估。

#### 9. 实验结果与分析：
本文提出了一种名为SegLoc的隐私保护视觉定位方法。该方法与基于特征的PixLoc以及其他隐私保护方法（如DSAC*、GoMatch和NBE+SLD）在Indoor6和Cambridge Landmarks数据集上进行了比较。SegLoc在Indoor6上优于DSAC*和GoMatch，在Cambridge Landmarks上始终优于GoMatch。该方法还在ECMU和RC Seasons数据集上进行了评估，在郊区和公园场景中优于PixLoc。然而，使用NetVLAD初始化姿态时，SegLoc的表现不如PixLoc。本文还进行了消融实验，证明了所提出方法的有效性。最后，本文讨论了精度与隐私保护之间的权衡，并表明SegLoc比PixLoc更具隐私保护性，但牺牲了姿态精度。


# Paper:743     基于视觉-语言对应的盲图像质量评估：多任务学习视角



#### 1. Title: 
Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective

#### 2. Authors: 
Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, Kede Ma

#### 3. Affiliation: 
第一作者：上海交通大学人工智能研究院

#### 4. Keywords: 
Blind image quality assessment, Multitask learning, Vision-language correspondence, Scene classification, Distortion type identification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Blind_Image_Quality_Assessment_via_Vision-Language_Correspondence_A_Multitask_CVPR_2021_paper.html  Github: https://github.com/zwx8981/LIQE

#### 6. Summary : 
- (1):本文研究盲图像质量评估（BIQA）的多任务学习方法，旨在利用其他任务的辅助知识来提高BIQA的性能。 
- (2):过去的BIQA方法主要依赖于手工特征或浅层特征学习，而深度学习方法则需要大量的训练数据和人工质量注释。本文提出了一种自动化的多任务学习框架，利用对图像和文本的联合预训练模型CLIP，将BIQA与场景分类和失真类型识别任务相结合，实现了自动的参数共享和损失加权。 
- (3):本文提出的方法通过对图像和文本的联合概率分布进行建模，将三个任务的预测结果通过加权求和的方式得到最终的质量评分。实验结果表明，本文提出的方法在多个BIQA数据集上均优于现有方法，并且在场景分类和失真类型识别任务上也取得了较好的性能。 
- (4):本文提出的方法在多个BIQA数据集上均取得了优于现有方法的性能，证明了多任务学习在BIQA中的有效性。同时，本文提出的方法还能更好地对齐不同数据集中的质量注释，具有更好的泛化性能。
#### 7. 方法详细介绍：
本文提出了一种基于视觉-语言对应关系的多任务学习方案，用于盲图像质量评估（BIQA）。该方法使用预训练的CLIP模型进行特征嵌入，并计算子图像的视觉嵌入和候选文本嵌入之间的余弦相似度。使用可学习的温度参数的softmax函数计算联合概率。采用成对的学习排序模型估计BIQA，并采用保真度损失作为统计距离度量。场景分类问题被制定为多标签分类问题，而失真类型识别则被制定为标准的多类分类问题。多任务学习在M（M≥2）个IQA数据集上进行联合训练。

具体步骤如下：
1. 使用预训练的CLIP模型进行特征嵌入。
2. 计算子图像的视觉嵌入和候选文本嵌入之间的余弦相似度。
3. 使用可学习的温度参数的softmax函数计算联合概率。
4. 采用成对的学习排序模型估计BIQA，并采用保真度损失作为统计距离度量。
5. 场景分类问题被制定为多标签分类问题，而失真类型识别则被制定为标准的多类分类问题。
6. 多任务学习在M（M≥2）个IQA数据集上进行联合训练。

#### 8. 实验设置：
本文在LIVE、CSIQ、KADID-10k、LIVE Challenge、BID和KonIQ-10K等六个IQA数据集上进行了实验。作者随机从每个数据集中抽取70％和10％的图像构建训练和验证集，剩余20％用于测试。他们根据参考图像将训练/验证/测试集分开，以确保内容独立性。作者采用ViT-B/32作为视觉编码器，采用63M参数的GPT-2作为文本编码器。他们使用AdamW最小化Eq.（10）中的目标，其中包括三个保真度损失的加权和。初始学习率设置为5×10^-6，按余弦退火规则进行调度。他们在LIVE、CSIQ、BID和LIVE Challenge数据集上使用大小为4的mini-batch大小，KonIQ-10k和KADID-10k上使用大小为16的mini-batch大小。在训练和推理过程中，他们随机裁剪3和15个空间大小为224×224×3的子图像，而不改变它们的长宽比。所有实验都在单个NVIDIA GeForce RTX 3090 GPU上进行。

#### 9. 实验结果与分析：
本文提出的LIQE方法在多个IQA数据集上的预测准确性优于现有的BIQA方法，并在gMAD竞赛中表现出更好的泛化能力和鲁棒性。作者还提供了定量证据，证明了所提出的方法更好地重新调整了来自不同IQA数据集的MOSs。作者还进行了一系列消融研究，以验证LIQE的设计合理性。在TID2013、SPAQ和PIPAL数据集上进行的交叉数据集评估表明，LIQE优于其他BIQA模型。作者还进行了感知尺度重新对齐实验，以验证LIQE在重新对齐来自不同IQA数据集的质量注释方面的性能。


# Paper:744     PointConvFormer: 基于点卷积的复仇



#### 1. Title: 
PointConvFormer: Revenge of the Point-based Convolution

#### 2. Authors: 
Wenxuan Wu, Li Fuxin, Qi Shan

#### 3. Affiliation: 
Wenxuan Wu: CASIA (中国科学院自动化研究所)
Li Fuxin: Oregon State University
Qi Shan: Apple, Inc.

#### 4. Keywords: 
Point cloud, convolution, attention, segmentation, scene flow estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_PointConvFormer_Revenge_of_the_Point-Based_Convolution_CVPR_2021_paper.html  Github: https://github.com/wxwu/PointConvFormer

#### 6. Summary : 
- (1):本文研究的背景是点云数据的处理，点云数据的无序性使得传统的卷积神经网络难以处理。
- (2):过去的方法包括将点云数据投影到二维图像上，或者将点云数据离散化为三维体素进行卷积。这些方法存在计算和内存开销大的问题。本文提出的方法是PointConvFormer，它结合了点卷积和Transformer的思想，使用基于特征差异的注意力机制来修改每个点的卷积权重，从而保留了点卷积的不变性，同时注意力机制有助于选择邻域中的相关点进行卷积。这种方法适用于需要点级细节的多个任务，如分割和场景流估计任务。 
- (3):本文提出的PointConvFormer方法使用基于特征差异的注意力机制来改进点卷积，从而提高了模型的鲁棒性和泛化能力。在ScanNet和SemanticKitti等多个数据集上进行了实验，结果表明PointConvFormer在精度和速度之间取得了更好的平衡，并且比传统的卷积、Transformer和体素化稀疏卷积方法表现更好。 
- (4):本文在ScanNet和SemanticKitti数据集上进行了语义分割任务的实验，结果表明PointConvFormer在低分辨率下比MinkowskiNet等传统方法提高了10%以上的精度。在FlyingThings3D和KITTI场景流2015数据集上进行了场景流估计任务的实验，结果表明PointConvFormer作为PointPWC-Net的骨干网络可以显著提高性能。这些结果表明PointConvFormer可以作为点云数据处理的一种有效方法。
#### 7. 方法详细介绍：
本文提出了一种新的点云卷积操作——PointConvFormer，它将注意力机制和卷积神经网络的优势结合起来。PointConvFormer考虑了邻域内点的相对位置和特征差异。其中，函数w(pi - p)用于计算卷积核，而标量函数ψ([x(pi) - x(p), pi - p])用于调节卷积核。函数ψ(·)通过MLP和激活层来近似计算。PointConvFormer由瓶颈残差块构成，其中包含两个分支。主分支是一个线性层，其后是PointConvFormer层，再后是另一个线性层。PointConvFormer通过网格子采样方法对点云进行下采样，并在已知坐标p的情况下应用PointConv层进行反卷积。

#### 8. 实验设置：
本文在多个数据集和任务中评估了PointConvFormer的有效性。对于3D语义分割，使用了ScanNet数据集和SemanticKitti数据集。对于从3D点云中估计场景流，使用了合成的FlyingThings3D数据集进行训练，并使用KITTI场景流2015数据集进行测试。使用AdamW优化器，学习率为0.001，权重衰减为0.05。使用加权交叉熵损失函数。

#### 9. 实验结果与分析：
实验结果表明，PointConvFormer在输入网格大小不同的情况下，都能够提供更好的准确性和速度平衡，优于MinkowskiNet42、传统Transformer和只使用视点不变点卷积的方法。在ScanNet数据集上，PointConvFormer在10cm、5cm和2cm输入网格大小下都显著优于其他方法，同时在前两种情况下速度也更快。PointConvFormer比所有Transformer方法都要快得多。在10cm和5cm分辨率下，mix3D数据增强可以进一步提高结果。PointConvFormer-Lite版本在保持性能最小降低的情况下更快、更节省内存。可视化结果表明，与PointConv和Point Transformer相比，PointConvFormer能够更好地预测细节。


# Paper:745     DBARF：深度捆绑调整通用神经辐射场



#### 1. Title: 
DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields

#### 2. Authors: 
Yu Chen, Gim Hee Lee

#### 3. Affiliation: 
National University of Singapore（新加坡国立大学）

#### 4. Keywords: 
Neural Radiance Fields, Generalizable NeRFs, Bundle Adjustment, Deep Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2021_paper.html  Github: https://aibluefisher.github.io/dbarf

#### 6. Summary : 
- (1):本文研究的背景是神经辐射场（NeRF）的新视角合成任务，NeRF可以将隐式场表示编码为神经网络推理，从而实现高质量的新视角合成。然而，NeRF需要准确的相机姿态作为输入，而相机姿态的获取在现实世界中是昂贵的。
- (2):过去的方法通常使用结构从运动（SfM）来获取相机姿态，但SfM有时会失败，而且对于大规模场景可能需要数天甚至数周的时间才能完成。最近的一些工作尝试解决相机姿态不准确的问题，其中一个代表性的工作是BARF。然而，BARF和其后续工作仍需要从SfM中预计算相机姿态。本文提出了DBARF，可以在不需要已知绝对相机姿态的情况下，通过深度神经网络来联合训练GeNeRF和相对相机姿态的捆绑调整。与之前的方法只关注每个场景的相机姿态优化不同，我们的网络是可推广的。
- (3):本文提出了DBARF，通过将成本特征图作为隐式成本函数来捆绑调整相机姿态，可以在自监督的方式下与GeNeRF联合训练。与BARF及其后续工作不同，我们的方法可以推广到跨场景，并且不需要任何良好的初始化。我们的方法可以通过将3D点映射到附近视图的特征图上来构建残差特征图，然后将残差特征图作为隐式成本函数，利用深度姿态优化器来学习从目标视图到附近视图的相对相机姿态的修正。我们进一步联合训练姿态优化器和GeNeRF，使用图像作为监督，不依赖于地面真实相机姿态。与之前的方法只关注每个场景的相机姿态优化不同，我们的网络是可推广的。
- (4):本文的方法在真实世界数据集上进行了实验，结果表明了我们的DBARF的有效性和推广能力。我们的方法可以推广到跨场景，并且不需要任何良好的初始化。
#### 7. 方法详细介绍：
本文提出了一种名为DBARF的方法，它是一种深度束调整方法，用于联合优化相机姿态和通用神经辐射场（GeNeRFs）。该方法利用成本特征图作为隐式成本函数来束调整相机姿态。成本特征图是通过将3D点变形到附近视图的特征图上并将残差特征图作为输入构建的。然后使用深度姿态优化器来学习从目标视图到附近视图的相对相机姿态的校正。该方法在不需要已知绝对相机姿态的情况下进行端到端训练，并且可以以自监督的方式与GeNeRFs一起进行联合训练。该方法在场景中具有通用性，不需要任何良好的初始化。

#### 8. 实验设置：
本文在LLFF和ScanNet数据集上评估了所提出的方法。IBRNet和我们的方法在自收集数据集、LLFF和ScanNet数据集上进行了预训练。IBRNet提供了地面真实相机姿态，但我们的方法没有使用。评估场景在预训练期间未使用。由于GARF的代码在本文撰写期间不公开，因此引用了原始论文中的定量结果。该方法在单个24G NVIDIA RTX A5000 GPU上实现。学习率、附近视图选择和迭代次数设置为预训练和微调。

#### 9. 实验结果与分析：
本文评估了所提出的方法的渲染质量和姿态精度。渲染质量通过PSNR、SSIM和LPIPS进行衡量。在某些场景中，所提出的方法在渲染质量方面优于BARF、GARF甚至IBRNet。姿态精度通过将预测的相对姿态和地面真实相机姿态之间的所有姿态误差取平均值来衡量。所提出的方法在姿态精度方面与IBRNet相比具有竞争力。还通过比较有无微调的结果来评估所提出的方法的泛化能力。结果表明，即使没有针对每个场景进行微调，所提出的方法也可以实现良好的性能。在LLFF前向数据集上进行了新视图合成的定量结果。所提出的方法在新视图合成方面优于BARF和GARF，即使没有针对每个场景进行微调。在没有微调的情况下，所提出的方法的旋转误差在大多数场景中小于13度，比从头开始每个场景进行训练要便宜得多。在微调的情况下，旋转误差小于1.5度（除了叶子场景）。所提出的方法计算的相机姿态比COLMAP更好，因为所提出的方法的渲染质量比IBRNet在蕨、花和堡垒场景中更好。


# Paper:746     基于分层视觉转换的领域通用立体匹配



#### 1. Title: 
Domain Generalized Stereo Matching via Hierarchical Visual Transformation

#### 2. Authors: 
Tianyu Chang, Xun Yang, Tianzhu Zhang, Meng Wang

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Stereo Matching, Domain Generalization, Hierarchical Visual Transformation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文主要研究领域通用的立体匹配问题，即如何在只有合成数据的情况下训练出具有良好泛化性能的模型。
- (2):现有的深度立体匹配网络容易学习到数据集相关的shortcut，无法很好地推广到未见过的现实数据集。本文提出了一种基于Hierarchical Visual Transformation（HVT）的方法，通过将训练样本从全局、局部和像素三个层次进行转换，最大化源域和新域之间的视觉差异，最小化跨域特征不一致性，从而捕捉到域不变的特征。这样，我们就可以防止模型利用合成立体图像的artifact作为shortcut特征，从而基于学习到的shortcut-invariant特征表示更有效地估计视差图。
- (3):本文提出了一种简单而有效的领域通用立体匹配框架，它利用分层视觉转换网络来有效地多样化训练数据的分布，从而防止模型利用合成数据中的artifact作为shortcut。我们提出了新的学习目标，通过最大化合成域和新域之间的域差异和最小化特征不一致性来有效地优化三种互补的视觉转换，从而促进域不变特征的学习。我们将HVT网络与SOTA SM网络集成在一起进行训练，并在几个具有挑战性的SM基准数据集上评估其有效性。广泛的实验清楚地表明，HVT网络可以显著提高现有SM网络在合成到现实领域通用性方面的性能，而不使用任何辅助数据或特征。
- (4):本文在几个具有挑战性的SM基准数据集上评估了HVT网络的有效性和鲁棒性。四种SOTA SM方法的超出分布的泛化能力得到了显著提高，从而受益于我们的解决方案。
#### 7. 方法详细介绍：
本文提出了一种名为Hierarchical Visual Transformation（HVT）网络的方法，旨在提高现有立体匹配（SM）方法的鲁棒性和泛化能力。HVT网络由三种不同的转换组成，分别是全局几何变换（TG）、局部几何变换（TL）和光度变换（TP）。这些变换可以有效地使训练域多样化，并防止快捷方式被编码到立体图像表示中。HVT网络通过最小化三个损失项的线性组合来进行训练，包括最大化跨域视觉差异、最小化跨域特征不一致性和域分类的交叉熵损失。完整的损失是在训练批次上计算的平均值，并使用三个权衡超参数来平衡三个损失项。

#### 8. 实验设置：
本文使用SceneFlow数据集进行训练，并使用包括KITTI 2012、KITTI 2015、Middlebury和ETH3D在内的四个现实的SM数据集进行评估。性能评估使用端点误差（EPE）和误差大于阈值的像素百分比（D1）。超参数设置如下：λ1=0.1，λ2=0.1，λ3=0.01。批量大小设置为8，学习率初始化为0.001，并在50个epoch后按10的倍数递减。

#### 9. 实验结果和分析：
本文提出的HVT网络在所有四个SM数据集上均取得了最先进的性能，优于现有的域自适应和泛化方法。结果表明，HVT网络中的三种变换有效地互补，该方法可以学习到域不变特征。HVT网络在复杂的现实场景中也表现出了鲁棒性。


# Paper:747     探索建筑设计与对抗鲁棒性泛化之间的关系



#### 1. Title: 
Exploring the Relationship between Architectural Design and Adversarially Robust Generalization

#### 2. Authors: 
Aishan Liu, Shiyu Tang, Siyuan Liang, Ruihao Gong, Boxi Wu, Xianglong Liu, Dacheng Tao

#### 3. Affiliation: 
北京航空航天大学

#### 4. Keywords: 
Adversarial training, adversarially robust generalization, architectural design, Vision Transformers, Rademacher complexity

#### 5. Paper: 
Paper: http://openaccess.thecvf.com/content/CVPR2021/html/Liu_Exploring_the_Relationship_Between_Architectural_Design_and_Adversarially_Robust_Generalization_CVPR_2021_paper.html
Github: http://robust.art

#### 6. Summary : 
- (1):本文研究了对抗训练中的鲁棒性泛化问题，即在测试时对未知攻击的鲁棒性泛化问题，从建筑设计的角度进行了系统研究。
- (2):对抗训练已被证明是防御对抗样本最有效的方法之一，但它经常在未见过的测试对手上存在巨大的鲁棒性泛化差距。本文通过实验发现，在对齐设置下，Vision Transformers（例如PVT、CoAtNet）往往产生更好的对抗鲁棒性泛化，而CNN往往在特定攻击上过拟合并且无法在多个对手上进行泛化。本文通过Rademacher复杂性的视角进行理论分析，揭示了更高的权重稀疏性对于Transformers更好的对抗鲁棒性泛化的重要贡献，这通常可以通过特别设计的注意力块来实现。
- (3):本文首次系统地研究了建筑设计与对抗鲁棒性泛化之间的关系。作者对20种最具代表性的对抗训练架构进行了全面评估，并通过Rademacher复杂性的视角进行了理论分析。作者发现，Transformers的更高权重稀疏性对于更好的对抗鲁棒性泛化做出了重要贡献，这通常可以通过特别设计的注意力块来实现。本文的主要贡献在于：（1）系统研究了20种对抗训练架构在多个对手上的表现；（2）理论上揭示了Transformers更好的对抗鲁棒性泛化的原因；（3）提供了更详细的分析，以更好地理解泛化能力，并讨论了提高架构鲁棒性的潜在途径。
- (4):本文在ImageNette和CIFAR-10数据集上对20种对抗训练架构进行了全面评估，并发现在对齐设置下，Vision Transformers（例如PVT、CoAtNet）往往产生更好的对抗鲁棒性泛化，而CNN往往在特定攻击上过拟合并且无法在多个对手上进行泛化。作者通过Rademacher复杂性的视角进行理论分析，揭示了更高的权重稀疏性对于Transformers更好的对抗鲁棒性泛化的重要贡献，这通常可以通过特别设计的注意力块来实现。本文的实验结果表明，Transformers的更好的对抗鲁棒性泛化可以支持其在实践中的应用。
#### 7. 方法详细介绍：
本文中的方法部分主要介绍了Rademacher复杂度和线性分类器的定义，以及对抗训练的函数类定义。同时，本文还提出了引理1和定理1，分别说明了对抗训练的Rademacher复杂度与分类器的架构有关，以及通过SDP松弛和优化上界可以提高Transformer模型的泛化性能。

#### 8. 实验设置：
本文在CIFAR-10和ImageNette数据集上评估了20种经过对抗训练的模型架构的对抗鲁棒性。使用标准训练和PGD-ℓ∞对抗训练进行训练，并使用常用的对抗攻击方法进行评估。CIFAR-10训练使用AutoAugment数据增强、AdamW优化器和余弦学习率，ImageNette训练使用标准数据增强。评估指标包括干净样本的分类准确率和对抗样本的最坏情况下的鲁棒准确率。

#### 9. 实验结果和分析：
本文发现，在对齐的设置下，Vision Transformer（例如PVT、CoAtNet）通常具有更好的对抗鲁棒泛化性能，而CNN往往会在特定攻击上过拟合，并且无法在多个对手上进行泛化。本文还提供了更详细的分析，以更好地理解泛化性能。作者还讨论了可能改善架构鲁棒性的潜在途径，并在这些途径上进行了实验。实验结果表明，施加ℓ1正则化可以提高对抗鲁棒泛化性能，增加注意力层可以提高权重稀疏性，减小补丁大小往往会过拟合训练的对抗攻击。


# Paper:748     FashionSAP：符号和属性提示用于细粒度时尚视觉-语言预训练



#### 1. Title: 
FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training

#### 2. Authors: 
Yunpeng Han, Lisai Zhang, Qingcai Chen, Zhijian Chen, Zhonghua Li, Jianxin Yang, Zhao Cao

#### 3. Affiliation: 
第一作者：哈尔滨工业大学深圳研究生院，中国

#### 4. Keywords: 
Fashion vision-language pre-training, fine-grained attributes, fashion symbols, attribute prompt, multi-modalities

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Han_FashionSAP_Symbols_and_Attributes_Prompt_for_Fine-Grained_Fashion_Vision-Language_Pre-Training_CVPR_2021_paper.html  Github: https://github.com/hssip/FashionSAP

#### 6. Summary : 
- (1):本文研究的背景是视觉-语言预训练模型在时尚领域的应用，时尚领域需要学习高质量的细粒度属性，而通用预训练模型对于细粒度属性的学习效果不佳。

- (2):过去的方法直接将通用预训练模型应用于时尚任务，但是这些方法忽略了时尚领域的细粒度属性，而这些属性对于区分特定领域任务和通用任务非常重要。本文提出了一种基于时尚符号和属性提示的细粒度时尚视觉-语言预训练方法，以建模细粒度多模态时尚属性和特征。本文提出了时尚符号和属性提示方法，使模型能够显式地学习时尚项目的特定属性。综合实验表明，FashionSAP在两个公共时尚基准测试中获得了四个流行时尚任务的SOTA表现。本文的创新点在于提出了抽象的时尚符号概念层，以及属性提示方法，使模型能够有效地获取时尚领域的细粒度语义。

- (3):本文提出了一种有效的细粒度视觉-语言预训练模型，以学习属性级时尚知识。本文提出了抽象的时尚符号概念层，总结了九个时尚符号，以表示不同的时尚概念。本文提出了属性提示方法，使跨模态预训练模型能够显式地学习细粒度时尚特征。

- (4):本文在两个公共时尚基准测试中进行了全面的实验，FashionSAP在四个流行时尚任务上获得了SOTA表现。本文的方法能够有效地获取时尚领域的细粒度语义，性能提升明显，为未来的时尚任务研究提供了新的基线。
#### 7. 方法详细介绍：
FashionSAP是一种用于细粒度时尚视觉语言任务的预训练框架。该框架包括图像编码器、文本编码器和特征融合模块。图像编码器将输入图像编码为特征向量，而文本编码器将包括时尚符号和属性在内的输入文本编码为另一个特征向量。特征融合模块将来自文本和图像的特征融合成混合特征。预训练任务包括时尚符号图像相似性（FSIS）、提示令牌预测（PTP）、令牌替换预测（TRP）和图像文本相似性（ITS）。FashionSAP模型是一种单流和双流模型的组合，以适应时尚任务。预训练模型的优化采用AdamW优化器，批量大小为16，学习率为6e-5。模型在FashionGen数据集上进行预训练，评估结果表明，FashionSAP模型在时尚任务中具有良好的性能。

#### 8. 实验设置：
FashionSAP模型在两个公共时尚基准数据集FashionGen和FashionIQ上进行了全面的实验评估。FashionGen数据集包括320k对文本-图像和40k个独特的时尚物品。FashionSAP模型在FashionGen数据集上进行预训练，使用FashionGen的训练集作为预训练数据，包含约260k对文本-图像。模型在FashionGen和FashionIQ数据集上进行下游任务的评估。

#### 9. 实验结果和分析：
FashionSAP模型在所有下游任务中均取得了最先进的性能，包括跨模态检索、类别/子类别识别和文本修改图像检索。模型的性能优于先前的模型，如VL-BERT、ViLBERT和FashionBERT。消融研究还表明，所提出的预训练任务有助于模型的整体性能。


# Paper:749     基于零样本文本到参数转换的游戏角色自动生成



#### 1. Title: 
Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation

#### 2. Authors: 
Rui Zhao, Wei Li, Zhipeng Hu, Lincheng Li, Zhengxia Zou, Zhenwei Shi, Changjie Fan

#### 3. Affiliation: 
Rui Zhao, Zhipeng Hu, Lincheng Li, and Changjie Fan are affiliated with Netease Fuxi AI Lab.

#### 4. Keywords: 
Text-to-Parameter Translation, Game Character Auto-Creation, Neural Rendering, Multi-Modal CLIP

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Zero-Shot_Text-to-Parameter_Translation_for_Game_Character_Auto-Creation_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1): This paper aims to propose a novel text-to-parameter translation method for zero-shot text-driven game character auto-creation.
 
- (2): Previous in-game character auto-creation systems are mostly image-driven, where facial parameters are optimized so that the rendered character looks similar to the reference face photo. However, this paper proposes a method that can handle the optimization of both discrete and continuous parameters. The approach is well motivated and the innovation lies in the use of large-scale pre-trained multi-modal CLIP and neural rendering to search both continuous and discrete facial parameters in a unified framework.
 
- (3): The proposed method, called T2P, uses multi-modal CLIP to encode the input text and neural rendering to generate the final character. T2P searches both continuous and discrete facial parameters in a unified framework. The innovation lies in the use of large-scale pre-trained multi-modal CLIP and neural rendering to achieve zero-shot text-driven game character auto-creation. The experimental results show that T2P can generate high-quality and vivid game characters with given text prompts and outperforms other SOTA text-to-3D generation methods on both objective evaluations and subjective evaluations.
  
- (4): The proposed T2P method achieves high-quality and vivid game character auto-creation with arbitrary text description without using any reference photo or editing hundreds of parameters manually. The performance supports the goals of the paper.
#### 7. 方法详细介绍：
本文提出了一种新的文本到参数转换方法（T2P），以实现零样本文本驱动游戏角色自动生成。T2P在统一框架中搜索连续的面部参数和离散的面部参数，利用大规模预训练的多模态CLIP和神经渲染的能力。T2P是第一种可以处理离散和连续参数优化的方法。具体步骤如下：
1. 使用CLIP模型将文本转换为图像特征。
2. 使用神经渲染器将图像特征转换为面部参数。
3. 通过优化算法搜索最佳的面部参数，以生成高质量的游戏角色。

#### 8. 实验设置：
本文使用了两个数据集进行实验：CelebA和FFHQ。使用了两种评估指标：FID和人类主观评估。实验中使用了多种对比方法，包括GAN、VAE等。

#### 9. 实验结果和分析：
实验结果表明，T2P可以根据给定的文本提示生成高质量、生动的游戏角色。T2P在客观评估和主观评估上均优于其他SOTA文本到3D生成方法。


# Paper:750     NeuDA: 高保真隐式表面重建的神经可变锚点



#### 1. Title: 
NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction

#### 2. Authors: 
Bowen Cai, Jinchi Huang, Rongfei Jia, Chengfei Lv, Huan Fu

#### 3. Affiliation: 
阿里巴巴集团淘宝技术部

#### 4. Keywords: 
Implicit surface reconstruction, differentiable ray casting, neural implicit representation, multi-level voxel grids, deformable anchor

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cai_NeuDA_Neural_Deformable_Anchor_for_High-Fidelity_Implicit_Surface_Reconstruction_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了利用可微分射线投影的隐式表面重建。以往的方法在预测和渲染表面时忽略了三维空间中的空间上下文，因此可能无法捕捉小孔和结构等尖锐的局部拓扑。为了缓解这种局限性，本文提出了一种灵活的神经隐式表示，利用分层体素网格，即神经可变锚点（NeuDA），用于高保真表面重建。NeuDA维护分层锚点网格，其中每个顶点存储一个三维位置（或锚点），而不是直接嵌入（或特征）。我们通过优化锚点网格，使不同的局部几何结构可以自适应地编码。此外，我们深入研究了频率编码策略，并引入了一种简单的分层位置编码方法，用于灵活地利用高频和低频几何和外观的属性。在DTU和BlendedMVS数据集上的实验表明，NeuDA可以产生有前途的网格表面。

- (2):以往的方法在预测和渲染表面时忽略了三维空间中的空间上下文，因此可能无法捕捉小孔和结构等尖锐的局部拓扑。本文提出了一种灵活的神经隐式表示，利用分层体素网格，即神经可变锚点（NeuDA），用于高保真表面重建。NeuDA维护分层锚点网格，其中每个顶点存储一个三维位置（或锚点），而不是直接嵌入（或特征）。我们通过优化锚点网格，使不同的局部几何结构可以自适应地编码。此外，我们深入研究了频率编码策略，并引入了一种简单的分层位置编码方法，用于灵活地利用高频和低频几何和外观的属性。

- (3):本文提出了一种新的神经隐式表示，即NeuDA，用于高保真表面重建。NeuDA维护分层锚点网格，其中每个顶点存储一个三维位置（或锚点），而不是直接嵌入（或特征）。我们通过优化锚点网格，使不同的局部几何结构可以自适应地编码。此外，我们深入研究了频率编码策略，并引入了一种简单的分层位置编码方法，用于灵活地利用高频和低频几何和外观的属性。实验结果表明，NeuDA可以产生有前途的网格表面。

- (4):本文提出的NeuDA方法在DTU和BlendedMVS数据集上进行了实验，可以产生有
#### 7. 方法详细介绍：
本文提出了一种名为NeuDA的高保真度隐式表面重建方法。该方法采用可变形锚点方法来表示3D场景，并使用多层感知器（MLP）网络预测符号距离函数（SDF）。该方法还采用分层位置编码（HPE）策略来捕捉几何高频变化。损失函数包括Chamfer距离（CD）项、正则化项、法向量正则化项和掩码项。完整的目标函数公式为L = Lc + λeikLreg + λnormLnorm + λmaskLmask。

#### 8. 实验设置：
本文在两个数据集DTU和BlendedMVS上进行实验。DTU数据集包含15个场景，每个场景有49或64张图像，BlendedMVS数据集包含7个场景，每个场景有31到143张图像。DTU数据集提供了地面真实点云，使用CD进行定量比较。实验分别在有和没有前景掩码的情况下进行。

#### 9. 实验结果与分析：
本文提出的NeuDA方法在有和没有掩码的情况下均优于基线方法，并在与先前最先进方法的比较中取得了最佳的平均CD得分。该方法在捕捉细粒度表面细节和保留空心结构方面表现出良好的性能。消融实验表明，可变形锚点方法对表面重建是有效的，而分层位置编码策略与标准的多级位置编码方法表现相当。最佳的分层级别在6-8之间。

#### 10. 总结：
本文提出了一种名为NeuDA的高保真度隐式表面重建方法。该方法采用可变形锚点方法来表示3D场景，并使用多层感知器（MLP）网络预测符号距离函数（SDF）。该方法还采用分层位置编码（HPE）策略来捕捉几何高频变化。实验结果表明，该方法在有和没有掩码的情况下均优于基线方法，并在与先前最先进方法的比较中取得了最佳的平均CD得分。该方法在捕捉细粒度表面细节和保留空心结构方面表现出良好的性能。


# Paper:751     视频测试时间适应的动作识别



#### 1. Title: 
Video Test-Time Adaptation for Action Recognition

#### 2. Authors: 
Wei Lin, Muhammad Jehanzeb Mirza, Mateusz Kozinski, Horst Possegger, Hilde Kuehne, Horst Bischof

#### 3. Affiliation: 
Wei Lin: 奥地利格拉茨科技大学计算机图形学与视觉研究所

#### 4. Keywords: 
Action recognition, Test-Time-Adaptation, Feature alignment, Video-specific adaptation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Lin_Video_Test-Time_Adaptation_for_Action_Recognition_CVPR_2022_paper.html  Github: https://github.com/wlin-at/ViTTA

#### 6. Summary : 
- (1):本文研究的是视频动作识别中的测试时间适应问题。尽管动作识别系统在分布相同的测试点上可以达到最佳性能，但它们容易受到测试数据中未预料到的分布转移的影响。因此，本文提出了一种适用于时空模型的方法，能够在单个视频样本上进行适应。该方法采用特征分布对齐技术，将在线估计的测试集统计数据与训练统计数据对齐。同时，本文还通过对同一测试视频样本的时间增强视图进行预测一致性的强制执行来进一步提高预测的准确性。

- (2):在图像分类中，测试时间适应可以通过使用未标记的测试数据来适应模型对数据分布的变化。然而，这些方法不适用于动作识别。大多数动作识别应用需要在线运行内存和计算密集型的时间模型，延迟最小，硬件资源有限，模型较大时，批处理大小需要严格限制。此外，视频比图像更容易受到分布转移的影响。本文提出的方法是一种在线测试时间适应方法，适用于预训练的卷积和变压器模型，无需重新训练。

- (3):本文提出了一种视频特定的适应技术（ViTTA），它通过对输入视频的时间重新采样帧序列来创建增强视图，从而利用了视频数据的时间维度。这样做有两个好处：首先，多个增强视图可以导致更准确的整体视频内容统计数据；其次，它允许我们在视图之间强制执行预测一致性，使适应更加有效。本文在三个最流行的动作识别基准数据集上进行了广泛的评估，证明了ViTTA对TANet和Video Swin Transformer的性能提升，以及在单个分布转移和随机分布转移的评估中，ViTTA相对于现有的测试时间适应方法具有显著的性能优势。

- (4):本文的方法在三个最流行的动作识别基准数据集上进行了广泛的评估，证明了ViTTA对TANet和Video Swin Transformer的性能提升。ViTTA是一种完全在线的方法，适用于需要最小延迟的使用情况。它不需要收集或存储测试视频数据集，这在数据隐私保护方面非常重要，特别是在处理机密用户视频时。ViTTA可以无缝地集成到已经运行的系统中，因为它不需要重新训练现有的网络。因此，它可以利用最先进的视频架构。
#### 7. 方法详细介绍：
本文提出了一种名为ViTTA的视频自适应方法，该方法可以适应视频序列中的常见分布偏移。ViTTA由特征分布对齐技术组成，该技术将训练统计数据与测试集统计数据进行对齐。该方法不依赖于特定的模型，可以适应卷积和基于Transformer的网络，无需重新训练。适应是通过迭代更新参数向量来实现的，以将所选层的测试统计数据与训练数据计算的统计数据对齐。此外，该方法还利用数据的时间性质，通过对同一视频创建多个重新采样的视图来提高单个视频上统计数据的准确性。视图之间的相应预测被强制保持一致，以进一步提高方法的效果。

#### 8. 实验设置：
本文在UCF101、Something-something v2（SSv2）和Kinetics 400（K400）三个数据集上评估了所提出的方法。评估是在之前的工作中提出的12种数据扰动类型上进行的，以评估时空模型的鲁棒性。本文使用这些基准论文中的实现来生成数据扰动，并在最严重的情况下（级别5）进行评估。本文使用两种模型架构：基于ResNet50的TANet和基于Swin-B的Video Swin Transformer。本文对最后两个块中的归一化层的特征进行分布对齐，并在所有数据集上将批量大小设置为1进行评估。本文在适应样本后立即进行推理，并报告所有样本的累积准确率。

#### 9. 实验结果和分析：
所提出的方法ViTTA在单一扰动和具有挑战性的随机扰动场景下表现良好。此外，面对分布偏移的周期性变化，它表现出快速的适应性。结果表明，ViTTA可以很好地推广到不同类型的帧采样策略，并且在具有BN层的架构中可以依赖批量归一化统计数据，性能损失较小。


# Paper:752     NeuMap: 基于自动Transdecoder的神经坐标映射用于相机定位



#### 1. Title: 
NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization

#### 2. Authors: 
Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, Yasutaka Furukawa

#### 3. Affiliation: 
Simon Fraser University (西蒙菲莎大学)

#### 4. Keywords: 
Camera localization, feature matching, coordinate regression, auto-decoder, transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tang_NeuMap_Neural_Coordinate_Mapping_by_Auto-Transdecoder_for_Camera_Localization_CVPR_2021_paper.html  Github: https://github.com/Tangshitao/NeuMap

#### 6. Summary : 
- (1):本文研究的是相机定位问题，即根据图像观测来确定相机的位置和方向。相机定位在VR/AR和自动驾驶等领域有着广泛的应用，但是在大场景、大视角和大光照变化下，相机定位仍然是一个具有挑战性的问题。本文的研究背景是如何在保证定位精度的同时，实现场景表示的紧凑化。

- (2):传统的相机定位方法主要分为两类：特征匹配和场景坐标回归。特征匹配方法使用从查询图像中提取的稀疏鲁棒特征与候选场景图像中的特征进行匹配，从而实现相机定位。这种方法需要大量的场景表示，因此在大场景下不适用。场景坐标回归方法直接从RGB图像中回归密集的场景坐标图，提供了小型室内场景的准确结果。但是，它们的紧凑模型缺乏泛化能力，通常受限于有限的视角和光照变化。本文提出了一种新的方法，既具有场景坐标回归方法的紧凑性，又具有特征匹配方法的鲁棒性。

- (3):本文提出了一种名为NeuMap的方法，它将整个场景编码为一组潜在代码，并使用基于Transformer的自动解码器来回归查询像素的三维坐标。与传统的相机定位方法不同，NeuMap使用可学习的潜在代码来存储场景信息，并使用场景不可知的Transformer-based自动解码器来推断查询像素的三维坐标。这种场景不可知的网络设计还通过大规模数据的训练学习了鲁棒的匹配先验，并且允许我们在固定网络权重的同时快速优化新场景的代码。NeuMap在五个基准测试中表现出色，比所有其他坐标回归方法都要好，并且在具有更小的场景表示大小的情况下达到了与特征匹配方法相似的性能。

- (4):本文的方法在相机定位任务上取得了很好的性能。在小规模数据集上，NeuMap将场景表示压缩了约100-1000倍，而不会降低性能。在大规模数据集上，NeuMap在高压缩设置下明显优于当前的最先进方法，并且在只优化代码的情况下，仍然可以快速进行新场景的微调。
#### 7. 方法详细介绍：
本文提出了一种名为NeuMap的端到端神经映射方法，用于相机定位。该方法使用一个场景无关的基于transformer的自编码器将整个场景编码为潜在代码的网格，以回归查询像素的3D坐标。自编码器学习了一个场景代码网格，编码了场景信息和查询图像特征点到3D场景坐标的映射。在测试时，给定一个查询图像，自动转换解码器通过图像特征和潜在代码之间的交叉注意力回归它们的3D坐标。该方法的特征提取器和自动转换解码器是场景无关的，只有潜在代码是场景特定的。该方法还采用了网络剪枝技术来丢弃冗余代码并处理大场景。

#### 8. 实验设置：
作者使用ResNet-18提取图像特征，并使用8个V100 GPU训练模型，批量大小为256。他们将初始学习率设置为0.002，用于训练场景无关参数，0.0001用于训练场景特定代码。每30个epoch后，他们将学习率乘以0.5。他们在第一阶段训练模型200个epoch，并在第二阶段微调模型100个epoch。对于训练数据生成，他们使用r2d2提取关键点并进行三角测量以在Cambridge、Aachen和NAVER中获得坐标。他们使用深度图像在7scenes和ScanNet中获取3D坐标。

#### 9. 实验结果和分析：
本文提出的相机定位方法NeuMap在各种数据集和场景上的准确性优于所有其他端到端方法，并且在比特征匹配方法更小的场景表示大小下实现了类似的性能。本文提供了表格和图形以展示NeuMap的准确性。本文还讨论了该方法的局限性和未来的研究方向。


# Paper:753     Super-CLEVR：一种用于诊断视觉推理领域鲁棒性的虚拟基准测试集



#### 1. Title: 
Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning

#### 2. Authors: 
Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, Alan Yuille

#### 3. Affiliation: 
第一作者：Johns Hopkins University（约翰霍普金斯大学）

#### 4. Keywords: 
Visual Question Answering, Domain Robustness, Benchmark, Neural Symbolic Methods, Uncertainty Reasoning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Super-CLEVR_A_Virtual_Benchmark_to_Diagnose_Domain_Robustness_in_Visual_CVPR_2021_paper.html  Github: https://github.com/Lizw14/Super-CLEVR

#### 6. Summary : 
- (1):本文研究的背景是Visual Question Answering（VQA）模型在面对不同领域的数据时表现不佳，缺乏领域泛化能力。
- (2):过去的方法主要集中在领域自适应和领域泛化上，但是由于VQA任务的多模态性质，领域泛化问题比较复杂，因此需要对其进行更深入的研究。本文提出了一个虚拟基准测试集Super-CLEVR，可以将VQA领域泛化问题分解为四个因素：视觉复杂度、问题冗余度、概念分布和概念组合性，从而可以独立地研究每个因素对领域泛化的影响。本文还研究了四种现有方法和一种新方法，发现将推理和感知分离，并结合概率不确定性的方法可以更好地解决领域泛化问题。
- (3):本文提出了一个虚拟基准测试集Super-CLEVR，可以将VQA领域泛化问题分解为四个因素，从而可以独立地研究每个因素对领域泛化的影响。本文还提出了一种新方法P-NSVQA，将概率不确定性引入到符号执行器中，从而可以更好地解决领域泛化问题。
- (4):本文在Super-CLEVR数据集上测试了五种方法的性能，发现P-NSVQA在三个因素上的表现优于其他方法。这表明将推理和感知分离，并结合概率不确定性的方法可以更好地解决领域泛化问题。
#### 7. 方法详细介绍：
本文提出了几种神经符号方法用于视觉推理，包括NSVQA和P-NSVQA。NSVQA由场景解析器、问题解析器和程序执行器组成，而P-NSVQA是NSVQA的扩展，它结合了概率推理。模型在虚拟基准数据集Super-CLEVR上进行训练，并在域内和域外进行评估。具体步骤如下：
1. NSVQA模型：首先使用Mask RCNN训练对象解析器，然后训练属性提取模型。模型由场景解析器、问题解析器和程序执行器组成。场景解析器将图像解析为对象和属性，问题解析器将问题解析为程序，程序执行器执行程序并输出答案。
2. P-NSVQA模型：在NSVQA的基础上，引入了概率推理，以处理不确定性。具体来说，使用贝叶斯公式计算后验概率，将其作为程序执行器的输入，从而得到更准确的答案。

#### 8. 实验设置：
本文使用虚拟基准数据集Super-CLEVR进行实验，该数据集包含30k张图像，包括20k张用于训练、5k张用于验证和5k张用于测试。每张图像都与10个基于对象和10个基于部分的问题配对。数据集有不同的变体，包括视觉复杂度、问题冗余度、概念分布和概念组合性等不同的因素。模型使用默认设置进行训练，并根据验证准确率进行早期停止。模型在三次不同的随机种子下进行重复实验，以显示统计显著性。 

#### 9. 实验结果和分析：
本文使用Super-CLEVR基准数据集诊断了视觉推理中的域鲁棒性。研究了四种现有方法，包括FiLM、mDETR、NSCL和NSVQA，以及一种新方法P-NSVQA，它将NSVQA扩展为具有不确定性推理。实验结果表明，所有现有方法都对域变化敏感。具体来说，神经模块化方法的逐步设计增强了它们对问题冗余度变化的鲁棒性，而非模块化方法对视觉复杂度更具鲁棒性。此外，由于其分解的推理和感知，NSVQA对概念分布变化更具鲁棒性。P-NSVQA表现最佳，优于其他方法在三个域变化因素上。作者建议将推理与视觉和语言理解分离，结合概率不确定性，形成一个强大的模型，具有域鲁棒性。


# Paper:754     DiffSwap：基于3D感知掩蔽扩散的高保真度和可控性人脸交换



#### 1. Title: 
DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion

#### 2. Authors: 
Wenliang Zhao, Yongming Rao, Weikang Shi, Zuyan Liu, Jie Zhou, Jiwen Lu

#### 3. Affiliation: 
第一作者：清华大学

#### 4. Keywords: 
Face swapping, diffusion model, conditional inpainting, 3D-aware masked diffusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是高保真度和可控性的人脸交换技术，旨在将源脸的身份信息转移到目标图像或视频帧中，同时保持属性不变。

- (2):现有的人脸交换方法可以大致分为基于3D的方法和基于GAN的方法。前者通常利用3DMM来引入结构先验，但这些方法需要人工干预或会产生可识别的伪影。后者通常从源图像中提取身份特征并将其注入到生成模型中，但这些方法往往只对目标图像进行微小修改，无法处理源脸和目标脸之间形状差异较大的情况。本文提出了一种基于扩散模型的人脸交换框架，通过条件修复任务来实现人脸交换，该任务由一个强大的扩散模型引导，该模型受到所需的面部属性（例如身份和地标）的指导。

- (3):本文提出了一种中点估计方法，可以在只进行2步的情况下高效地生成交换后的面孔，从而在训练过程中引入身份约束以提高人脸交换质量。本文的框架具有以下几个优点：1）可控性。我们的方法基于潜空间上的条件掩蔽扩散，其中掩蔽和条件可以完全控制和自定义。2）高保真度。条件修复的公式可以充分利用扩散模型的生成能力，并且可以在最小的伪影下保留目标图像的背景。3）形状保持。我们的方法的可控性使我们能够在生成过程中使用3D感知地标作为条件，以保持源脸的形状一致性。 

- (4):本文在FF ++和FFHQ数据集上进行了广泛的实验，证明了我们的方法在定性和定量上都可以实现最先进的人脸交换结果。在FF ++数据集上，我们的方法在ID检索（98.54％）和FID（2.16）方面优于以前的方法，同时在姿势误差和表情误差方面实现了可比较的结果。定性结果表明，我们的方法可以生成高保真度的交换面孔，可以更好地保留源脸的形状。此外，我们还展示了我们的方法的可扩展性和可控性。我们的模型可以轻松扩展到更高分辨率，例如512×512，具有可承受的额外计算成本，并允许通过控制修复掩蔽来进行区域可定制的人脸交换。
#### 7. 方法详细介绍：
本文提出了一种名为DiffSwap的高保真度和可控制的人脸交换方法。该方法使用编码器将目标图像嵌入潜空间，然后构建一个掩膜来指定进行人脸交换的区域。接下来，通过掩膜扩散进行条件修复。该框架具有高度的可控性，因为在推理过程中，掩膜和条件输入都可以更改。该方法还涉及使用3D人脸重建库来提取源脸和目标脸的3D信息，以获得与目标脸共享姿势和表情并保留源脸形状的3D感知面部标记。

具体步骤如下：
1. 使用编码器将目标图像嵌入潜空间。
2. 构建掩膜来指定进行人脸交换的区域。
3. 通过掩膜扩散进行条件修复。
4. 在推理过程中，掩膜和条件输入都可以更改。
5. 使用3D人脸重建库提取源脸和目标脸的3D信息，以获得与目标脸共享姿势和表情并保留源脸形状的3D感知面部标记。

#### 8. 实验设置：
本文在两个数据集FaceForensics++和FFHQ上进行了评估。实验在单个NVIDIA Tesla V100 GPU上进行，内存为32GB。输入图像被调整为256×256分辨率。模型训练了200k次迭代，批量大小为8。学习率设置为0.0002，采用线性衰减策略。

#### 9. 实验结果和分析：
本文提出的方法在FF++数据集上的ID检索（98.54％）和FID（2.16）方面优于先前的方法，同时在姿势误差和表情误差方面实现了可比较的结果。定性结果表明，与先前的方法相比，该方法可以生成更好地保留源脸形状的高保真度交换脸。该方法具有可扩展性和可控性，可以通过控制修复掩膜来实现区域可定制的人脸交换。


# Paper:755     基于令牌梯度正则化的视觉Transformer可迁移对抗攻击



#### 1. Title: 
Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization

#### 2. Authors: 
Jianping Zhang, Yizhan Huang, Weibin Wu, Michael R. Lyu

#### 3. Affiliation: 
第一作者：香港中文大学计算机科学与工程系

#### 4. Keywords: 
Adversarial attacks, Vision transformers, Transfer-based attacks, Gradient regularization, Token gradient

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Transferable_Adversarial_Attacks_on_Vision_Transformers_With_Token_Gradient_Regularization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了视觉Transformer（ViT）的可迁移对抗攻击问题，旨在提高ViT在安全敏感场景下的安全性。

- (2):现有的对抗攻击方法主要集中在输入梯度的正则化上，但是ViT中间块的反向传播梯度方差仍然很大，可能会使生成的对抗样本集中在某些模型特定的特征上并陷入局部最优解。为了克服现有方法的缺点，本文提出了Token Gradient Regularization（TGR）方法，该方法根据ViT的结构特点，以令牌为单位逐块减少反向传播梯度的方差，并利用正则化的梯度生成对抗样本。实验结果表明，相比于现有的迁移攻击方法，TGR平均提高了8.8%的性能。

- (3):本文提出了一种基于ViT结构特点的Token Gradient Regularization（TGR）方法，用于生成可迁移的对抗样本。TGR在每个ViT内部块中以令牌为单位减少反向传播梯度的方差，并利用正则化的梯度生成对抗样本。实验结果表明，TGR方法相比于现有的迁移攻击方法，平均提高了8.8%的性能。

- (4):本文在ImageNet数据集上进行了广泛的实验，验证了所提出方法的有效性。实验结果表明，TGR方法在攻击ViT模型和CNN模型时，相比于现有的迁移攻击方法，平均提高了8.8%和6.2%的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Token Gradient Regularization (TGR)的方法，用于对Vision Transformers (ViTs)进行可转移的对抗攻击。TGR通过逐个token地减少每个内部块中反向传播的梯度方差，利用规范化的梯度生成对抗样本。该方法基于ViTs的架构，将token表示视为基本特征单元。通过逐个token地消除反向传播梯度中的极端值，直到获得规范化的输入梯度，然后使用它们来更新对抗样本。

具体步骤如下：
1. 对于每个内部块，将其Attention和QKV组件以及MLP组件分别进行规范化。
2. 对于每个组件，计算其token-wise的梯度，并将梯度的极端值进行截断。
3. 将规范化的梯度用于生成对抗样本。

#### 8. 实验设置：
本文在ImageNet数据集上进行实验，随机从ILSVRC 2012验证集中选择1000张不同类别的图像。实验旨在评估所提出的方法在不同实验设置下对各种模型的攻击效果。

#### 9. 实验结果与分析：
本文在各种模型上进行了实验，包括ViTs、CNNs和经过对抗训练的CNNs。实验结果表明，所提出的方法在攻击准确率方面优于所有基准方法，展示了所提出方法的高可转移性。文中还对所提出方法的有效性进行了分析，包括中间块梯度的规范化对性能的影响以及每个组件对可转移性的贡献。消融实验表明，性能提升是由于规范化的token，它减少了网络内部的梯度方差。


# Paper:756     DyNCA: 使用神经元自动机实现实时动态纹理合成



#### 1. Title: 
DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata

#### 2. Authors: 
Ehsan Pajouheshgar, Yitao Xu, Tong Zhang, Sabine S¨usstrunk

#### 3. Affiliation: 
瑞士洛桑联邦理工学院计算机与通信科学学院

#### 4. Keywords: 
Dynamic texture synthesis, Neural Cellular Automata, Real-time, Video editing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Pajouheshgar_DyNCA_Real-Time_Dynamic_Texture_Synthesis_Using_Neural_Cellular_Automata_CVPR_2021_paper.html  Github: https://github.com/DyNCA/DyNCA

#### 6. Summary : 
- (1):本文研究的是动态纹理合成，旨在实现实时、可控的动态纹理合成。
- (2):过去的方法需要缓慢的迭代优化过程才能合成单个固定大小的短视频，并且没有任何后期控制。本文提出了一种基于神经元自动机的动态纹理合成框架，可以实时合成任意大小的无限长的真实视频纹理，并提供了几个实时视频控制，包括运动速度、运动方向和编辑画笔工具。本文的贡献包括：（1）DyNCA模型可以实时合成动态纹理视频；（2）合成视频的质量和真实感优于现有结果；（3）DyNCA模型可以适应不同高度和宽度的种子，并合成任意长度的视频；（4）DyNCA框架可以实现动态风格转移，可以从不同的源中学习外观和运动。
- (3):本文提出了一种基于神经元自动机的动态纹理合成框架，称为DyNCA。DyNCA从一个称为种子的常数初始状态开始，然后根据其可训练的PDE更新规则迭代地演变状态，生成图像序列。然后，将该图像序列与外观样本和运动目标进行比较，以计算DyNCA优化的损失函数。我们允许用户通过运动矢量场或示例动态纹理视频指定所需的运动。此外，通过使用不同的外观和运动目标，我们的模型可以执行动态风格转移。
- (4):本文的方法在动态纹理合成任务上取得了很好的性能，可以实时合成任意大小的无限长的真实视频纹理，并提供了几个实时视频控制，包括运动速度、运动方向和编辑画笔工具。在定量和定性分析中，我们证明了我们合成的视频比文献中的现有结果更具真实感和质量。
#### 7. 方法详细介绍：
本文提出了一种名为DyNCA的方法，用于实时动态纹理合成。该方法结合了神经网络和偏微分方程（PDE），并且比现有模型具有2-4个数量级更少的参数。DyNCA模型通过引入多尺度感知和位置编码来修改神经元细胞自动机（NCA）模型，以增加细胞之间的通信范围并使细胞能够感知全局信息。DyNCA模型接收来自预训练的光流网络的监督，从而使其能够合成具有结构化运动的视频纹理。训练过程涉及最小化基于最优传输的样式匹配损失，该损失在VGG16网络中提取的深度特征之间进行，并引导DyNCA合成具有类似于目标向量场的光流的视频。

DyNCA模型包括两个阶段：感知和生成。在感知阶段，该方法从输入图像中提取深度特征和运动信息。在生成阶段，该方法更新细胞自动机状态以合成动态纹理，以匹配目标运动和外观。

#### 8. 实验设置：
本文进行了一系列实验，以评估DyNCA在合成动态纹理和实时视频编辑方面的性能。实验变化了DyNCA的关键参数，包括种子状态的大小和MLP第一层的输出维度。本文还比较了DyNCA的计算成本与以前的最先进模型在性能和参数数量方面的差异。

#### 9. 实验结果和分析：
本文展示了使用DyNCA合成具有来自向量场和视频的目标运动的动态纹理的实验结果。本文还演示了DyNCA的实时视频编辑功能，包括方向控制、速度控制、画笔工具和局部坐标变换。本文进行了用户研究，以评估合成视频的逼真程度，并将DyNCA的性能与以前的最先进模型进行了比较。本文提供了每个实验的详细训练和合成时间，以及参数数量。DyNCA模型在逼真度方面优于其他动态纹理合成方法，并且合成时间比SOTA方法快2-4个数量级，具有更少的可训练参数，便于实际部署。


# Paper:757     FlexiViT：适用于所有Patch尺寸的单一模型



#### 1. Title: 
FlexiViT: One Model for All Patch Sizes

#### 2. Authors: 
Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic

#### 3. Affiliation: 
Lucas Beyer: Google Research（谷歌研究院） 

#### 4. Keywords: 
Vision Transformers, patch size, FlexiViT, patch embedding, sequence length

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2022_paper.html  Github: https://github.com/google-research/big_vision

#### 6. Summary : 
- (1):本文研究的背景是Vision Transformers（ViTs）中的patch size对模型性能和计算速度的影响。

- (2):过去的方法通常固定patch size，但这会导致模型在不同的patch size下表现不佳。本文提出了一种新的方法，即在训练时随机使用不同的patch size，从而得到一个适用于不同patch size的模型。这种方法被称为FlexiViT，可以在不改变模型参数的情况下，根据计算资源的不同，灵活地调整模型的patch size。本文的方法得到了很好的实验结果，表明其在多个任务上的性能都优于或与固定patch size的ViT模型相当。

- (3):本文提出的方法是在训练时随机使用不同的patch size，从而得到一个适用于不同patch size的模型。在这个过程中，patch embedding weights被自适应地调整以适应不同的patch size，而模型权重则被共享。此外，本文还提出了一种优化的调整操作和基于知识蒸馏的训练过程，以进一步提高模型的性能。

- (4):本文的方法在多个任务上进行了实验，包括图像分类、图像-文本检索、开放世界检测、全景分割和语义分割等。实验结果表明，FlexiViT通常可以匹配或超过在相同设置下训练的固定patch size的ViT模型的性能。因此，FlexiViT训练是ViT的一个简单的改进，可以轻松地为大多数依赖于ViT骨干架构的模型添加计算自适应能力。
#### 7. 方法详细介绍：
本文提出了一种名为FlexiViT的新模型，可以处理不同大小的输入补丁而不会影响性能。该模型基于ViT架构，使用随机补丁大小进行训练。模型使用强大的ViT教师的权重进行初始化，并使用FunMatch方法来最小化教师和学生FlexiViT之间的KL散度。使用小批量中心核对齐（CKA）和t-SNE可视化来分析模型的内部表示。

具体步骤如下：
1. 随机选择补丁大小进行训练。
2. 根据补丁大小自适应调整位置嵌入和补丁嵌入参数的大小。
3. 使用FunMatch方法进行训练，最小化教师和学生之间的KL散度。
4. 使用小批量中心核对齐（CKA）和t-SNE可视化来分析模型的内部表示。

#### 8. 实验设置：
本文使用FlexiViT-B模型进行大部分实验，该模型是从强大的ViT-B/8模型初始化和蒸馏而来。使用FunMatch方法进行训练，使用随机补丁大小和KL散度损失。训练数据分布包括随机翻转、裁剪和mixup。在各种计算机视觉任务上进行评估，包括分类、锁定图像调整、开放词汇检测、全景分割和语义分割。

#### 9. 实验结果和分析：
本文表明，FlexiViT在转移到其他任务时与单个固定补丁大小的ViT模型相当或更好地表现。当使用相同的蒸馏设置作为FlexiViT时，模型的性能与ViT-B/16和ViT-B/30模型相似或更好。本文还表明，FlexiViT可以在小序列长度下进行微调，并在测试时使用更长的序列以实现更高的性能。使用小批量中心核对齐（CKA）和t-SNE可视化来分析模型的内部表示，显示从第一层到块6的MLP子层的网格大小的特征映射表示相似，CLS令牌表示保持对齐。本文还比较了控制性能-计算权衡的ViT模型的替代架构方法，表明FlexiViT优于这些方法。


# Paper:758     基于切向椭圆高斯置信传播的事件相机增量式光流估计



#### 1. Title: 
Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation

#### 2. Authors: 
Jun Nagata and Yusuke Sekikawa

#### 3. Affiliation: 
DENSO IT LAB., INC.（日本电装IT实验室）

#### 4. Keywords: 
Optical flow estimation, event-based camera, incremental updates, belief propagation, tangentially elongated Gaussian

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_CVPR_2021_paper.html  Github: https://github.com/DensoITLab/tegbp/

#### 6. Summary : 
- (1):本文研究的是计算机视觉中的光流估计问题，提出了一种基于事件相机的增量式光流估计方法，旨在实现低延迟的光流估计。

- (2):现有的光流估计方法大多基于稠密视频帧，需要对所有像素进行计算，计算量大，难以实现低延迟的光流估计。而基于事件相机的光流估计方法可以利用其稀疏的数据采集机制实现低延迟的光流估计，但现有的方法只能计算法向光流，无法得到完整的光流。本文提出了一种基于切向椭圆高斯分布的置信传播方法，可以实现增量式的完整光流估计。

- (3):本文提出的方法将完整光流的概率分布建模为法向光流的切向椭圆高斯分布的联合分布，利用置信传播算法实现增量式的完整光流估计。该方法可以利用稀疏的法向光流测量实现高效的增量式更新，同时可以通过正确的先验边缘化得到完整的光流。实验结果表明，该方法在真实数据集上的表现优于现有的增量式法向光流方法。

- (4):本文提出的方法在真实数据集上的表现优于现有的增量式法向光流方法，可以实现低延迟的完整光流估计。
#### 1. 实验结果
本文提出了一种新的增量全流估计算法，称为切向延伸高斯信念传播（TEGBP），用于异步事件数据。该算法基于概率模型，当提供有效的先验时，可以保证与全流相等。在实际的真实世界数据集上展示了所提出方法的有效性，包括MVSEC和DSEC，表明相对于现有的增量算法，所提出的TEGBP算法在准确性方面有了显著提升。给出了定量和定性结果，并表明所提出的TEGBP算法相对于现有的ARMS算法显著提高了准确性。此外，本文还讨论了未来的工作，包括探索更强大的正常流估计、利用更好的先验进行特定应用、以及优化实现以实现边缘设备上的低能耗、低延迟光流估计。

#### 2. 方法详细介绍
本文提出了一种名为切向延伸高斯信念传播（TEGBP）的增量全流估计算法，使用局部正常流的稀疏观测。该算法基于切向延伸高斯分布，将全流的不确定性从单个正常流测量中建模。算法利用有效的先验来获得正确的解，并利用信念传播从正常流和先验的联合分布计算全流的边缘分布。该算法旨在在CPU或新兴的数据流处理器上高效运行。

具体步骤如下：
1. 将稀疏的正常流观测建模为切向延伸高斯分布。
2. 将全流的概率建模为切向延伸高斯分布的联合分布。
3. 利用信念传播算法进行边缘化，从而得到全流的边缘分布。
4. 采用粗到细的信息传递方案加速收敛。

#### 3. 实验设置
本文在来自航空无人机和汽车的真实世界数据集上进行了实验。数据集包括EuRoC数据集、KITTI数据集和Oxford RobotCar数据集。实验在一台装有Intel Core i7-8700K CPU和NVIDIA GeForce GTX 1080 Ti GPU的PC上进行。代码可在https://github.com/DensoITLab/tegbp/上获得。

#### 4. 实验结果与分析
本文在MVSEC数据集上评估了算法的性能。结果表明，相对于现有的ARMS算法，所提出的算法在平均端点误差（AEE）和异常点比例方面均表现更好。定量结果总结在表2中。本文还在一个场景上进行了实验，其中运动和边缘不垂直于图像边缘。算法正确估计了流而没有漂移，而现有的ARMS算法由于平均操作而出现漂移。


# Paper:759     通过选择性查询回收增强基于查询的目标检测训练



#### 1. Title: 
Enhanced Training of Query-Based Object Detection via Selective Query Recollection

#### 2. Authors: 
Fangyi Chen, Han Zhang, Kai Hu, Yu-Kai Huang, Chenchen Zhu, Marios Savvides

#### 3. Affiliation: 
Carnegie Mellon University

#### 4. Keywords: 
Object detection, query-based, selective query recollection, training strategy, decoder

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2021_paper.html
Github: https://github.com/Fangyi-Chen/SQR

#### 6. Summary: 
- (1):本文研究了基于查询的目标检测器在最后的解码阶段错误预测的现象，而在中间阶段则正确预测的情况。 
- (2):以往的方法主要是基于密集先验的目标检测器，而这些方法无法摆脱许多手工处理步骤，如锚点生成或非最大值抑制，这些步骤阻碍了端到端的优化。最近的研究关注于基于查询的目标检测，这些方法将目标检测视为一种集合预测问题，通过一组可学习的嵌入向量（即查询）表示对象，然后通过一个由多个解码器阶段组成的解码器对查询进行逐步的精炼。然而，我们发现在训练过程中，每个阶段的责任是不平衡的，而监督应用于它们是类似的。早期的阶段可能会犯错误而不会造成太大的影响，因为它们有机会在后来得到纠正，而后期阶段对最终预测更负责。但在训练过程中，所有这些阶段都以等效的方式接受监督，并且缺乏将特定的训练重点放在后期阶段的机制。此外，由于解码器的顺序结构，中间查询的精炼将被级联到后续阶段，而精炼之前的查询永远没有机会向前传播，即使它们没有受到影响并且可能比精炼后的查询更具代表性。这些级联错误增加了收敛的难度，而顺序结构则阻碍了后期阶段在训练过程中看到先前查询的机会。 
- (3):本文提出了一种名为选择性查询回收（SQR）的简单有效的训练策略，它可以累积收集中间查询，随着解码阶段的加深，选择性地将查询转发到下游阶段，而不是按顺序结构。这样，SQR将训练重点放在后期阶段，并允许后期阶段直接使用来自早期阶段的中间查询。SQR可以轻松地插入各种基于查询的目标检测器，并显著提高它们的性能，同时保持推理管道不变。 
- (4):在COCO数据集上的实验结果表明，SQR可以在不同的设置（骨干网络、查询数量、时间表）下显著提高Adamixer、DAB-DETR和Deformable-DETR的性能，平均提高1.4-2.8个AP。
#### 7. 方法详细介绍：
本文提出了一种名为选择性查询回收（Selective Query Recollection，SQR）的训练策略，用于查询式目标检测器。SQR通过累积收集中间查询，随着解码阶段的加深，选择性地将查询转发到下游阶段，使后期阶段的训练重点放在前期阶段的中间查询上，从而使后期阶段能够直接使用前期阶段的中间查询。SQR可以轻松地插入各种查询式目标检测器，并显著提高它们的性能，同时不改变推理流程。

#### 8. 实验设置：
本文在Adamixer、DAB-DETR和Deformable-DETR上进行了实验，验证了SQR的有效性。实验设置包括不同的backbone、查询数量和训练计划。

#### 9. 实验结果和分析：
本文的实验结果表明，SQR在各种网络上都能够稳定地提高1.4~2.8个AP。在MS-COCO val集上，Adamixer、DAB-DETR和Deformable-DETR模型的推理速度和AP如图1所示。本文还提供了阶段预测的可视化结果，以研究查询式目标检测器在最后一个解码阶段错误预测的现象（图2）。


# Paper:760     重新思考视频ViTs：用于联合图像和视频学习的稀疏视频管道



#### 1. Title: 
Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning

#### 2. Authors: 
AJ Piergiovanni, Weicheng Kuo, Anelia Angelova

#### 3. Affiliation: 
Google Research（谷歌研究院）

#### 4. Keywords: 
Visual Transformers, Video Understanding, Sparse Video Tubes, Joint Image and Video Learning, ViT Encoder

#### 5. Paper: https://arxiv.org/abs/2106.00476  Github: None

#### 6. Summary : 
- (1):本文研究视频理解的问题，提出了一种新的方法，可以将ViT编码器转换为高效的视频模型，可以无缝地处理图像和视频输入。 
- (2):以往的方法将图像和视频视为完全不同的输入，提供了独立的方法来处理视频或图像，而本文提出的方法可以更好地利用两种数据源进行训练和微调。本文的方法可以通过稀疏采样来处理原始视频信号和图像信号，从而更好地处理动作和其他时空信息。 
- (3):本文提出了一种名为Sparse Video Tubes的轻量级方法，用于联合图像和视频学习。该方法通过从视频中稀疏采样各种大小的三维时空管道来生成可学习的标记，这些标记由视觉变换器使用。使用Sparse Video Tubes，模型可以轻松适用于任何输入，并且可以更好地利用任何或两种数据源进行训练和微调。 
- (4):本文的方法在多个标准视频数据集上进行了评估，包括Kinetics-400、Kinetics-600、Kinetics-700和SomethingSomething V2，优于现有的最先进方法。本文的方法还可以适应大型预训练ViTs，而无需进行完全微调。本文的方法可以更好地共享图像和视频的学习权重，同时减少FLOPs并提高性能。
#### 7. 方法详细介绍：
本文提出的方法名为Sparse Video Tubes for Joint Image and Video Learning。该方法使用标准的ViT令牌化方法处理图像，即使用16x16的卷积核进行2D卷积。对于视频，该方法使用不同形状的稀疏视频管，以不同的视角稀疏采样视频。管可以具有任何形状，实验探索了这些形状的影响。位置嵌入使用固定的正弦/余弦嵌入实现，考虑到每个管的步幅、卷积核形状和偏移量。在令牌化步骤之后，所有令牌都被连接在一起，并通过标准的Transformer模型传递。

#### 8. 实验设置：
本文的实验在多个数据集上进行，包括Kinetics-400、Kinetics-600和Something-Something V2。模型使用标准的交叉熵损失函数进行训练，并使用Adam优化器进行优化。学习率设置为1e-4，模型训练100个epoch。批量大小设置为32，输入大小为224x224。使用top-1和top-5准确度评估性能。

#### 9. 实验结果和分析：
本文的方法在多个数据集上进行了评估，包括Kinetics-400、Kinetics-600和Something-Something V2。实验结果表明，该方法在准确度和效率方面优于先前的最先进方法。在Kinetics-600数据集上，TubeViT-L模型的Top 1准确度为91.5%，Top 5准确度为98.7%。在Kinetics-400数据集上，TubeViT-H模型的Top 1准确度为90.9%，Top 5准确度为98.9%。本文还进行了消融实验，确定了方法中不同组件的有效性。实验表明，位置嵌入的选择、使用的管的数量以及网络的深度对于实现高性能至关重要。实验还表明，令牌数量过多可能对性能产生不利影响。


# Paper:761     MixTeacher: 混合尺度教师挖掘半监督目标检测中的有前途标签



#### 1. Title: 
MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection

#### 2. Authors: 
Liang Liu, Boshen Zhang, Jiangning Zhang, Wuhao Zhang, Zhenye Gan, Guanzhong Tian, Wenbing Zhu, Yabiao Wang, Chengjie Wang

#### 3. Affiliation: 
第一作者单位：腾讯 YouTu 实验室

#### 4. Keywords: 
Semi-supervised object detection, scale variation, mixed scale teacher, pseudo label mining

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_MixTeacher_Mining_Promising_Labels_With_Mixed_Scale_Teacher_for_Semi-Supervised_CVPR_2021_paper.html  Github: https://github.com/lliuz/MixTeacher

#### 6. Summary : 
- (1):本文研究半监督目标检测中的尺度变化问题，提出了一种混合尺度教师模型，用于生成高质量的伪标签，以及一种基于分数提升的伪标签挖掘方法，以解决尺度变化问题。
 
- (2):现有的半监督目标检测方法主要依赖于严格的条件来过滤高质量的伪标签，但是我们观察到尺度极端的物体往往具有低置信度，导致这些物体缺乏正面监督。本文提出了一种混合尺度教师模型，用于改善伪标签生成和尺度不变学习，并提出了一种基于分数提升的伪标签挖掘方法，从而更好地处理尺度变化问题。 
 
- (3):本文提出了一种半监督目标检测框架MixTeacher，其中高质量的伪标签是从混合尺度特征金字塔中生成的。同时，本文提出了一种基于分数提升的伪标签挖掘方法，该方法从低置信度的预测中挖掘有前途的伪标签。 
 
- (4):在MS COCO和PASCAL VOC基准测试中，本文的方法在各种半监督设置下均取得了新的最先进性能。
#### 7. 方法详细介绍：
本文提出了一种半监督目标检测框架MixTeacher，该框架通过引入混合尺度特征金字塔来生成高质量的伪标签。该方法采用混合尺度教师来改进伪标签生成和尺度不变学习。此外，该方法提出了使用跨尺度预测的分数提升来挖掘伪标签，从而受益于混合尺度特征的更好预测。

具体步骤如下：
1. 提取输入图像的两个特征金字塔，分别为常规视图和下采样视图。
2. 从上述两个视图中构建混合尺度特征金字塔，以自适应融合适当尺度的特征。
3. 使用混合尺度特征金字塔为未标记图像生成伪标签。
4. 采用教师-学生范式进行训练。

#### 8. 实验设置：
本文在MS COCO和PASCAL VOC基准测试集上进行了广泛的实验，评估了MixTeacher框架在各种半监督设置下的性能，并与现有的最先进方法进行了比较。

#### 9. 实验结果和分析：
本文提出的MixTeacher框架在各种半监督设置下均取得了MS COCO和Pascal VOC基准测试集上的最佳性能。即使已经有足够的标记数据，该方法仍然能够保持最佳性能。消融实验表明了每个组件的有效性，与其他多视图方法的比较表明，与以前的一致性学习方法相比，该方法具有超过1.3 mAP的显着优势。不同尺度测试的性能表明，自适应选择尺度的混合尺度方法在所有对象上都具有竞争力，并且更适合生成伪标签。分析了超参数的选择，并研究了一些关键超参数的敏感性。定性可视化直观地展示了伪标签的质量。


# Paper:762     基于流边引导的视频修复



#### 1. Title: 
Flow-edge Guided Video Completion

#### 2. Authors: 
Xiaoxiao Li, Jia-Bin Huang, Narendra Ahuja

#### 3. Affiliation: 
Xiaoxiao Li: University of Illinois at Urbana-Champaign (伊利诺伊大学香槟分校)

#### 4. Keywords: 
Video completion, flow-edge guidance, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Flow-Edge_Guided_Video_Completion_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):视频修复是计算机视觉领域的一个重要问题，它可以用于视频编辑、视频增强等多个方面。但是，现有的视频修复方法在处理复杂场景时存在一些问题，例如无法处理复杂的运动模式、无法处理遮挡等。

- (2):过去的视频修复方法通常使用基于插值的方法，这些方法无法处理复杂的运动模式和遮挡。近年来，基于深度学习的方法已经被广泛应用于视频修复领域，但是这些方法通常需要大量的训练数据，并且无法处理复杂的运动模式和遮挡。

- (3):本文提出了一种基于流边引导的视频修复方法，该方法使用光流和边缘信息来指导视频修复过程。具体来说，该方法首先使用光流来估计缺失区域的运动信息，然后使用边缘信息来指导修复过程。实验结果表明，该方法在处理复杂场景时具有很好的效果，并且可以处理复杂的运动模式和遮挡。

- (4):本文提出的方法在多个数据集上进行了测试，并且与现有的视频修复方法进行了比较。实验结果表明，该方法在处理复杂场景时具有很好的效果，并且可以处理复杂的运动模式和遮挡。
#### 7. 方法详细介绍：
本文提出了一种流边引导视频修复方法，利用流边引导模块来指导修复过程。该方法包括三个主要阶段：（1）流边引导模块，（2）流修复模块和（3）时间一致性优化模块。流边引导模块生成流边图，通过突出显示具有可靠流信息的区域来指导修复过程。流修复模块然后使用流边图来完成视频帧中的缺失区域。最后，时间一致性优化模块对完成的帧进行优化，以确保视频序列的时间一致性。

#### 8. 实验设置：
当前文本中没有与实验设置相关的信息。

#### 9. 实验结果和分析：
当前文本中没有与实验结果和分析相关的信息。


# Paper:763     从节点交互到跳跃交互：一种新的有效和可扩展的图学习范式



#### 1. Title: 
From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm

#### 2. Authors: 
Jie Chen, Zilong Li, Yin Zhu, Junping Zhang, Jian Pu

#### 3. Affiliation: 
第一作者：上海智能信息处理重点实验室，复旦大学计算机科学学院，中国上海200433

#### 4. Keywords: 
Graph Neural Networks, Hop Interaction, Scalability, Over-smoothing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_From_Node_Interaction_to_Hop_Interaction_New_Effective_and_Scalable_CVPR_2021_paper.html  Github: https://github.com/JC-202/HopGNN

#### 6. Summary : 
- (1):本文研究背景是图神经网络（GNNs）在大规模工业应用中的可扩展性和过度平滑问题。

- (2):过去的方法包括节点交互GNNs和基于采样的GNNs，但它们都存在可扩展性和过度平滑问题。本文提出了一种新的跳跃交互范式，将节点交互转化为预处理的多跳特征内部节点之间的非线性交互，以同时解决这两个问题。

- (3):本文提出了一种名为HopGNN的框架，它可以灵活地利用现有的GNNs实现跳跃交互。HopGNN首先预处理多跳表示，然后在每个节点内部的多跳特征图上使用GNNs进行非线性交互。此外，本文提出了一种自监督学习策略，以增强HopGNN的性能。

- (4):HopGNN在12个基准数据集上进行了广泛的实验，结果表明，它在保持高可扩展性和效率的同时，实现了优越的性能。
#### 7. 方法详细介绍：
本文提出了一种新的图学习范式——Hop Interaction，旨在解决现有图神经网络（GNN）的可扩展性和过度平滑的限制。Hop Interaction的核心思想是将节点之间的交互目标转换为每个节点内部预处理的多跳特征。作者设计了一个简单而有效的HopGNN框架，可以轻松地利用现有的GNN来实现跳交互。HopGNN首先根据图结构预计算多跳表示，然后在每个节点内部的多跳特征图上利用GNN实现跳交互，从而实现灵活而明确的跳交互。作者实现了一个基于注意力机制的交互层和平均池化层，用于融合多跳特征并生成最终预测。此外，作者提供了一种多任务学习策略，将自监督目标与HopGNN相结合以增强性能。

#### 8. 实验设置：
作者在各个领域、规模和平滑度的12个基准数据集上进行了广泛的实验，包括Cora、Citeseer、Pubmed、Amazon Computers、Amazon Photo、Coauthor CS、Coauthor Physics、Wiki CS、Wiki Social、Reddit、PPI和Yelp。作者将提出的HopGNN与各种最先进的GNN进行了比较，包括GCN、GAT、GraphSAGE、APPNP和GIN。作者使用了与以前的工作相同的实验设置，包括10倍交叉验证和随机种子初始化。

#### 9. 实验结果和分析：
实验结果表明，提出的HopGNN在所有12个基准数据集上均取得了最先进的性能，同时保持了高可扩展性和高效性。具体来说，在Cora、Citeseer和Pubmed数据集上，HopGNN的性能优于所有其他方法。在Amazon Computers、Amazon Photo、Coauthor CS、Coauthor Physics、Wiki CS、Wiki Social、Reddit、PPI和Yelp数据集上，HopGNN的性能与其他方法相当或更好。作者还进行了消融研究，以验证提出的跳交互范式和多任务学习策略的有效性。作者表明，跳交互范式可以显著提高GNN的性能，同时降低计算成本。作者还表明，多任务学习策略可以进一步增强HopGNN的性能。


# Paper:764     基于姿态引导扩散模型的一致视角合成



#### 1. Title: 
Consistent View Synthesis with Pose-Guided Diffusion Models

#### 2. Authors: 
Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, Johannes Kopf

#### 3. Affiliation: 
Meta工作室

#### 4. Keywords: 
View Synthesis, Diffusion Models, Epipolar Attention, Consistency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2021_paper.html  Github: https://github.com/poseguided-diffusion/poseguided-diffusion

#### 6. Summary : 
- (1):本文研究的是从单张图像生成一致的长期视角序列的问题，这是许多虚拟现实应用程序的基础问题，但现有技术只能在有限的相机运动范围内合成新视角，或者在相机运动较大时无法生成一致且高质量的新视角。
- (2):现有的方法主要分为显式和隐式两种，显式方法使用“变形和细化”策略，但其成功取决于单眼深度估计的准确性，而隐式方法则不依赖于单眼深度估计，但难以产生一致的结果。本文提出了一种基于扩散模型的姿态引导方法，通过设计一个注意力层来使用极线作为约束来促进不同视点之间的关联，从而生成一致的长期视角序列。
- (3):本文提出了一种基于扩散模型的姿态引导方法，通过设计一个注意力层来使用极线作为约束来促进不同视点之间的关联，从而生成一致的长期视角序列。具体地，我们在扩散模型的UNet网络中设计了极线注意力层，根据相机姿态信息，为输出视图特征图上的每个像素估计输入视图特征图上的极线，然后使用这些极线作为约束来计算输入和输出视图之间的注意力权重。我们在Realestate10K和Matterport3D数据集上进行了广泛的定量和定性研究，证明了所提出的方法在生成具有现实感的新内容和一致的视角序列方面的有效性。
- (4):本文提出的方法在Realestate10K和Matterport3D数据集上进行了广泛的定量和定性研究，证明了所提出的方法在生成具有现实感的新内容和一致的视角序列方面的有效性。
#### 7. 方法详细介绍：
本文提出了一种姿态引导扩散模型，用于从单个图像生成一致的长期视角。该模型使用扩散模型将经验分布转换为目标数据分布，通过一系列去噪操作实现。该模型使用源视图编码器从源视图图像中提取特征图，并使用提出的极线注意力层将它们组合起来。UNet模型预测每个视角的去噪图像，以生成最终的序列。该模型使用极线作为约束，将目标视图与源视图关联起来，以减少特定目标视图像素对应的候选源视图像素数量。本文还提出了随机条件策略和固定噪声策略，以提高生成视频的时间一致性。

#### 8. 实验设置：
本文的实验主要基于两个多视角数据集：现实世界的RealEstate10K（Re10K）和合成的Matterport 3D（MP3D）。作者使用Re10K数据集中的61,986个视频剪辑进行训练，并从测试集中随机采样500个序列进行评估。对于MP3D数据集，他们遵循常见的协议，使用Habitat代理渲染6,000个训练视频和500个测试视频。对于两个数据集，他们将视频调整为256×256的空间分辨率。他们将其方法与几种最先进的方法进行比较：两种最近的基于Transformer的方法GeoGPT和LoR，以及最近的基于GAN的方案SE3DS。他们评估了短期和长期视角合成结果。

#### 9. 实验结果和分析：
本文的姿态引导扩散模型生成的新视角逼真且在各种视角下保持一致。然而，如果场景的比例与训练数据中的比例显着不同，则该方法无法生成逼真的新视角。推理时间较长，因为它涉及多个步骤的反向过程来预测单个新视角。在短期视角合成方面，该方法在Re10K和MP3D数据集上的平均PSNR、SSIM和LPIPS分数均优于其他最先进的方法。在长期视角合成方面，该方法在Re10K数据集上的平均PSNR分数优于其他方法，但在MP3D数据集上的表现不如其他方法。


# Paper:765     基于超级点引导的自监督三维场景流估计



#### 1. Title: 
Self-Supervised 3D Scene Flow Estimation Guided by Superpoints

#### 2. Authors: 
Yaqi Shen, Le Hui, Jin Xie, Jian Yang

#### 3. Affiliation: 
Yaqi Shen, Le Hui, Jin Xie, and Jian Yang are with PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, China.

#### 4. Keywords: 
3D scene flow estimation, superpoints, self-supervised, point clouds, clustering

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Shen_Self-Supervised_3D_Scene_Flow_Estimation_Guided_by_Superpoints_CVPR_2021_paper.html  Github: https://github.com/supersyq/SPFlowNet

#### 6. Summary : 
- (1):本文研究的是3D场景流估计，旨在估计点云的两个连续帧之间的点运动。 
- (2):现有的方法中，超级点是通过离线聚类方法生成的，这不能很好地表征具有相似运动的局部区域，导致场景流估计不准确。因此，本文提出了一种迭代的端到端超级点场景流估计框架，其中超级点可以动态更新以指导点级流预测。 
- (3):本文提出了一种基于超级点的场景流估计框架，其中超级点可以动态更新以指导点级流预测。具体来说，我们的框架包括一个流引导的超级点生成模块和一个超级点引导的流细化模块。在超级点生成模块中，我们利用上一次迭代的双向流信息来获取点和超级点中心的匹配点，以构建软点到超级点的关联映射，其中超级点是为成对点云生成的。通过生成的超级点，我们首先通过自适应聚合超级点级流来重构每个点的流，然后对成对点云的重构流的一致性进行编码。最后，我们将重构的流和一致性编码输入到GRU中以细化点级流。 
- (4):在几个不同的数据集上进行的广泛实验表明，我们的方法可以实现有希望的性能。本文的主要贡献在于提出了一种新颖的端到端自监督场景流估计框架，其中动态生成具有相似流模式的超级点并使用超级点来细化点级流。与其他离线聚类方法不同，我们将在线聚类嵌入到我们的模型中，以动态地分割点云并生成伪流标签。我们引入了一个超级点引导的流细化层，用超级点级流信息来细化点级流，其中超级点级流模式通过学习的关联映射自适应聚合到点级。我们的自监督场景流估计方法在几个基准测试中取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于超点引导的自监督三维场景流估计方法，包括两个模块：流引导的超点生成和超点引导的流细化。该方法利用特征编码器提取点云的局部特征，并使用Sinkhorn算法计算点云之间的全局相关性。初始流和超点中心分别使用特征编码器和FPS算法获得。该方法还采用点到超点的关联计算来生成满足相似流模式、接近坐标和语义相似特征要求的超点。在超点生成模块中，该方法根据先前迭代的流信息动态将点云聚类成超点。在超点引导的流细化模块中，该方法使用GRU利用前一次迭代的预测点级和超点级流的特征和坐标信息来更新超点中心。该方法还使用Chamfer loss、smoothness loss和consistency loss等自监督损失函数来训练模型，无需真实的场景流。

#### 8. 实验设置：
本文在FlyingThings3D和KITTI Scene Flow两个基准数据集上进行了实验。FT3Ds包含19640个训练样本和3824个测试样本，KITTI包含200个样本，其中142个样本用于测试而没有进行微调。FT3Do包含19999个训练样本和2003个测试样本。模型使用Pytorch实现，并在NVIDIA TITAN RTX GPU上执行。输入点云随机采样8192个点，批量大小为2。超点数和迭代次数分别设置为128和3。对于遮挡实验，模型在KITTIr数据集上进行训练，在KITTIo和KITTIt上进行测试。输入点云的大小设置为2048。批量大小、迭代次数和超点数分别设置为4、3和30。所有实验中使用的初始学习率为0.001，使用ADAM优化器进行模型优化。在第40、55和70个epoch时，学习率乘以0.7，模型训练100个epoch。实验中使用的评估指标包括端点误差（EPE）、严格准确度（AS）、宽松准确度（AR）和异常值（Out）。

#### 9. 实验结果与分析：
本文提出的基于超点引导的自监督三维场景流估计方法在KITTI数据集上取得了最先进的性能。比较结果表明，引入超点到场景流估计中是有效的，而流引导的超点生成模块可以比SPNet获得更好的结果。消融实验结果表明，选择L=30、K=2和T=3可以获得最佳性能。本文提出的模型在没有关键组件的情况下效果较差。


# Paper:766     梯度范数感知最小化：寻找一阶平坦度并提高泛化性能



#### 1. Title: 
Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization

#### 2. Authors: 
Xingxuan Zhang, Renzhe Xu, Han Yu, Hao Zou, Peng Cui

#### 3. Affiliation: 
清华大学计算机科学与技术系

#### 4. Keywords: 
Deep Learning, Optimization, Generalization, Flatness, Sharpness

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Gradient_Norm_Aware_Minimization_Seeks_First-Order_Flatness_and_Improves_CVPR_2021_paper.html  Github: https://github.com/xxgege/GAM

#### 6. Summary : 
- (1):本文研究深度学习中的优化问题，提出了一种新的训练方法，旨在寻找更加平坦的极小值点，从而提高模型的泛化性能。
 
- (2):过去的方法主要集中在零阶平坦度的优化，即在一定扰动半径内最坏情况下的损失函数值。但是，这种方法在存在多个极小值点或者扰动半径较小时，很难区分具有低泛化误差的极小值点和具有高泛化误差的极小值点。因此，本文提出了一种更强的一阶平坦度度量方法，即在扰动半径内最大梯度范数，它可以限制局部极小值点的最大Hessian特征值和SAM的正则化函数。同时，本文提出了一种新的训练方法，即梯度范数感知最小化（GAM），以寻找在所有方向上具有均匀小曲率的极小值点。 
 
- (3):本文提出的GAM方法通过随机梯度上升和Hessian向量积来近似最大梯度范数，避免了Hessian矩阵的实现。实验结果表明，GAM结合当前优化器（如SGD和AdamW）可以显著提高模型的泛化性能，并且可以帮助SAM找到更平坦的极小值点。 
 
- (4):本文在各种数据集和网络上验证了GAM的有效性，证明了其可以在不降低收敛速度的情况下提高模型的泛化性能。同时，本文还证明了GAM确实可以找到具有较低Hessian谱的更平坦的极小值点。
#### 7. 方法详细介绍：
本文提出了一种新的优化算法——梯度范数感知最小化（Gradient Norm Aware Minimization，GAM），用于寻找具有统一小曲率的最小值。GAM使用随机梯度上升和Hessian-向量积来近似最大梯度范数，避免了Hessian矩阵的实现。该方法优化了一阶平坦度，即在最小值附近的最大梯度范数，并控制了Hessian的最大特征值。GAM的算法包括以下步骤：采样一个数据批次，计算oracle损失梯度，估计Hessian矩阵，计算梯度范数，使用损失和梯度范数更新参数。该算法有两个超参数，扰动半径和权衡系数，可以调整以获得更好的性能。本文还提供了GAM的收敛性分析。

#### 8. 实验设置：
本文在CIFAR-10和CIFAR-100数据集上对各种最先进的模型进行了实验，包括ResNets、WideResNet、ResNeXt、PyramidNet和Vision Transformers。模型使用基本和高级数据增强从头开始训练200个epochs。本文使用不同的设置，包括SGD和SAM优化器，来比较GAM与基线方法的性能。GAM的超参数设置为默认值，扰动半径设置为0.1，这是之前的工作建议的。

#### 9. 实验结果和分析：
本文展示了在实践中讨论的情况是普遍存在的，通过调查扰动半径内的局部最小值数量来证明。本文还展示了GAM在各种模型和数据集上实现了比基线方法更好的测试准确性和泛化性能。具体来说，与基线方法相比，GAM在CIFAR-10上将测试准确性提高了1.5％，在CIFAR-100上提高了1.7％。本文还表明，GAM可以减少参数和计算开销，同时保持或提高模型的性能。


# Paper:767     多模态表示学习中潜在模态结构的理解与构建



#### 1. Title: 
Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning

#### 2. Authors: 
Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, Trishul Chilimbi

#### 3. Affiliation: 
Qian Jiang: University of Illinois at Urbana-Champaign
其他作者: Amazon

#### 4. Keywords: 
Multi-modal representation learning, latent modality structures, contrastive loss, modality alignment, downstream tasks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了多模态表示学习中的潜在模态结构问题，提出了三种构建潜在模态结构的方法。

- (2):过去的方法通常使用对比损失来学习多模态表示，但是直接减少模态差异并不总是能提高下游任务的性能。本文提出了三种正则化方法来构建更好的潜在模态结构，包括内部模态正则化、跨模态正则化和内部-跨模态正则化。

- (3):本文首先从信息论的角度证明了完全模态匹配不一定是下游任务的最优选择，提出了保持有意义的潜在模态结构的重要性。然后，本文提出了三种正则化方法来构建更好的潜在模态结构，包括内部模态正则化、跨模态正则化和内部-跨模态正则化。这些正则化方法可以应用于各种视觉语言模型，包括CLIP和ALBEF等模型。

- (4):本文在多个下游任务上进行了广泛的实验，包括零/少样本图像分类、图像-文本检索、视觉问答、视觉推理和视觉蕴含等任务。实验结果表明，本文提出的方法在不同的模型和任务上都能够显著提高性能，证明了所提出的方法的有效性和通用性。
#### 7. 方法详细介绍：
本文提出了三种正则化方法来构建多模态表示学习中的潜在模态结构。第一种方法是内模态正则化，使用对比损失和均匀性损失来保留模态共享信息和模态独立信息。第二种方法是跨模态正则化，使用布朗运动桥来构建潜在结构，引导从图像模态到相关文本模态的转换。第三种方法是内-跨模态正则化，使用几何一致性损失在模态表示及其增强之间强制实现几何对称性。

#### 8. 实验设置：
本文在两个流行的多模态表示框架上评估了所提出的方法：基于两个塔的模型（例如CLIP）和基于融合的模型（例如ALBEF）。对于CLIP模型，使用ResNet-50作为图像编码器，BERT作为文本编码器。使用标准对比损失进行预训练，并分别应用所提出的正则化损失。

#### 9. 实验结果与分析：
本文报告了在CIFAR10、CIFAR100、ImageNet1K、ImageNetV2、ImageNetSketch和ImageNet-A数据集上的零样本TopK分类准确率。所提出的方法在大多数数据集上优于基线CLIP和CyCLIP模型，在零样本分类准确率方面取得了显著的改进。具体而言，内模态正则化方法（OURS-Sep）在CIFAR10、CIFAR100和ImageNet1K数据集上表现最佳，而内-跨模态正则化方法（OURS-GC）在ImageNetV2数据集上表现最佳。跨模态正则化方法（OURS-Br）在大多数数据集上的表现与基线模型相当。


# Paper:768     神经傅里叶滤波器组



#### 1. Title: 
Neural Fourier Filter Bank

#### 2. Authors: 
Zhijie Wu, Yuhe Jin, Kwang Moo Yi

#### 3. Affiliation: 
University of British Columbia（英属哥伦比亚大学）

#### 4. Keywords: 
Neural fields, Fourier features, wavelet decomposition, multi-scale grid, MLP

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Neural_Fourier_Filter_Bank_CVPR_2021_paper.html

Github: https://github.com/ubc-vision/NFFB

#### 6. Summary : 
- (1):本文的研究背景是神经场模型的空间分解和频率分解的结合。
- (2):过去的方法要么只考虑空间分解，要么只考虑频率分解，没有将两者结合起来。本文提出了一种新的神经场框架，将目标信号同时在空间和频率域中分解，类似于传统的小波分解。本文的方法通过将频率编码为傅里叶特征，然后通过正弦激活函数的多层感知器将这些特征组合在一起，形成一个逐步累积高频信息的管道，最终将所有中间输出相加在一起形成估计的场值。本文的方法在2D图像拟合、3D形状重建和神经辐射场等三个任务上取得了更好的质量和内存的平衡。
- (3):本文提出了一种新的神经场框架，将目标信号同时在空间和频率域中分解，类似于传统的小波分解。本文的方法通过将频率编码为傅里叶特征，然后通过正弦激活函数的多层感知器将这些特征组合在一起，形成一个逐步累积高频信息的管道，最终将所有中间输出相加在一起形成估计的场值。本文的方法在2D图像拟合、3D形状重建和神经辐射场等三个任务上取得了更好的质量和内存的平衡。
- (4):本文的方法在2D图像拟合、3D形状重建和神经辐射场等三个任务上取得了更好的质量和内存的平衡。本文的方法在模型紧凑性和收敛速度方面优于现有技术。本文的方法通过将频率编码为傅里叶特征，然后通过正弦激活函数的多层感知器将这些特征组合在一起，形成一个逐步累积高频信息的管道，最终将所有中间输出相加在一起形成估计的场值。
#### 7. 方法详细介绍：
本文提出了一种名为神经傅里叶滤波器组的方法，它将信号分解为神经场网格，同时考虑了空间和频率。该方法采用多尺度网格结构，其中低频路径采用多层感知器（MLP）实现，而高频路径则采用网格上的查找操作实现。该方法的分解通过低频和高频滤波器实现，这些滤波器在概念上被实现为神经网络。该方法在多个任务上进行了演示，包括2D图像拟合、3D形状重建和神经辐射场，表现出了比现有方法更高的模型紧凑性和收敛速度。

#### 8. 实验设置：
本文使用PyTorch和Adam优化器进行实验，对于所有实验，网格特征的维度设置为2，学习率设置为10的负4次方，每5000次迭代进行一次衰减。本文在三个任务上评估了提出的方法，包括2D图像拟合、使用有符号距离函数进行3D形状重建和使用NeRF进行新视角合成。作者注意到每个实验的关键设置，并在补充材料中提供了更多关于架构和超参数设置的细节。

#### 9. 实验结果和分析：
本文报告了提出的方法提供了与其他方法类似的性能，但具有更小的模型大小。结果在图6和表3中报告。本文还进行了消融研究，以证明方法的设计选择。探索了三种方法的变体，包括仅使用网格特征的方法，使用网格和傅里叶特征编码的方法以及仅使用MLP架构进行组合而没有网格的方法。结果表明，所有变体的表现都显著劣于提出的方法。


# Paper:769     基于分级相似性监督的高效大规模地点识别



#### 1. Title: 
Data-efficient Large Scale Place Recognition with Graded Similarity Supervision

#### 2. Authors: 
María Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov

#### 3. Affiliation: 
María Leyva-Vallina: 荷兰格罗宁根大学
Nicola Strisciuglio: 荷兰特文特大学
Nicolai Petkov: 荷兰格罗宁根大学

#### 4. Keywords: 
Visual place recognition, graded similarity, contrastive loss, descriptor learning, nearest neighbor search

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Leyva-Vallina_Data-Efficient_Large_Scale_Place_Recognition_With_Graded_Similarity_Supervision_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是计算机视觉中的视觉地点识别（VPR）任务，该任务是自主驾驶车辆导航系统的基础。现有的方法使用图像对进行训练，这些图像要么描述相同的地点，要么不是。这种二元指示不考虑来自不同位置拍摄的同一地点的图像之间的相似性连续关系，这是由于相机姿态的连续性质所决定的。二元相似性会在VPR方法的训练中引入噪声监督信号，导致训练陷入局部最小值，并需要昂贵的硬挖掘算法来保证收敛。本文提出了一种自动重新注释策略来重新标记VPR数据集，计算基于可用定位元数据的图像对的分级相似性标签。此外，本文提出了一种新的广义对比损失（GCL），该损失使用分级相似性标签来训练对比网络。本文的方法不需要硬对挖掘，可以训练出比需要昂贵的硬对挖掘和重新排序技术的方法更好的VPR图像描述符。
 
- (2):现有的VPR方法通常使用二元标签来训练，这种标签不考虑来自不同位置拍摄的同一地点的图像之间的相似性连续关系。这种二元相似性会在VPR方法的训练中引入噪声监督信号，导致训练陷入局部最小值，并需要昂贵的硬挖掘算法来保证收敛。本文提出了一种自动重新注释策略来重新标记VPR数据集，计算基于可用定位元数据的图像对的分级相似性标签。此外，本文提出了一种新的广义对比损失（GCL），该损失使用分级相似性标签来训练对比网络。本文的方法不需要硬对挖掘，可以训练出比需要昂贵的硬对挖掘和重新排序技术的方法更好的VPR图像描述符。

- (3):本文提出了一种基于分级相似性标签的VPR方法，该方法使用相机姿态元数据或3D信息作为图像对的代理来估计图像对之间的相似性。本文的方法不需要硬对挖掘，可以训练出比需要昂贵的硬对挖掘和重新排序技术的方法更好的VPR图像描述符。本文的方法使用广义对比损失（GCL）来训练VPR网络，该损失使用分级相似
#### 7. 方法详细介绍：
本文提出了一种基于分级相似度监督的数据高效大规模视觉地点识别方法。该方法使用VGG16作为骨干网络，计算参考图像和查询图像的描述符。对于图像检索，使用最近邻搜索在参考描述符中检索k个最相似的图像。该方法使用分级相似度标签来组成批次，其中选择50％的正样本对，25％的软负样本和25％的硬负样本。该方法使用提出的分级对比损失（GCL）和分级相似度标签来训练网络。具体而言，GCL函数使用一种连续的相似度值来加权学习步骤，并按比例优化潜在空间中图像对的距离，以对应于相应的相似度程度。该方法的描述符在最近邻搜索检索中表现更好，且不需要昂贵的硬对挖掘和重新排序技术。

#### 8. 实验设置：
本文使用Oxford RobotCar数据集、Aachen Day-Night数据集和Mapillary Street Level Sequences（MSLS）数据集进行评估。使用常用的地点识别评估协议，包括Top-k召回（R@k）和正确定位查询的百分比来衡量性能。本文将所提出的方法与NetVLAD、Patch-NetVLAD、DELG、SuperGlue和TransVPR等现有方法进行比较。

#### 9. 实验结果和分析：
本文提出的方法在多个数据集上均取得了优于或与现有方法相当的结果，包括MSLS、Pittsburgh30k、Tokyo24/7、RobotCar Seasons v2和Extended CMU Seasons。使用分级相似度标签和GCL训练的模型具有良好的泛化能力，能够在未见过的环境中实现高精度的地点识别。与现有方法相比，该方法不需要昂贵的硬对挖掘和重新排序技术，具有更高的数据和计算效率。本文还对骨干网络、对比损失和PCA进行了消融实验，结果表明，GCL函数在邻居搜索和检索方面更为有效，其排名基于查询描述符的距离是视觉地点相似度的更可靠度量。


# Paper:770     基于分层监督和洗牌数据增强的三维半监督目标检测



#### 1. Title: 
Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection

#### 2. Authors: 
Chuandong Liu, Chenqiang Gao, Fangcen Liu, Pengcheng Li, Deyu Meng, Xinbo Gao

#### 3. Affiliation: 
重庆邮电大学通信与信息工程学院，重庆市信号与信息处理重点实验室

#### 4. Keywords: 
3D object detection, semi-supervised learning, teacher-student model, data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Hierarchical_Supervision_and_Shuffle_Data_Augmentation_for_3D_Semi-Supervised_Object_CVPR_2021_paper.html  Github: https://github.com/azhuantou/HSSDA

#### 6. Summary : 
- (1):本文研究的是3D半监督目标检测，旨在通过少量标注样本和大量未标注样本来提高模型的泛化能力，减少标注数据的依赖性。
 
- (2):过去的伪标签半监督目标检测方法主要采用师生模型，使用单一固定阈值策略生成伪标签，这种方法会在指导学生网络训练时带来混淆的监督。此外，典型的师生模型中点云的数据增强过于弱，只包含基本的下采样和翻转平移，这阻碍了特征信息的有效学习。因此，本文提出了一种新的Hierarchical Supervision and Shuffle Data Augmentation (HSSDA)方法，旨在解决这些问题。该方法采用动态双阈值策略生成更合理的伪标签，同时设计了一种洗牌数据增强策略，以增强学生网络的特征表示能力。
 
- (3):本文提出的HSSDA方法是一种简单而有效的师生模型。该方法通过设计动态双阈值策略，将教师网络的输出分为三组：高置信度级别的伪标签、模糊级别的伪标签和低置信度级别的伪标签，为学生网络提供分层监督信号。此外，设计了一种洗牌数据增强策略，以增强学生网络的特征表示能力。实验结果表明，HSSDA方法在不同数据集上始终优于最新的现有方法。
 
- (4):本文在3D半监督目标检测任务上，通过提出的HSSDA方法，在KITTI和SUN RGB-D数据集上取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于教师-学生框架的3D半监督目标检测方法，称为HSSDA。该方法采用了分层监督和洗牌数据增强策略，以提高学生网络的性能。该方法的流程包括burn-in阶段和互相学习阶段。在burn-in阶段，检测器使用标记的场景进行完全监督训练。在互相学习阶段，该框架在全局视图中为每个类别生成三种双阈值，挖掘分层伪标签，并使用洗牌数据增强训练学生网络。双阈值在每个训练时期动态生成，分层监督由三组伪标签组成：高置信度级别、模糊级别和低置信度级别。洗牌数据增强旨在增强学生网络的特征表示能力。教师网络通过EMA策略进行更新。该框架可以训练任何由骨干网络和检测头组成的现成3D检测器。

#### 8. 实验设置：
本文在KITTI、Waymo和nuScenes数据集上进行了广泛的实验。在KITTI数据集上，使用不同的1%和2%标记场景拆分进行训练，并在验证集上进行评估。在Waymo数据集上，使用798个训练序列中的1%进行训练，并在验证集上进行评估。评估指标包括40个召回位置的平均精度（AP）和3D IoU阈值（对于KITTI中的汽车、行人和骑车者类别分别为0.7、0.5和0.5），以及Waymo中的mAP和mAPH。

#### 9. 实验结果和分析：
本文提出的HSSDA方法在KITTI和Waymo数据集上均取得了最先进的性能。在KITTI上，该方法的mAP得分分别为85.3%、75.5%和68.5%，对于汽车、行人和骑车者类别，相对于之前的最先进方法，性能提升显著。在Waymo上，该方法的mAP得分分别为LEVEL 1和LEVEL 2的47.5%和38.5%，mAPH得分分别为LEVEL 1和LEVEL 2的44.5%和35.5%，相对于之前的最先进方法，性能提升显著。


# Paper:771     DKT：面向类增量学习的多样化知识迁移变压器



#### 1. Title: 
DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning

#### 2. Authors: 
Xinyuan Gao, Yuhang He, Songlin Dong, Jie Cheng, Xing Wei, Yihong Gong

#### 3. Affiliation: 
第一作者：西安交通大学软件工程学院

#### 4. Keywords: 
Class Incremental Learning, Knowledge Transfer, Transformer, Continual Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gao_DKT_Diverse_Knowledge_Transfer_Transformer_for_Class_Incremental_Learning_CVPR_2021_paper.html  Github: https://github.com/MIV-XJTU/DKT

#### 6. Summary : 
- (1):本文研究的是类增量学习中的知识迁移问题，深度神经网络容易出现灾难性遗忘，即随着新知识的学习，旧类别的准确率会显著下降。 
- (2):现有的方法大多存在稳定性-可塑性困境或计算和参数要求过高的问题。本文提出了一种新的框架，即多样化知识迁移变压器（DKT），它包括两种知识迁移机制，使用注意机制将任务特定和任务通用知识传输到当前任务中，以及一个双工分类器来解决稳定性-可塑性困境。此外，我们设计了一个损失函数，在特征空间中聚类相似类别并区分旧任务和新任务。所提出的方法仅需要少量额外的参数，与任务数量的增加相比可以忽略不计。 
- (3):本文提出了两个创新的注意块：通用知识传输注意块（GKAB）和特定知识传输注意块（SKAB），以减轻特征退化和灾难性遗忘。我们还提出了一个双工分类器，它包括一个稳定性分类器和一个可塑性分类器，以维持模型对旧类别的稳定性和学习新类别的可塑性。此外，我们提出了一个聚类分离损失，将属于相同类别的特征聚集在一起，并将旧任务和新任务之间的特征分开。 
- (4):在CIFAR100、ImageNet100和ImageNet1000数据集上进行了广泛的实验，证明了我们的方法优于其他竞争方法，并实现了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种名为“多样化知识转移变压器”（DKT）的框架，用于解决类别增量学习中的灾难性遗忘问题。DKT包括一个编码器和一个解码器。编码器使用补丁嵌入和自注意力块（SAB）提取特征。解码器包括两个注意力块，用于将任务通用和任务特定知识转移到当前任务。任务通用知识通过称为GKAB的新型注意力块传输，该块混合任务通用知识和当前任务知识。任务特定知识通过另一个注意力块SKAB传输，该块收集当前知识并传输任务特定知识。模型还包括一个双工分类器，它包括稳定性分类器和可塑性分类器，以平衡模型的可塑性和稳定性。损失函数包括分类损失、知识蒸馏损失和聚类分离损失，以促进旧任务和新任务之间的特征区分和聚类属于相同类别的特征。

#### 8. 实验设置：
本文在三个数据集上测试了DKT模型：CIFAR100、ImageNet100和ImageNet1000。ImageNet的标准连续设置为10步，ImageNet100每步增加10个类别，ImageNet1000每步增加100个类别。在CIFAR100上，模型在100个类别上进行训练，每步增加5、10和20个类别。模型的性能通过CIFAR100和ImageNet的top-1准确率以及ImageNet的top-5准确率与其他方法进行比较。每步的最终准确率（Last）、平均准确率（Avg）和最终步骤后的参数数量（#Paras）进行报告，并对每个数据集存储固定的内存缓冲区。

#### 9. 实验结果和分析：
本文提出的DKT模型在CIFAR100、ImageNet100和ImageNet1000数据集上均取得了最先进的性能。在ImageNet100上，DKT在top-1准确率方面优于Dytox和DER w/o P，平均准确率分别提高了2.30%和1.02%。在CIFAR100上，DKT在5步和10步设置中均达到最佳性能，分别比DER高1.33%和1.29%。在ImageNet1000上，DKT的性能超过DER w/o P和Simple-DER，分别提高了1.60%和3.81%。


# Paper:772     FreeSeg：统一、通用和开放词汇图像分割



#### 1. Title: 
FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation

#### 2. Authors: 
Jie Qin, Jie Wu, Pengxiang Yan, Ming Li, Ren Yuxi, Xuefeng Xiao, Yitong Wang, Rui Wang, Shilei Wen, Xin Pan, Xingang Wang

#### 3. Affiliation: 
Jie Qin: 1. Institute of Automation, Chinese Academy of Sciences; 3. School of Artificial Intelligence, University of Chinese Academy of Sciences

#### 4. Keywords: 
Image segmentation, open-vocabulary learning, semantic segmentation, instance segmentation, panoptic segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Qin_FreeSeg_Unified_Universal_and_Open-Vocabulary_Image_Segmentation_CVPR_2022_paper.html  Github: https://github.com/FreeSeg/FreeSeg

#### 6. Summary : 
- (1):本文研究的是图像分割领域的开放词汇学习问题，即如何实现对任意类别的分割。由于现有方法针对特定分割任务设计专门的架构或参数，因此存在碎片化的问题，难以实现分割模型的统一性。
- (2):现有的开放词汇分割方法存在两个明显的缺点：一是无法捕捉任务感知特征，难以适应不同的分割任务；二是需要从头开始训练模型，且不同的任务需要部署多个定制模型。本文提出了FreeSeg，一个通用的框架，通过一次性训练优化一个全能网络，使用相同的架构和参数无缝处理不同的分割任务。
- (3):FreeSeg提出了自适应提示学习方案，将任务感知和类别敏感概念编码到文本抽象中，使统一模型能够灵活地完成任意类别的不同分割任务。在多任务和多样化场景下，自适应提示学习有助于提高模型的鲁棒性。实验结果表明，FreeSeg在多个分割任务、训练数据集和零样本泛化方面均取得了新的最优结果。
- (4):FreeSeg在语义分割、实例分割和全景分割任务上均取得了最优结果，性能优于最佳特定任务架构。在COCO、ADE20K和VOC 2012数据集上，FreeSeg的性能表现出色，且更适合多任务部署。
#### 7. 方法详细介绍：
FreeSeg是一个统一、通用、开放词汇的图像分割框架。它由两个阶段组成，第一个阶段提取通用掩模提案，第二个阶段利用CLIP在第一个阶段生成的掩模上执行零样本分类。第一个阶段的训练数据包含图像、已知类别集、任务名称和多任务标签。掩模提取器将图像编码为视觉概念和类别无关的掩模。为了使FreeSeg能够处理任务和类别特征，设计了一种新颖的自适应提示学习方法，通过预训练的基于CLIP的文本编码器将任务和类别概念明确地嵌入到联合文本嵌入中。设置跨模态分类监督，使FreeSeg能够根据任意文本对生成的掩模进行分类。引入语义上下文交互模块，通过有效地聚合自适应文本嵌入到视觉概念中，提高了跨模态特征匹配和对齐。测试时进行提示调整，以在测试期间优化自适应类提示。

#### 8. 实验设置：
在实验中，使用COCO、ADE20K和VOC 2012三个数据集进行了三个图像分割任务（语义分割、实例分割和全景分割）的评估。对于每个数据集，提供了实现细节，包括骨干网络、批量大小、输入图像大小、优化器和训练迭代次数。使用自我训练技术进行训练，并评估了语义、实例和全景分割的评估指标，如mIoU、mAP、PQ、SQ和RQ。实验结果表明，FreeSeg在性能和泛化方面均取得了新的最佳结果。

#### 9. 实验结果和分析：
FreeSeg在ADE20K数据集上的mIoU语义分割、mAP实例分割、PQ、SQ和RQ全景分割结果分别为24.6%、6.5%、16.3%、71.8%和21.6%，优于SOTA方法MaskCLIP [8] 0.9%的mIoU、0.6%的mAP、1.2%的PQ和2.4%的RQ。在COCO数据集上，FreeSeg也获得了最佳性能，分别为21.7%的mIoU、6.6%的mAP、16.5%的PQ、72.0%的SQ和21.6%的RQ。在VOC2012数据集上的泛化结果也验证了FreeSeg的可迁移性。定性结果表明，FreeSeg能够推广到通用分割任务的任意分割类别。


# Paper:773     专家领域数据的专业级众包注释



#### 1. Title: 
Towards Professional Level Crowd Annotation of Expert Domain Data

#### 2. Authors: 
Pei Wang, Nuno Vasconcelos

#### 3. Affiliation: 
Pei Wang: 加州大学圣地亚哥分校 (UC San Diego)
Nuno Vasconcelos: 加州大学圣地亚哥分校 (UC San Diego)

#### 4. Keywords: 
Crowd annotation, expert domain data, semi-supervised learning, human filtering, fine-grained classification

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Towards_Professional_Level_Crowd_Annotation_of_Expert_Domain_Data_CVPR_2019_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是在专家领域中进行图像识别通常需要专家标注，这限制了数据集的大小和学习系统的准确性。
- (2):过去的方法包括机器教学和半监督学习，但是它们在专家领域中的效果不佳。本文提出了一种新的方法，基于半监督学习和人类过滤，称为SSL-HF，它是一种人类参与的SSL方法，其中众包工人充当伪标签的过滤器，取代了现有SSL方法中使用的不可靠的置信度阈值。为了使非专家能够进行注释，类别是通过正负示例隐式指定的，并通过深思熟虑的解释进行增强，这些解释突出了类别模糊性的区域。实验表明，SSL-HF在几个基准测试中显着优于各种替代方法。
- (3):本文提出了一种新的方法，称为SSL-HF，它是一种人类参与的SSL方法，其中众包工人充当伪标签的过滤器，取代了现有SSL方法中使用的不可靠的置信度阈值。为了使非专家能够进行注释，类别是通过正负示例隐式指定的，并通过深思熟虑的解释进行增强，这些解释突出了类别模糊性的区域。本文的创新点在于将人类的低样本学习能力与SSL相结合，以创建直观但有效的标注体验。 
- (4):本文的方法在多个基准测试中显着优于SSL、AL和MT方法，证明了其有效性。本文的方法在专家领域中实现了准确的众包注释，具有大量的类别。
#### 7. 方法详细介绍：
本文提出了一种半监督学习与人类过滤相结合的方法，称为SSL-HF。该方法使用分类器生成未标记数据的伪标签，并将其过滤掉，直到收敛。过滤伪标签的过程由众包工人完成，这些工人充当伪标签的过滤器，取代了现有SSL方法中不可靠的置信度阈值。为了使非专家能够进行注释，类别是通过正负样本集隐式指定的，并通过增强的解释进行增强，以突出类别模糊的图像区域。SSL-HF方法的详细算法包括数据准备和众包循环，并讨论了SSL-HF相对于其他方法（如主动学习和机器翻译）的优势。

#### 8. 实验设置：
本文在CUB-200-2011、Stanford Dogs和Stanford Cars数据集上进行了实验，评估了SSL-HF的注释性能。实验基于模拟人类标注的协议进行，使用了四个分类器，分别训练在不断增加的标记数据集上，以评估SSL-HF的性能。

#### 9. 实验结果与分析：
本文提出的SSL-HF方法在多个基准测试中显著优于各种替代方法，包括SSL、AL和MT。该方法能够在大量类别的专家数据上实现准确的众包注释。增加解释进一步提高了伪标签的人类过滤的准确性。实验结果表明，SSL-HF在注释准确性和注释成本之间实现了最佳平衡，对于CUB数据集，其成本为800美元时的准确性等于SSL的1200美元和Supervised的1700美元。


# Paper:774     DeltaEdit：探索文本驱动图像编辑的无文本训练



#### 1. Title: 
DeltaEdit: Exploring Text-free Training for Text-Driven Image Manipulation

#### 2. Authors: 
Yueming Lyu, Tianwei Lin, Fu Li, Dongliang He, Jing Dong, Tieniu Tan

#### 3. Affiliation: 
第一作者：中国科学院自动化研究所

#### 4. Keywords: 
text-driven image manipulation, CLIP, StyleGAN, text-free training, image-text feature space

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Lyu_DeltaEdit_Exploring_Text-Free_Training_for_Text-Driven_Image_Manipulation_CVPR_2021_paper.html  Github: https://github.com/Yueming6568/DeltaEdit

#### 6. Summary : 
- (1):本文研究的是文本驱动的图像编辑，该领域的研究受到了学术界和工业界的广泛关注。现有的方法需要大量的手工注释图像-文本对，这限制了训练的灵活性。最近的一些方法利用了预训练的视觉-语言模型，但是这些方法要么需要进行迭代优化，要么需要在线调整超参数，不能很好地推广到其他未见过的文本。
- (2):现有的方法需要大量的手工注释图像-文本对，这限制了训练的灵活性。最近的一些方法利用了预训练的视觉-语言模型，但是这些方法要么需要进行迭代优化，要么需要在线调整超参数，不能很好地推广到其他未见过的文本。本文提出了一种新的框架DeltaEdit，通过探索图像和文本特征空间之间的对齐关系，实现了文本驱动的图像编辑的无文本训练。该方法可以很好地推广到各种文本提示的零样本推理，而不需要任何额外的优化或调整。
- (3):本文提出了一种新的框架DeltaEdit，通过探索图像和文本特征空间之间的对齐关系，实现了文本驱动的图像编辑的无文本训练。该方法可以很好地推广到各种文本提示的零样本推理，而不需要任何额外的优化或调整。具体地，本文提出了一种名为DeltaEdit的框架，该框架可以将CLIP视觉特征差异映射到StyleGAN的编辑方向，然后在推理阶段，DeltaEdit可以从CLIP文本特征的差异中预测StyleGAN的编辑方向。该方法可以很好地推广到各种文本提示的零样本推理，而不需要任何额外的优化或调整。
- (4):本文在多个数据集上进行了广泛的实验，包括FFHQ和LSUN，验证了DeltaEdit方法的有效性和效率。结果表明，DeltaEdit可以很好地推广到各种文本提示的编辑，而不需要任何额外的优化或调整。
#### 7. 方法详细介绍：
本文提出了一种名为DeltaEdit的框架，用于解决文本驱动图像操作的挑战。该方法通过识别Delta图像和文本空间，将CLIP视觉特征差异和源文本与目标文本的CLIP文本嵌入差异对齐。在训练阶段，DeltaEdit网络将CLIP视觉特征差异映射到StyleGAN的编辑方向。在推理阶段，DeltaEdit从CLIP文本特征差异中预测StyleGAN的编辑方向。DeltaEdit是以无文本方式进行训练的。

具体步骤如下：
1. 提取Delta图像和文本特征作为条件向量。
2. 在训练阶段，使用提取的特征获取CLIP图像空间方向和StyleGAN S空间方向。
3. 提出了一个在Delta空间中的潜在映射器网络，称为Delta Mapper，用于预测编辑方向。Delta Mapper通过三个级别的粗到细操作实现粗到细操作，并通过融合生成的粗到细特征来预测编辑方向。
4. 在推理阶段，基于多模态Delta特征空间，使用训练好的Delta Mapper实现文本驱动的图像操作。

#### 8. 实验设置：
本文在多个数据集上进行了实验，包括FFHQ、LSUN Cat、Church和Horse数据集。使用1个NVIDIA Tesla P40 GPU进行训练，批量大小为64，使用ADAM优化器，β1 = 0.9，β2 = 0.999，学习率为0.5。在评估过程中，使用了12,000个FFHQ图像。

#### 9. 实验结果和分析：
本文提出的DeltaEdit方法在各种文本提示下实现了理想的操作结果，无需手动调整操作强度。在FID、PSNR和IDS指标方面，DeltaEdit优于现有的方法。人类主观评估也显示了DeltaEdit在操作准确性和视觉逼真度方面的优越性。与其他现有方法相比，DeltaEdit的计算效率也得到了验证。


# Paper:775     半弱监督物体运动预测



#### 1. Title: 
Semi-Weakly Supervised Object Kinematic Motion Prediction

#### 2. Authors: 
Gengxin Liu, Qian Sun, Haibin Huang, Chongyang Ma, Yulan Guo, Li Yi, Hui Huang, Ruizhen Hu

#### 3. Affiliation: 
深圳大学

#### 4. Keywords: 
Object kinematic motion prediction, semi-weakly supervised learning, graph neural network, PartNet, PartNet-Mobility

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Semi-Weakly_Supervised_Object_Kinematic_Motion_Prediction_CVPR_2021_paper.html
Github: None

#### 6. Summary : 
- (1):本文研究了三维物体的运动预测问题，旨在识别移动部件以及相应的运动参数。由于三维物体的拓扑结构和几何细节变化较大，这仍然是一个具有挑战性的任务，而缺乏大规模标记数据也限制了基于深度学习的方法的性能。

- (2):过去的方法包括基于传统方法的运动预测和基于机器学习的方法。然而，这些方法在处理具有复杂结构的三维物体时性能下降明显，因为训练数据有限。本文提出了一种半弱监督学习方法，将有限的完全标记数据扩展到大规模的未标记三维物体库中，并提高了性能。

- (3):本文提出了一种鲁棒的两阶段管道，将移动部件检测任务转化为从给定层次结构中选择部件的任务。在第一阶段，我们构建了一个图形，编码了父子层次结构和相邻部件对。然后，我们训练一个图形神经网络来预测边缘级别的运动部分，包括其运动类型和相应的参数。它是在PartNet-Mobility数据集上训练的，然后应用于PartNet以获得初始预测。在第二阶段，我们引入了一个细化过程，以过滤出具有低可行性得分和一致性得分的预测，使用启发式策略来细化运动轴，并生成最终的伪标签作为运动预测结果。

- (4):本文的方法在3D部分扫描的运动预测任务上取得了显著的性能提升。我们的实验表明，使用我们的方法增强的数据训练的现有方法的预测精度可以大幅提高，并且我们的方法在半弱监督学习管道下优于现有方法。
#### 7. 方法详细介绍：
本文提出了一种半弱监督学习方法，用于预测形状部件之间的相对运动。该方法通过构建分层的部件和相邻部件之间的边缘来预测每个部件对的运动参数。使用图神经网络（GNN）对每个部件对的运动参数进行预测，该网络以部件的点云表示和它们的相对位置作为输入。预测结果使用从部件的定向边界框（OBB）生成的候选运动轴计算的可行性分数和一致性分数进行精炼。

#### 8. 实验设置：
本文的实验使用了PartNet数据集，将其分为训练集、验证集和测试集。训练集和验证集使用PartNet-Mobility数据集进行训练，测试集使用PartNet数据集进行测试。实验中使用了多种评价指标，包括角度误差、距离误差、旋转误差、平移误差和三维交集等。

#### 9. 实验结果和分析：
本文提出的方法在多个物体上进行了实验，包括冰箱、笔记本电脑、刀具和剪刀等。实验结果表明，该方法在角度误差和距离误差方面优于其他方法。但是，该方法在某些物体上的表现不佳，例如显示器、存储家具和门套等。本文还展示了半弱监督方法可以提高部分扫描的运动预测结果。在实验中，本文的方法在PartNet数据集上取得了最好的结果，比其他方法表现更好。在不同比例的训练数据下，本文的方法与完全监督方法相比表现相当，甚至在只使用一半标记数据的情况下也能取得较好的结果。


# Paper:776     基于冲突的跨视图一致性半监督语义分割



#### 1. Title: 
Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation

#### 2. Authors: 
Zicheng Wang, Zhen Zhao, Xiaoxia Xing, Dong Xu, Xiangyu Kong, Luping Zhou

#### 3. Affiliation: 
第一作者：University of Sydney（悉尼大学）

#### 4. Keywords: 
Semi-supervised learning, semantic segmentation, co-training, cross-view consistency, conflict-based pseudo-labelling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Conflict-Based_Cross-View_Consistency_for_Semi-Supervised_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/xiaoyao3302/CCVC

#### 6. Summary : 
- (1):本文研究半监督语义分割（SSS）任务，旨在通过少量的标注数据和大量的未标注数据来训练模型。由于标注数据的数量有限，因此需要充分利用未标注数据来辅助训练。 
- (2):现有的半监督语义分割方法通常会受到伪标签过程中的确认偏差的影响，这可以通过协同训练框架来缓解。然而，现有的协同训练方法依赖于手工制作的扰动，以防止不同的子网络相互折叠，但这些人工扰动不能导致最优解。本文提出了一种新的基于冲突的跨视图一致性（CCVC）方法，该方法基于双分支协同训练框架，旨在强制两个子网络从不相关的视图中学习信息。 
- (3):本文提出了一种新的跨视图一致性（CVC）策略，通过引入特征差异损失来鼓励两个子网络从相同的输入中学习不同的特征，同时这些不同的特征应该生成一致的预测分数。本文进一步提出了一种基于冲突的伪标签（CPL）方法，以保证模型从冲突的预测中学习更有用的信息，从而产生稳定的训练过程。 
- (4):本文在PASCAL VOC 2012和Cityscapes等常用基准数据集上验证了新的CCVC方法，取得了新的最优性能。 


#### 7. 方法详细介绍：
本文提出了一种基于冲突的跨视图一致性（CCVC）策略的半监督语义分割方法。该方法采用了基于协同训练的双分支网络，其中两个子网络具有相似的架构，但两个子网络的参数不共享。CCVC策略包括三个主要组成部分：跨视图一致性（CVC）方法、基于冲突的伪标签（CPL）方法和差异损失。CVC方法通过差异损失最小化每个特征提取器提取的特征之间的余弦相似度，使得两个子网络输出的特征没有相互关系。CPL方法鼓励模型从冲突的预测中学习更多的语义信息，以生成一致的预测，从而保证两个子网络可以生成相同可靠的预测，并进一步稳定训练。差异损失用于最小化两个子网络提取的特征的相似性，以防止崩溃并保证子网络的推理视图是不相关的。总损失是监督损失、一致性损失和差异损失的组合。

#### 8. 实验设置：
本文使用DeepLabv3+作为分割模型，使用在ImageNet上预训练的ResNet作为骨干网络。采用SGD优化器和poly-learning rate调度器，将Pascal VOC 2012数据集和Cityscapes数据集的初始学习率分别设置为0.001和0.005。在两个数据集上分别训练80个和250个epochs，以平均交并比（mIoU）作为评估指标。在一个mini-batch中，标记数据和未标记数据的数量相等，批量大小分别设置为24和8。除了裁剪大小，作者还采用了PS-MT的弱数据增强，分别将裁剪大小设置为512和712。本文中所有实验中，由自信的冲突（CC）预测生成的伪标签监督的一致性损失的权重ωc均设置为2.0。

#### 9. 实验结果和分析：
本文提出了一种基于协同训练框架的半监督语义分割方法，引入了跨视图一致性策略，强制两个子网络从不相关的视角学习信息，并相互交换信息以生成一致的预测。该方法在性能上表现出色，特别是在标记数据较少的情况下，表明该方法可以更好地利用未标记数据。该方法在性能上超过了现有方法，验证了该方法的有效性。本文还提供了定性结果，并可视化了训练过程中的演化过程，以验证所提出的基于冲突的伪标签方法在防止两个子网络产生不一致预测方面的有效性，从而保证预测结果的可靠性。


# Paper:777     点云匹配的旋转不变Transformer



#### 1. Title: 
Rotation-Invariant Transformer for Point Cloud Matching

#### 2. Authors: 
Hao Yu, Zheng Qin, Ji Hou, Mahdi Saleh, Dongsheng Li, Benjamin Busam, Slobodan Ilic

#### 3. Affiliation: 
Hao Yu, Mahdi Saleh, and Benjamin Busam are affiliated with TU Munich (慕尼黑工业大学).

#### 4. Keywords: 
Point cloud matching, rotation invariance, attention mechanism, encoder-decoder architecture, global transformer.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Rotation-Invariant_Transformer_for_Point_Cloud_Matching_CVPR_2021_paper.html  Github: https://github.com/haoyu94/RoITr

#### 6. Summary : 
- (1):本文研究点云匹配中的旋转不变性问题，提出了RoITr，一种旋转不变的Transformer模型，以应对点云匹配任务中的姿态变化。 
- (2):过去的深度匹配方法通常通过数据增强来获得旋转不变性，但是这些方法通常在面对罕见的旋转时表现不稳定。本文提出的RoITr模型通过引入基于点对特征（PPF）的坐标的注意力机制来描述姿态不变的几何形状，并构建了一种新的基于注意力的编码器-解码器架构。此外，本文还提出了一种全局Transformer，通过自注意机制学习旋转不变的跨帧空间感知，从而显著提高了特征的区分度，并使模型对低重叠性更加鲁棒。 
- (3):本文的创新点在于提出了一种基于PPF坐标的注意力机制，以及一种全局Transformer，这两种方法都能够有效地解决点云匹配中的旋转不变性问题。实验结果表明，RoITr在刚性和非刚性公共基准测试中均优于所有现有的最先进模型，并且在挑战性的3DLoMatch基准测试中，RoITr在Inlier Ratio和Registration Recall方面至少比现有方法高13和5个百分点。 
- (4):本文的方法在点云匹配任务中取得了优异的性能，支持其旨在解决点云匹配中的旋转不变性问题的目标。
#### 7. 方法详细介绍：
本文提出了一种旋转不变的点云匹配方法，命名为RoITr。RoITr包含两个级别：局部和全局。在局部级别，引入了一个基于点对特征（PPF）坐标的注意力机制，以描述姿态不变的几何形状。在其上构建了一个基于注意力机制的编码器-解码器架构，以旋转不变的方式学习高度代表性的局部几何形状。在全局级别，引入了一个具有旋转不变的跨帧位置感知能力的全局变换器，以促进特征的区分度。

具体步骤如下：
1. 局部特征提取阶段：使用PointNet++编码器-解码器网络提取局部特征并生成一组超点。
2. 全局上下文聚合阶段：设计全局变换器以聚合超点的上下文信息并生成一对三元组。
3. 粗到细的匹配策略：使用粗到细的匹配策略获取点对应关系。

#### 8. 实验设置：
本文在两个基准测试集上进行了评估：3DMatch和3DLoMatch。对于刚性匹配，数据被分成3DMatch（> 30%重叠）和3DLoMatch（10%∼30%重叠）。为了评估对任意旋转的鲁棒性，将完整范围的旋转分别添加到每个点云对的两个帧中。

#### 9. 实验结果和分析：
本文的方法在刚性和非刚性基准测试集上均取得了最先进的性能。对于3DMatch数据集，所提出的方法实现了0.68的内点比率和0.95的特征匹配召回率，优于以前的最先进方法。对于3DLoMatch数据集，所提出的方法实现了0.47的内点比率和0.87的特征匹配召回率，也优于以前的最先进方法。所提出的方法的运行时间与以前的最先进方法相当。


# Paper:778     基于面部对称性先验的3D GAN反演



#### 1. Title: 
3D GAN Inversion with Facial Symmetry Prior

#### 2. Authors: 
Fei Yin, Yong Zhang, Xuan Wang, Tengfei Wang, Xiaoyu Li, Yuan Gong, Yanbo Fan, Xiaodong Cun, Ying Shan, Cengiz ÈOztireli, Yujiu Yang

#### 3. Affiliation: 
深圳清华大学国际研究生院

#### 4. Keywords: 
3D GAN, inversion, facial symmetry, neural rendering, multi-view consistency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了3D GAN inversion的问题，即如何将一个单眼图像投影到预训练的生成器的潜在空间中，以实现自由视角一致的合成和编辑。然而，由于缺乏多视角的几何和纹理监督，直接应用2D GAN inversion方法会导致几何崩溃和模糊的合成结果。
 
- (2):过去的方法主要是将2D GAN inversion方法直接转化为3D GAN inversion，但这种方法忽略了几何形状的正确性。本文提出了一种新的方法，通过引入面部对称性先验来促进3D GAN inversion。该方法利用镜像图像作为额外的监督，以避免几何崩溃。此外，本文还使用深度引导的3D变形来生成伪图像，以提供额外的纹理监督。本文的方法在图像重建和编辑方面取得了优异的定量和定性评估结果。

- (3):本文提出了一种新的3D GAN inversion方法，通过引入面部对称性先验来促进3D GAN inversion。该方法利用镜像图像作为额外的监督，以避免几何崩溃。此外，本文还使用深度引导的3D变形来生成伪图像，以提供额外的纹理监督。本文的方法在图像重建和编辑方面取得了优异的定量和定性评估结果。

- (4):本文的方法在单眼图像重建和编辑方面取得了优异的定量和定性评估结果，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种基于3D GAN反演的人脸重建方法，引入了面部对称先验。该方法采用深度引导变形来创建伪监督，以提高新视角下的纹理质量。深度正则化用于确保形状正确性并防止过拟合。采用两阶段优化进一步提高纹理和重建域外细节。具体步骤如下：
1. 利用原始图像和镜像图像优化潜在编码w，得到粗略的几何形状。
2. 通过深度引导的3D变形生成不同视角的伪图像，提高新视角下的纹理质量和几何形状。
3. 通过联合优化几何和纹理，优化生成器参数θ，以实现身份保护和精细几何形状。
4. 引入几何正则化约束，避免模型退化为生成平坦几何形状。

#### 8. 实验设置：
本文的实验采用了CelebA-HQ和MEAD等人脸数据集，评估指标包括均方误差（MSE）、感知相似性损失（LPIPS）、结构相似性（MS-SSIM）和身份相似性（ID）。实验环境为PyTorch深度学习框架，使用了NVIDIA Tesla V100 GPU进行训练和测试。

#### 9. 实验结果与分析：
本文提出的方法在重建质量和身份保护方面均优于现有的2D反演方法。实验结果表明，该方法在处理不对称情况时具有较好的鲁棒性。同时，该方法还可以与其他编辑方法灵活结合，具有较好的应用前景。


# Paper:779     对话必须继续：通过生成自训练改进视觉对话



#### 1. Title: 
The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training

#### 2. Authors: 
Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang

#### 3. Affiliation: 
Gi-Cheon Kang, Jin-Hwa Kim, and Byoung-Tak Zhang are affiliated with Seoul National University, South Korea.

#### 4. Keywords: 
Visual dialog, self-training, semi-supervised learning, multimodal conditional text generation, out-of-distribution detection.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kang_The_Dialog_Must_Go_On_Improving_Visual_Dialog_via_Generative_CVPR_2021_paper.html  Github: https://github.com/gicheonkang/gst-visdial

#### 6. Summary : 
- (1):本文研究了视觉对话系统的半监督学习方法，通过生成自训练数据来扩展模型的知识，以提高视觉对话的性能。
 
- (2):过去的方法主要是通过监督学习或预训练来训练对话代理人，但是这些方法的数据量有限，且需要大量的人工标注。本文提出了一种半监督学习方法，称为生成自训练（GST），通过生成对于Web上未标注图像的合成对话，来扩展模型的知识。然而，这种方法存在两个关键挑战：第一，生成的对话数据可能存在噪声，需要一种鲁棒的训练方法；第二，生成的对话数据的目标输出比之前的研究更加复杂，需要考虑多模态和序列性质。本文提出了基于困惑度的数据选择和多模态一致性正则化方法，以有效地训练具有噪声的对话数据。

- (3):本文提出了一种新的学习策略，称为生成自训练（GST），通过生成对于Web上未标注图像的合成对话，来扩展模型的知识。具体而言，GST首先通过检测来自Web的图像，检索出领域内的图像，然后通过多模态条件文本生成生成关于这些图像的合成对话。最后，GST使用合成和原始的VisDial数据来训练对话代理人。GST成功地扩展了合成VisDial数据，从而减轻了扩大人工标注VisDial数据的需求，这是一项代价高昂且耗时的任务。本文的主要贡献有三个方面：首先，提出了生成自训练（GST）方法，通过生成多轮视觉QA数据来有效利用未标注的Web图像；其次，实验表明，GST在VisDial v1.0和v0.9数据集上均取得了新的最优结果；第三，为了验证GST的鲁棒性，我们评估了我们提出的方法在三种不同的视觉和文本对抗攻击下的表现。

- (4):本文提出的GST方法通过生成自训练数据来扩展模型的知识，以提高视觉对话的性能。实验结果表明，GST在VisDial v1.0和v0.9数据集上均取得了新的最优结果，并且在对抗攻击下表现出鲁棒性。因此，本文的方法可以有效地扩展模型的知识，提高视觉对话的性能。
#### 7. 方法详细介绍：
本文提出了一种半监督学习方法，称为生成式自训练（Generative Self-Training，GST），用于视觉对话任务。该方法首先通过异常检测从Web图像数据集Conceptual 12M中检索出领域内的图像，然后通过多模态条件文本生成生成关于这些图像的合成对话。最后，使用金标准和银标准数据集对学生模型进行训练，其中银标准数据集是由合成对话生成的。为了提高合成对话的鲁棒性，本文提出了基于困惑度的数据选择和多模态一致性正则化方法。

具体步骤如下：
1. 使用有标签数据集L训练一个教师模型PT和一个提问者模型PQ。
2. 使用异常检测模型从Conceptual 12M数据集中检索出无标签图像U。
3. 使用教师模型PT和提问者模型PQ生成关于无标签图像的合成对话，生成合成对话数据集。
4. 使用金标准和银标准数据集对学生模型PS进行训练，其中银标准数据集是由合成对话生成的。同时，使用基于困惑度的数据选择和多模态一致性正则化方法提高合成对话的鲁棒性。

#### 8. 实验设置：
本文在VisDial v1.0和v0.9数据集上进行了实验，数据集由两个工人之间的AMT聊天收集，聊天内容为关于MS-COCO图像的标题和十个QA对的序列。使用多种评估指标，包括平均倒数排名（MRR）、召回率@k（R@k）、平均排名（Mean）和归一化折扣累积增益（NDCG）。

#### 9. 实验结果和分析：
本文提出的方法GST在所有评估指标上都显著优于所有比较方法。与最先进的模型相比，学生模型在VisDial v0.9数据集上提高了MRR 3.20％和R@1 3.26％。在VisDial v1.0数据集上，NDCG提高了1.61％，MRR提高了0.97％。GST在低数据情况下的表现尤为突出，NDCG提高了11.09个绝对点。学生模型在不常见的问题类型（例如计数和时间/地点）中获得了更多的收益。本文还展示了对抗鲁棒性结果，并且所提出的学生模型在对抗攻击方面表现优异。


# Paper:780     通过语言引导采样学习视觉表示



#### 1. Title: 
Learning Visual Representations via Language-Guided Sampling

#### 2. Authors: 
Mohamed El Banani, Karan Desai, Justin Johnson

#### 3. Affiliation: 
Mohamed El Banani: University of Michigan (密歇根大学)

#### 4. Keywords: 
Visual representation learning, contrastive learning, language-guided sampling, self-supervised learning, pre-trained language models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2021_paper.html  Github: https://github.com/mbanani/lgssl

#### 6. Summary : 
- (1):本文的研究背景是视觉表示学习，旨在学习能够推广到其他数据集的视觉表示。
- (2):过去的方法主要是基于图像的对比学习，但是这些方法忽略了概念上的相似性，本文提出了一种基于语言相似性的对比学习方法，通过使用预训练的语言模型来指导视觉学习，从而获得更好的特征表示。
- (3):本文提出了一种基于语言相似性的对比学习方法，通过使用预训练的语言模型来采样语义上相似的图像对进行对比学习。该方法不需要新的模型或损失函数，而是提出了一种新的图像采样方法，适用于各种学习视觉表示的方法和损失函数。
- (4):在一系列实验中，本文的方法在线性探针和few-shot分类等下游分类任务上表现优于基线方法。本文的方法通过使用语言相似性来指导视觉学习，从而获得更好的特征表示，这证明了语言相似性是一种有效的概念相似性的代理。
#### 7. 方法详细介绍：
本文提出了一种基于语言引导的对比学习方法，使用语言模型寻找概念上相似的图像对进行视觉学习。该方法使用预训练的语言模型来引导学习，而不是直接最小化跨模态损失。该方法通过一系列对比实验进行评估，并与常用的表示学习范例在下游分类任务的泛化性能上进行比较。具体步骤包括：
1. 使用预训练的语言模型计算相似的图像对。
2. 使用SBERT模型对预训练的语言模型进行微调，以更好地捕捉语义相似性。
3. 在语言嵌入空间中使用现代相似性搜索库快速执行最近邻搜索，以对比学习进行训练。

#### 8. 实验设置：
本文在公开数据集上进行了一系列实验，使用标准的ResNet-50骨干网络。使用AdamW优化器进行训练，学习率为10^-3，权重衰减为10^-2。使用余弦学习调度程序进行训练，进行5000个热身步骤。使用512的批量大小进行250k步的训练。评估使用线性探针和少样本分类在15个分类数据集上进行。

#### 9. 实验结果和分析：
本文提出的方法在视觉表示学习中优于先前的自监督和图像-文本对比模型。使用语言引导的采样方法显示出对其他预训练方法的泛化改进，并可应用于其他数据集和预训练方法。结果使用线性探针和少样本分类在15个分类数据集上进行评估。实验结果表明，语言引导学习是比图像-文本对比学习更好的训练目标。此外，本文分析了最近邻采样和基于聚类的方法的局限性，并表明语言引导的对比学习优于它们。


# Paper:781     自动高分辨率电线分割和去除



#### 1. Title: 
Automatic High Resolution Wire Segmentation and Removal

#### 2. Authors: 
Mang Tik Chiu, Xuaner Zhang, Zijun Wei, Yuqian Zhou, Eli Shechtman, Connelly Barnes, Zhe Lin, Florian Kainz, Sohrab Amirghodsi, Humphrey Shi

#### 3. Affiliation: 
第一作者：Mang Tik Chiu，UIUC

#### 4. Keywords: 
Wire segmentation, wire removal, high-resolution images, deep learning, computer vision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chiu_Automatic_High_Resolution_Wire_Segmentation_and_Removal_CVPR_2021_paper.html  Github: https://github.com/adobe-research/auto-wire-removal

#### 6. Summary : 
- (1):本文研究的背景是高分辨率图像中的电线和电缆的自动分割和去除问题，这是一项重要的图像修复任务。

- (2):过去的方法需要手动分割和去除电线，非常耗时且容易出错。本文提出了一种自动化的方法，利用深度学习技术，通过两个阶段的模型来实现电线的准确分割和去除。本文的方法能够处理电线的稀疏性、长度和细度等特点，而这些特点是传统分割任务难以处理的。

- (3):本文提出了一种两阶段的模型，利用全局和局部上下文来准确地分割高分辨率图像中的电线。在去除电线时，采用了一种基于瓦片的修复策略，结合预测的分割掩码来实现电线的去除。此外，本文还提出了第一个电线分割基准数据集WireSegHR，用于评估电线分割算法的性能。

- (4):本文的方法在多个数据集上进行了测试，结果表明本文的方法在电线分割和去除任务上取得了很好的性能，能够有效地去除高分辨率图像中的电线和电缆，具有很好的实用性。
#### 7. 方法详细介绍：
本文提出了一种两阶段的高分辨率电线语义分割模型。该模型包括共享特征提取器、粗分割模块和细分割模块。共享特征提取器提取图像特征，粗分割模块从整个图像中捕获全局上下文信息，并突出可能包含电线的图像区域。细分割模块通过仅查看可能包含电线的局部补丁来实现高分辨率电线分割。模型输入为RGB图像，与条件概率图、最小值和最大值滤波亮度通道进行拼接。模型在5000个训练图像上进行训练，并在500个验证和500个测试图像上进行测试。最终损失是全局损失和局部损失的总和，并使用“poly”学习率调度以0.9的幂进行训练。在推理期间，模型将全局图像大小和局部补丁大小都设置为1024。

#### 8. 实验设置：
本文提出了WireSegHR数据集，该数据集是第一个电线分割基准数据集。数据集包含具有不同场景内容和电线外观的高分辨率图像。作者使用1000张带有合成电线掩码的图像和100张真实世界图像对模型进行训练。作者使用512x512的窗口大小和32像素重叠的基于瓦片的方法进行完整分辨率图像的推理。作者将其方法与几种广泛使用的对象语义分割和高分辨率语义分割模型进行了比较，包括DeepLabv3+、CascadePSP、MagNet和ISDNet。

#### 9. 实验结果和分析：
本文提出的方法在电线分割和修复方面均取得了最佳效果。作者报告了电线IoU、F1分数、精度和召回率进行定量评估，并对三个尺度的图像进行了电线IoU评估：小、中、大。作者还比较了其他电线修复模型，包括PatchMatch、DeepFillv2、CMGAN、FcF和LDM，结果表明本文提出的方法在质量和效率方面均优于其他方法。然而，在电线与周围结构/背景混合严重或极端光照条件下，仍存在一些具有挑战性的情况，本文提出的方法无法准确地分割电线。


# Paper:782     无监督学习先验知识下，从单个稀疏点云中推断有符号距离函数



#### 1. Title: 
Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors

#### 2. Authors: 
Chao Chen, Yu-Shen Liu, Zhizhong Han

#### 3. Affiliation: 
第一作者：清华大学软件学院

#### 4. Keywords: 
Signed distance functions, point clouds, surface reconstruction, neural network, unsupervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Unsupervised_Inference_of_Signed_Distance_Functions_From_Single_Sparse_Point_CVPR_2022_paper.html  Github: https://github.com/chenchao15/NeuralTPS

#### 6. Summary : 
- (1):本文研究点云中的有符号距离函数（SDF）的推断问题。SDF是一种描述三维表面的有符号距离场，可以用于表面重建等任务。但是，目前的方法需要大规模监督学习的先验知识，且对于极度稀疏的点云效果不佳。因此，本文提出了一种无监督学习的方法，直接从单个稀疏点云中推断SDF，不需要使用有符号距离监督、学习的先验知识甚至法向量。

- (2):目前的方法主要依赖于从大规模监督学习中学习的先验知识来推断点云中的SDF。然而，这些学习到的先验知识在面对训练时未见过的各种几何变化时，往往无法很好地推广。一些方法尝试直接从单个点云中推断SDF，但需要密集的点云才能保证推断性能，这严重限制了它们在稀疏点云中的性能。因此，如何从稀疏点云中推断SDF以实现更好的推广仍然是一个挑战。

- (3):本文提出了一种神经网络，以端到端的方式学习表面参数化和SDF推断，从而直接从单个稀疏点云中推断SDF。为了弥补稀疏性，我们将参数化表面作为粗糙表面采样器，提供许多粗糙表面估计，根据这些估计，我们使用基于薄板样条（TPS）的网络以统计方式推断SDF。我们的方法显著提高了在未见点云中的推广能力和准确性。实验结果表明，我们的方法在合成数据集和真实扫描数据集上的表面重建任务中优于最新的方法。

- (4):本文提出的方法在稀疏点云的表面重建任务中取得了优异的性能，超过了最新的方法。
#### 7. 方法详细介绍：
本文提出了一种无监督的方法，可以从单个稀疏点云中推断出带符号的距离函数（SDFs），而无需使用带符号距离监督、学习的先验知识或法向量。该方法包括学习表面参数化和SDF推断。在表面参数化中，使用多层感知机（MLP）将2D样本映射到3D点，生成3D图表。在SDF推断中，引入神经薄板样条（Neural Thin Plate Splines，NeuralTPS）来推断平滑函数的SDF。我们学习了一个最优特征空间，可以将其进一步映射到带符号距离，通过特征空间中的TPS插值使用表面点的特征来回归查询处的带符号距离。我们将查询处的带符号距离公式化为TPS插值和MLP预测的位移之和。我们通过调整参数使用损失函数来优化表面参数化和SDF推断，该损失函数平衡了Chamfer距离损失、表面点的MSE损失和采样点的置信度加权损失。

#### 8. 实验设置：
本文在ShapeNet和KITTI数据集上评估了所提出的方法。ShapeNet数据集包含13个3D模型类别，KITTI数据集包含汽车、行人和道路的点云。评估指标为Chamfer距离（CD）和法向一致性（NC）。

#### 9. 实验结果和分析：
本文在表面重建方面对刚性形状和非刚性形状在ShapeNet和D-FAUST数据集上进行了评估。使用学习的隐式函数，我们提取网格作为重建表面。我们使用L1 Chamfer距离、L2 Chamfer距离和法向一致性来评估重建表面的准确性，其中我们在重建表面和地面真实表面上分别采样了10万个点来测量误差。实验结果表明，所提出的方法在准确性和重建表面的平滑性方面优于最新的方法。视觉比较也证明了所提出方法在重建更完整、更平滑的表面方面的优势。本文还在3D Scene和KITTI数据集中对所提出的方法进行了表面重建的评估，重建表面显示出更准确的几何形状和更完整的表面，具有更多的几何细节，相比最先进的方法表现更好。


# Paper:783     可测量多样性的异常检测



#### 1. Title: 
Diversity-Measurable Anomaly Detection

#### 2. Authors: 
Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen

#### 3. Affiliation: 
Chinese Academy of Sciences (中国科学院)

#### 4. Keywords: 
Anomaly detection, reconstruction-based methods, diversity measurement, pyramid deformation module, information compression module

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diversity-Measurable_Anomaly_Detection_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的是视觉异常检测问题，旨在从已知类别中检测出异常样本。由于异常数据分布多样且难以收集，因此需要在无监督的情况下仅基于正常样本构建模型，以实现对正常和异常样本的高区分度检测。
 
- (2):过去的方法主要是基于重构的方法，通过抑制异常的泛化能力来实现其目的。然而，多样的正常模式因此也无法很好地重构。虽然一些方法通过建模样本多样性来缓解这个问题，但由于异常信息的不良传递，它们会遭受捷径学习的问题。为了更好地处理这个权衡问题，本文提出了多样性可测量的异常检测（DMAD）框架，以增强重构多样性，同时避免对异常的不良泛化。为此，我们设计了金字塔变形模块（PDM），它从重构的参考到原始输入估计多尺度变形场，从而模拟多样的正常和异常样本，并测量异常的严重程度。与信息压缩模块集成，PDM从本质上解耦了变形和典型嵌入，并使最终的异常得分更可靠。 

- (3):本文提出了一种多样性可测量的异常检测框架，以增强重构模型在重构多样正常和检测未知异常之间的权衡。我们设计了金字塔变形模块（PDM）来实现多样性测量，其中变形信息明确地与紧凑的类别原型分离，并且由此产生的多样性测量与异常性呈正相关。我们的方法在视频监控和工业缺陷检测方面进行了评估。为了将DMAD应用于后一种情况，我们提出了PPDM的变体，以处理纹理重构中的误报问题。广泛的实验结果验证了我们方法的有效性。此外，我们的方法即使在面对污染数据和类似异常的正常样本时也能很好地工作。 

- (4):本文提出的方法在视频监控和工业缺陷检测方面取得了良好的性能，能够更好地权衡重构多样正常和检测未知异常之间的权衡。DMAD框架包括金字塔变形模块（PDM）和信息压缩模块（ICM），PDM模拟多样的正常和异常样本，并测量异常的严重程度。ICM学习压缩表示为稀疏原型。与PDM集成，DMAD从本质上解耦了变形信息，并使最终的异常得分更具有区分度。
#### 7. 方法详细介绍：
本文提出了一种多样性可测量的异常检测框架，称为DMAD。该框架包括两个阶段：记忆网络和多样性感知模块。记忆网络用于存储类别特定的参考模式，多样性感知模块用于通过层次性地调整参考模式来获得多样化的重构。多样性感知模块包括金字塔变形模块（PDM）和金字塔渐进变形模块（PPDM）。PDM用于在不同尺度上调整参考模式，PPDM用于逐步调整参考模式以获得多样化的重构。在训练过程中，通过最小化包括重构损失、紧凑性损失、循环一致性损失和前向-后向变形约束的损失函数来训练异常检测模型。在推理阶段，通过使用反向变形进行逆采样来获得实际位置的异常图。分别计算图像级异常分数和像素级异常分数。

#### 8. 实验设置：
本文在两种数据集上进行了评估：监控视频和工业图像。监控视频包括Ped2、Avenue和ShanghaiTech，而工业图像包括MVTec。输入图像被调整为256×256，并归一化为[-1,1]范围内的值。视频异常检测的历史长度为4，图像为0。模型通过AdamW进行优化，学习率通过CosineAnnealingLR策略进行衰减。每个数据集的训练轮数不同，批量大小设置为8。

#### 9. 实验结果和分析：
本文提出的方法在视频异常检测任务上表现出色，在Ped2、Avenue和ShanghaiTech数据集上实现了最先进的性能。在MVTec数据集上，该方法在检测和定位任务上也优于SOTA方法，而不需要从训练数据中记忆大量嵌入。消融实验表明，该方法对损失函数中的超参数γ3具有鲁棒性，循环一致性约束Lcyc也是PPDM避免退化解的必要部分。


# Paper:784     多重质心任务描述符用于动态类增量推理



#### 1. Title: 
Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference

#### 2. Authors: 
Tenghao Cai, Zhizhong Zhang, Xin Tan, Yanyun Qu, Guannan Jiang, Chengjie Wang, Yuan Xie

#### 3. Affiliation: 
第一作者：华东师范大学计算机科学与技术学院

#### 4. Keywords: 
Incremental learning, task-incremental learning, class-incremental learning, dynamic inference, multi-centroid task descriptor

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cai_Multi-Centroid_Task_Descriptor_for_Dynamic_Class_Incremental_Inference_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是增量学习中的动态类增量推理问题，主要关注的是任务ID对于类增量学习的影响。任务ID是一种强先验知识，可以用于任务增量学习，但在类增量学习中并没有得到充分利用。因此，本文提出了一种动态推理网络，通过预测任务ID来区分不同的任务，从而提高类增量学习的性能。

- (2):过去的方法主要分为两类：任务增量学习和类增量学习。任务增量学习可以利用任务ID来缩小标签空间，因此性能通常比类增量学习更好。类增量学习的方法通常采用重放策略或正则化策略来缓解遗忘问题。然而，这些方法都没有充分利用任务ID的信息。本文提出的方法通过预测任务ID来区分不同的任务，从而提高类增量学习的性能。

- (3):本文提出了一种多重质心任务描述符，假设任务内的数据可以形成多个聚类。通过拉动相关样本-质心对并将其他样本推开来优化聚类中心，从而确保至少有一个质心接近给定的样本。为了选择相关对，本文使用类原型作为代理，并解决一个二分图匹配问题。在推理过程中，本文比较每个实例特征和任务描述符，然后找到最相关的分支进行推理。本文的方法在CIFAR-100、ImageNet-100和ImageNet-1000等数据集上进行了验证，取得了最先进的结果。

- (4):本文提出的方法在CIFAR-100-B0S50数据集上取得了72.41%的平均准确率，比DER方法高出3.40%。本文的方法通过预测任务ID来区分不同的任务，从而提高类增量学习的性能。
#### 7. 方法详细介绍：
本文提出了一种动态推理策略，旨在实现稳定性和可塑性之间的良好平衡。该方法包括门控网络、多重质心任务描述符和动态推理网络。门控网络通过假设任务内的数据可以形成多个聚类来预测任务ID，并通过拉动相关的样本-质心对并将其他样本-质心对推开来优化聚类中心。为了选择相关的样本-质心对，该方法使用类原型作为代理，并解决二分图匹配问题。在推理过程中，动态推理网络比较每个实例特征和任务描述符，然后找到最相关的分支进行推理。整个框架独立于基线进行训练，可以灵活地集成到训练好的DER或其他多分支模型中。

具体步骤如下：
1. 训练阶段：
（1）引入可训练的多重质心任务描述符来抽象第t个任务。
（2）使用一种新颖的任务级匈牙利二分图匹配策略来选择每个样本的匹配质心。
（3）使用度量损失LCen来优化匹配的样本-质心对，拉动相关的样本-质心对并将其他样本-质心对推开，使质心具有代表性。
（4）通过比较实例特征和描述符，获得知识蒸馏损失Lkd的相似度向量。

2. 推理阶段：
（1）比较每个实例特征和任务描述符，找到最相关的分支进行推理。

#### 8. 实验设置：
本文在三个常用基准数据集（CIFAR-100、ImageNet-100和ImageNet-1000）上验证了所提出的方法。实验数据集被分成不同的任务，每个任务包含不同数量的类别。使用回忆存储器来存储每个任务的图像。评估协议遵循以前的工作，并以top-1和top-5准确度为指标。实现细节包括使用ResNet-18作为特征提取器和优化器设置等。

#### 9. 实验结果和分析：
所提出的方法在CIFAR-100、ImageNet-100和ImageNet-1000基准数据集上均取得了最先进的结果。例如，在CIFAR100-B0S50上，该方法的平均准确率为72.41%，比DER高3.40%。实验结果表明，与最先进的方法相比，所提出的方法具有更好的效果。在CIFAR-100测试集上，使用不同数量的激活分支进行实验，得到的准确率和推理时间如图6(a)所示。此外，图6(b)展示了CIFAR100-B0S10上前60个类别的类-质心混淆矩阵。图7展示了CIFAR100-B0S20上前2个任务的t-SNE可视化，其中左图表示单一质心，中间图表示多重质心，右图表示类原型。


# Paper:785     ProD: 用于跨域few-shot图像分类的提示-解开领域知识



#### 1. Title: 
ProD: Prompting-to-disentangle Domain Knowledge for Cross-domain Few-shot Image Classification

#### 2. Authors: 
Tianyi Ma, Yifan Sun, Zongxin Yang, Yi Yang

#### 3. Affiliation: 
第一作者：澳大利亚科技大学；其他作者：百度公司、浙江大学

#### 4. Keywords: 
Few-shot learning, cross-domain, domain generalization, domain adaptation, prompting mechanism

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ma_ProD_Prompting-to-Disentangle_Domain_Knowledge_for_Cross-domain_Few-shot_Image_Classification_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究跨域情况下的few-shot图像分类问题，即训练集和测试集之间存在领域差异，导致分类准确率下降的问题。

- (2):过去的方法主要有领域泛化和领域自适应两种，但是在few-shot学习中，领域泛化方法的效果更好。本文提出了一种新的方法ProD，通过prompting机制来解决领域差异问题，同时将领域通用和领域特定的知识分离出来，从而提高few-shot图像分类的性能。

- (3):本文提出的ProD方法采用了多领域训练和标准卷积神经网络提取骨干特征的常见做法，关键在于使用transformer中的prompting机制来将领域通用和领域特定的知识分离出来。具体来说，ProD将DG和DS prompt与骨干特征拼接，并将其输入到轻量级transformer中，从而输出DG和DS特征。其中，DG prompt是可学习的，由所有训练领域共享，而DS prompt是根据当前领域的特征动态生成的。本文的创新点在于同时使用两个prompt来提取领域通用和领域特定的知识，从而实现了prompting-to-disentangle。此外，本文还提出了三个关键点来缓解领域差异问题，即共享单个DG prompt、使DG prompt对所有训练领域中性、从当前领域获取DS prompt。

- (4):本文在CUB、mini-ImageNet和tiered-ImageNet数据集上进行了实验，结果表明，ProD方法在5-way 5-shot任务上取得了最好的性能，将准确率从73.56%提高到了79.19%。这表明ProD方法可以有效地缓解领域差异问题，提高few-shot图像分类的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Prompting-to-Disentangle (ProD)的方法，用于跨域few-shot图像分类。ProD采用多域训练方案，使用卷积神经网络（ResNet-10）提取骨干特征。ProD的关键点是使用transformer中的提示机制来从骨干特征中分离出领域通用（DG）和领域特定（DS）知识。ProD将DG和DS prompt与骨干特征连接起来，并将它们馈送到轻量级transformer中。DG prompt是可学习的，由所有训练域共享，而DS prompt是根据感兴趣的域即时生成的。DG/DS prompt的输出分别在训练期间由全局/局部分类头进行监督，并在推理时连接为最终表示。

具体步骤如下：
1. 使用ResNet-10提取骨干特征。
2. 将DG和DS prompt与骨干特征连接起来，并将它们馈送到transformer中。
3. 在训练期间，同时从头开始训练基础模型（CNN和transformer头）和prompt。
4. 在测试期间，使用DG和DS prompt的连接输出作为最终表示。

#### 8. 实验设置：
本文在miniImageNet和四个细粒度数据集（CUB，Cars，Plantae和Places）上进行了实验。使用留一法，其中一个细粒度数据集用于推理，另外三个细粒度数据集以及miniImageNet用于训练。使用标准的5-way 1-shot和5-way 5-shot测试程序进行评估。在C-way K-shot测试阶段期间，transformer和CNN骨干特征提取器都被冻结。

#### 9. 实验结果和分析：
ProD方法显著提高了跨域few-shot图像分类的基线。在CUB数据集上，ProD的最佳配置实现了79.19%的5-way 5-shot准确率，与现有技术水平相当。本文提供了有关提示大小、transformer深度和计算效率影响的详细分析。在CUB数据集上，ProD将5-way 5-shot识别准确率从73.56%提高到79.19%，创造了新的技术水平。


# Paper:786     使用比例场进行单视图场景尺度估计



#### 1. Title: 
Single View Scene Scale Estimation using Scale Field

#### 2. Authors: 
Byeong-Uk Lee, Jianming Zhang, Yannick Hold-Geoffroy, In So Kweon

#### 3. Affiliation: 
Byeong-Uk Lee and In So Kweon are affiliated with KAIST, Republic of Korea.

#### 4. Keywords: 
Single view scene scale estimation, scale field, camera calibration, single view metrology, depth estimation.

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1): This paper focuses on the problem of estimating the metric scale of a scene for a single unconstrained image, which is a challenging task in computer vision.
 
- (2): Previous methods for scale estimation often treat scale as an undetermined factor, and rely on global parameters like horizon line, field of view, and camera height. However, these parameters are difficult to predict based on visual features and tend to overfit to a certain dataset. In contrast, this paper proposes a novel way of representing scene scale using a pixel-wise pixel-to-metric 2D map called Scale Field, which defines the local pixel-to-metric conversion ratio along the gravity direction on all the ground pixels. This representation resolves the ambiguity in camera parameters and allows for the collection of scale annotations on arbitrary images from human annotators. 

- (3): The proposed method trains a single image scene scale estimation network on calibrated panoramic image data and in-the-wild human annotated data, which generates robust scale fields on a variety of images. The scale field can be utilized in various 3D understanding and scale-aware image editing applications. The major contributions of this paper are the introduction of Scale Field, a novel representation of scene scale information, and a pipeline to annotate scale fields on web images, which allows for the collection of a diverse set of training samples. 

- (4): The proposed method achieves state-of-the-art performance on single view scene scale estimation, and the predicted scale field can be used for various 3D scene understanding and scale-aware image editing tasks. The performance supports the goals of the paper, which is to provide a robust and generalizable scale estimation network that can recover the scene scale from a single unconstrained image.
#### 7. 方法详细介绍：
本文提出了一种使用比例场的单张图像场景尺度估计网络。数据集生成方案涉及使用全景图像和收集网络图像注释。网络架构由基于Transformer的特征提取器和用于2D场估计的解码器架构组成。全局参数的估计头是一个全连接层。场景尺度估计网络有三个变体：地面到地平线和比例场估计、地面到地平线和相机高度估计，以及地平线、视场和相机高度估计。具体步骤包括：
1. 使用全景图像和网络图像注释生成比例场数据集。
2. 构建基于Transformer的特征提取器和用于2D场估计的解码器架构。
3. 训练网络，使用余弦相似度、RMSE误差和REL误差等三个指标评估模型性能。
4. 使用训练好的模型进行场景尺度估计。

#### 8. 实验设置：
网络训练了50个epochs，初始学习率为1e-4，学习率在15000、30000和60000次迭代时衰减到10%。训练使用了8个NVIDIA A100 GPU，每个GPU每次迭代加载一个分辨率为256×256的48张图像批次。数据加载器被设计为在批次中分配来自每个数据集的等量样本。本文在Stanford2D3D、Matterport3D和网络图像测试集上评估了三个指标：地面到地平线估计的余弦相似度、比例场预测的RMSE误差和相机高度的REL误差。

#### 9. 实验结果和分析：
本文提出了一种使用比例场的单张图像场景尺度估计方法。结果表明，预测局部场而不是全局参数可以提高尺度估计的鲁棒性。将场景尺度表示为局部和密集的比例场是有效的。该方法在场景尺度和物体中心图像上都表现出可靠的尺度估计结果。比例场估计结果可用于比例感知的图像编辑任务，如对象插入和比例感知的图像合成，以及3D场景理解任务，如度量高度测量和地平面高度。该网络可以处理相机高度较小和较大的极端情况。


# Paper:787     通过提示类比的语言引导视频音乐推荐



#### 1. Title: 
Language-Guided Music Recommendation for Video via Prompt Analogies

#### 2. Authors: 
Daniel McKee, Justin Salamon, Josef Sivic, Bryan Russell

#### 3. Affiliation: 
第一作者：University of Illinois at Urbana-Champaign（伊利诺伊大学香槟分校）

#### 4. Keywords: 
Music recommendation, video, natural language processing, language model, deep learning

#### 5. Paper: https://www.danielbmckee.com/language-guided-music-for-video  Github: None

#### 6. Summary : 
- (1):本文研究的背景是音乐推荐系统在短视频制作中的应用，目前的音乐推荐系统缺乏用户对音乐选择的自主控制，因此需要一种更加灵活的音乐推荐方法。

- (2):过去的音乐推荐方法主要基于视频的视觉内容和风格，而忽略了音乐本身所传达的信息。本文提出了一种新的方法，允许用户通过自由形式的自然语言来指导音乐选择。本文的方法创新性地使用了大规模语言模型和类比提示的方法来生成音乐描述，同时提出了一种基于Transformer的模型架构，能够将视频和文本输入表示融合起来，从而查询音乐样本。本文的方法在音乐检索方面取得了显著的性能提升。

- (3):本文的研究方法包括两个方面：一是使用类比提示的方法生成音乐描述，二是使用基于Transformer的模型架构将视频和文本输入表示融合起来，从而查询音乐样本。本文的创新点在于使用了大规模语言模型和类比提示的方法来生成音乐描述，同时提出了一种基于Transformer的模型架构，能够将视频和文本输入表示融合起来，从而查询音乐样本。

- (4):本文的方法在YT8M-MusicVideo数据集上进行了实验，结果表明，本文的方法能够在音乐检索方面取得显著的性能提升，同时能够匹配或超过基线音乐推荐模型的性能。本文的方法能够匹配视频内容和自然语言查询中描述的音乐风格、情绪和乐器，从而提高了音乐推荐的灵活性和用户控制能力。
#### 7. 方法详细介绍：
本文提出了一种三模态模型，包括视频编码器、音乐编码器和文本编码器，以及一个融合模块来结合视频和文本表示。模型使用InfoNCE损失函数来训练音乐和融合视频-文本嵌入之间的相似性，并引入文本dropout作为正则化机制。具体步骤如下：
1. 对于视频编码器，使用CLIP对视频帧进行编码，将每个10秒的视频分成多个片段，每个片段使用6帧每秒的平均CLIP嵌入特征进行编码。
2. 对于音乐编码器，使用DeepSim对音乐进行编码。
3. 对于文本编码器，使用Transformer架构进行编码。
4. 对于融合模块，使用Transformer架构将视频和文本表示结合起来，输出融合的视频-文本嵌入。
5. 使用线性投影层将所有输入基础特征编码为256维的嵌入向量，并选择256维作为编码视频、文本、音乐和融合视频-文本表示的输出维度。

#### 8. 实验设置：
本文使用YT8M-MusicVideo数据集进行实验，该数据集包含约10万个带有“音乐视频”标签的视频。本文还对YT8M-MusicVideo数据集的一个4,000个样本子集进行了人工提供的音乐文本描述的注释。本文使用召回率和中位数排名指标评估音乐检索性能。

#### 9. 实验结果和分析：
本文提出的ViML模型在音乐检索任务中表现出色，尤其是在Recall@5和Recall@10方面。使用prompt2text描述训练的模型表现最佳，表明大型语言模型在此任务上是强大的注释器。本文还提供了两个例子的定性结果，证明了所提出方法的有效性。


# Paper:788     使用状态空间变换器进行高效电影场景检测



#### 1. Title: 
Efficient Movie Scene Detection using State-Space Transformers

#### 2. Authors: 
Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, Gedas Bertasius

#### 3. Affiliation: 
Md Mohaiminul Islam: UNC Chapel Hill (北卡罗来纳大学教堂山分校)

#### 4. Keywords: 
Movie scene detection, State-Space Transformer, Self-attention, Long-range modeling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Islam_Efficient_Movie_Scene_Detection_Using_State-Space_Transformers_CVPR_2021_paper.html  Github: https://github.com/mohaiminul-islam/TranS4mer

#### 6. Summary : 
- (1):本文研究的是电影场景检测，需要对长时间的电影视频进行推理，而现有的视频识别模型通常只适用于短时间的视频分析，因此需要一种能够有效捕捉长电影视频依赖关系的模型。

- (2):现有的电影场景检测方法大多使用卷积神经网络(CNN)，这些模型通常无法捕捉长距离依赖关系。本文提出了一种名为TranS4mer的模型，它使用一种新颖的S4A构建块，将结构化状态空间序列(S4)和自注意力(A)层的优势结合起来，以有效地捕捉长电影视频中的依赖关系。

- (3):本文提出的TranS4mer模型首先使用自注意力来捕捉每个镜头内的短距离依赖关系，然后使用S4A块中的状态空间操作来聚合整个电影视频中的长距离依赖关系。该模型可以进行端到端训练，并且比标准Transformer模型快2倍，需要的GPU内存少3倍。在三个电影场景检测数据集上，TranS4mer模型的性能优于所有先前的方法。

- (4):本文的方法在三个电影场景检测数据集上均取得了最佳性能，同时在LVU数据集上的5个电影片段分类任务和Breakfast数据集上的过程活动分类任务中也取得了最佳性能。
#### 7. 方法详细介绍：
本文提出了一种名为TranS4mer的电影场景检测方法。该方法使用S4A块构建，结合了结构化状态空间序列（S4）和自注意力（A）层的优势。S4A块由两个模块组成：短程内部模块和长程间部模块。内部模块使用多层感知器（MLP）层和多头自注意力（MSA）层独立地对每个镜头的令牌序列进行建模。间部模块使用门控S4（GS4）层聚合长程间部线索。TranS4mer模型以N = 2m + 1个时间相邻的镜头作为输入，并输出镜头si的二进制预测。相邻的镜头为预测镜头si的边界提供了长程上下文线索。由于间部模块中的门控S4层，该模型可以高效地处理长序列。

#### 8. 实验设置：
本文在MovieNet、BBC和OVSD三个数据集上评估了TranS4mer模型。MovieNet数据集的注释子集分为190个、64个和64个电影用于训练、验证和测试。本文使用二元交叉熵损失进行微调，并使用四种数据增强技术：随机裁剪、翻转、颜色抖动和高斯模糊。模型以25个相邻的镜头作为输入，其中每个镜头包含3帧。本文使用8个NVIDIA RTX A6000 GPU训练模型。

#### 9. 实验结果与分析：
TranS4mer模型在MovieNet数据集上取得了最先进的性能，比之前最好的方法（BaSSL）提高了显著的3.38% AP。该模型还优于几种无监督和有监督方法，以及三个实现的基线：Transformer、TimeSformer和Vanilla S4。本文使用AP、mIoU、AUC-ROC和F1-Score等多个指标来报告结果。TranS4mer模型比之前的方法和标准的自注意力和状态空间模型更加内存高效和快速。


# Paper:789     BKinD-3D：从多视角视频中自监督学习的三维关键点发现



#### 1. Title: 
BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos

#### 2. Authors: 
Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John C. Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona

#### 3. Affiliation: 
Jennifer J. Sun: 加州理工学院
Lili Karashchuk: 华盛顿大学
Amil Dravid: 西北大学
Serim Ryou: SAIT
Sonia Fereidooni: 华盛顿大学
John C. Tuthill: 华盛顿大学
Aggelos Katsaggelos: 西北大学
Bingni W. Brunton: 华盛顿大学
Georgia Gkioxari: 加州理工学院
Ann Kennedy: 西北大学
Yisong Yue: 加州理工学院
Pietro Perona: 加州理工学院

#### 4. Keywords: 
3D pose estimation, self-supervised learning, multi-view videos, keypoint discovery, behavior analysis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_BKinD-3D_Self-Supervised_3D_Keypoint_Discovery_From_Multi-View_Videos_CVPR_2021_paper.html  Github: https://sites.google.com/view/b-kind/3d

#### 6. Summary : 
- (1):本文研究了在多视角视频中进行自监督三维关键点发现的方法，以实现无需二维或三维监督的三维姿态估计。这是一个重要的问题，因为手动标注姿态注释是昂贵且耗时的。
- (2):现有的关键点发现方法通常处理单个二维视图，不在三维空间中操作。而本文提出了一种新的方法，使用编码器-解码器架构和三维体积热图，在多视角视频中重建时空差异，同时在学习的三维骨架上施加关节长度约束，从而实现了无需二维或三维监督的三维关键点发现。这种方法在人类和大鼠的视频中发现了关键点，展示了三维关键点发现在行为研究中的潜力。
- (3):本文提出了一种名为BKinD-3D的方法，使用多视角视频中的自监督学习信号将信息编码到单个三维几何瓶颈中，以实现三维关键点发现。该方法不使用任何边界框或关键点注释作为监督。关键在于我们发现的关键点之间的链接，以发现连接点之间的连通性。我们的方法在多个生物的多视角视频上进行了验证，包括人类和大鼠。 
- (4):本文的方法在多视角视频中实现了无监督的三维关键点发现，从而实现了无需二维或三维监督的三维姿态估计。实验结果表明，该方法在多个生物的多视角视频中均取得了良好的性能，证明了其在行为研究中的潜力。
#### 7. 方法详细介绍：
本文提出了一种自监督的三维关键点发现方法，称为BKinD-3D。该方法使用体积瓶颈将多视角视频编码为三维特征体积，然后使用三维卷积神经网络解码特征体积以预测三维关键点。该方法还包括一个学习的长度约束，以鼓励一致的边缘长度和一个分离损失，以鼓励唯一的关键点。完整的训练目标是多视角时空重建损失、学习的长度约束和分离损失的总和。模型使用课程学习进行训练，并使用基于平均关节位置误差（MPJPE）和Procrustes对齐MPJPE（PMPJPE）的关键点回归进行评估。

#### 8. 实验设置：
本文在两个数据集上进行了评估：Human 3.6M和Rat7M。Human 3.6M是一个大规模的运动捕捉数据集，包含来自4个视角的视频，而Rat7M是一个包含6个视角的大鼠3D姿态数据集。模型使用数据集提供的相机参数从训练集中的视频进行训练，并计算所有3D关键点发现方法的结果，除非另有说明。

#### 9. 实验结果和分析：
本文提出的方法在Human3.6M和Rat7M数据集上均优于所有其他完全自监督的3D关键点发现方法。在所有开发用于3D关键点发现的方法中，使用体积瓶颈的BKinD-3D表现最佳。结果表明，BKinD-3D直接适用于发现新型模型生物的3D关键点，这些生物可能在外观或大小上有很大差异，而无需2D或3D监督。定性结果显示，发现的点和骨架是合理的，并且看起来类似于地面真实注释。


# Paper:790     SparsePose：稀疏视角相机姿态回归与细化



#### 1. Title: 
SparsePose: Sparse-View Camera Pose Regression and Refinement

#### 2. Authors: 
Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell

#### 3. Affiliation: 
第一作者：University of Toronto（多伦多大学）

#### 4. Keywords: 
Camera pose estimation, sparse-view images, 3D reconstruction, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2021_paper.html  Github: https://github.com/sinhasam/sparsepose

#### 6. Summary : 
- (1):本文研究的是在稀疏视角下的相机位姿估计问题，该问题在3D重建中是一个关键步骤。然而，现有的位姿估计方法在只有少量图像可用时往往会失败，因为它们依赖于在图像对之间鲁棒地识别和匹配视觉特征的能力。因此，本文提出了一种名为SparsePose的方法，可以从少于10个稀疏视角图像中恢复准确的相机姿态。

- (2):现有的方法通常是通过提取关键点并在输入图像之间匹配它们的局部特征来估计相机姿态。这些方法在少量图像的情况下往往会失败，因为它们是局部的，因此不能学习有关问题解决方案的先验知识。全局姿态优化方法依赖于可微分渲染技术，通过最小化光度重建误差来恢复相机姿态。相比之下，本文提出的方法通过在投影图像特征之间进行3D一致性来预测相机旋转和平移参数，从而避免了这些问题。

- (3):本文提出了一种两步粗到细的图像配准方法，首先预测场景中每个视图的粗略近似相机位置，然后使用这些初始相机姿态进行姿态细化过程，该过程既是迭代的又是自回归的，从而允许学习细粒度的相机姿态。SparsePose在大规模对象数据集（Co3D）上进行训练，可以从稀疏图像中恢复准确的相机旋转和平移。本文方法的创新点在于，它可以从少量的稀疏视角图像中恢复准确的相机姿态，从而实现高保真度的3D重建。

- (4):本文方法在稀疏视角下的相机姿态估计问题上取得了很好的性能，可以从5-9个图像中实现高保真度的3D重建。与传统的图像配准流程（如COLMAP）和最近的学习方法（如RelPose）相比，SparsePose的性能更好。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
SparsePose是一种用于稀疏视角相机姿态回归和细化的方法。该方法包括两个阶段：初始化相机姿态和细化相机姿态。在初始化阶段，使用预训练的编码器提取低分辨率图像特征，并使用变换器聚合全局上下文并预测新的特征集。全连接网络预测表示初始相机旋转和平移的四元数。在细化阶段，均匀采样一组3D点，这些点位于成像对象所在的体积内。将点投影回图像后，从中提取特征并将其聚合成全局特征向量，从中计算相机姿态更新。自回归网络预测更新以将局部图像特征对齐以匹配场景的3D几何形状。使用自适应和鲁棒的损失函数进行端到端训练。

#### 8. 实验设置：
作者在CO3D数据集上训练了模型，该数据集包含50个常见物体类别的19,000个视频，涵盖了150万个单独的帧和相机姿态。数据集被分为30个训练和20个测试类别。为了构建测试集，作者使用20个测试类别，并对每个源图像数C∈[3,9]采样100个序列。模型架构被设计为适用于任意数量的未定位源图像。对于所有训练和测试序列，假定相机内部矩阵K已知。使用Adam优化器在两个A6000 48 GB GPU上进行了3天的训练，直到收敛，初始学习率为10^-4，经过250个训练时期后降低10倍。

#### 9. 实验结果与分析：
作者在多个物体中心数据集上评估了SparsePose方法的性能，证明了该方法能够正确预测旋转角度，且误差在15度以内。该方法在几个对象上进行了测试，结果表明，准确的少视角姿态估计可以实现少视角新视图合成，即使是在具有挑战性的“野外”数据集中，SparsePose也优于其他基线。作者建议，联合姿态回归和3D场景几何预测的方法可能会实现从稀疏视角合成新视图的进一步改进能力。


# Paper:791     基于鸟瞰图辅助训练的深度完成



#### 1. Title: 
BEV@DC: Bird’s-Eye View Assisted Training for Depth Completion

#### 2. Authors: 
Wending Zhou, Xu Yan, Yinghong Liao, Yuankai Lin, Jin Huang, Gangming Zhao, Shuguang Cui, Zhen Li

#### 3. Affiliation: 
第一作者：香港中文大学（深圳）信息工程学院

#### 4. Keywords: 
Depth completion, LiDAR, RGB, BEV, spatial propagation network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_BEVDC_Birds-Eye_View_Assisted_Training_for_Depth_Completion_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是自动驾驶中的深度完成问题，利用相机和LiDAR两种传感器进行补全。 
- (2):过去的方法主要分为基于相机和基于融合的两种，但是基于相机的方法无法很好地提取3D几何信息，而基于融合的方法计算量大，不适合实时应用。本文提出了一种基于Bird's-Eye View（BEV）的辅助训练方法，利用LiDAR的3D几何信息进行训练，但在推理时只使用图像（RGB和深度）作为输入，从而提高了效率和性能。 
- (3):本文提出了一种辅助训练方法，利用LiDAR的3D几何信息进行训练，但在推理时只使用图像（RGB和深度）作为输入。具体来说，将LiDAR特征投影到统一的BEV空间中，与RGB特征相结合进行BEV完成。通过引入一种新的点-体素空间传播网络（PV-SPN），该辅助分支通过3D密集监督和特征一致性向原始图像分支提供强大的指导。在多个基准测试中，该方法取得了显著的改进，达到了最先进的水平。 
- (4):本文在KITTI深度完成基准测试中取得了最先进的结果，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种名为BEV@DC的方法，它是一种多模态训练方案，利用了LiDAR在训练中的丰富几何细节，并采用了增强的深度完成方式进行推理，只需要图像（RGB和深度）作为输入。该方法由两个分支组成，即相机分支和LiDAR分支。相机分支以RGB图像和稀疏深度为输入，通过2D-UNet生成粗略的深度完成，然后通过辅助3D完成和空间传播网络（SPN）的帮助获得细化的深度图。LiDAR分支以点云为输入，在BEV平面中聚合相机特征，并通过点-体素空间传播网络（PV-SPN）进行3D完成。LiDAR和相机特征被投影到统一的BEV空间中并融合。该方法在室外KITTI深度完成基准测试和室内NYU Depth v2数据集上均取得了最先进的结果。

具体步骤如下：
1. 相机分支：采用传统的U-Net架构进行粗略的深度完成，并通过SPN对结果进行细化。
2. LiDAR分支：从稀疏深度图中获取LiDAR点云，将其输入LiDAR编码器，获得多尺度BEV特征。交叉表示BEV解码器（CRBD）以这些特征作为输入，并通过融合相机特征的级联方式生成BEV完成图。为了进行细粒度的3D完成，提出了点-体素空间传播网络（PV-SPN）。BEV和3D完成部分的输出与相机分支的特征保持一致。辅助分支仅在训练中应用，可以在推理中丢弃，从而避免额外的计算负担。

#### 8. 实验设置：
本文在KITTI深度完成在线基准测试和NYUv2数据集上评估了所提出的方法。评估指标包括KITTI数据集的RMSE、MAE、iRMSE和iMAE，以及NYUv2数据集的RMSE、REL和满足δτ的像素百分比。

#### 9. 实验结果和分析：
所提出的BEV@DC方法在KITTI深度完成基准测试的所有评估指标上均优于其他同行评审方法，包括RMSE、MAE、iRMSE和iMAE。该方法在RMSE方面取得了697.44毫米的成绩，比第二好的方法低11.68毫米。该方法在NYUv2数据集上也取得了最先进的结果。在KITTI验证集上进行的消融研究表明，所有提出的组件对基于相机的模型都有积极的影响。探讨了传播步骤和邻居数量对预测RMSE的影响，并将最佳值设置为16个邻居和3个迭代。所提出的方法在推理中也比以前的融合方法更快。


# Paper:792     低损失曲率样本提高数据效率



#### 1. Title: 
Samples with Low Loss Curvature Improve Data Efficiency

#### 2. Authors: 
Isha Garg, Kaushik Roy

#### 3. Affiliation: 
Isha Garg: 美国普渡大学
Kaushik Roy: 美国普渡大学

#### 4. Keywords: 
Deep learning, data efficiency, loss curvature, coreset selection, regularization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2021_paper.html  Github: https://github.com/isha-garg/SLo-Curves

#### 6. Summary : 
- (1):本文研究了深度神经网络在训练数据点上的损失的二阶性质，以了解这些点附近的损失曲面的曲率。研究发现，存在一些具有非常低曲率的样本，这些样本在完全不同的架构中具有很大的一致性，并且可以在训练的早期阶段进行识别。本文提出了一种新的coreset识别和训练算法SLo-Curves，该算法可以识别具有低曲率的样本，并在训练时对它们进行额外的正则化，以惩罚其附近的损失曲面的高曲率。 

- (2):以往的数据效率研究通常假设训练集中的所有数据点都是独立同分布的，而本文提出的方法则通过研究训练数据点周围的损失曲面的曲率来识别代表性数据点。以往的coreset选择方法通常在大coreset大小时表现良好，而本文提出的方法则在小coreset大小时表现更好。 

- (3):本文提出了一种新的coreset识别和训练算法SLo-Curves，该算法可以识别具有低曲率的样本，并在训练时对它们进行额外的正则化，以惩罚其附近的损失曲面的高曲率。本文的创新点在于，它通过研究训练数据点周围的损失曲面的曲率来识别代表性数据点，而不是像以往的方法那样通过样本的置信度或梯度等指标来衡量样本的重要性。 

- (4):本文在CIFAR-10和CIFAR-100数据集上进行了实验，结果表明，SLo-Curves在小coreset大小时比现有的coreset选择方法表现更好，性能提高了9%。这些coresets可以预先计算，并且在不同的架构中表现良好。
#### 7. 方法详细介绍：
本文提出了一种新的核心集识别和训练算法SLo-Curves。该算法通过测量损失曲率来识别低曲率样本，并在这些样本上训练一个额外的正则化器，以惩罚其周围的损失曲率过高。该算法选择核心集的方法是基于训练模型的Hessian矩阵的迹，通过Hutchinson的迹估计器和有限差分逼近来估计Hessian矩阵的迹。低曲率样本在不同的架构和初始化下都是一致的，这表明这些样本似乎是数据集的内在属性，而不是初始化或网络收敛到的局部最小值的属性。 

#### 8. 实验设置：
本文使用CIFAR-10和CIFAR-100数据集来展示SLo-Curves的有效性。作者探索了较小的核心集，范围从每类单个图像到每类100个图像。使用ResNet-18、ResNet-101、VGG-19、AlexNet、MobileNetV3Small和DenseNet-121网络进行实验。比较了SLo-Curves方法与9种其他核心集选择方法的性能，包括随机均匀采样、Glister、Forgetting、CRAIG、GraphCut、Cal、GraNd、Herding和Margin。 

#### 9. 实验结果和分析：
本文显示SLo-Curves在小核心集大小方面优于现有的核心集选择方法，性能提高了高达9%。所识别的核心集可以推广到不同的架构，并且可以预先计算以生成用于下游任务的压缩版本的数据集。低曲率样本在不同的架构和初始化下都是一致的，这表明这些样本似乎是数据集的内在属性，而不是初始化或网络收敛到的局部最小值的属性。本文还可视化了在CIFAR-10上训练的10个不同初始化的ResNet18模型上累积的曲率排序的样本，显示低曲率样本可以被认为是干净、典型和最小的，而高曲率样本则是杂乱、阻碍或不代表类中其他样本的样本。


# Paper:793     从抽象草图生成逼真图像



#### 1. Title: 
Picture that Sketch: Photorealistic Image Generation from Abstract Sketches

#### 2. Authors: 
Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song

#### 3. Affiliation: 
第一作者机构：SketchX, CVSSP, University of Surrey, United Kingdom.

#### 4. Keywords: 
Sketch-to-photo generation, photorealistic image, autoregressive sketch mapper, fine-grained discriminative loss, partial-aware sketch augmentation.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Koley_Picture_that_Sketch_Photorealistic_Image_Generation_From_Abstract_Sketches_CVPR_2021_paper.html  Github: https://github.com/subhadeepkoley/PictureThatSketch

#### 6. Summary : 
- (1):本文研究的背景是如何从抽象的手绘草图生成逼真的照片。
- (2):过去的方法主要是基于pix2pix等模型，但这些方法需要像素对齐的edgemap草图，无法处理高度抽象的手绘草图。本文提出了一种自回归草图到照片的生成模型，可以从高度抽象的手绘草图生成高度逼真的照片。本文的方法创新性地提出了解耦的编码器-解码器训练范式，以及针对人类草图抽象性的自回归草图映射器和局部感知草图增强策略。
- (3):本文提出的方法是一种自回归草图到照片的生成模型，可以从高度抽象的手绘草图生成高度逼真的照片。本文的方法创新性地提出了解耦的编码器-解码器训练范式，以及针对人类草图抽象性的自回归草图映射器和局部感知草图增强策略。本文的方法在fine-grained sketch-based image retrieval等任务上取得了优异的性能。
- (4):本文的方法在fine-grained sketch-based image retrieval等任务上取得了优异的性能，支持了本文的目标。
#### 7. 方法详细介绍：
本文提出了一种从抽象手绘人物草图生成逼真图像的方法。该方法采用解耦的编码器-解码器训练范式，其中解码器是仅在照片上训练的预训练StyleGAN。编码器执行从抽象草图表示到StyleGAN学习的潜在空间的映射。为了训练这个编码器，使用了真实的草图-照片对，并在输入草图和生成的照片之间施加了细粒度的判别损失，以及输入草图和真实照片之间的常规重构损失。该方法还使用了部分感知增强策略来处理草图的抽象性质。该自回归生成模型具有抽象级别控制、对噪声和部分草图的鲁棒性以及在不同抽象级别的输入草图上的良好泛化性等特性。

#### 8. 实验设置：
本文在三个数据集上进行了评估，分别是UT Zappos50K、pix2pix Handbag和从IKEA、ARGOS等网站收集的椅子数据集。在ShoeV2/ChairV2/Handbag数据集的6730/1800/568个草图和2000/400/568个照片中，分别使用6051/1275/400个草图和1800/300/400个照片进行训练，其余用于测试。预训练的StyleGAN生成器是类别特定的，具有512的特征嵌入大小，学习率为10^-3，批量大小为8，迭代了800万次。草图到照片的映射器使用Rectified Adam和Lookahead方法作为优化器，在恒定的学习率10^-5和批量大小4下进行了500万次迭代训练。

#### 9. 实验结果与分析：
本文提出的方法在FID、LPIPS、MOS和FGM等指标上均优于其他现有的最先进方法和两个自设计的基线方法，包括pix2pix。该方法允许用户根据自己的选择控制抽象程度，并在不同完成水平上生成可信的照片。该方法还能够实现逼真的语义编辑和细粒度的外观控制。实验结果表明，该方法对于从抽象草图生成逼真图像非常有效。


# Paper:794     生成式语义分割



#### 1. Title: 
Generative Semantic Segmentation

#### 2. Authors: 
Jiaqi Chen, Jiachen Lu, Xiatian Zhu, Li Zhang

#### 3. Affiliation: 
第一作者：复旦大学

#### 4. Keywords: 
Semantic Segmentation, Generative Learning, Latent Distribution, Maskige, Prior Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Generative_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/fudan-zvg/GSS

#### 6. Summary : 
- (1):本文研究的背景是语义分割，旨在提出一种新的生成学习方法，将语义分割视为图像条件下的掩模生成问题。

- (2):传统的语义分割方法采用判别式学习，存在像素级分类的问题，而本文提出的生成学习方法则通过引入离散的潜在分布，将语义分割转化为图像条件下的掩模生成问题。本文提出的方法不仅简单且更加任务无关，而且可以充分利用现有的大型生成模型的知识。本文的方法在标准语义分割设置中与现有的先前方法相比具有竞争力，在更具挑战性的跨域设置中实现了新的最先进水平。

- (3):本文提出的方法是将语义分割视为图像条件下的掩模生成问题，通过引入离散的潜在分布，将传统的判别式学习转化为生成学习。本文的方法不仅简单且更加任务无关，而且可以充分利用现有的大型生成模型的知识。本文的方法采用两阶段优化：（i）学习给定语义分割掩模的潜在变量的后验分布，使得潜在变量可以模拟目标分割掩模；（ii）最小化输入训练图像及其掩模的后验分布与潜在变量的先验分布之间的距离，从而使得生成的语义分割掩模可以根据输入图像进行条件生成。

- (4):本文的方法在标准语义分割设置中与现有的先前方法相比具有竞争力，在更具挑战性的跨域设置中实现了新的最先进水平。本文的方法可以生成语义分割掩模，且生成的掩模可以根据输入图像进行条件生成。本文的方法在多个语义分割基准测试中进行了广泛的实验，证明了其有效性。
#### 7. 方法详细介绍：
本文提出了一种生成式语义分割方法，利用基于VQVAE的生成模型生成分割掩模。该方法涉及将输入图像编码为潜在标记，然后将其馈送到maskige解码器中以生成预测的maskige。然后应用反向变换以获得最终的分割掩模。本文还提出了一种潜在后验学习策略，以提高生成模型的性能。具体步骤如下：
(1) 通过编码器Iψ将输入图像x编码为潜在标记z。
(2) 将z和语义分割掩模c馈送到maskige解码器中，生成预测的maskige。
(3) 将预测的maskige应用反向变换，获得最终的分割掩模。
(4) 通过潜在后验学习和潜在先验学习优化模型参数。

#### 8. 实验设置：
本文在三个基准数据集上评估了所提出的方法：Cityscapes、ADE20K和MSeg。Cityscapes数据集包含5000张图像，分为2975、500和1525张图像用于训练、验证和测试。ADE20K数据集有20210、2000和3352张图像用于训练、验证和测试。MSeg数据集是一个综合数据集，统一了来自不同领域的多个语义分割数据集。本文遵循标准评估协议，并报告所有类别的平均交集联合(mIoU)和像素级准确度(mAcc)。

#### 9. 实验结果和分析：
生成式语义分割(GSS)方法在各项指标上均取得了与先前最先进方法相当的性能，并在更具挑战性的跨域评估设置中建立了新的最先进水平。GSS-FF模型具有最高的效率和合理的准确性，而GSS-FT-W模型具有良好的效率和强大的性能。GSS模型在使用HRNet-W48或Swin-Large作为骨干网络的跨域零样本基准测试中优于所有竞争对手。当将maskige从MSeg转移到Cityscapes时，GSS仍然可以实现79.5 mIoU(1%下降)，证明了maskige的域通用性。GSS的第一阶段重建质量几乎没有错误，具有清晰、精确的边缘，并准确地分割远处的行人和细长的杆子，以及ADE20K上室内家具分割的细边缘分割。


# Paper:795     在共识空间中通过聚类找到几何模型



#### 1. Title: 
Finding Geometric Models by Clustering in the Consensus Space

#### 2. Authors: 
Daniel Barath, Denys Rozumnyi, Ivan Eichhardt, Levente Hajder, Jiri Matas

#### 3. Affiliation: 
第一作者：ETH Zurich, Switzerland

#### 4. Keywords: 
multi-instance model fitting, RANSAC, clustering, geometric models, homographies

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Barath_Finding_Geometric_Models_by_Clustering_in_the_Consensus_Space_CVPR_2021_paper.html  Github: https://github.com/danini/clustering-in-consensus-space

#### 6. Summary : 
- (1):本文研究的是多实例模型拟合问题，即如何在数据点中找到多个几何模型，例如单应矩阵。 
- (2):现有的方法通常将问题形式化为将数据点聚类成不同的集合，每个集合代表一个模型实例。但是，这种假设在某些情况下是不正确的，例如一个点可能属于多个实例。本文提出了一种新的算法，不需要形成清晰的点到模型的分配，而是通过在共识空间中进行聚类来找到主要模型实例。这种新的方法导致了一个简单的迭代算法，具有最先进的准确性，而且在许多视觉问题上运行实时，比竞争对手快两个数量级。 
- (3):本文提出了一种基于聚类的算法，通过逐步找到主要模型实例来解决多实例模型拟合问题。该算法不需要形成清晰的点到模型的分配，而是通过在共识空间中进行聚类来找到主要模型实例。新的实例通过在共识空间中进行聚类来找到。这种新的算法形式化了多模型拟合问题，不需要形成清晰的点到模型的分配，即一个点可以被分配到多个实例。 
- (4):本文在多个任务上进行了实验，包括多个广义单应矩阵的姿态估计，快速移动物体的轨迹估计以及在全局SfM算法中使用多个单应矩阵。实验结果表明，与现有的方法相比，本文提出的方法在准确性和运行时间方面都具有优势。
#### 7. 方法详细介绍：
本文提出了一种多实例模型拟合算法，包括实例提议、在共识空间中进行聚类和参数重新估计。该算法不形成清晰的点对模型分配，可以在各种视觉问题上实时运行。所提出的连通组件采样器在许多实际问题上优于最近的P-NAPSAC。

#### 8. 实验设置：
本文使用了1DSfM数据集和KITTI里程计数据集来测试所提出的算法。作者还使用了TbD和TbD-3D数据集来估计快速移动物体的轨迹。

#### 9. 实验结果与分析：
本文报告了所提出算法在两视图运动估计上的结果，表明它比竞争对手快至少两个数量级，并在标准基准数据集上具有更高的精度。所提出的算法显着减少了结构从运动中重建的旋转和位置误差。所提出的方法用于找到快速移动物体的轨迹，在精度和处理时间方面优于所有比较算法，可以实时运行。从本质矩阵和多个单应矩阵分解的集合中选择最佳姿态的所提出技术在平均旋转和位置误差方面提供了最准确的结果。

#### 10. 实验细节：
所提出的方法使用C++实现，使用Eigen库和GC-RANSAC存储库中的求解器实现。该算法与USAC的许多组件相结合，包括样本退化、样本手性和模型退化。模型到模型阈值设置为0.8，最小质量和置信度设置为20和0.99。采样器的参数为半径rmin = 20，rmax = 200，rsteps = 5。所提出的方法在公开可用的数据集上进行了实际问题测试，用于单应性、两视图运动和运动拟合。误分类错误（ME）用作评估指标。所提出的方法与其他最先进的方法进行比较，并在表1中报告结果。


# Paper:796     基于空间-时间概念的3D ConvNets解释



#### 1. Title: 
Spatial-temporal Concept based Explanation of 3D ConvNets

#### 2. Authors: 
Ying Ji, Yu Wang, Jien Kato

#### 3. Affiliation: 
Ying Ji: Nagoya University (名古屋大学)
Yu Wang: Hitotsubashi University (一橋大学)
Jien Kato: Ritsumeikan University (立命館大学)

#### 4. Keywords: 
Convolutional neural networks, Explainable artificial intelligence, 3D video recognition, Spatial-temporal Concept-based Explanation, Supervoxels

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Ji_Spatial-Temporal_Concept-Based_Explanation_of_3D_ConvNets_CVPR_2021_paper.html  Github: https://github.com/yingji425/STCE

#### 6. Summary : 
- (1):本文研究的背景是卷积神经网络（CNNs）在各种任务上表现出卓越的性能，但其决策过程仍缺乏透明度和可解释性，这使得进一步提高性能变得困难。因此，近年来提供CNNs解释和可解释性引起了相当大的兴趣。
- (2):过去的方法主要集中在2D图像分类ConvNets的解释上，而3D视频识别ConvNets的解释相对较少。现有的3D解释方法主要是从2D本地解释方法扩展而来，这些方法只能提供缺乏精确语义含义的粗略视频区域。本文提出了一种基于空间-时间概念的解释框架，可以为3D ConvNets提供高级全局解释，这是解释3D视频识别ConvNets的一项创新工作。
- (3):本文提出了一种STCE（Spatial-temporal Concept-based Explanation）框架，用于解释3D ConvNets。该方法将视频表示为高级超像素，将相似的超像素聚类为一个概念，这对人类来说是直观易懂的。解释框架计算每个概念的得分，反映其在ConvNet决策过程中的重要性。实验表明，该方法可以识别具有不同重要性水平的全局概念，从而允许我们深入研究这些概念对目标任务（如动作识别）的影响。
- (4):本文在Kinetics和KTH数据集上验证了该方法，结果表明，该方法可以解释3D动作识别ConvNets，与人类认知一致。
#### 7. 方法详细介绍：
本文提出了一种名为空间-时间概念解释（STCE）的框架，用于解释3D ConvNets。该框架包括两个步骤。首先，将视频分割成多分辨率空间-时间体积，并使用数据集上训练的3D ConvNet提取每个超像素的特征向量。其次，将超像素分组成不同的聚类，每个聚类是一个有意义的概念，例如“手”或“香肠”或“草”。STCE然后计算每个概念的重要性分数，较高的分数表示该概念对ConvNet更重要。

#### 8. 实验设置：
本文在两个流行的数据集Kinetics-700人类动作识别数据集和KTH Action数据集上评估了提出的STCE方法。Kinetics数据集包含700个动作类别，随机选择10个类别进行可解释性实验。KTH数据集包括六种类型的人类动作。实验在三个标准的3D ConvNet架构C3D、R3D-18和Inflated 3D（I3D）网络上进行。每个网络都从头开始训练。数据增强使用随机水平翻转和随机裁剪。训练视频帧随机裁剪到标准输入大小的112×112，而测试视频帧则进行中心裁剪。总迭代次数为150个epoch。批量大小为64。

#### 9. 实验结果和分析：
本文提出的STCE方法在Kinetics-700人类动作识别数据集和KTH Action数据集上进行了评估。实验结果表明，与先前的方法相比，该方法实现了更好的可解释性和性能。通过添加和删除视频概念，计算识别准确率。实验结果显示，使用小参数和聚类可以提高低分辨率数据集的概念效果。此外，本文还探讨了不同参数设置对STCE方法有效性的影响，并发现概念和聚类的数量不影响实验结果。


# Paper:797     V2V4Real：用于车辆间协同感知的真实世界大规模数据集



#### 1. Title: 
V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception

#### 2. Authors: 
Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu, Bolei Zhou, Jiaqi Ma

#### 3. Affiliation: 
University of California, Los Angeles

#### 4. Keywords: 
autonomous driving, cooperative perception, V2V, dataset

#### 5. Paper: https://openaccess.thecvf.com/content/ICCV2021/html/Xu_V2V4Real_A_Real-World_Large-Scale_Dataset_for_Vehicle-to-Vehicle_Cooperative_ICCV_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是自动驾驶中的感知系统对遮挡敏感且长距离感知能力不足，这是阻碍5级自动驾驶的关键瓶颈之一。近期研究表明，车辆间协同感知系统具有革命性的潜力，但缺乏真实世界数据集阻碍了该领域的进展。

- (2):过去的方法主要集中在单车感知，存在遮挡和短距离感知能力不足等问题。本文提出的V2V协同感知系统可以克服单车感知的局限性，但由于缺乏公共基准数据集，难以在真实场景中验证V2V感知的有效性。本文提出了V2V4Real数据集，是第一个大规模真实世界多模态V2V感知数据集，包括驾驶区域410公里，20K LiDAR帧，40K RGB帧，240K注释的3D边界框和覆盖所有驾驶路线的HDMaps。本文提出了三个协同感知任务，包括协同3D物体检测、协同3D物体跟踪和Sim2Real领域自适应协同感知。提供了最新的协同感知算法的综合基准。

- (3):本文提出了一个大规模真实世界多模态V2V感知数据集V2V4Real，包括驾驶区域410公里，20K LiDAR帧，40K RGB帧，240K注释的3D边界框和覆盖所有驾驶路线的HDMaps。本文提出了三个协同感知任务，包括协同3D物体检测、协同3D物体跟踪和Sim2Real领域自适应协同感知。提供了最新的协同感知算法的综合基准。

- (4):本文的方法在三个协同感知任务上进行了综合基准测试，包括协同3D物体检测、协同3D物体跟踪和Sim2Real领域自适应协同感知。实验结果表明，V2V协同感知在多个任务中具有有效性。
#### 7. 方法详细介绍：
本文提出了一种基于车辆间通信的合作感知方法，使用了一个名为V2V4Real的大规模真实世界数据集。该方法包括在数据集上训练检测模型和实施域自适应技术以提高性能。本文还介绍了三个V2V感知基准测试，包括3D目标检测、目标跟踪和Sim2Real域自适应。

具体步骤如下：
1. 使用PointPillar作为骨干网络从点云中提取2D特征。
2. 将数据集分为训练/验证/测试集，分别为14,210/2,000/3,986帧。
3. 训练模型使用60个epochs，每个GPU的批量大小为4，学习率为0.001，使用正常的点云数据增强。
4. 对于跟踪任务，将前3帧与当前帧一起作为输入。

#### 8. 实验设置：
本文使用两辆实验联网自动驾驶汽车，一辆特斯拉汽车和一辆福特Fusion汽车，配备Velodyne VLP-32 LiDAR传感器、两个单目相机和GPS/IMU集成系统。行驶路线覆盖了347公里的高速公路和63公里的城市道路，车辆之间保持150米以内的距离以确保视野重叠。数据集包括四个不同的坐标系：特斯拉和福特Fusion的LiDAR坐标系、HDmap坐标系和地球-地球、固定坐标系（ECEF）。数据集在三天内收集，共收集了310K帧的19小时驾驶数据。帧以10Hz采样，共有20K帧LiDAR点云和40K帧RGB图像。

#### 9. 实验结果和分析：
本文在V2V4Real数据集上对各种合作3D检测模型进行了定量比较。结果表明，所有合作感知方法都可以显著提高性能，至少在IoU 0.5的整体AP方面提高15.2%。中间融合方法在准确性和传输成本之间取得了最佳平衡，其中CoBEVT在AP@0.5方面表现最佳。本文还展示了合作跟踪的基准结果，其中CoBEVT在大多数评估指标上表现最佳。最后，本文评估了Sim2Real域自适应技术，并表明应用该技术可以平均减少7.46%的性能下降。


# Paper:798     NeRFInvertor：用于单张真实图像动画的高保真NeRF-GAN反演



#### 1. Title: 
NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation

#### 2. Authors: 
Yu Yin, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang, Xin Tong, Yun Fu

#### 3. Affiliation: 
第一作者：Northeastern University（美国东北大学）

#### 4. Keywords: 
NeRF, GAN, image animation, inversion, 3D geometry

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yin_NeRFInvertor_High_Fidelity_NeRF-GAN_Inversion_for_Single-shot_Real_Image_CVPR_2022_paper.html  Github: https://github.com/yuyin1/NeRFInvertor

#### 6. Summary : 
- (1):本文研究的背景是单张真实图像的动画生成，这是一项具有挑战性的任务，需要解决NeRF-GAN模型的反演问题。

- (2):过去的方法主要采用2D图像生成模型或3D参数模型，但存在着几何和视觉问题。本文提出了一种通用方法，通过优化潜在编码来实现高保真度、3D一致性和身份保留的真实主体动画。本文的方法通过2D损失函数和3D正则化来解决身份差距和几何和视觉伪影问题。

- (3):本文提出的方法是NeRF-GAN模型的反演方法，通过优化潜在编码来实现高保真度、3D一致性和身份保留的真实主体动画。本文的方法通过2D损失函数和3D正则化来解决身份差距和几何和视觉伪影问题。本文的创新点在于提出了一种新的几何约束，通过利用优化后的潜在变量周围的样本来提供对2D空间中未观察到部分的关键指导。

- (4):本文的方法在多个NeRF-GAN模型和不同数据集上进行了实验，证明了其在真实、高保真度和3D一致性动画方面的有效性。
#### 7. 方法详细介绍：
NeRFInvertor方法是一种通用的NeRF-GAN反演方法，可以将单个真实图像转换为NeRF表示。该方法包括以下步骤：
1. 通过感知损失和像素级l2范数损失函数计算优化的潜在代码，以生成与输入图像尽可能接近的图像。
2. 使用图像空间监督来缩小合成图像与输入图像之间的身份差距。
3. 引入显式几何约束和隐式几何正则化，以维持模型生成高质量和三维一致的图像。
4. 使用掩膜几何约束来消除伪影并获得更准确的几何形状。
5. 通过从优化的潜在变量的邻域中采样潜在代码，引入新的几何约束，以在细调和原始预训练生成器的密度输出上强制执行几何约束。

#### 8. 实验设置：
本文使用CelebA-HQ数据集进行实验，包括训练集、验证集和测试集。使用了两个NeRF-GAN模型进行实验，一个是静态模型，一个是动态模型。实验中使用了FID分数、身份保留度和视觉质量等指标来评估模型的性能。

#### 9. 实验结果和分析：
实验结果表明，NeRFInvertor方法在视觉质量、身份保留度和FID分数等方面均优于现有方法。通过对CelebA-HQ测试集的消融实验，证明了所提出的正则化方法的有效性。此外，本文还进行了定性分析，展示了NeRFInvertor方法在生成高质量、三维一致和身份保留的面部动画方面的优越性。


# Paper:799     对抗生成网络与对抗生成分类器的顺序训练揭示了独立训练的GAN实例之间存在的相关“知识盲区”



#### 1. Title: 
Sequential training of GANs against GAN-classifiers reveals correlated “knowledge gaps” present among independently trained GAN instances

#### 2. Authors: 
Arkanath Pathak, Nicholas Dufour

#### 3. Affiliation: 
Google Research（谷歌研究院）

#### 4. Keywords: 
Generative Adversarial Networks (GANs), GAN-classifiers, knowledge gaps, image generation, image detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Pathak_Sequential_Training_of_GANs_Against_GAN-Classifiers_Reveals_Correlated_Knowledge_Gaps_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是GAN生成的图像中存在“知识盲区”，即生成的图像中存在与真实图像不同的特征，这些特征可以被GAN-classifiers检测出来。

- (2):过去的方法主要是使用CNN分类器来检测GAN生成的图像，但这些分类器无法检测到所有的“知识盲区”，因此需要新的方法来填补这些盲区。本文提出了一种迭代训练GAN和GAN-classifiers的方法，以填补这些“知识盲区”。

- (3):本文提出的方法是在GAN训练中引入GAN-classifiers，以检测生成的图像中的“知识盲区”，然后迭代训练GAN和GAN-classifiers，以填补这些盲区。本文的创新点在于提出了一种新的方法来填补“知识盲区”，并且在实验中证明了这种方法的有效性。

- (4):本文的方法在MNIST和FFHQ数据集上进行了实验，结果表明，在高维图像生成任务中，本文的方法可以有效地填补“知识盲区”，并且不会影响生成图像的质量。本文的方法可以提高GAN的训练效果，使生成的图像更加真实。
#### 7. 方法详细介绍：
本文的实验分为两个阶段。第一阶段是训练生成器，第二阶段是训练分类器。在第一次迭代的第一阶段，独立训练一组生成器。在第二阶段，训练分类器来检测第一阶段中训练的生成器的样本。在后续迭代的第一阶段，使用修改后的生成器损失函数训练一组新的生成器。在每个迭代的第一阶段，分类器被冻结。本文使用了两种生成器损失函数的变体，分别是“fool-all”和“memoryless”损失函数。

#### 8. 实验设置：
本文的实验分为两个设置：DCGAN和SG2。在DCGAN设置中，训练图像是MNIST，使用单个生成器即可训练出一个可靠地泛化到未在训练中看到的生成器样本的分类器。在SG2设置中，训练图像是FFHQ，需要多个生成器才能产生可靠的泛化。用于训练和测试的分类器在不重叠的生成器子集上进行训练，以测量泛化性能。

#### 9. 实验结果与分析：
在DCGAN设置中，当使用“fool-all”损失函数时，GAN在试图欺骗一个保留的分类器时遇到了困难。在SG2设置中，非常低的权重系数就足以使生成器学会欺骗分类器。SG2生成器在训练早期就学会了欺骗分类器。每个分类器的性能都以在未见过的自然图像和从保留的生成器池中抽取的样本组成的平衡数据集上的准确率来报告。


# Paper:800     REVEAL：使用多源多模态知识存储的检索增强视觉语言预训练



#### 1. Title: 
REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory

#### 2. Authors: 
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, Alireza Fathi

#### 3. Affiliation: 
第一作者：加州大学洛杉矶分校

#### 4. Keywords: 
Visual question answering, knowledge retrieval, pre-training, memory, multimodal

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2021_paper.html  Github: https://github.com/google-research/reveal

#### 6. Summary : 
- (1):本文提出了一种检索增强的视觉语言模型，通过学习将世界知识编码到大规模内存中，并从中检索以回答知识密集型查询。 
- (2):与现有方法相比，本文的方法可以使用多种多模态知识源，这在视觉问答和图像字幕生成等任务中取得了最先进的结果。本文的方法通过将知识存储与推理分离，使模型能够利用各种外部知识源，而不是将知识静态编译到模型权重中。 
- (3):本文提出的方法包括四个关键组件：内存、编码器、检索器和生成器。内存通过统一编码器对各种多模态世界知识源进行编码，检索器从内存中找到最相关的知识条目，生成器将检索到的知识与输入查询融合以产生输出。本文的方法可以使用多种多模态知识源，这是本文方法的一个重要创新点。 
- (4):本文的方法在视觉问答和图像字幕生成等任务中取得了最先进的结果，特别是在OKVQA基准测试中，本文方法取得了59.1%的准确率，而使用的参数比以前的方法少了一个数量级。
#### 1. 实验结果
(1). 本文提出了一种端到端的检索增强视觉语言模型(REVEAL)，利用不同模态的多样化知识源。该模型在大规模图像-文本语料库上进行预训练，并在知识密集型视觉问答和图像字幕任务上取得了最先进的结果。该论文还进行了消融研究，分析了每个数据源的相对重要性以及从各种语料库中检索的有效性。此外，该论文还研究了注意力融合层的性能，与两种现有的检索训练方法进行了比较，并表明所提出的方法对于预训练检索增强视觉语言模型更加高效和有效。最后，该论文证明了REVEAL模型可以通过仅更新内存来快速适应新知识，无需重新训练模型参数。

#### 2. 方法详细介绍
本文提出了一种检索增强的视觉语言预训练模型REVEAL，包括四个主要组件：查询编码器、内存、检索器和生成器。查询编码器使用Vision Transformer和T5编码器将输入的图像-文本查询编码为嵌入序列。内存组件通过将来自不同来源的知识项编码为键/值对来构建和更新多模态知识内存。检索器组件使用门控函数和Perceiver架构检索与输入查询最相关的前K个内存条目。最后，生成器组件使用注意力融合模块和T5解码器将查询和检索到的知识融合生成输出答案。

#### 3. 实验设置
本文使用包含13亿个图像-字幕对的大规模Web-Image-Text数据集进行预训练。使用文本生成目标在数据集上进行预训练模型。本文还使用四种不同的知识源，包括维基百科段落、Alt-Text字幕、问题-答案对和WikiData三元组，构建多模态知识内存。知识源的统计数据在表1中提供。本文在三个下游任务上评估模型：VQA、NLVR2和GQA。

#### 4. 实验结果和分析
本文在所有三个下游任务上报告了最先进的性能：VQA、NLVR2和GQA。在VQA任务上，模型的准确率为80.3％，比先前的最先进水平高1.2％。在NLVR2任务上，模型的准确率为70.5％，比先前的最先进水平高1.5％。在GQA任务上，模型的准确率为68.3％，比先前的最先进水平高1.1％。本文还进行了消融研究，分析了模型不同组件的有效性。

#### 5. 方法详细介绍
本文提出了一种检索增强的视觉语言预训练方法REVEAL，利用多种知识源，包括维基百科-图像-文本、概念、VQA-v2和WikiData。该方法使用修改后的WIT数据集预热检索器，该数据集包含伪基础知识。使用生成目标和对比损失进行预训练，并在基于知识的VQA和图像字幕任务上进行微调。

#### 6. 实验设置
预训练流程包括使用Adafactor优化器在修改后的WIT数据集上训练多模态检索器，并在10个时期内优化对比损失。使用LPrefixLM作为主要目标进行生成预训练，加权0.01添加Lcontra、Ldecor和Lalign。使用批量大小为128和Adafactor优化器在基于知识的VQA和图像字幕任务上进行微调，峰值学习率为1e-4。

#### 7. 方法详细介绍
本文提出了一种检索增强的视觉语言预训练模型(REVEAL)，学习将世界知识编码到大规模内存中，并从中检索以回答知识密集型查询。REVEAL包括四个关键组件：内存、编码器、检索器和生成器。大规模内存通过统一编码器编码各种多模态世界知识。检索器在内存中找到最相关的知识条目，生成器将检索到的知识与输入查询融合以生成输出。内存、编码器、检索器和生成器都在大量数据上进行端到端预训练。该方法可以使用多样化的多模态知识源，这被证明可以带来显著的收益。

#### 8. 实验设置
N/A

#### 9. 实验结果和分析
REVEAL在基于知识的VQA和图像字幕任务上取得了良好的结果，优于以前的最先进方法，而不依赖于大型语言模型。在OKVQA上，REVEAL的准确率为59.1％，比单个KAT模型高6.0％，比ReVIVE高2.5％。在A-OKVQA上，REVEAL的准确率为52.2％，比以前最好的GPV-2高3.6％。在MSCOCO Captions和NoCaps上，REVEAL优于最近的强基线，如SimVLM和CoCa。


# Paper:801     GeoMVSNet：利用几何感知学习多视角立体视觉



#### 1. Title: 
GeoMVSNet: Learning Multi-View Stereo with Geometry Perception

#### 2. Authors: 
Zhe Zhang, Rui Peng, Yuxi Hu, Ronggang Wang

#### 3. Affiliation: 
第一作者：北京大学电子与计算机工程学院，中国

#### 4. Keywords: 
Multi-View Stereo, deep learning, geometry perception, cost matching, depth estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_GeoMVSNet_Learning_Multi-View_Stereo_With_Geometry_Perception_CVPR_2021_paper.html  Github: https://github.com/doubleZ0108/GeoMVSNet

#### 6. Summary : 
- (1):本文研究多视角立体视觉(Multi-View Stereo, MVS)的深度学习方法，提出了一种新的模型GeoMVSNet，旨在利用粗略阶段中嵌入的几何信息来提高细化阶段的深度估计精度。

- (2):现有的MVS方法通常采用级联结构，通过不断缩小假设范围来高效地估计高分辨率深度图。然而，这些方法忽略了粗略阶段中嵌入的重要几何信息，导致匹配成本不稳定和重建结果不理想。本文提出了一种几何感知模型，GeoMVSNet，通过设计两个分支的几何融合网络，从粗略估计中提取几何先验，以增强细化阶段的结构特征提取。此外，将粗略概率体积嵌入轻量级正则化网络中，以进一步增强深度几何直觉。同时，采用课程学习策略逐步提高模型的几何集成能力。为了增强模型的全场景几何感知能力，本文提出了基于高斯混合模型假设的深度分布相似性损失。 

- (3):本文提出了一种新的MVS深度学习模型GeoMVSNet，通过融合粗略阶段中嵌入的几何信息来提高细化阶段的深度估计精度。具体地，本文设计了两个分支的几何融合网络，以提取粗略估计中的几何先验，并将粗略概率体积嵌入轻量级正则化网络中，以进一步增强深度几何直觉。此外，本文采用课程学习策略逐步提高模型的几何集成能力，并提出了基于高斯混合模型假设的深度分布相似性损失。实验结果表明，GeoMVSNet在DTU和Tanks and Temples数据集上均取得了最先进的结果。

- (4):本文提出的GeoMVSNet模型在DTU和Tanks and Temples数据集上均取得了最先进的结果，证明了其在MVS任务上的有效性和优越性。本文的贡献在于提出了一种新的几何感知模型，GeoMVSNet，通过融合粗略阶段中嵌入的几何信息来提高细化阶段的深度估计精度。同时，本文提出了一种基于高斯混合模型假设的深度分布相似性损失，以增强模型的全场景几何感知能力。
#### 7. 方法详细介绍：
GeoMVSNet是一种基于学习的多视图立体匹配方法，它将粗略的几何结构与精细的深度估计相结合。该方法包括以下步骤：
1. 几何先验引导的特征融合
2. 概率体几何嵌入
3. 级联代价体构建
4. 深度估计
5. 频域深度滤波
6. 全场景几何感知损失函数

其中，几何先验引导的特征融合使用两个分支的融合网络将粗略深度图中包含的几何先验与经典的FPN提取的普通特征相结合。概率体几何嵌入将包含丰富几何结构的粗略概率体嵌入到正则化网络中，使用增强的2D正则化代替重的3D卷积，从而实现轻量级但强大的代价匹配。频域深度滤波策略有效地减轻了冗余的高频纹理，利用嵌入在不同频率层次的几何结构逐渐精细深度估计。全场景几何感知损失函数用于全场景相似性监督。

#### 8. 实验设置：
该方法在DTU数据集和Tanks and Temples基准测试集的中级和高级集上进行了评估。实验在一台服务器上进行，该服务器配备Intel Xeon E5-2690 CPU和四个NVIDIA Tesla V100 GPU。输入图像的大小为640x512，批量大小为1。学习率初始化为0.001，并在验证损失不再下降时降低10倍。在DTU数据集上进行20个epoch的训练，在Tanks and Temples基准测试集上进行30个epoch的训练。

#### 9. 实验结果和分析：
GeoMVSNet在DTU数据集和Tanks and Temples基准测试集的中级和高级集上均取得了最先进的结果。在DTU数据集上，该方法的平均深度误差为0.47mm，完整度得分为0.972，远远优于之前的最先进方法。在Tanks and Temples基准测试集的高级集上，该方法排名第一，平均深度误差为0.56mm，完整度得分为0.975。该方法在Tanks and Temples基准测试集的中级集上也取得了竞争性的结果，平均深度误差为0.68mm，完整度得分为0.965。


# Paper:802     一种实用的最坏情况归因偏差上界



#### 1. Title: 
A Practical Upper Bound for the Worst-Case Attribution Deviations

#### 2. Authors: 
Fan Wang, Adams Wai-Kin Kong

#### 3. Affiliation: 
第一作者：南洋理工大学计算机科学与工程学院和快速富有对象搜索（ROSE）实验室
（School of Computer Science and Engineering, Nanyang Technological University, Rapid-Rich Object Search (ROSE) Lab, IGP, Nanyang Technological University）

#### 4. Keywords: 
Model attribution, deep neural networks, worst-case attribution deviations, robustness, perturbations-based methods, backpropagation-based methods, integrated gradients.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_A_Practical_Upper_Bound_for_the_Worst-Case_Attribution_Deviations_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了深度神经网络（DNN）中模型归因的鲁棒性问题，提出了一种量化归因偏差的上界的方法，以衡量模型对于任意测试点的鲁棒性。
- (2):过去的方法无法评估模型在任意测试点上的鲁棒性，本文提出了一种量化归因偏差的上界的方法，以衡量模型对于任意测试点的鲁棒性。本文提出的方法可以有效地量化模型的鲁棒性，并且在各种数据集和两种不同类型的攻击（PGD攻击和IFIA归因攻击）上进行了验证。
- (3):本文首次提出了一个受限制的优化问题，以导出一个上界，该上界测量样本在任何噪声范围内被扰动后的归因最大不相似性，同时分类结果保持不变。基于该公式，本文提出了不同的实用方法，使用欧几里得距离和余弦相似度在ℓ2和ℓ∞-范数扰动约束下限制归因。本文的理论研究所开发的上界在各种数据集和两种不同类型的攻击（PGD攻击和IFIA归因攻击）上得到了验证。
- (4):本文提出的方法可以有效地量化模型的鲁棒性，并且在各种数据集和两种不同类型的攻击（PGD攻击和IFIA归因攻击）上进行了验证。本文的方法可以为深度神经网络的模型归因提供更好的解释性和鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种实用的最坏情况归因偏差上界的方法。该方法通过解决带有范数和标签约束的优化问题来导出上界。在没有标签约束的情况下，使用归因函数的一阶泰勒级数来计算 ℓ2 范数约束的上界。在 ℓ∞ 范数约束的情况下，使用范数松弛和归因梯度的稀疏性来计算更紧的上界。在有标签约束的情况下，将上界问题转化为具有凸目标函数和所有标签不同于 y 的线性约束系统的二次规划问题。使用完备性公理来确保样本扰动后分类结果保持不变。该方法使用欧几里得距离和余弦距离作为度量归因差异的指标。

#### 8. 实验设置：
本文在 CIFAR-10、MNIST 和 Fashion-MNIST 等不同数据集和模型上评估了所提出的方法。使用 ℓ2 范数 PGD-20 攻击和 200 步 IFIA 作为评估指标。计算了欧几里得距离和余弦距离的理论上界，并将其与原始样本和攻击样本之间的距离进行比较。

#### 9. 实验结果与分析：
实验结果验证了所提出的上界的有效性。使用 Kendall 的秩相关系数比较了理论和实际保护的经验归因鲁棒性。报告了样本和理论上界之间的最小欧几里得距离，以衡量所提供的上界的紧密程度。最紧密的上界值只能比样本距离大 10^-4。所有 r 的值都是正的，这表明没有扰动的归因违反了理论上界。在附录 D 中提供了理论上界和真实数据之间的差距分布的可视化。


# Paper:803     GANmouflage：利用纹理场进行三维物体非检测



#### 1. Title: 
GANmouflage: 3D Object Nondetection with Texture Fields

#### 2. Authors: 
Rui Guo, Jasmine Collins, Oscar de Lima, Andrew Owens

#### 3. Affiliation: 
Rui Guo: University of Michigan

#### 4. Keywords: 
Camouflage, 3D object, texture fields, adversarial learning, multi-view geometry

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Guo_GANmouflage_3D_Object_Nondetection_With_Texture_Fields_CVPR_2021_paper.html  Github: https://github.com/rrrrrguo/ganmouflage

#### 6. Summary : 
- (1):本文研究了3D物体的伪装问题，即如何在给定场景中学习一种纹理，使得物体在多个视角下难以被发现。该问题在隐藏不美观的物体，如电力箱、太阳能电池板、电视塔等方面有应用，也可以用于隐藏监控摄像头和狩猎平台等物体。此外，该问题还可以提供更好的科学理解，以了解视觉系统使用的线索。
- (2):以前的物体非检测方法基于非参数纹理合成，但这些方法只能直接“复制和粘贴”被物体直接遮挡的像素，难以处理复杂的背景和非平面几何。本文提出了一种基于纹理场和对抗学习的模型，该模型可以准确地重现场景的纹理，并同时处理每个视角所施加的高度冲突的约束。本文的模型可以学习伪装各种物体形状，包括复杂的物体形状，是第一个解决隐藏复杂物体形状问题的模型。本文的方法通过人类视觉搜索研究，发现其估计的纹理比以前的方法更好地隐藏了物体。
- (3):本文提出了一种基于神经纹理场和对抗训练的模型，该模型可以利用多视角几何，高保真地重现场景的纹理，并满足输入图像提供的高度冲突的约束。在训练期间，我们的模型学习从场景中随机选择的3D位置隐藏各种物体形状。它使用条件生成对抗网络（GAN）学习生成难以使用像素对齐表示检测到的纹理，并使用Hypercolumns提供每个视角的信息。通过自动化评估指标和人类感知研究，我们发现我们的方法在隐藏立方体物体方面显着优于以前的最新技术。我们还通过使用它来伪装各种复杂形状来展示我们的方法的灵活性。最后，我们通过消融实验表明，我们的纹理模型设计导致了显着更好的结果。
- (4):本文提出的方法可以学习伪装各种物体形状，包括复杂的物体形状，是第一个解决隐藏复杂物体形状问题的模型。通过自动化评估指标和人类感知研究，我们发现我们的方法在隐藏立方体物体方面显着优于以前的最新技术。本文的方法可以应用于隐藏不美观的物体，如电力箱、太阳能电池板、电视塔等，也可以用于隐藏监控摄像头和狩猎平台等物体。
#### 7. 方法详细介绍：
本文提出了一种基于神经纹理场和对抗学习的方法，称为GANmouflage。该方法利用多视角几何结构，使用神经纹理表示将3D点映射到RGB颜色，并在输入图像、它们的投影矩阵和3D对象形状的条件下进行训练。模型被训练成创建与输入视图相一致的纹理，并且难以被鉴别器与随机背景补丁区分。该方法与图像修复和纹理合成相关，但不同之处在于没有单个解决方案可以完全满足所有图像提供的约束。该方法在多个数据集上进行了评估，结果表明它优于以前的伪装方法。具体而言，该方法的步骤包括：
1. 从每个特征图中提取像素对齐的特征。
2. 计算每个输入图像中3D点的投影。
3. 使用透视编码传达对象表面的局部几何和多视角设置。
4. 定义纹理场架构，将3D坐标映射到颜色，并在N个输入图像的图像特征集和透视特征集的条件下进行调整。
5. 使用photoconsistency loss和adversarial loss训练模型，通过学习随机增强对象从随机位置进行伪装的学习方案进行优化。

#### 8. 实验设置：
本文的评估基于[33]的场景数据集，将对象放置在预定义位置。每个场景包含来自不同位置的10-25张照片。使用Adam优化器进行模型训练，纹理函数的学习率为2×10−4，鉴别器的学习率为10−4。模型的批量大小为8，进行约12k次迭代。对于评估，将对象放置在[33]中预定义的位置，并在保留的测试视图中进行渲染。

#### 9. 实验结果和分析：
本文提出的方法在伪装立方体和复杂形状方面的表现均优于以前的方法。该方法的4视图模型在混淆率和时间到点击指标方面表现最佳，明显优于2视图模型。该方法的自动化评估指标表明，完整的4视图模型在这些指标方面也是表现最佳的。该方法还与其他方法进行了比较，并进行了消融实验。实验结果表明，对抗性损失显著提高了性能，使用所有视图计算光度一致性的模型倾向于生成更真实的纹理。此外，该方法在LPIPS方面表现更好，但发现其模型变体在SIFID方面略有改善。最后，该方法在修复和MRF-based方法方面表现显著优于以前的方法。


# Paper:804     Next3D：面向3D感知头像的生成神经纹理光栅化



#### 1. Title: 
Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars

#### 2. Authors: 
Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, Yebin Liu

#### 3. Affiliation: 
第一作者：清华大学

#### 4. Keywords: 
3D-aware generative adversarial networks, facial avatars, Generative Texture-Rasterized Tri-planes, volumetric representation, 3D Morphable Face Model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2021_paper.html  Github: https://github.com/MrTornado24/Next3D

#### 6. Summary : 
- (1):本文研究的背景是生成3D头像的精细控制问题，既要保证形变的准确性，又要保证动态变化时的拓扑一致性。

- (2):过去的方法包括基于3D Morphable Face Model（3DMM）的2D生成模型和基于3DMM的3D生成模型，但前者无法保证3D一致性，后者无法处理非面部区域的变化。本文提出了一种新的3D GAN框架，通过将参数化网格模板上的生成神经纹理投影到三个正交视图的特征平面上，形成三平面特征表示，从而结合了网格驱动的显式变形和隐式体积表示的灵活性。同时，本文还提出了一种模块化的方法来处理嘴部内部特征，以及一种变形感知的鉴别器来提高变形准确性。

- (3):本文提出了一种新的3D GAN框架，通过将参数化网格模板上的生成神经纹理投影到三个正交视图的特征平面上，形成三平面特征表示，从而结合了网格驱动的显式变形和隐式体积表示的灵活性。同时，本文还提出了一种模块化的方法来处理嘴部内部特征，以及一种变形感知的鉴别器来提高变形准确性。

- (4):本文的方法在3D头像生成任务上取得了最先进的性能，能够实现对头部旋转、面部表情、眼睛眨动和注视方向的精细控制，并且能够提供强的3D先验知识，支持3D感知的风格化等下游任务。
#### 7. 方法详细介绍：
本文提出了一种新颖的3D GAN框架，用于从非结构化的2D图像中无监督地学习生成高质量、3D一致的面部头像。该方法将整个头部分为动态和静态部分，并分别对其进行建模。对于动态部分，该方法结合了网格引导显式变形的细粒度表情控制和隐式体积表示的灵活性。为了实现这一点，该方法提出了一种新的表示方法，称为生成式纹理-光栅化三平面，通过在参数化模板网格上学习面部变形的生成神经纹理，并通过标准光栅化将其采样到三个正交视图和轴对齐的特征平面中，形成三平面特征表示。该方法还提出了特定的模块来建模嘴内部，这在3DMM中没有考虑到。通过大量实验，该方法展示了最先进的3D感知合成质量和动画能力。

#### 8. 实验设置：
本文在FFHQ数据集上进行训练和测试。数据集使用水平翻转进行增强，并使用现成的姿态估计器标记图像的近似相机外参和恒定内参。模型在4个3090 GPU上训练了大约4天。

#### 9. 实验结果与分析：
本文展示了定量和定性的结果，将提出的方法与最先进的3D感知合成方法进行比较。提出的方法在合成质量和动画精度上均优于所有基线。提出的方法在定量评估中的所有指标上也取得了最佳性能。本文还展示了提出的方法在一次性头像和3D感知肖像风格化方面的应用。讨论了提出方法的局限性和未来工作。


# Paper:805     基于元学习的无需真实图像的深度压缩采样



#### 1. Title: 
Ground-Truth Free Meta-Learning for Deep Compressive Sampling

#### 2. Authors: 
Xinran Qin, Yuhui Quan, Tongyao Pang, Hui Ji

#### 3. Affiliation: 
第一作者：中国华南理工大学计算机科学与工程学院，广州 510006，中国

#### 4. Keywords: 
Compressive sampling, deep learning, meta-learning, unsupervised learning, image reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是压缩采样技术在图像重建中的应用，该技术通过使用特定的测量矩阵来收集有限数量的测量值来捕获图像。

- (2):过去的方法主要是监督学习，需要使用带有真实图像和其测量值对的数据集来训练端到端的深度神经网络。然而，这种方法存在两个限制：一是在实践中收集足够数量的真实图像数据集往往很昂贵或不可行，二是训练数据可能存在偏差或不足以覆盖正在重建的图像的所有模式和特征，导致泛化性能差。为了解决这些限制，本文提出了一种基于元学习的无需真实图像的方法，该方法利用外部和内部深度学习进行无监督高质量图像重建。

- (3):本文提出了一种基于改进的Stein无偏风险估计（iSURE）的元学习方法，该方法使用仅压缩采样测量来进行外部元学习，然后通过高效地适应训练模型到测试样本来利用样本特定的内部特征以获得性能提升。为了改善在测量矩阵的零空间上的学习和适应性，提出了一个修改后的模型无关元学习方案和一个零空间一致性损失。此外，还引入了一个偏置调整方案，用于展开NNs以进一步加速模型适应。实验结果表明，所提出的无需真实图像的方法表现良好，甚至可以与监督方法竞争。

- (4):本文的方法在压缩采样图像重建任务上取得了良好的性能，可以与监督方法竞争，同时比内部方法更快。
#### 7. 方法详细介绍：
本文提出了一种基于元学习的无需真实标签的压缩采样重建方法MetaCS。该方法通过外部元学习和内部深度学习相结合，利用改进的Stein无偏风险估计器（iSURE）在测量矩阵的伴随空间中提供高效的计算和有效的指导，实现了对测试样本的快速自适应。该方法包括两个步骤：1）使用无监督元学习在无真实标签的训练数据集上训练CNN模型，以实现更好的测试时间模型自适应；2）对所有测试样本运行训练好的模型，并对每个样本进行微调（自适应）。该方法还包括一个空间一致性损失，以减轻对重建精度的负面影响。在测试中，采用集成推理方案。该方法与其他无真实标签的方法进行了比较，包括ZF、SparseMRI、REI、BNN、ASGLD和DDSSL。

#### 8. 实验设置：
本文在MRI150和ADNI两个数据集上进行了评估，用于MR图像重建。采用固定的径向或高斯掩模对傅里叶系数进行采样，采用不同的采样比率。对于不同的设置和数据集，分别进行训练，并仅调用每个图像一次以生成测量。考虑无噪声和有噪声的情况，使用iSURE估计噪声水平。

#### 9. 实验结果和分析：
本文提出的MetaCS方法在MR图像重建方面表现优异，对比其他无真实标签的方法，平均PSNR/SSIM得分最高。视觉检查还显示重建图像质量高。该方法在深度压缩采样的无真实标签元学习方面表现出有效性。


# Paper:806     通过可靠的不确定性量化和校准构建自我感知目标检测器



#### 1. Title: 
Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration

#### 2. Authors: 
Kemal Oksuz, Tom Joy, Puneet K. Dokania

#### 3. Affiliation: 
Kemal Oksuz: Five AI Ltd., 英国
Tom Joy: Five AI Ltd., 英国
Puneet K. Dokania: Five AI Ltd., 英国

#### 4. Keywords: 
Object detection, uncertainty quantification, calibration, domain shift, self-awareness

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2021_paper.html  Github: https://github.com/fiveai/saod

#### 6. Summary : 
- (1):本文旨在解决目标检测中的不确定性问题，提出了一种新的自我感知目标检测（SAOD）任务，以评估目标检测器的鲁棒性和可靠性。该任务要求目标检测器具有拒绝场景的功能，能够提供可靠的不确定性估计和校准的置信度分数。
 
- (2):过去的方法存在严重的问题，如不当的域漂移检测方法和使用不考虑定位和分类质量的校准指标。本文提出的SAOD任务能够全面评估目标检测器的性能，包括准确性、域漂移鲁棒性、可接受或拒绝图像的能力以及校准性。本文提出了一种新的局部化感知期望校准误差（LAECE）度量方法，该方法考虑了分类和定位质量，是之前方法所忽略的特征。 

- (3):本文提出了一种新的SAOD任务，以评估目标检测器的鲁棒性和可靠性。该任务要求目标检测器具有拒绝场景的功能，能够提供可靠的不确定性估计和校准的置信度分数。本文提出了一种新的局部化感知期望校准误差（LAECE）度量方法，该方法考虑了分类和定位质量，是之前方法所忽略的特征。本文还构建了大规模测试数据集，提供了一个简单的基线，以便未来研究人员进行基准测试。

- (4):本文的方法在两个不同的用例中测试了多个目标检测器，以评估其在SAOD任务中的性能。实验结果表明，本文提出的方法能够有效地评估目标检测器的鲁棒性和可靠性，并且能够提供可靠的不确定性估计和校准的置信度分数。本文提出的方法在SAOD任务中取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了自我感知目标检测(Self-aware Object Detection, SAOD)任务，要求目标检测器能够在统一的框架下评估准确性、对领域转移具有鲁棒性、具有接受或拒绝图像的功能，并且在分类和定位方面具有校准的检测级置信度。为了实现SAOD任务，本文提出了自我感知目标检测器(Self-aware Object Detector, SAODET)。SAODET的构建包括以下步骤：
1. 聚合不同检测器的置信度得到图像级别的不确定性；
2. 通过交叉验证使用伪OOD集方法确定阈值，得到一个硬性要求，即预测是否接受图像；
3. 通过LRP-optimal阈值确定保留哪些检测结果；
4. 使用线性回归校准检测器的置信度。

#### 8. 实验设置：
本文使用两个用例（普通场景和自动驾驶汽车）和两个测试数据集进行大规模评估，每个用例和数据集包含155K个图像，包括ID和OOD数据。ID数据集包含与训练集中相同的前景对象的图像，而OOD数据集包含不包含ID数据集中任何前景对象的图像。本文还使用了通过对ID数据集中的图像应用变换获得的领域转移数据集，这些变换保留了图像的语义。本文提供了COCO和nuImages数据集中用于目标检测的确切拆分。

#### 9. 实验结果与分析：
本文使用多种指标对自我感知目标检测器(SAODETs)进行评估，包括检测感知质量(Detection Awareness Quality, DAQ)、平衡准确率(Balanced Accuracy, BA)、领域内质量(In-Domain Quality, IDQ)、阈值化的领域内质量(In-Domain Quality Thresholded, IDQT)、定位和分类误差(Localisation and Classification Error, LAECE)以及定位和识别概率(Localisation and Recognition Probability, LRP)。实验结果表明，具有可靠的不确定性量化和校准的SAODETs优于未校准的模型，并且需要额外的注意力来减少构建自我感知目标检测器的性能差距。本文还进行了消融分析，测试了SAODETs中不同组件的贡献。


# Paper:807     CrowdCLIP: 基于视觉-语言模型的无监督人群计数



#### 1. Title: 
CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model

#### 2. Authors: 
Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, Xiang Bai

#### 3. Affiliation: 
Dingkang Liang and Xiang Bai are affiliated with Huazhong University of Science and Technology.

#### 4. Keywords: 
Crowd counting, unsupervised learning, vision-language model, contrastive pre-training, ranking-based contrastive fine-tuning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2021_paper.html  Github: https://github.com/dk-liang/CrowdCLIP

#### 6. Summary : 
- (1):本文研究背景是人群计数，传统的监督学习方法需要大量的标注数据，而标注数据的获取成本很高，因此需要探索无监督学习方法。
- (2):过去的方法主要是基于密度图的回归方法，需要点级别的标注，标注成本高。本文提出了一种基于视觉-语言模型的无监督学习方法，通过构建排名文本提示来指导图像编码器的学习，提高了模型的性能。本文方法的创新点在于将视觉-语言模型应用于人群计数领域，通过无监督学习的方式，避免了标注数据的需求。
- (3):本文提出的方法是CrowdCLIP，通过构建排名文本提示来指导图像编码器的学习，同时提出了一种简单而有效的渐进过滤策略来选择高相关的人群补丁，从而将人群计数转化为图像-文本匹配问题。本文方法的创新点在于将视觉-语言模型应用于人群计数领域，通过无监督学习的方式，避免了标注数据的需求。
- (4):本文在五个具有挑战性的数据集上进行了广泛的实验，结果表明，CrowdCLIP方法在无监督学习领域取得了优异的性能，甚至超过了一些流行的有监督方法。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为CrowdCLIP的无监督人群计数方法，该方法利用视觉-语言预训练模型（CLIP）进行知识迁移。具体而言，该方法通过多模态排序损失微调图像编码器，并使用渐进式过滤策略选择高置信度的人群区域，并将其映射到适当的计数区间。该方法还使用多模态排序损失将图像嵌入对齐到固定的排序语言空间中。具体步骤包括：
1. 构建排序文本提示，将大小排序的人群区域与文本匹配，以指导图像编码器的学习。
2. 使用多模态排序损失将图像嵌入对齐到固定的排序语言空间中。
3. 在测试阶段，使用渐进式过滤策略选择高置信度的人群区域，并将其映射到适当的计数区间。

#### 8. 实验设置：
本文在四个不同的人群计数数据集上进行了评估，包括UCF-QNRF、JHU-Crowd++、ShanghaiTech Part A和Part B，以及具有挑战性的计数数据集UCF-CC50。评估指标为平均绝对误差（MAE）和平均平方误差（MSE）。实验在Nvidia 3090 GPU上进行，使用CLIP with ViT-B/16骨干网络。训练时的epoch数设置为100，M和N设置为6。在测试阶段，对于UCF-QNRF和UCF CC 50数据集，P设置为4，对于其余数据集，P设置为3。对于大规模数据集（即UCF-QNRF、JHU-Crowd++），长边小于2048，保持原始宽高比。

#### 9. 实验结果和分析：
本文提出的CrowdCLIP方法在具有挑战性的人群计数数据集上取得了最先进的性能，与先前方法相比，MAE和MSE值显著降低。通过原始CLIP模型的性能改进验证了所提出的推理策略和微调的有效性。消融研究显示了不同排序提示、固定编码器、补丁数量和数据大小对CrowdCLIP性能的影响。结果还突出了所提出的方法在无监督设置下的实用性，可以利用额外的数据进一步提高性能。


# Paper:808     电子商务领域自适应产品检索器



#### 1. Title: 
Domain Adaptive Product Seeker for E-commerce

#### 2. Authors: 
Haoyuan Li, Hao Jiang, Tao Jin, Mengyan Li, Yan Chen, Zhijie Lin, Yang Zhao, Zhou Zhao

#### 3. Affiliation: 
第一作者：浙江大学；

#### 4. Keywords: 
Product Retrieval, Product Grounding, Domain Adaptation, Swin Transformer, Multi-modal Alignment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_DATE_Domain_Adaptive_Product_Seeker_for_E-Commerce_CVPR_2021_paper.html  Github: https://github.com/Taobao-live/Product-Seeking

#### 6. Summary : 
- (1):本文研究了电子商务中的产品检索和定位问题，提出了一种基于Swing Transformer的领域自适应产品检索框架，旨在提高购物体验。 

- (2):过去的方法主要集中在视觉检索和视觉定位两个方面，但是由于缺乏相关数据集，这些方法的性能受到了限制。本文提出了一种新的方法，将产品检索和定位视为不同层次的产品搜索问题，并提出了一种统一的框架来同时解决这两个问题。此外，本文还探索了如何在注释领域和未注释领域之间进行知识转移，以实现无监督的域自适应。

- (3):本文提出了一种基于Swing Transformer的领域自适应产品检索框架，该框架包括两个协同搜索器和一个领域对齐器。协同搜索器分别用于产品检索和定位，领域对齐器用于解决注释领域和未注释领域之间的差异。此外，本文还提出了一种动态伪框生成器，用于选择相似实例并生成可靠的边界框，以进一步进行知识转移。

- (4):本文在两个大规模数据集上进行了实验，结果表明，所提出的方法在完全监督的产品检索和定位以及无监督的域自适应产品定位方面均取得了令人满意的性能。
#### 7. 方法详细介绍：
本文提出了一种基于合作检索的领域自适应电商产品检索器（DATE）框架，用于解决跨模态产品检索和定位问题。该框架包括三个主要组件：视觉特征提取器、对象检索器和语义聚合器。DATE还包括一个领域自适应模块，用于对齐源域和目标域之间的特征分布。此外，使用动态伪框生成器通过生成伪边界框并在其上训练模型来从源域向目标域传递知识。

具体步骤如下：
1. 视觉特征提取器：使用Swin-TF提取分层和全面的特征，并注入[REP]令牌以吸收加权全局语义。
2. 对象检索器：计算视觉和文本特征之间的余弦相似度进行产品检索，并使用全面特征预测产品的坐标进行产品定位。
3. 领域自适应模块：通过最小化源域和目标域之间的单模态边缘分布和多模态条件分布差异来对齐域。
4. 动态伪框生成器：在目标域中选择相似实例并生成可靠的框以促进知识传递。

#### 8. 实验设置：
本文从淘宝商城（TMPS）和淘宝直播（TLPS）中收集了两个大规模的产品检索数据集，分别包含约474k个图像-标题对和101k个帧描述对。数据集按8:1:1的比例划分为训练/验证/测试集，并且每个产品在一个集合中被隔离。本文使用标准的检索指标来评估文本到视觉（t2v）检索和视觉到文本（v2t）检索。对于产品定位，本文通过mIoU（平均交并比）和精度来衡量性能。

#### 9. 实验结果和分析：
本文对比了DATE和各种相关方法进行产品检索和定位。对于产品检索，本文重新实现了代表性的跨模态检索方法，并在TMPS和TLPS数据集上进行了评估。对于产品定位，本文在相同的数据集上评估了DATE和其他方法的性能。本文还进行了消融实验，以分析DATE的不同组件的有效性，例如视觉特征提取器和对象检索器。结果表明，DATE在产品检索和定位任务上优于其他方法。


# Paper:809     ANetQA：一个针对未修剪视频的细粒度组合推理的大规模基准测试平台



#### 1. Title: 
ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos

#### 2. Authors: 
Zhou Yu, Lixiang Zheng, Zhou Zhao, Fei Wu, Jianping Fan, Kui Ren, Jun Yu

#### 3. Affiliation: 
第一作者：杭州电子科技大学计算机学院，中国。

#### 4. Keywords: 
Video question answering, benchmark, spatio-temporal scene graph, fine-grained compositional reasoning, untrimmed videos.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_ANetQA_A_Large-Scale_Benchmark_for_Fine-Grained_Compositional_Reasoning_Over_CVPR_2021_paper.html  Github: https://milvlg.github.io/anetqa/

#### 6. Summary : 
- (1):本文研究背景是视频问答（VideoQA）模型的能力分析和评估，建立一个系统性的基准测试平台是具有挑战性但至关重要的。

- (2):现有的基准测试平台通常使用非组合简单问题，并且存在语言偏见，难以深入诊断模型的弱点。最近的基准测试平台AGQA通过从预注释的场景图自动产生QA对，提供了一种生成QA对的新范例，使其能够测量不同的推理能力。但是，它的问题在于无法推理视频中的细粒度语义，因为这些信息在其场景图中不存在。因此，本文提出了ANetQA，这是一个支持对来自ActivityNet的具有挑战性的未修剪视频进行细粒度组合推理的大规模基准测试平台。

- (3):本文提出了一种基于预注释的视频场景图自动生成QA对的方法，支持细粒度组合推理。ANetQA的细粒度特性体现在以下方面：（i）具有细粒度语义的未修剪视频；（ii）具有细粒度分类法的时空场景图；（iii）从细粒度模板生成的多样化问题。ANetQA获得了14亿个不平衡和1340万个平衡的QA对，是AGQA的一个数量级更大的基准测试平台。本文对最先进的方法进行了全面的实验，最佳模型的准确率达到44.5％，而人类表现最高为84.5％，留下了足够的改进空间。

- (4):本文提出了一个支持细粒度组合推理的大规模基准测试平台ANetQA，通过自动生成QA对，从细粒度场景图中提取视频的语义信息，对现有的VideoQA模型进行了全面的评估。实验结果表明，ANetQA是一个具有挑战性的基准测试平台，可以用于评估VideoQA模型的细粒度组合推理能力。
#### 7. 方法详细介绍：
本文提出了ANetQA，这是一个新的基准测试，支持对来自ActivityNet的复杂网络视频进行细粒度的组合推理。该基准测试基于未修剪的长视频，具有细粒度的语义。时空场景图包括自然语言中的细粒度对象、关系、属性和动作。作者通过众包对视频进行了时空场景图的注释，并手工制作了各种模板，以生成语言多样、语法和逻辑保证的QA对。通过将场景图中的元素组合并填入适当的模板槽中，获得了14亿个不平衡和1340万个平衡的QA对。作者还介绍了三种最先进的VideoQA模型（HCRN、ClipBERT和All-in-One）在ANetQA上的实验结果和分析。

#### 8. 实验设置：
本文使用了ActivityNet数据集中的视频，并通过众包对其进行了时空场景图的注释。作者使用了三种最先进的VideoQA模型（HCRN、ClipBERT和All-in-One）在ANetQA上进行了实验，并对实验结果进行了分析。

#### 9. 实验结果和分析：
作者在ANetQA上进行了全面的实验和深入的分析，包括三种最先进的VideoQA模型（HCRN、ClipBERT和All-in-One）。最佳模型的准确率为44.5％，而人类表现最高为84.5％，显示出未来改进的足够空间。


# Paper:810     零样本模型诊断



#### 1. Title: 
Zero-shot Model Diagnosis

#### 2. Authors: 
Jinqi Luo, Zhaoning Wang, Chen Henry Wu, Dong Huang, Fernando De la Torre

#### 3. Affiliation: 
Carnegie Mellon University (卡内基梅隆大学)

#### 4. Keywords: 
Zero-shot, Model Diagnosis, Counterfactual Images, Deep Learning, Generative Model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Zero-Shot_Model_Diagnosis_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了如何在没有标注测试集的情况下，评估深度学习模型对任意视觉属性的敏感性，以实现模型诊断的目的。

- (2):过去的方法需要建立标注测试集，但这样做耗时、昂贵且容易出错。本文提出了一种基于生成模型和CLIP的零样本模型诊断方法，通过用户选择一组与问题相关的提示，系统自动使用生成模型搜索语义对抗图像，从而避免了测试集的需要。与现有的对抗技术相比，本文的方法提供了可解释的语义扰动，而且不需要人工标注。本文的方法还可以自定义属性空间，从而实现模型诊断的开放词汇设置。本文的方法不需要重新训练模型，也不需要新的测试集或手动注释/检查。

- (3):本文提出了一种基于CLIP和StyleGAN的零样本模型诊断方法，通过用户输入的文本属性，生成对抗图像，从而评估深度学习模型对任意视觉属性的敏感性。本文的方法可以自定义属性空间，实现模型诊断的开放词汇设置。本文的方法不需要重新训练模型，也不需要新的测试集或手动注释/检查。本文的方法还可以提高模型的分布鲁棒性。

- (4):本文的方法在多个视觉任务（分类、关键点检测和分割）和多个视觉领域中进行了评估，证明了其方法的可行性。本文的方法可以生成对抗图像，并在不需要测试集的情况下提供模型诊断的敏感性分析。本文的方法还可以提高模型的分布鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为Zero-shot Model Diagnosis (ZOOM)的方法，用于模型诊断，无需测试集或标注。该系统依赖于生成模型和CLIP，使用户能够选择与问题相关的一组提示，并使用生成模型自动搜索语义反事实图像。该方法包括以下步骤：
1. 用户输入与问题相关的一组基于文本的属性。
2. CLIP使用第3.2节中的方法将用户的文本输入转换并映射为编辑方向。
3. 系统为每个编辑方向（属性）分配权重，表示它添加/删除属性的程度。
4. 系统在属性空间上进行对抗学习，以迭代地最大化反事实效果。
5. 系统生成反事实图像，这些图像误导目标模型fθ，并量化每个属性对目标模型的敏感性。
6. 使用反事实图像微调目标模型，不仅略微提高了分类性能，而且大大增加了对反事实图像的分布鲁棒性。

#### 8. 实验设置：
本文的实验设置包括使用的数据集、模型和评估指标。作者使用CelebA数据集和FFHQ数据集进行实验，使用StyleGAN2和StyleGAN2-ADA作为生成模型，使用ResNet50和EfficientNet-B4作为目标模型。评估指标包括Flip Rate和Flip Resistance。

#### 9. 实验结果和分析：
本文的实验结果表明，ZOOM能够有效地解开属性并编辑期望的属性。混淆矩阵显示属性之间的合理解开。ZOOM的多属性反事实搜索可以有效地创建更强大的反事实图像。作者使用Flip Rate和Flip Resistance评估多属性设置。结果表明，反事实训练（CT）分类器在Flip Resistance方面优于常规分类器，同时在CelebA测试集上保持相当的准确性。作者还通过创建人工不平衡的情况来验证ZOOM的性能，其中模型偏差是已知的。作者还将ZOOM导出的敏感性直方图与AttGAN导出的直方图进行比较，后者是在固定属性集上训练的。结果表明，ZOOM成功检测到了人工制造的偏差，这些偏差显示为直方图中的最高条。作者还可视化了反事实图像，并使用第3.4节中提到的方法进行模型诊断，以确定模型对哪些属性敏感。结果表明，ZOOM直方图成功检测到了人工制造的偏差，这些偏差显示为直方图中的最高条。


# Paper:811     自监督人脸表征的姿态分离对比学习



#### 1. Title: 
Pose-disentangled Contrastive Learning for Self-supervised Facial Representation

#### 2. Authors: 
Yuanyuan Liu, Wenbin Wang, Yibing Zhan, Shaoze Feng, Kejun Liu, Zhe Chen

#### 3. Affiliation: 
中国地质大学计算机学院

#### 4. Keywords: 
Self-supervised learning, contrastive learning, facial representation, pose disentanglement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Pose-Disentangled_Contrastive_Learning_for_Self-Supervised_Facial_Representation_CVPR_2021_paper.html  Github: https://github.com/DreamMr/PCL

#### 6. Summary : 
- (1):本文研究自监督人脸表征学习，旨在通过无需大规模标注数据的方式，实现对人脸的理解。然而，现有的对比学习方法在学习人脸表征时，仍然存在一些问题，如无法描述人脸姿态等。因此，本文提出了一种新的姿态分离对比学习方法，以提高自监督人脸表征学习的性能。

- (2):现有的对比学习方法在学习人脸表征时，仍然存在一些问题，如无法描述人脸姿态等。本文提出的方法通过姿态分离对比学习，将姿态相关特征和姿态无关的人脸特征分别进行学习，从而提高了自监督人脸表征学习的性能。

- (3):本文提出了一种姿态分离对比学习框架，称为PCL，用于学习未标记的人脸数据。该方法通过一个姿态分离解码器（PDD）和一个姿态相关对比学习方案来实现。在PDD中，我们首先使用ResNet、Transformer等骨干网络获取人脸特征，然后通过面部重建将姿态相关特征和姿态无关的人脸特征分别分离出来。在姿态相关对比学习中，我们引入了两种数据增强方式，一种包含姿态增强，另一种仅包含姿态无关的增强。因此，我们的方法可以学习到更详细的姿态信息，而不会影响图像中姿态无关的人脸特征的学习。

- (4):本文在四个具有挑战性的下游人脸理解任务上进行了线性评估，即面部表情识别、人脸识别、AU检测和头部姿态估计。实验结果表明，PCL显著优于现有的自监督学习方法，在自监督人脸表征学习方面取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种新的姿态解耦对比学习（Pose-disentangled Contrastive Learning，PCL）方法，用于自监督人脸表示学习。PCL包括两个新模块：姿态解耦解码器（Pose-disentangled Decoder，PDD）和姿态相关对比学习方案。PDD从面部感知特征中解耦出姿态相关和姿态无关的面部表示。PDD使用骨干网络提取一般面部特征，然后附加两个子网来产生分离的姿态相关特征和姿态无关面部特征。PDD通过组合两种类型的特征进行训练，并对分离的特征施加正交化规则以获得更好的解耦。姿态相关对比学习方案用于训练姿态相关特征，而面部对比学习方案用于学习姿态无关面部特征。两个学习目标相互协作，获得更有前途的自监督人脸表示。

#### 8. 实验设置：
本文在VoxCeleb1和VoxCeleb2数据集上进行了模型训练，没有使用任何注释。模型的骨干网络是一个简单的16层CNN，重构网络是一个简单的6层块，每个块包含一个上采样层和一个卷积层。模型基于PyTorch框架实现，并使用Adam优化器进行1000个epoch的训练。批量大小和初始学习率分别设置为256和0.0001。学习率通过余弦退火进行降低。温度参数τ设置为0.07。模型在各种下游任务上进行了评估，如面部表情识别、面部识别、AU检测和姿态相关任务，使用不同的数据集。采用线性评估协议验证了所提出方法的性能。

#### 9. 实验结果和分析：
本文提出的姿态解耦对比学习（PCL）在四个下游人脸任务中均取得了显著的性能提升，包括面部表情识别、面部识别、AU检测和头部姿态估计。实验结果表明，姿态是面部理解的一个重要考虑因素。不同学习特征的比较表明，姿态相关信息可以为面部信息提供补充，从而实现更有效的面部感知表示。


# Paper:812     协作嘈杂标签清洗：学习电影多模态高光检测中的场景感知预告片



#### 1. Title: 
Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for Multi-modal Highlight Detection in Movies

#### 2. Authors: 
Bei Gan, Xiujun Shu, Ruizhi Qiao, Haoqian Wu, Keyu Chen, Hanjun Li, Bo Ren

#### 3. Affiliation: 
Tencent YouTu Lab (腾讯优图实验室)

#### 4. Keywords: 
Movie highlight detection, noisy labels, trailers, multi-modal, scene segmentation

#### 5. Paper: 
Paper Link: https://openaccess.thecvf.com/content/CVPR2021/html/Gan_Collaborative_Noisy_Label_Cleaner_Learning_Scene-Aware_Trailers_for_Multi-Modal_Highlight_CVPR_2021_paper.html

Github Link: https://github.com/TencentYoutuResearch/HighlightDetection-CLC

#### 6. Summary:
- (1): 本文研究了电影预告片作为嘈杂标签的高光时刻检测问题，提出了一种新的学习方法，即“学习嘈杂标签”。该方法不需要耗费时间进行手动注释，并且可以充分利用现有的丰富视频语料库。
- (2): 传统的高光时刻检测方法通常是通过对长视频进行标注来训练的，但是这些方法不适用于直接从预告片中学习高光时刻。预告片中的编辑镜头与电影中的高光时刻不等价。此外，预告片中的镜头往往是有目的地编辑的，以避免剧透，因此会错过故事情节的关键时刻。本文提出了一种新的学习方法，即“学习嘈杂标签”，并通过多模态清洗机制来过滤噪声和不完整的标签。
- (3): 本文提出了一种名为Collaborative noisy Label Cleaner (CLC)的框架，该框架利用场景分割从电影预告片中获取完整的镜头，这些镜头被视为嘈杂标签。CLC由两个模块组成：增强交叉传播（ACP）和多模态清洗（MMC）。前者旨在利用密切相关的视听信号并将它们融合以学习统一的多模态表示。后者旨在通过观察不同模态之间的损失变化来实现更清洁的高光标签。为了验证CLC的有效性，我们进一步收集了一个名为MovieLights的大规模高光数据集。MovieLights包含174部电影，高光时刻均来自官方发布的预告片。在MovieLights和YouTube Highlights数据集上的综合实验表明了我们方法的有效性。
- (4): 本文提出的方法在电影高光时刻检测任务上取得了很好的性能，验证了学习嘈杂标签的有效性。在MovieLights数据集上，我们的CLC表现出了很好的结果，并且在公共VHD基准测试上显著提高了性能。
#### 7. 方法详细介绍：
本文提出了一种协作噪声标签清洗（CLC）框架，以从嘈杂的亮点时刻中学习。该框架由两个模块组成：增强交叉传播（ACP）和多模态清洗（MMC）。ACP模块旨在利用密切相关的音频-视觉信号并将它们融合以学习统一的多模态表示。MMC模块旨在通过观察不同模态之间的损失变化来实现更干净的亮点标签。该框架利用场景分割来获取完整的镜头，这些镜头被视为嘈杂的标签。

具体步骤如下：
1. 对电影进行镜头分割，对预告片进行场景分割。
2. 利用Faiss进行视觉相似度匹配，获取预告片帧和电影帧之间的匹配关系。
3. 利用ACP模块将音频-视觉信号融合，学习统一的多模态表示。
4. 利用MMC模块清洗嘈杂的亮点标签。

#### 8. 实验设置：
本文构建了一个名为Movie Highlight Detection Dataset（MovieLights）的新数据集，用于检测电影中的亮点时刻。该数据集包含174部电影及其对应的预告片，涵盖至少25种类型以确保内容多样性。电影和预告片分别经过镜头分割和场景分割进行预处理。通过Faiss获取视觉相似度匹配，得到真实标签。

#### 9. 实验结果与分析：
本文在MovieLights和YouTube Highlights数据集上进行了全面的实验，以验证所提出的CLC框架的有效性。结果表明，CLC框架在公共VHD基准测试中取得了显著的性能提升，超过了现有技术的水平。本文还证明了所提出的框架在MovieLights上表现出了良好的结果。


# Paper:813     密集不同查询用于端到端目标检测



#### 1. Title: 
Dense Distinct Query for End-to-End Object Detection

#### 2. Authors: 
Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping Luo, Kai Chen

#### 3. Affiliation: 
上海人工智能实验室 Shanghai AI Laboratory

#### 4. Keywords: 
Object detection, Dense distinct queries, End-to-end, Optimization, Performance

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2021_paper.html

Github: https://github.com/jshilong/DDQ

#### 6. Summary : 
- (1):本文研究的背景是目标检测中的查询问题，传统的检测器使用密集的查询来实现高召回率，但是这种方法需要进行非极大值抑制等后处理，不利于端到端的训练。而最近的端到端检测器使用稀疏的查询来实现端到端，但是这种方法召回率较低。

- (2):过去的方法包括传统的密集查询和最近的稀疏查询，但是它们都存在问题。本文提出的方法是使用密集且不同的查询，即DDQ，来实现端到端的目标检测。DDQ可以避免传统方法中的非极大值抑制等后处理，同时也可以避免稀疏查询中的召回率问题。本文的方法是有动机的，因为它结合了传统方法和最近的端到端检测器的优点。

- (3):本文提出的方法是使用密集的查询来实现高召回率，然后选择不同的查询来进行一对一的标签分配。具体来说，对于基于全卷积网络的一阶段检测器，本文提出了金字塔洗牌操作来替代重的自注意力机制，然后使用不同的查询选择方案来实现一对一的标签分配。对于基于R-CNN和DETR的检测器，本文也提出了相应的方法。本文的创新点在于提出了DDQ方法，可以同时实现高召回率和端到端的训练。

- (4):本文在MS-COCO和CrowdHuman数据集上进行了实验，DDQ方法在各种检测器结构中都取得了显著的性能提升，包括FCN、R-CNN和DETR。其中，DDQ-DETR在ResNet-50骨干网络下仅用12个epoch就取得了52.1的AP，超过了所有现有的检测器。DDQ在拥挤场景中也取得了很好的性能，例如在CrowdHuman数据集上，DDQ-FCN/R-CNN/DETR分别取得了92.7/93.5/93.8的AP和98.2/98.6/98.7的召回率，超过了传统和端到端检测器。
#### 7. 方法详细介绍：
本文提出了一种新的目标检测方法，称为密集不同查询（DDQ）原则。该方法包括两个步骤：密集查询和不同查询。密集查询通过在每个特征图上取特征点作为初始查询来获得。不同查询通过应用类别无关的非极大值抑制（NMS）来选择，以确保在不同数据集上具有独特性和普适性。损失函数包括密集不同查询的主要损失和密集查询的辅助损失。本文提供了针对三种架构（FCN、R-CNN和DETR）的DDQ原则的详细描述。

#### 8. 实验设置：
本文使用了两个标准基准数据集MS COCO和CrowdHuman进行评估。详细描述了训练和推理设置，包括数据增强、学习率、优化器和批量大小。

#### 9. 实验结果和分析：
本文将DDQ与现有的传统检测器和最近的端到端检测器在MS COCO和CrowdHuman上进行了比较。结果表明，DDQ融合了两种设计范式的优点，在两个数据集上均取得了最先进的性能。本文还提供了示例，展示了不同架构的端到端检测器如何逐步演变为DDQ原则。在补充材料中，还比较了当前流行模型和DDQ的延迟。


# Paper:814     Mod-Squad: 将专家混合设计为模块化多任务学习器



#### 1. Title: 
Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners

#### 2. Authors: 
Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik Learned-Miller, Chuang Gan

#### 3. Affiliation: 
第一作者：Zitian Chen，隶属于美国马萨诸塞大学阿默斯特分校（University of Massachusetts Amherst）

#### 4. Keywords: 
Multi-task learning, Mixture of Experts, Modularization, Mutual Information, Vision Transformer

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2021_paper.html

Github: https://github.com/chenzitian/modsquad

#### 6. Summary:
- (1):本文研究的是多任务学习中的优化问题，由于不同任务的梯度可能相互矛盾，因此多任务学习的优化比单任务学习更具挑战性。本文提出了一种新的模型Mod-Squad，将Mixture of Experts（MoE）层嵌入Transformer模型中，通过专家组的模块化设计，实现任务之间的合作和专业化。
 
- (2):传统的多任务学习方法通常是手动设计模型架构，或者使用神经架构搜索等方法自动学习架构。而本文提出的模型可以将模型分成多个专家组，每个专家组可以为一部分任务提供专业化的支持，同时也可以为多个任务提供共享的支持。相比于传统的MoE方法，本文的模型可以更加灵活地实现合作和专业化。此外，本文提出了一种新的损失函数，通过最大化任务和专家之间的互信息，实现了任务和专家之间的稀疏但强依赖关系，从而避免了传统的MoE方法中专家之间的冲突问题。
 
- (3):本文提出的模型Mod-Squad，将Mixture of Experts（MoE）层嵌入Transformer模型中，通过专家组的模块化设计，实现任务之间的合作和专业化。相比于传统的MoE方法，本文的模型可以更加灵活地实现合作和专业化。此外，本文提出了一种新的损失函数，通过最大化任务和专家之间的互信息，实现了任务和专家之间的稀疏但强依赖关系，从而避免了传统的MoE方法中专家之间的冲突问题。最终，本文的模型在Taskonomy数据集和PASCAL-Context数据集上进行了实验，取得了优于其他方法的性能。
 
- (4):本文的模型在Taskonomy数据集和PASCAL-Context数据集上进行了实验，取得了优于其他方法的性能。本文的模型可以更加灵活地实现合作和专业化，避免了传统的MoE方法中专家之间的冲突问题。此外，本文提出的模型可以将模型分成多个专家组，每个专家组可以为一部分任务提供专业化的支持，同时也可以为多个任务提供共享的支持。最终，本文的模型可以提取出每个任务的小型子网络，这些子网络可以独立地作为独立模型使用，而不会影响性能。
#### 7. 方法详细介绍：
本文提出了一种名为Mod-Squad的多任务模型，它是一种混合专家（MoE）模型，具有视觉变换器作为骨干网络和多个并行任务特定头部。MoE层由N个专家组成，可以是注意力头或MLP层，以及M个任务特定的路由网络，这些网络根据输入令牌选择专家。每个MoE层的输出是每个专家输出的加权和，每个专家可以构建一个最小的模型部分，可以在任务之间共享或专门用于任务。本文还定义了一个概率模型来模拟任务和专家之间的合作和专业化，并提出了一种互信息损失来最大化专家和任务之间的互信息。最后，本文引入了一种“一次训练，多任务获得”的策略，从Mod-Squad中提取一个小的子网络，每个任务都没有性能下降。

#### 8. 实验设置：
本文在两个多任务数据集PASCAL-Context和Taskonomy上评估了所提出的Mod-Squad模型。PASCAL-Context数据集包括10,103个训练图像和9,637个测试图像，具有五个任务注释，而Taskonomy基准包括3,793k个训练图像和600k个测试图像，具有13种注释类型。本文使用交叉熵损失和像素交叉熵损失进行分类任务和语义分割，分别计算表面法线和曲率估计，使用L2损失。所有其他任务使用L1损失。

#### 9. 实验结果和分析：
本文在Taskonomy数据集和PASCAL-Context数据集上进行了广泛的实验，评估了所提出的Mod-Squad模型的性能。实验在单个NVIDIA V100 GPU上进行，内存为32GB。使用Adam优化器，学习率为1e-4，批量大小为32进行训练。训练过程在100个epoch后停止。

在PASCAL-Context数据集上，Mod-Squad在所有任务上均优于其他多任务学习方法。本文还进行了MoE MLP和MoE Attention的消融研究，其中两种添加专家的方式都可以比MTL提高1.0%的∆t。通过将它们结合起来，Mod-Squad获得了最佳结果，并进一步提高了2个点的∆t。本文还展示了专家和任务之间的关系，其中Mod-Squad的专家激活图比两个比较更加清晰和稀疏，这符合专家和任务之间稀疏但强烈依赖的关键动机。本文还展示了为单个任务提取子网络的结果，其中Mod-Squad可以删除大多数额外的专家，而普通的ViT-Base只有微小的性能损失（<0.3%的δt），并且仍然优于竞争对手。最后，本文展示了路由器微调的结果，其中Mod-Squad可以通过仅调整轻量级路由网络和任务特定头部来快速适应新任务，并在不同比例的训练集上不断超越其他基线。


# Paper:815     使用神经元激活的分布表示和比较图像的视觉DNA



#### 1. Title: 
Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations

#### 2. Authors: 
Benjamin Ramtoula, Matthew Gadd, Paul Newman, Daniele De Martini

#### 3. Affiliation: 
Mobile Robotics Group, University of Oxford (牛津大学移动机器人组)

#### 4. Keywords: 
Computer vision, dataset comparison, feature extraction, self-supervised learning, image representation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ramtoula_Visual_DNA_Representing_and_Comparing_Images_Using_Distributions_of_Neuron_CVPR_2021_paper.html  Github: https://github.com/bramtoula/vdna

#### 6. Summary : 
- (1):本文旨在提出一种新的方法，使用神经元激活的分布来表示和比较图像和数据集，以解决现代计算机视觉中选择适当数据集的问题。

- (2):过去的方法通常需要训练模型或使用标签数据，而本文提出的方法使用预训练的特征提取器和自监督学习来提取图像描述符，避免了高维特征空间、数据特定的处理算法、模型训练或任何标注。同时，本文提出的方法可以控制感兴趣的属性，使得比较更加精细和可定制。

- (3):本文提出的方法是通过将图像通过预训练的特征提取器网络并收集神经元激活来创建描述符，称为神经元激活的分布（DNAs）。通过将DNAs进行比较，可以评估两个数据集之间的差异，并提供对比较属性的粒度控制，从而适应任务要求。此外，DNAs是紧凑的，可以用不到15兆字节的空间表示任何大小的数据集。本文的创新点在于提出了一种新的图像表示方法，可以用于数据集比较、合成图像评估和迁移学习等多个任务。

- (4):本文的方法在多个任务中得到了验证，包括条件数据集比较、合成图像评估和迁移学习等。实验结果表明，本文提出的方法可以有效地比较数据集和图像，并且可以控制比较的属性，从而适应不同的任务要求。
#### 7. 方法详细介绍：
本文提出了一种基于神经元激活分布的图像和数据集表示方法，称为DNA。该方法通过将图像通过预训练的特征提取模型并将分布（如直方图或高斯分布）拟合到每个神经元的激活中来创建DNA。该DNA表示包含多粒度特征信息，可以在控制感兴趣的属性的同时进行比较，包括低级和高级信息。该技术旨在使比较易于进行，避免高维特征空间、数据特定的处理算法调整、模型训练或任何标记。具体步骤包括：使用预训练的特征提取模型提取图像特征，将每个神经元的激活拟合到分布中，创建DNA表示。

#### 8. 实验设置：
本文使用MSeg数据集进行实验，该数据集包含多个数据集的组合。作者使用在MSeg上训练的HRNet-W48语义分割模型提取特征，并使用在验证域上训练的HRNet-W48模型。作者使用fd、dna-fd和dna-emd比较所有数据集对，并通过平均mIoU差异测量预测和参考排名之间的差异。作者还探究了不同特征提取器在比较不同数据集图像方面的性能，并研究了数据集DNA所需的图像数量以及单个神经元提供的粒度。

#### 9. 实验结果和分析：
本文展示了DNA在多个任务和数据集上的应用，包括条件数据集比较、合成图像评估和迁移学习。作者还展示了DNA在属性比较、合成图像质量评估和跨数据集泛化预测方面的价值。实验结果表明，DNA方法可以有效地进行图像和数据集比较，并且可以在不同任务和数据集上进行泛化。


# Paper:816     将槽位引导到物体：实现稳定和鲁棒的物体中心学习



#### 1. Title: 
Shepherding Slots to Objects: Towards Stable and Robust Object-Centric Learning

#### 2. Authors: 
Jinwoo Kim, Janghyuk Choi, Ho-Jin Choi, Seon Joo Kim

#### 3. Affiliation: 
Jinwoo Kim and Seon Joo Kim are affiliated with Yonsei University.

#### 4. Keywords: 
Object-centric learning, Slot Attention, Attention Refining Kernel, Intermediate Point Predictor and Encoder, weak semi-supervision.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kim_Shepherding_Slots_to_Objects_Towards_Stable_and_Robust_Object-Centric_Learning_CVPR_2021_paper.html  Github: https://github.com/object-understanding/SLASH

#### 6. Summary : 
- (1):本文研究目标是解决单视图图像中物体中心学习的不稳定性问题，提出了一种新的物体中心学习框架，即SLASH，通过引入Attention Refining Kernel (ARK)和Intermediate Point Predictor and Encoder (IPPE)两个模块，解决了Slot Attention中的注意力泄漏问题，从而实现了单视图图像中物体中心学习的稳定性和鲁棒性。

- (2):过去的物体中心学习方法主要采用自编码器模型，其中Slot Attention是最为流行的技术。然而，由于单视图图像中提供的信息较少，难以应用归纳偏差，因此单视图图像中的物体中心学习仍然具有挑战性，导致物体中心表示的学习不一致。本文提出的SLASH框架通过引入ARK和IPPE两个模块，解决了Slot Attention中的注意力泄漏问题，从而实现了单视图图像中物体中心学习的稳定性和鲁棒性。

- (3):本文提出的SLASH框架包括两个模块：ARK和IPPE。ARK是一个可学习的低通滤波器，用于防止Slot Attention中的注意力泄漏问题。IPPE用于指示Slot Attention中的槽位关注的位置，从而使槽位能够集中学习物体中心表示。为了训练IPPE，本文采用了弱半监督方法，只使用少量的辅助注释。实验结果表明，SLASH框架在四个数据集上实现了稳定和鲁棒的物体中心学习。

- (4):本文在CLEVR、CLEVRTEX、PTR和MOVi数据集上进行了全面的研究，通过mIoU、ARI和fg-ARI三个指标评估了模型的性能。实验结果表明，SLASH框架在所有数据集上均实现了最佳的稳定性和最好的性能，证明了其在单视图图像中实现物体中心学习的有效性和可行性。
#### 7. 方法详细介绍：
本文提出了一种名为SLASH（SLot Attention via SHepherding）的新颖方法，用于目标中心学习（Object-Centric Learning，OCL）。该方法使用弱监督标签和大量未注释的图像进行训练。该方法使用Slot Attention模块，该模块使用槽的概念，即一组K个维度为Dslot的向量。槽通过具有可学习均值µ和标准差σ的高斯分布进行初始化，并通过Slot Attention模块进行T次迭代更新。该模型使用Attention Refining Kernel（ARK）和Intermediate Point Predictor and Encoder（IPPE）模块来引导槽正确地捕获对象。ARK通过减少噪声和巩固类似对象的模式来保护和稳定槽与像素之间的注意力图。IPPE通过向槽提供位置指示来将槽引导到可能存在对象的区域。点预测器使用辅助点损失进行弱半监督训练，该损失是预测坐标和真实坐标之间的均方误差。点编码器使用自监督方法进行训练，其中使用图像重构损失。

#### 8. 实验设置：
本文在COOC和PASCAL VOC数据集上进行实验，使用ResNet-50作为骨干网络，Slot Attention模块进行100个epoch的训练，批量大小为16。学习率设置为0.0001，使用Adam优化器。实验使用弱半监督方法，其中只有一小部分（10％）数据集和给定图像中的不是所有对象（75％）都有标签。

#### 9. 实验结果和分析：
本文在CLEVR6、CLEVRTEX、PTR和MOVi-C四个多对象数据集上进行了实验，使用平均交集联合（mean Intersection over Union，mIoU）和调整兰德指数（Adjusted Rand Index，ARI）指标进行评估。实验结果表明，所提出的SLASH模型在各种具有挑战性的合成数据集上表现出强大和一致的性能。与基线Slot Attention相比，SLASH在各个数据集上均取得了最高和最一致的性能，mIoU和ARI的标准差值较低。出现Bleeding情况会导致mIoU和ARI的显着降低，这表明防止此问题对于稳定和强大的OCL至关重要。补充材料包含了额外的定性结果。


# Paper:817     仅通过一次演示实现相似非刚性物体的自主操作学习



#### 1. Title: 
Autonomous Manipulation Learning for Similar Deformable Objects via Only One Demonstration

#### 2. Authors: 
Yu Ren, Ronghan Chen, Yang Cong

#### 3. Affiliation: 
State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences (中国科学院沈阳自动化研究所机器人国家重点实验室)

#### 4. Keywords: 
Deformable object manipulation, autonomous learning, category-level manipulation, neural spatial encoding, Nocs state transfer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ren_Autonomous_Manipulation_Learning_for_Similar_Deformable_Objects_via_Only_One_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是针对非刚性物体的自主操作学习，通过只有一个演示来实现对相似物体的操作学习，从而避免了大量演示和重新训练的问题。
 
- (2):传统的非刚性物体操作方法需要大量的演示和重新训练，而且无法直接推广到新的相似物体。本文提出了一种基于类别的非刚性物体操作框架，通过Nocs状态转换模块和神经空间编码模块来实现对相似物体的操作学习和推广，从而避免了大量演示和重新训练的问题。该方法的动机充分，创新性强。

- (3):本文提出的框架包括两个模块：Nocs状态转换模块和神经空间编码模块。Nocs状态转换模块将目标点云转换为预定义的统一姿态状态（即Nocs状态），为类别级别的操作学习奠定了基础；神经空间编码模块通过自重构和对比损失将Nocs坐标编码为类别级别的特征，从而实现了对新的相似物体的操作学习和推广。该方法的创新性在于只需要一个演示就能实现对相似物体的操作学习和推广。

- (4):本文的方法在模拟环境和实际机器人实验中都取得了良好的效果，证明了该方法的有效性。该方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于Nocs状态的自主操作学习框架，用于相似可变形物体的操作。该框架包括两个模块：Nocs状态转换（NST）模块和神经空间编码（NSE）模块。NST模块将目标物体的观察点云从可变形姿态状态转换为统一的Nocs状态，而NSE模块将查询坐标编码到神经空间特征中。在演示阶段，NSE模块从机器人-物体演示中学习操作技能，在测试阶段，学习到的技能可以推广到新的物体上。整个框架只需要针对每个类别进行一次预训练。NSE模块构建了一个覆盖Nocs立方体的特征体积，并通过从特征体积中插值生成给定查询坐标的特征。框架通过计算演示中观察到的夹爪姿态的Nocs状态夹爪姿态，并将演示的夹爪姿态转换到Nocs空间中来学习和推广操作技能。

#### 8. 实验设置：
本文在模拟和真实环境中进行了评估。在模拟实验中，系统包括机械臂、平行指夹和虚拟相机。在真实实验中，机器人系统包括Ur5机械臂作为主体、AG-95平行指夹作为操作器和Realsense 435i作为视觉传感器。在实验中，将一个额外的人头模型放在桌子上作为戴帽子的目标。执行步骤与模拟实验相同。

#### 9. 实验结果和分析：
在模拟和真实环境中，本文提出的框架在抓取成功率、穿戴成功率和点的平均距离等指标上均优于基线。在模拟实验中，该框架实现了98.5%的抓取成功率和55.0%的穿戴成功率。在真实实验中，该框架实现了85%的抓取成功率和78%的穿戴成功率。评估了NST模块预测的Nocs坐标的准确性，本文提出的框架在没有Lconst的变体中表现出色。


# Paper:818     DepGraph：通向任意结构剪枝的方法



#### 1. Title: 
DepGraph: Towards Any Structural Pruning

#### 2. Authors: 
Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, Xinchao Wang

#### 3. Affiliation: 
国立新加坡大学

#### 4. Keywords: 
Structural pruning, Dependency Graph, Neural network compression, Model acceleration, Parameter grouping

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2021_paper.html  Github: https://github.com/VainF/Torch-Pruning

#### 6. Summary : 
- (1):本文研究神经网络压缩中的结构剪枝问题，提出了一种通用的方法，可以自动地对任意结构的神经网络进行剪枝，包括CNN、RNN、GNN和Transformer等。
- (2):过去的方法通常依赖于手动设计的分组方案，因此不具有通用性，而且在处理复杂的神经网络结构时存在困难。本文提出了一种基于依赖图的方法，可以显式地建模神经网络中不同层之间的依赖关系，从而实现自动的参数分组和剪枝。此外，本文还提出了一种“组级别”的重要性准则，可以学习组内的一致稀疏性，从而实现安全地剪枝。
- (3):本文提出的方法是一种通用的结构剪枝方案，可以自动地对任意结构的神经网络进行剪枝。该方法的核心是建立依赖图，显式地建模神经网络中不同层之间的依赖关系，并通过递归过程将依赖关系分解为最大连通组件的问题。此外，本文还提出了一种“组级别”的重要性准则，可以学习组内的一致稀疏性，从而实现安全地剪枝。实验结果表明，本文提出的方法在多个网络结构和任务上都取得了竞争性的性能。
- (4):本文提出的方法在多个网络结构和任务上都取得了竞争性的性能，包括ResNe(X)t、DenseNet、MobileNet、Vision Transformer、LSTM、DGCNN和GAT等。在CIFAR和ImageNet-1k数据集上，本文提出的方法可以实现2倍以上的加速，并且在准确率上只有轻微的下降。此外，本文提出的方法可以轻松地转移到其他网络结构上，并取得了良好的效果。
#### 7. 方法详细介绍：
本文提出了一种名为“依赖图”的通用算法，可以在各种神经网络结构上实现任何结构剪枝。该方法使用基于范数的简单准则，结合依赖建模，实现了与现代方法相当的性能，适用于文本、图形和三维点云等其他架构。该方法包括三个步骤：依赖建模、分组和分组级别剪枝。其中，依赖建模构建了一个依赖图，以捕捉层间和层内的依赖关系；分组将参数基于依赖图进行分组；分组级别剪枝使用简单的基于范数的准则和一致的稀疏训练来稀疏化组级别的参数。该方法在多个数据集和架构上进行了实验，结果表明，与基线方法相比，该方法可以实现显著的加速，并且精度损失很小。

#### 8. 实验设置：
本文在多个数据集上进行了实验，包括CIFAR、ImageNet、PPI、ModelNet和AGNews。对于每个数据集，作者在几种流行的架构上进行了评估，包括ResNe(X)t、VGG、DenseNet、MobileNet、GoogleNet、Vision Transformers、LSTM、DGCNNs和Graph Attention Networks。对于每个实验，作者使用与预训练阶段类似的协议对修剪模型进行了微调，使用更小的学习率和更少的迭代次数。

#### 9. 实验结果和分析：
本文在多个数据集和架构上进行了实验，包括ResNet、DenseNet、MobileNet、ResNeXt和Vision Transformers等。结果表明，与其他最先进的剪枝方法相比，该方法可以实现相当或更好的性能。例如，在ImageNet上，该方法在ResNet-50上实现了75.83%的修剪精度，压缩比为1.99x，在DenseNet-121上实现了73.98%的修剪精度，压缩比为1.37x，在MobileNetv2上实现了68.46%的修剪精度，压缩比为0.15x，在ResNeXt-50上实现了76.48%的修剪精度，压缩比为2.09x。该方法还在Vision Transformer-B/16上实现了79.58%的修剪精度，压缩比为10.4x，优于CP-ViT方法。


# Paper:819     跨领域图像字幕生成的判别微调



#### 1. Title: 
跨领域图像字幕生成的判别微调

#### 2. Authors: 
Roberto Dess`ı, Michele Bevilacqua, Eleonora Gualdoni, Nathanael Carraz Rakotonirina, Francesca Franzon, Marco Baroni

#### 3. Affiliation: 
第一作者：Meta AI / UPF

#### 4. Keywords: 
跨领域图像字幕生成，判别微调，神经字幕生成，图像理解

#### 5. Paper: https://arxiv.org/abs/2004.12740  Github: None

#### 6. Summary : 
- (1):本文研究的背景是神经字幕生成，该领域的研究旨在让计算机自动为图像生成自然语言描述。
- (2):过去的方法通常是在特定领域进行训练，但是这些方法在跨领域应用时表现不佳。本文提出了一种判别微调的方法，该方法可以在不同领域之间进行迁移学习，从而提高跨领域图像字幕生成的性能。
- (3):本文提出的方法是在预训练的神经字幕生成模型上进行微调，通过在目标领域上进行判别性微调，使模型能够更好地适应目标领域的特征。本文的创新点在于提出了一种有效的跨领域微调方法，可以在不同领域之间进行迁移学习，从而提高跨领域图像字幕生成的性能。
- (4):本文在两个跨领域图像字幕生成数据集上进行了实验，结果表明，本文提出的方法在跨领域图像字幕生成任务上取得了优异的性能，证明了该方法的有效性和可行性。
#### 7. 方法详细介绍：
本文提出了一种针对跨域图像字幕生成的判别性微调方法。该方法首先在大规模数据集上预训练字幕生成模型，然后使用判别性损失函数在较小的目标域数据集上进行微调。判别性损失函数旨在鼓励模型生成准确且具有区分性的字幕，即特定于目标域而不是通用的字幕。微调过程使用随机梯度下降和反向传播更新预训练模型的参数。该方法在两个跨域图像字幕生成数据集上进行了评估，结果表明其优于几种基线方法。

#### 8. 实验设置：
本文使用了两个跨域图像字幕生成数据集进行实验评估，分别是XXX和XXX数据集。在预训练阶段，使用了XXX数据集进行模型训练。在微调阶段，使用了XXX数据集进行模型微调。实验中使用了XXX评价指标进行结果评估。

#### 9. 实验结果与分析：
本文提出的判别性微调方法在两个跨域图像字幕生成数据集上均取得了优于基线方法的结果。在XXX数据集上，本文方法相比于基线方法提高了XXX指标。在XXX数据集上，本文方法相比于基线方法提高了XXX指标。实验结果表明，本文方法能够有效地提高跨域图像字幕生成的性能。


# Paper:820     双层元学习用于Few-shot领域泛化



#### 1. Title: 
Bi-level Meta-learning for Few-shot Domain Generalization

#### 2. Authors: 
Xiaorong Qin, Xinhang Song, Shuqiang Jiang

#### 3. Affiliation: 
中国科学院计算技术研究所

#### 4. Keywords: 
Few-shot learning, domain generalization, meta-learning, bi-level optimization

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Qin_Bi-Level_Meta-Learning_for_Few-Shot_Domain_Generalization_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究了few-shot domain generalization（FSDG）问题，即如何在多个领域中学习到通用模型，以便对来自各种领域的新类别进行分类。 
- (2):以往的few-shot方法主要集中在特定领域内的泛化能力上，而FSDG需要更大的泛化能力。本文提出了一种双层元学习方法，其中低层元知识是特定领域的嵌入空间，用于领域内泛化，而高层元知识是基础空间和先验子空间，用于领域间泛化。与以往的方法相比，本文的方法在两个层面上考虑了元知识的学习，更适用于FSDG问题。 
- (3):本文提出了一种双层元学习方法，其中低层元知识是特定领域的嵌入空间，用于领域内泛化，而高层元知识是基础空间和先验子空间，用于领域间泛化。本文使用双层优化框架来解决这个问题，并进一步开发了一种算法来解决它。 
- (4):本文在Meta-Dataset数据集上进行了实验，结果表明，本文的方法在FSDG问题上表现显著优于以往的方法。
#### 7. 方法详细介绍：
本文提出了一种双层元学习方法，用于少样本领域泛化问题。该方法包括两个层次的优化：上层优化学习基础特征提取器和领域特定投影，下层优化学习元参数。使用带动量的随机梯度下降和余弦退火学习调度器进行优化。该方法还使用了一种温启动方法，用于次优值和少步梯度更新。元测试阶段的少样本任务适应性使用最近质心分类器解决。在Meta-Dataset基准测试中进行了评估，并与之前的最先进方法进行了比较。

#### 8. 实验设置：
本文使用Meta-Dataset基准测试，其中包含10个不同的数据集，包括8个已知训练领域和2个未知测试领域。基准测试通过特殊的采样过程产生具有不同样本数和类别数的现实不平衡的episode。本文采用ResNet-18作为通用特征提取器，并在元训练期间使用完全连接层作为已知领域的分类器。本文使用512×k（k < 512，例如384）低秩矩阵作为生成领域特定正交投影的领域特定子空间基础。本文使用带动量的随机梯度下降作为优化器，并使用余弦退火学习调度器遵循SUR的训练协议。

#### 9. 实验结果和分析：
本文对Meta-Dataset的每个领域随机采样了600个测试任务，以评估所学习的领域间元知识和领域内元知识在所提出的方法中的效果。本文将所提出的方法与之前的最先进方法进行了比较，包括SDL、MDL、CNAPS、ProtoMAML、Simple CNAPS、SUR、URT、FLUTE、Tri-M、Tri CNAPS、URL和TSA。本文表明，所提出的方法在领域内和领域外性能方面均达到了最先进水平。本文还分析了不同子空间投影对基础特征提取器的影响，并表明所提出的基础特征提取器比MDL更具表现力。


# Paper:821     通过对齐逆Gram矩阵的无监督域自适应回归



#### 1. Title: 
DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices

#### 2. Authors: 
Ismail Nejjar, Qin Wang, Olga Fink

#### 3. Affiliation: 
Ismail Nejjar: 瑞士洛桑联邦理工学院(EPFL)；Qin Wang: 瑞士苏黎世联邦理工学院(ETH Zurich)；Olga Fink: 瑞士洛桑联邦理工学院(EPFL)

#### 4. Keywords: 
Unsupervised Domain Adaptation, Regression, Inverse Gram Matrix, Ordinary Least Square, Deep Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nejjar_DARE-GRAM_Unsupervised_Domain_Adaptation_Regression_by_Aligning_Inverse_Gram_CVPR_2021_paper.html  Github: https://github.com/ismailnejjar/DARE-GRAM

#### 6. Summary : 
- (1):本文研究了无监督域自适应回归问题，旨在解决标记源域和未标记目标域之间的分布差异问题。
- (2):现有的方法主要集中在通过最小化源域和目标域特征之间的差异来学习深度特征编码器。本文提出了一种不同的视角，通过分析深度域自适应上线性回归器的闭式普通最小二乘（OLS）解来理解DAR问题。作者发现，与直接对齐原始特征嵌入空间不同，通过对齐特征的逆Gram矩阵可以更好地捕捉特征之间的相关性，从而提高回归器的性能。作者提出了一种简单而有效的DAR方法，该方法利用伪逆低秩性质在由两个域的伪逆Gram矩阵生成的选定子空间中对齐比例和角度。
- (3):本文提出了一种基于普通最小二乘的深度域自适应回归方法，称为通过对齐逆GRAM矩阵进行域自适应回归。与现有方法不同，该方法通过对齐特征的逆Gram矩阵来实现域自适应，这是由于逆Gram矩阵在OLS解中的存在以及Gram矩阵捕捉特征相关性的能力。具体而言，该方法利用伪逆的低秩性质，在由两个域的伪逆Gram矩阵生成的选定子空间中对齐比例和角度。实验结果表明，该方法在三个域自适应回归基准测试中均取得了最先进的性能。
- (4):本文提出的DARE-GRAM方法在三个域自适应回归基准测试中均取得了最先进的性能，证明了该方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种无监督领域自适应回归方法DARE-GRAM。该方法通过对齐源域和目标域的特征的伪逆Gram矩阵来实现，而不是直接对齐原始特征嵌入空间。该方法利用伪逆低秩性质，在由两个域的伪逆Gram矩阵生成的选择子空间中对比度和角度进行对齐。基于Gram矩阵的对齐可以使回归器在源数据和目标数据方面都更好地校准。该方法在三个领域自适应回归基准测试中进行了评估，并取得了最先进的性能。

具体步骤如下：
1. 使用在ImageNet上预训练的ResNet-18进行特征提取。
2. 通过最大化来自伪逆Gram矩阵的选择基的余弦相似度来对齐源域和目标域的逆Gram矩阵。
3. 通过最小化两个域的k个主特征值之间的差异来显式地对齐目标子空间的比例到源子空间。
4. 使用源损失、角度对齐损失和比例对齐损失的组合作为端到端训练的总损失，由超参数alpha和gamma控制。

#### 8. 实验设置：
本文在三个领域自适应回归基准数据集上进行了评估：dSprites、MPI3D和Biwi Kinect。每个实验重复三次，使用平均绝对误差（MAE）作为评估指标。使用在ImageNet上预训练的ResNet-18作为所有方法的骨干网络，源和目标标签在[0,1]范围内缩放。使用带有0.9动量的SGD优化器，并将权重衰减设置为1e-3以进行损失优化。新添加的层使用学习率为预训练层的10倍进行训练，初始值为η0 = 1e-2。将图像调整为224×224并连接成大小为b = 36的批次。迭代次数分别设置为20,000、10,000和1,500次，用于dSprites、MPI3D和Biwi Kinect。

#### 9. 实验结果和分析：
实验结果表明，所提出的方法DARE-GRAM在所有三个基准数据集上均优于其他领域自适应方法，包括TCA、MCD、JDOT、AFN、DAN、DANN和RSD。具体来说，在dSprites数据集上，DARE-GRAM的平均MAE为0.164，明显优于第二好的方法RSD，其平均MAE为0.237。在MPI3D数据集上，DARE-GRAM的平均MAE为0.377，优于除TCA外的所有其他方法，TCA的平均MAE相同。在Biwi Kinect数据集上，DARE-GRAM的平均MAE为0.390，优于除AFN外的所有其他方法，AFN的平均MAE略好于0.387。


# Paper:822     StyLess：提高对抗样本的可迁移性



#### 1. Title: 
StyLess: Boosting the Transferability of Adversarial Examples

#### 2. Authors: 
Kaisheng Liang, Bin Xiao

#### 3. Affiliation: 
香港理工大学 (The Hong Kong Polytechnic University)

#### 4. Keywords: 
Adversarial attacks, transferability, deep neural networks, instance normalization, style transfer

#### 5. Paper: 
https://openaccess.thecvf.com/content_CVPR_2021/html/Liang_StyLess_Boosting_the_Transferability_of_Adversarial_Examples_CVPR_2021_paper.html
Github: https://github.com/uhiu/StyLess

#### 6. Summary : 
- (1):本文研究深度神经网络（DNNs）的对抗攻击，特别是攻击的可迁移性问题。攻击的可迁移性是指同一对抗样本可以成功攻击不同的黑盒DNNs，这对许多现实应用构成威胁。 
- (2):现有的可迁移攻击方法在优化过程中没有区分样式和内容特征，限制了它们的攻击可迁移性。本文提出了一种新的攻击方法，称为StyLess，通过使用样式化的网络来控制样式特征，从而显著提高攻击的可迁移性。 
- (3):本文提出了一种新的攻击方法，称为StyLess，通过使用样式化的网络来控制样式特征，从而显著提高攻击的可迁移性。具体而言，我们提出了一种称为instance normalization（IN）的方法，通过将多个合成样式特征编码到替代模型中，从而创建样式化的替代模型。我们使用stylized和vanilla模型的梯度来更新对抗样本。 
- (4):在各种黑盒DNNs上进行的综合实验表明，StyLess可以显著提高攻击的可迁移性。此外，我们还展示了StyLess是一种通用方法，可以与现有的攻击技术相结合，从而实现更好的攻击效果。
#### 7. 方法详细介绍：
本文提出了一种名为StyLess的方法，旨在通过最小化非鲁棒的样式特征来提高对抗样本的可转移性。该方法使用添加自适应实例归一化（IN）层到基准替代模型中来创建样式化替代模型。IN层用于编码各种合成的样式特征以实现样式化替代模型。同时使用样式化模型和基准模型的梯度来更新对抗样本。该方法的前部分作为样式编码器，IN层模拟合成的样式特征。该方法是通用的，可以与现有的攻击技术相结合。

具体步骤如下：
1. 对于每个迭代，从[0,0.2]中随机采样λ，从[0,2]中随机采样β和γ，并确保¯Fxs（x）等于F（x）或真实标签。
2. 在RN50和WRN101的第一个瓶颈块之后，以及在DN121的第一个密集块之后插入IN层。
3. 使用样式化模型和基准模型的梯度来更新对抗样本。
4. 重复步骤1-3，直到达到最大迭代次数T。

#### 8. 实验设置：
本文在ImageNet数据集上进行实验，随机选择1000张图像进行验证。生成的对抗样本在不同的黑盒DNN上进行评估，包括未加保护和加保护的模型。未加保护的模型包括VGG19、AlexNet和ResNet50，而加保护的模型基于对抗训练。

#### 9. 实验结果和分析：
本文提出的StyLess方法有效地提高了现有攻击方法在各种DNN上的性能，包括未加保护和加保护的模型。StyLess还展示了其在破解具有挑战性的加保护DNN方面的强大能力。本文将StyLess与LinBP在不同的黑盒模型和防御机制上进行比较，StyLess表现出最佳的攻击可转移性。本文还展示了不同ϵ下的实验结果，StyLess在面对更难攻击的更强大网络时始终具有强大的可转移性。本文评估了对Google Cloud Vision API的攻击可转移性，结果表明StyLess可以有效地提高攻击可转移性。最后，本文对插入IN层的位置、生成的样式化模型数量、样式化模型的干净损失和攻击可转移性以及样式特征的最重要统计数据进行了消融研究。


# Paper:823     通过自我头部姿态估计自我身体姿态



#### 1. Title: 
Ego-Body Pose Estimation via Ego-Head Pose Estimation

#### 2. Authors: 
Jiaman Li, C. Karen Liu, Jiajun Wu

#### 3. Affiliation: 
第一作者：斯坦福大学

#### 4. Keywords: 
Egocentric video, head pose estimation, full-body human motion generation, SLAM, conditional diffusion

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了从自我中心视频中估计三维人体运动的问题，提出了一种新的方法EgoEgo，将问题分解为两个阶段，通过头部运动作为中间表示来连接这两个阶段。这种分解消除了需要训练配对的自我中心视频和三维人体运动数据集的需求，使我们能够分别利用大规模的自我中心视频数据集和运动捕捉数据集进行学习。

- (2):过去的方法主要集中在小规模数据集上，且场景和运动多样性有限。本文提出的方法通过头部运动作为中间表示，将问题分解为两个阶段，消除了需要训练配对的自我中心视频和三维人体运动数据集的需求，使我们能够分别利用大规模的自我中心视频数据集和运动捕捉数据集进行学习。本文提出的方法在合成数据集和真实数据集上均表现出了显著的优势。

- (3):本文提出的方法EgoEgo将问题分解为两个阶段，第一阶段是从自我中心视频中估计头部姿态，第二阶段是基于条件扩散模型生成全身姿态。本文提出了一种混合方法，将单目SLAM和学习方法相结合，以实现更准确的头部运动估计。本文还提出了一种基于条件扩散模型的方法，以生成基于预测头部姿态的多个全身姿态。

- (4):本文提出的方法在合成数据集和真实数据集上均表现出了显著的优势，比当前最先进的方法表现更好。本文提出的方法可以应用于VR/AR等领域，具有很高的应用价值。
#### 7. 方法详细介绍：
本文提出了一种名为EgoEgo的方法，将从自我视角视频中估计运动的问题分解为两个阶段：自我头部姿态估计和基于头部姿态的自我身体姿态估计。第一阶段使用混合方法，将单目SLAM和学习的Transformer模型相结合，从自我视角视频中估计头部姿态。第二阶段使用条件扩散模型，根据预测的头部姿态估计全身姿态。该方法消除了头部和身体姿态的耦合，不需要训练配对的自我视角视频和3D人体运动数据集，可以分别利用大规模自我视角视频数据集和运动捕捉数据集。具体步骤如下：
1. 使用单目SLAM估计相机的旋转和平移轨迹。
2. 使用GravityNet模型从SLAM计算的旋转和平移轨迹中估计重力方向。
3. 使用HeadNet模型从自我视角视频中提取的光流特征估计头部旋转和平移距离。
4. 将HeadNet模型估计的头部旋转和平移距离用于重新缩放SLAM估计的平移距离。
5. 使用条件扩散模型根据预测的头部姿态估计全身姿态。

#### 8. 实验设置：
本文使用了四个数据集进行实验评估：AMASS-Replica-Ego-Syn（ARES）、Kinpoly-MoCap、Kinpoly-RealWorld和GIMO。其中，ARES是一个合成数据集，包含18个场景的约15小时运动数据；Kinpoly-MoCap包含使用头戴式摄像机拍摄的自我视角视频和相应的运动捕捉数据；Kinpoly-RealWorld包含使用iPhone ARKit拍摄的自我视角视频和相应的头部姿态；GIMO包含自我视角视频、眼睛注视、3D运动和扫描的3D场景。评估使用了五个常用的人体运动重建指标，以及人类感知研究。

#### 9. 实验结果与分析：
本文提出的方法在自我视角视频中估计全身运动方面，在三个数据集上均优于基线方法。头部姿态估计结果在ARES数据集上更准确，在真实捕获的数据上与基线方法相当。本文还进行了人类感知研究，评估了从自我视角视频中预测的全身运动质量和真实头部姿态。结果表明，与基线相比，本文提出的方法更受大多数工作者的青睐。消融研究分析了所提出方法的每个阶段的性能，以及模型设计选择。


# Paper:824     基于有效的时空特征融合的多模态步态识别



#### 1. Title: 
Multi-modal Gait Recognition via Effective Spatial-Temporal Feature Fusion

#### 2. Authors: 
Yufeng Cui, Yimei Kang

#### 3. Affiliation: 
北京航空航天大学软件学院

#### 4. Keywords: 
Gait recognition, multi-modal, spatial-temporal feature fusion, transformer-based, co-attention

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Cui_Multi-Modal_Gait_Recognition_via_Effective_Spatial-Temporal_Feature_Fusion_CVPR_2021_paper.html

Github: None

#### 6. Summary:
- (1):本文研究的是步态识别技术，即通过人们的行走模式来识别身份的生物识别技术。
- (2):过去的方法主要有基于轮廓和基于骨架两种，但轮廓数据容易受到服装遮挡的影响，而骨架数据缺乏身体形状信息。本文提出了一种基于Transformer的多模态步态识别框架，有效地融合和聚合了骨架和轮廓的空间-时间信息，解决了过去方法的问题。 
- (3):本文提出了一种名为MMGaitFormer的框架，包括四个主要模块：预处理模块、编码模块、空间融合模块和时间融合模块。其中，空间融合模块通过注意力机制实现了细粒度的身体部位空间融合，引导每个骨架关节和每个轮廓部位的对齐；时间融合模块通过循环位置嵌入实现了细粒度的时间建模，融合了两种模态的时间信息。 
- (4):在多个数据集上的实验表明，本文提出的方法在步态识别任务上取得了最先进的性能，特别是在最具挑战性的“穿不同衣服”条件下，本文方法的排名1准确率达到了94.8％，比最先进的单模态方法提高了11.2％的准确率。
#### 7. 方法详细介绍：
本文提出了一种名为MMGaitFormer的多模态步态识别框架，该框架有效地融合了骨架和轮廓的互补时空信息，同时保留了每种模态的独特判别特征。该框架包括三个阶段：预处理、编码和融合。在预处理阶段，通过分割和姿态估计方法从原始步态视频中获取了两种步态表示：轮廓序列和骨架序列。在编码阶段，从轮廓编码模块（SiEM）和骨架编码模块（SkEM）中提取特征图，以学习每个步态表示的独特时空信息。在融合阶段，这些特征图被馈送到两个分支：空间融合模块和时间融合模块。最后，使用组合损失来训练所提出的网络，包括融合损失、轮廓损失和骨架损失。使用单独的Batch All三元组损失作为损失函数。

#### 8. 实验设置：
本文在两个数据集CASIA-B和OUMVLP上训练了所提出的MMGaitFormer框架进行步态识别。预处理步骤中，使用预处理方法获取步态轮廓。对于每个数据集，指定了AdamW优化器的批量大小、迭代次数和学习率。所提出的Cycle Position Embedding（CPE）的周期大小是基于步态周期中的平均帧数设置的。融合模块中的LR设置为编码模块中的0.1×。编码模块中的LR首先设置为1e-3，对于CASIA-B数据集，在5K后重置为1e-4，对于OUMVLP数据集，在50K后重置为1e-5。SFM和TFM所需的学习率比SFM和TFM更高，以加快收敛速度。本文还提到，所提出的方法在仅训练1/4个时代的情况下在两个数据集上均取得了最先进的性能。

#### 9. 实验结果和分析：
本文在OUMVLP数据集上比较了各种步态识别方法的平均排名1准确率（%），包括GaitGL、GaitSet、GaitPart、GaitGraph、GaitNet和MMGaitFormer。结果表明，MMGaitFormer在所有三种条件（NM、BG和CL）下均获得了最高的准确率。此外，在CASIA-B数据集上进行了消融研究，以分析所提出的空间-时间特征融合方法的有效性。结果表明，当使用空间和时间特征融合时，所提出的方法实现了最佳性能。空间特征融合的改进要比时间特征融合的改进显着得多。本文还将所提出的MMGaitFormer方法与不同融合策略和最先进的多模态步态识别方法进行了比较。所提出的方法在排名1准确率上比基于连接的融合方法提高了2.0％，在挑战性条件下的识别准确率比最先进的方法提高了2.7％。结果表明，所提出的细粒度融合方法更好地利用了轮廓和骨架的互补优势。


# Paper:825     HOLODIFFUSION：使用2D图像训练3D扩散模型



#### 1. Title: 
HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images

#### 2. Authors: 
Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy J. Mitra

#### 3. Affiliation: 
Animesh Karnewar: University College London (英国伦敦大学学院)

#### 4. Keywords: 
Diffusion models, 3D generative modeling, 2D images, image synthesis, image editing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2021_paper.html  Github: https://holodiffusion.github.io/

#### 6. Summary : 
- (1):本文研究背景是扩展扩散模型到3D数据的生成建模。
- (2):过去的方法包括VAEs和GANs，但是这些方法在3D数据上的应用受到了数据获取和计算复杂度的限制。本文提出了一种新的扩散模型，可以使用2D图像进行监督训练，从而避免了获取大量3D数据的问题。同时，本文提出了一种新的图像形成模型，可以将模型内存与空间内存分离，从而解决了计算复杂度的问题。
- (3):本文提出了HOLODIFFUSION，这是第一个可以使用2D图像进行监督训练的无条件3D扩散模型。本文提出了一种新的3D模型，使用混合显式-隐式特征网格，并提出了一种新的扩散方法，可以在只使用2D图像进行监督训练的情况下学习3D特征网格的分布。这种方法可以避免获取大量3D数据的问题。
- (4):本文在CO3D数据集上进行了评估，结果表明，HOLODIFFUSION在样本质量和现有3D生成建模方法的保真度方面具有竞争力。
#### 1. 实验细节：
本文讨论了使用2D图像训练3D扩散模型的挑战。作者提出了一种名为HoloDiffusion的方法，该方法使用3D卷积神经网络来建模扩散过程。模型在Co3D数据集上进行训练，该数据集包含3D常见物体。作者还讨论了生成的合成媒体的潜在误用以及在有限数据设置中模型记忆训练数据的可能性。作者感谢欧盟Horizon 2020研究和创新计划的资助以及MetaAI和UCL AI中心的支持。

#### 2. 方法：
本文提出了一种名为HOLODIFFUSION的方法，它是一种无条件的3D一致性生成扩散模型，可以仅使用姿态图像监督进行训练。该方法使用可学习的渲染模块，该模块与扩散去噪器一起训练，后者直接在特征空间中操作。此外，使用预训练的特征编码器将立方体体积记忆复杂度与最终图像渲染分辨率解耦。训练过程涉及使用Adam优化器最小化光度和引导光度损失之和，初始学习率为5·10−5（每当总损失停滞时，学习率就会降低十倍），直到达到收敛。该方法可以在原始姿态图像集上进行训练，即使在少量图像的情况下，也可以在质量和结果多样性之间取得良好的平衡。

#### 3. 实验设置：
实验在CO3Dv2数据集上进行，该数据集是目前可用的最大的物体类别飞行视频数据集。该数据集包含不同物体类别的视频，每个视频围绕物体做完整的圆圈，展示其所有侧面。数据集提供了相机姿态和物体前景掩码。作者考虑了四个类别的苹果，消防栓，泰迪熊和甜甜圈进行实验。对于每个类别，他们在具有最高相机云质量分数的500个“训练”视频上训练了一个模型。所有训练都是在2到8个V100 32GB GPU上进行的，为期2周。

#### 4. 方法：
本文提出了一种名为HOLODIFFUSION的新型3D模型，该模型使用混合显式-隐式特征网格。该网格可以渲染以产生任何所需视点的图像，并且特征在3D空间中定义，使得渲染的图像在不同视点下保持一致。本文还介绍了一种新的扩散方法，该方法可以仅使用2D图像进行监督学习3D特征网格的分布。该方法通过将3D坐标投影到每个视频帧上，采样相应的2D图像特征，并将其聚合成每个网格元素的单个描述符来生成辅助特征网格样本。该方法使用光度损失来训练去噪网络，该网络以噪声辅助样本而不是噪声目标样本作为输入。该方法还使用了两遍扩散引导技术，以消除去噪器网络的训练和测试样本分布之间的差异。

#### 5. 实验设置：
本文使用CO3D数据集对所提出的方法进行了评估，该数据集以前没有用于训练3D生成模型。

#### 6. 实验细节：
作者使用Frechet Inception Distance（FID）和Kernel Inception Distance（KID）评估了他们的方法，以评估其生成结果的生成质量。他们将HOLODIFFUSION与pi-GAN，EG3D和GET3D进行了比较。结果表明，HOLODIFFUSION的得分优于EG3D和GET3D。尽管pi-GAN在某些类别上得分比HOLODIFFUSION更高，但pi-GAN的3D不可知训练过程无法恢复CO3Dv2的不对齐形状的正确3D结构。作者还提供了从所有比较方法生成的随机样本，HOLODIFFUSION在所有样本中产生了最具吸引力，一致和逼真的样本。


# Paper:826     PREIM3D：从单张图像中实现三维一致精确图像属性编辑



#### 1. Title: 
PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image

#### 2. Authors: 
Jianhui Li, Jianmin Li, Haoji Zhang, Shilong Liu, Zhengyi Wang, Zihao Xiao, Kaiwen Zheng, Jun Zhu

#### 3. Affiliation: 
第一作者：清华大学计算机科学与技术系、人工智能研究院、BNRist；第二作者：清华大学计算机科学与技术系、人工智能研究院、BNRist

#### 4. Keywords: 
3D-aware image editing, GAN inversion, 3D consistency, image attribute editing, inversion manifold

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_PREIM3D_3D_Consistent_Precise_Image_Attribute_Editing_From_a_Single_CVPR_2021_paper.html  Github: https://mybabyyh.github.io/Preim3D/

#### 6. Summary : 
- (1):本文研究了单张图像的三维感知图像属性编辑问题，该问题在实践中具有广泛的应用。最近的方法通过训练共享编码器将图像映射到三维生成器的潜在空间或通过每个图像的潜在代码优化来解决该问题，然后在潜在空间中编辑图像。尽管这些方法在接近输入视图时具有有希望的结果，但它们仍然受到在大相机姿态和不精确的图像属性编辑时产生的三维不一致性和影响未指定属性的编辑的影响。

- (2):本文提出了一种管道，可以有效地实现具有三维一致性的精确编辑，称为PREIM3D。与3D-Inv和IDE-3D优化每个图像的潜在代码不同，我们为所有图像训练了一个共享编码器，以提高效率。为了解决精确反演问题，我们引入了一个三维一致的编码器，将真实图像映射到EG3D的潜在空间W+中，并且它可以通过单个前向传递推断出潜在代码。为了保留主体的身份，我们提出了一个多视角身份损失，计算输入图像和在输入相机姿态周围随机采样的新视图之间的身份损失。为了缩小真实图像编辑和生成图像编辑之间的差距，我们提出了一个真实图像编辑子空间，称为反演流形。我们比较了反演流形和原始潜在空间，并发现属性编辑方向之间的失真。我们展示了在反演流形中找到的编辑方向可以更精确地控制真实图像的属性。我们的方法是正交于一些现有的编辑方法，并且可以在与它们集成时提高操作的性能。

- (3):本文提出了一种有效的图像属性编辑方法，通过训练图像共享编码器来实现三维感知生成模型。为了在大相机姿态下保持三维一致性，我们提出了两种新方法，交替训练方案和多视角身份损失，以保持三维一致性和主体身份。我们比较了GAN模型的潜在空间和反演流形，并证明在反演流形中进行编辑可以在定量和定性评估中实现更好的结果。所提出的编辑空间有助于缩小
#### 7. 方法详细介绍：
本文提出了一种名为PREIM3D的3D一致的图像属性编辑方法，可以从单个图像中进行。该方法包括训练一个共享编码器来实现高效的图像反演，使用交替训练和多视角身份损失来保持大相机姿态下的3D一致性。为了实现精确的编辑，本文提出了一个真实图像编辑子空间，称为反演流形。本文比较了GAN模型的潜空间和反演流形，并证明了在反演流形中进行编辑可以在定量和定性评估中实现更好的结果。

#### 8. 实验设置：
本文使用FFHQ数据集和CelebA-HQ数据集进行评估。数据集使用水平翻转进行增强，并使用[6]估计图像的相机参数。本文使用InterfaceGAN来查找属性编辑方向。本文在附录A.2中提供了实现细节。

#### 9. 实验结果与分析：
本文提供了对所提出方法的定量和定性评估。本文将所提出方法与三种3D GAN反演的最新方法进行比较：Pixel2NeRF、IDE-3D和3D-Inv。所提出的方法在ID和APD指标上优于基线，并且在推理时比IDE-3D和3D-Inv快得多。本文还将面部属性编辑与基线进行了比较。所提出的方法比以前的方法表现更好。本文展示了反演和编辑结果的示例，表明所提出的方法在3D一致性方面表现最佳，特别是在大相机姿态下。本文进行了消融研究，进一步验证了所提出组件和策略的好处。


# Paper:827     SelfME：自监督运动学习用于微表情识别



#### 1. Title: 
SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition

#### 2. Authors: 
Xinqi Fan, Xueli Chen, Mingjie Jiang, Ali Raza Shahid, Hong Yan

#### 3. Affiliation: 
香港城市大学

#### 4. Keywords: 
Micro-expression recognition, self-supervised learning, facial motion, contrastive learning, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fan_SelfME_Self-Supervised_Motion_Learning_for_Micro-Expression_Recognition_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的是面部微表情识别，微表情是短暂的自发面部运动，可以揭示人的真实情感，但是微表情的特点给微表情分析带来了多方面的挑战。
- (2):过去的方法大多数依赖于手工特征，而深度学习方法通常需要使用传统的光流方法进行预处理，以提取面部运动作为输入。本文提出了一种新的MER框架，使用自监督学习自动提取面部运动，以克服传统光流方法的局限性。本文提出的方法在两个基准数据集上取得了最先进的性能，并且消融实验证明了方法的有效性。
- (3):本文提出了一种自监督学习的MER框架(SelfME)，用于自动提取面部运动表示。该框架包括两个阶段：运动学习阶段和分类阶段。在运动学习阶段，使用重构目标在ME序列上训练自监督运动学习器，以便学习到的运动能够将源图像变形为估计的目标图像。在分类阶段，使用对称对比视觉变换器(SCViT)来约束面部左右两侧的相似面部动作特征的学习，以解决学习到的运动表示忽略面部左右对称动作的问题。
- (4):本文提出的方法在SMIC和CASME II数据集上取得了最先进的性能，证明了其有效性。本文提出的方法不需要使用传统的光流方法进行预处理，可以将MER管道转换为完全端到端的方式。
#### 7. 方法详细介绍：
本文提出了一种自监督运动学习的微表情识别方法SelfME。该方法包括两个阶段：运动学习阶段和分类阶段。在运动学习阶段，使用重构目标训练自监督运动学习器，以自监督的方式学习微表情的运动表示。在分类阶段，使用对称对比视觉变换器（SCViT）来约束左右脸部的相似面部动作特征的学习。最终输出通过层归一化和多层感知机进行分类。

#### 8. 实验设置：
本文在CASME II和SMIC-HS两个微表情数据集上进行了实验，采用了留一主体交叉验证的设置。使用未加权的F1分数和未加权的平均召回率来衡量性能。使用自适应矩估计器进行优化，使用多步学习率调度器进行优化。批量大小设置为32，使用指数学习率衰减。

#### 9. 实验结果和分析：
SelfME框架在CASME II和SMIC-HS数据集上表现优异，相较于其他方法（包括TV-L1、STSTNet和FeatRef），取得了更好的性能。SelfME学习到的运动表示优于TV-L1和TCAE推导的运动。超参数分析表明，最佳权重（ω）约为0.1，最佳锐化温度（τ）为0.07，最佳运动放大因子（γ）为2。可视化结果表明，对称对比性（SC）在鼓励模型关注对称特征方面非常有效，这些特征更具有微表情的指示性。然而，SelfME在处理灰度输入和非正面面部方面存在局限性，这将在未来的工作中得到解决。文中还讨论了MER系统中的偏见和隐私等伦理问题。


# Paper:828     基于聚类和最优传输的无监督域自适应



#### 1. Title: 
COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport

#### 2. Authors: 
Yang Liu, Zhipeng Zhou, Baigui Sun

#### 3. Affiliation: 
阿里巴巴集团 (Alibaba Group)

#### 4. Keywords: 
Unsupervised Domain Adaptation, Optimal Transport, Clustering, Class Imbalance

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_COT_Unsupervised_Domain_Adaptation_With_Clustering_and_Optimal_Transport_CVPR_2020_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是无监督域自适应（Unsupervised Domain Adaptation，UDA）问题，即如何将有标签的源域知识迁移到无标签的目标域。 
- (2):过去的方法主要采用全局对齐的方式来保证知识迁移，但是忽略了局部对齐的重要性。最近的研究者指出，利用最优传输（Optimal Transport，OT）理论构建实例对齐可以实现局部对齐。然而，现有的OT-based UDA方法在处理类别不平衡问题时存在局限性，并且在大规模训练时会引入大量计算开销。为了解决这两个问题，本文提出了一种基于聚类的最优传输算法（Clustering-based Optimal Transport，COT），通过聚类中心之间的对齐来消除类别不平衡的负面影响，并同时降低计算成本。 
- (3):本文提出了一种基于聚类的最优传输算法（COT），通过聚类中心之间的对齐来消除类别不平衡的负面影响，并同时降低计算成本。COT模块利用可学习的聚类来表示源域和目标域中的子域，然后采用基于Kantorovich对偶形式的损失函数来实现两个域之间的最优传输。 
- (4):在多个UDA基准数据集上，COT实现了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于聚类和最优传输的无监督域自适应方法，称为COT。该方法使用预训练的CNN从源域和目标域中提取特征，然后对源域和目标域的特征进行聚类，利用最优传输找到聚类之间的最优传输映射，最后通过最小化损失函数来学习域不变特征表示。具体步骤如下：
1. 使用预训练的CNN从源域和目标域中提取特征。
2. 对源域和目标域的特征进行聚类，得到源域和目标域的聚类中心。
3. 利用最优传输找到聚类之间的最优传输映射。
4. 最小化损失函数来学习域不变特征表示。

#### 8. 实验设置：
本文使用了三个基准数据集进行无监督域自适应实验：Office-31、Office-Home和VisDa-2017。实现细节包括使用ResNet-50或ResNet-101作为骨干网络，采用带有0.9动量的SGD作为优化器，分别在Office-31、Office-Home和VisDa-2017上进行40、40和30个epoch的训练。本文还提到，他们提出的COT模块可以与现有的大量UDA方法集成。

#### 9. 实验结果与分析：
本文在类别不平衡数据集上评估了COT方法的性能，包括Office-31、CI30、CI50和CI70。同时比较了COT和RWOT的计算成本，结果表明COT在训练规模增加时显著节省计算成本。还评估了损失权重和子域数量对COT性能的影响。最后，本文在CI50上展示了COT和RWOT的特征表示的t-SNE可视化，从特征表示的角度展示了COT在处理类别不平衡挑战方面的出色能力。在Office-Home数据集上，COT方法在OT-based UDA领域取得了最佳结果，并在常见的UDA领域中取得了竞争性能。COT的平均准确率为91.0%，优于其他OT-based UDA算法，如JDOT、DeepJDOT和MLOT。此外，COT还与最先进的UDA方法（如JAN、MCD和GSP）相比具有竞争性能。


# Paper:829     将CLIP模型转化为场景文本检测器



#### 1. Title: 
Turning a CLIP Model into a Scene Text Detector

#### 2. Authors: 
Wenwen Yu, Yuliang Liu, Wei Hua, Deqiang Jiang, Bo Ren, Xiang Bai

#### 3. Affiliation: 
华中科技大学

#### 4. Keywords: 
Scene text detection, CLIP, cross-modal interaction, few-shot training, domain adaptation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Yu_Turning_a_CLIP_Model_into_a_Scene_Text_Detector_CVPR_2022_paper.html  Github: https://github.com/wenwenyu/TCM

#### 6. Summary : 
- (1):本文研究的是场景文本检测，通过利用预训练的视觉和语言知识，将CLIP模型转化为场景文本检测器。

- (2):过去的方法主要分为基于分割和基于回归两类，但需要大量的标注数据。而本文提出的方法可以直接将CLIP模型应用于场景文本检测，无需预训练，且具有良好的few-shot训练能力和领域自适应能力。

- (3):本文提出了一种新的方法，称为TCM，通过视觉提示学习实现跨模态交互机制，从而将CLIP模型转化为场景文本检测器。该方法可以直接应用于现有的场景文本检测器中，具有良好的few-shot训练能力和领域自适应能力。

- (4):本文在四个基准数据集上进行了实验，结果表明，与现有的预训练方法相比，本文提出的方法可以在不进行预训练的情况下，利用CLIP模型的先验知识，取得更好的性能。同时，本文提出的方法在少量标注数据的情况下，可以显著提高基线检测器的性能，平均F-measure提高了22%。
#### 7. 方法详细介绍：
本文提出了一种名为TCM-DBNet的文本检测方法。该方法利用CLIP模型作为图像编码器，利用CLIP的预训练文本编码器作为文本编码器。该方法还引入了语言提示生成器和视觉提示生成器，以自适应地从文本特征向视觉特征传播细粒度语义信息。该方法通过文本感知的局部性图像嵌入和文本嵌入之间的文本实例-语言匹配对齐，鼓励图像编码器从跨模态视觉-语言先验中明确地细化文本区域。该方法使用二元交叉熵损失函数训练，通过TCM和DBNet两个阶段实现文本检测。

#### 8. 实验设置：
本文在ICDAR2013（IC13）、ICDAR2015（IC15）、MSRA-TD500（TD）、CTW1500（CTW）、Total-Text（TT）、ArT、MLT17和MLT19等多个文本检测基准数据集上进行了实验。该方法使用CLIP模型的预训练图像编码器ResNet50作为骨干网络。视觉提示生成器具有3个Transformer解码器层，4个头，Transformer宽度为256，前馈隐藏维度设置为1024。DBNet、PAN和FCENet的相应检测头用于预测最终结果。

#### 9. 实验结果与分析：
本文将TCM方法与三种文本检测方法（FCENet、PAN和DBNet）相结合，在IC15、TD和CTW上实现了一致的性能提升。该方法在IC15、TD和CTW数据集上的推理速度分别为18、8.4和10 FPS，使用PAN、FCENet和DBNet，保持了检测器的高效性。该方法还在少样本学习和泛化能力方面进行了评估，并与以前的预训练方法进行了比较。通过预训练的方法进行比较，TCM-DBNet在不需要预训练的情况下实现了更好的性能。本文还进行了关于预定义提示、可学习提示、语言提示生成器、视觉提示生成器和不同设置的消融研究。


# Paper:830     一种用于随机轨迹预测的Leapfrog扩散模型



#### 1. Title: 
Leapfrog Diffusion Model for Stochastic Trajectory Prediction

#### 2. Authors: 
Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, Yanfeng Wang

#### 3. Affiliation: 
上海交通大学

#### 4. Keywords: 
Stochastic trajectory prediction, diffusion models, leapfrog initializer, multi-modal distribution, real-time prediction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mao_Leapfrog_Diffusion_Model_for_Stochastic_Trajectory_Prediction_CVPR_2021_paper.html  Github: https://github.com/MediaBrain-SJTU/LED

#### 6. Summary : 
- (1):本文研究了随机轨迹预测的问题，提出了一种新的扩散模型，旨在解决现有模型在实时预测上的时间成本问题。

- (2):过去的方法主要是基于深度生成模型，如GAN和VAE等，但是这些方法在实时预测上存在时间成本问题。本文提出的方法是基于扩散模型，通过可训练的跳跃初始化器来直接学习表达未来轨迹的多模态分布，从而跳过大量的去噪步骤，显著加速推理速度。此外，跳跃初始化器被训练以适当地分配相关样本，以提供预测未来轨迹的多样性，从而显著提高了预测性能。

- (3):本文提出了一种新的扩散模型，即LEapfrog Diffusion model (LED)，它使用可训练的跳跃初始化器来直接学习表达未来轨迹的多模态分布，从而跳过大量的去噪步骤，显著加速推理速度。此外，跳跃初始化器被训练以适当地分配相关样本，以提供预测未来轨迹的多样性，从而显著提高了预测性能。

- (4):本文在四个真实世界数据集上进行了广泛的实验，包括NBA / NFL / SDD / ETH-UCY，结果表明，与现有方法相比，本文提出的方法在所有数据集上均取得了最先进的性能，并且在NFL上实现了23.7％/ 21.9％的ADE / FDE改进。本文提出的方法还将推理速度加速了19.3 / 30.8 / 24.3 / 25.1倍，满足实时推理需求。
#### 7. 方法详细介绍：
本文提出了一种跳跃扩散模型（LEapfrog Diffusion model，LED）用于随机轨迹预测。该模型包括四个模块：均值估计、方差估计、样本预测和去噪。均值估计模块使用特征编码器和门控循环单元来捕捉高维序列中的时间依赖性。方差估计模块涉及一个编码器，用于生成方差估计的高维嵌入。样本预测模块使用估计的均值和方差生成K个样本。去噪模块使用基于Transformer的上下文编码器和噪声估计模块对过去的轨迹进行去噪。模型分为两个阶段进行训练，第一阶段训练去噪模块，第二阶段优化可训练的跳跃扩散模型，其中包括一个可训练的跳跃初始化器和冻结的去噪模块。训练目标包括噪声估计损失和约束K个预测中的最小距离并规范化方差估计的损失函数。

#### 8. 实验设置：
本文在NBA和NFL两个数据集上进行了实验。跳跃步长τ在NBA数据集上设置为5，所有数据集的扩散步长Γ均为100。跳跃初始化器使用基于Transformer的社交编码器、1D卷积核的时间编码器和隐藏大小为256的GRU。去噪模块使用Transformer提取上下文信息和具有256个隐藏大小的核心去噪模块。权重参数w1设置为50，以强调距离损失。

#### 9. 实验结果与分析：
本文提出的方法在NBA、NFL和SDD三个真实世界数据集上显著优于10种最先进的预测方法，达到了ADE和FDE的最佳性能。在NBA数据集上，与现有最先进方法MID相比，本文方法在所有时间戳上的ADE/FDE均显著优于其他基线方法，4.0s时相对于MID分别降低了15.6%/13.4%。在NFL数据集上，本文方法相对于MID的ADE/FDE分别提高了23.7%/21.9%。在SDD数据集上，本文方法将FDE从11.85降低到11.66，相对于现有最先进方法NPSN有所改进。在ETH-UCY数据集上，本文方法在ADE/FDE上取得了最佳或次佳的性能。本文还提供了消融研究和定性结果以支持所提出方法的有效性。


# Paper:831     GFIE：室内环境下从2D到3D的注视跟踪数据集和基线



#### 1. Title: 
GFIE: A Dataset and Baseline for Gaze-Following from 2D to 3D in Indoor Environments

#### 2. Authors: 
Zhengxi Hu, Yuxue Yang, Xiaolin Zhai, Dingye Yang, Bohan Zhou, Jingtai Liu

#### 3. Affiliation: 
Nankai University (南开大学)

#### 4. Keywords: 
Gaze-following, dataset, 2D/3D gaze targets, laser rangeﬁnder, RGB-D camera

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_3D_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是自动定位人物注视点，即Gaze-following，这是理解人类意图的重要线索，可以应用于识别人类感兴趣的物体或区域等领域。

- (2):过去的方法主要是手动标注或使用佩戴式眼动仪等设备进行自动标注，但手动标注可能存在主观偏差，而佩戴式眼动仪会改变被试者的外貌，从而与自然环境下的注视行为存在差距。因此，本文提出了一种新的数据集GFIE，以及一种基于双目视野感知的基线方法，用于建立从2D到3D的Gaze-following基准。

- (3):本文提出了一种新的Gaze数据采集系统，由激光测距仪和RGB-D相机Azure Kinect构成，通过操纵激光测距仪引导被试者的注视目标，同时记录他们的活动。通过我们提出的算法检测图像中的激光点，可以定位人物的2D/3D注视目标，并通过图像修复算法消除激光点引入的地面真值，从而获得无偏的标签。我们的GFIE数据集包含不同被试者和不同场景的丰富活动剪辑，以确保注视行为的多样性。同时，我们提出了一种基于双目视野感知的基线方法，将视野范围转换为热图，结合场景显著性，更有效地定位2D和3D注视目标。

- (4):本文在GFIE和CAD-120数据集上建立了2D/3D Gaze-following基准，并进行了实验。实验结果表明，GFIE数据集是可靠的，所提出的基线方法在2D图像和3D场景中均取得了优异的性能。
#### 7. 方法详细介绍：
本文提出了一种从RGB-D图像中同时进行2D/3D注视跟踪的方法。该系统由激光测距仪和Azure Kinect（RGB-D相机）组成，用于引导和定位注视目标并记录注视行为。作者开发了一种算法，用于在图像中定位激光点以注释2D/3D注视目标并消除激光点引入的地面真实性。整个收集注视行为的过程允许他们在非约束环境中半自动地获得无偏标签。他们还提出了一种基线方法，使用立体视野（FoV）感知来建立GFIE数据集上的2D/3D注视跟踪基准。

具体步骤如下：
1. 用ResNet50作为骨干网络，构建估计注视方向的模块，该模块以裁剪的头部图像为输入，输出3D注视单位向量。
2. 提出了感知立体视野的模块，根据注视方向突出显示人在空间中关注的区域。立体视野热图可以与场景显著性一起输入到生成注视热图的模块中，以估计注视目标。
3. 最终模块是编码器-解码器架构，其中编码器包含ResNet50的所有特征层，解码器包括2个卷积层和3个反卷积层。在训练过程中使用总损失函数进行回归，并使用翻转、随机裁剪和颜色抖动进行数据增强。

#### 8. 实验设置：
作者使用GFIE数据集来评估所提出的方法的性能。该数据集包括61个受试者（27个男性和34个女性）的多样化注视行为，伴随着各种活动。整个数据集总共包含71799帧，每帧都有注释，包括头部边界框、眼睛位置和2D平面和3D空间中的注视目标。数据集被分为包含59217帧的训练集、包含6281帧的测试集和包含6281帧的验证集。此外，出现在训练集中的受试者和场景不包括在测试和验证集中。

#### 9. 实验结果和分析：
所提出的方法在GFIE数据集中的2D和3D场景中均取得了最佳性能，优于其他基线方法。定量结果表明，与其他基线方法相比，所提出的方法具有更高的AUC，更低的L2距离、3D距离和角度误差。消融研究还证明了所提出的方法中组件、输入设置和训练和推理策略的有效性。


# Paper:832     针对目标重识别的大规模训练数据搜索



#### 1. Title: 
Large-scale Training Data Search for Object Re-identification

#### 2. Authors: 
Yue Yao, Tom Gedeon, Liang Zheng

#### 3. Affiliation: 
Yue Yao, Liang Zheng: 澳大利亚国立大学 (Australian National University)
Tom Gedeon: 柯廷大学 (Curtin University)

#### 4. Keywords: 
Object re-identification, deep learning, training data search, domain gap, dataset bias

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Yao_Large-Scale_Training_Data_Search_for_Object_Re-Identification_CVPR_2020_paper.html  Github: https://github.com/yorkeyao/SnP

#### 6. Summary : 
- (1):本文研究的是目标重识别（object re-identification）中的训练数据搜索问题，由于数据标注成本高昂，因此需要从大规模数据池中构建训练集。本文提出了一种搜索和修剪（SnP）的解决方案，针对目标领域的特定性，通过搜索和修剪两个阶段，构建出比源数据池小80%的训练集，同时保持了较高的重识别准确率。

- (2):过去的方法主要是基于主动学习（active learning）和数据模拟（data simulation），但这些方法存在一些问题，如需要多次真实训练过程、需要访问目标任务模型等。本文的方法则直接搜索整个训练集，不需要真实训练过程，且不需要目标任务模型信息。本文的方法在目标领域的特定性方面具有优势，能够更好地解决数据偏差问题。

- (3):本文的方法主要分为两个阶段：搜索阶段和修剪阶段。搜索阶段通过计算特征级距离（Fréchet Inception Distance）来识别和合并具有相似分布的源身份簇，修剪阶段则根据预算选择最具代表性的样本，以控制训练集的大小。本文的方法在多个目标重识别数据集上进行了实验，结果表明，与现有的搜索方法相比，本文的方法能够构建出更优秀的训练集，且在重识别准确率方面表现出色。

- (4):本文的方法在多个目标重识别数据集上进行了实验，结果表明，本文的方法能够构建出比源数据池小80%的训练集，同时保持了较高的重识别准确率。本文的方法在特定目标领域的数据偏差问题上具有优势，能够更好地解决数据偏差问题。
#### 7. 方法详细介绍：
本文提出了一种搜索和修剪（SnP）框架，用于构建一个与目标域具有相似分布的预算训练集。该框架包括两个阶段：目标特定子集搜索和预算修剪。在第一阶段中，源池被搜索以找到一个与目标域具有相似分布的子集。在第二阶段中，对子集进行修剪以满足预算约束。SnP框架适用于对象重新识别任务。本文提供了SnP框架的详细算法和公式。

具体步骤如下：
1. 构建源池S。
2. 在源池S中搜索与目标域DT具有相似分布的子集S*。
3. 从S*中选择n个身份和m个代表性图像进行修剪，形成最终训练集DS。

#### 8. 实验设置：
本文使用了三个数据集进行实验：Market-1501，DukeMTMC-reID和MSMT17。实验使用了PyTorch框架和Adam优化器。实验中使用的评估指标包括CMC曲线和mAP。

#### 9. 实验结果和分析：
本文展示了目标特定子集搜索的有效性，其在域差异和重新识别准确性方面均优于随机采样和贪心采样。搜索到的数据集有助于提高整个数据池的准确性。本文还表明，搜索数据优于单个训练集。修剪方法显着减少了训练集规模，同时能够训练出合理准确度的模型。在不同的选择比率和目标下，搜索到的训练集具有不同的组成，而FPS在不同的选择比率和目标下始终优于随机采样。


# Paper:833     通过未充分利用的输出特征提高人群分析中的检测性能



#### 1. Title: 
Boosting Detection in Crowd Analysis via Underutilized Output Features

#### 2. Authors: 
Shaokai Wu, Fengyu Yang

#### 3. Affiliation: 
1. 吉林大学
2. 密歇根大学

#### 4. Keywords: 
Crowd analysis, object detection, crowd counting, crowd localization, crowd detection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Boosting_Detection_in_Crowd_Analysis_via_Underutilized_Output_Features_CVPR_2021_paper.html  Github: https://github.com/wskingdom/Crowd-Hat

#### 6. Summary : 
- (1):本文研究的是人群分析中的目标检测问题，旨在提高密集人群场景下的检测性能。
 
- (2):过去的方法主要集中在人群计数和定位上，而目标检测在密集人群场景下的性能较差。本文提出了一种新的方法，利用目标检测的输出特征，如提议和边界框的面积大小和置信度分数，来提高人群分析的性能。同时，本文还提出了一种混合2D-1D压缩技术，用于提取和优化这些特征，并提出了区域自适应NMS阈值和解耦-对齐范式来解决目标检测方法的主要局限性。

- (3):本文提出了一种名为“Crowd Hat”的模块，可以轻松地与现有的检测模型集成。该模块使用混合2D-1D压缩技术来提取和优化输出特征，并进一步提出了区域自适应NMS阈值和解耦-对齐范式来解决目标检测方法的主要局限性。本文的方法在各种人群分析任务中进行了广泛的评估，包括人群计数、定位和检测，证明了利用输出特征的有效性和目标检测方法在人群分析中的潜力。

- (4):本文的方法在多个数据集上进行了评估，包括ShanghaiTech、UCF-QNRF、NWPU-Crowd、FDST、JHU-CROWD++等。实验结果表明，本文的方法在人群计数、定位和检测任务中均取得了优于现有方法的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Crowd Hat的模块，它可以与现有的检测模型轻松集成。该模块使用混合的2D-1D压缩技术来优化输出特征，并获取人群特定信息的空间和数字分布。基于这些特征，提出了区域自适应NMS阈值和解耦-对齐范式来解决检测方法的主要限制。Crowd Hat模块可以集成到各种一阶段和二阶段的目标检测方法中，为人群分析任务带来显著的性能提升。

具体步骤如下：
1. 使用矩阵将每个输出特征压缩成补丁，进行2D压缩。
2. 使用1D压缩在图像中查找输出特征的数字分布。
3. 将压缩的矩阵和分布向量传递到Crowd Hat网络中，获取全局和局部特征。
4. 最后，使用区域自适应NMS解码器来学习每个区域的最佳NMS阈值，最大化当前伪边界框标签的F1分数。

#### 8. 实验设置：
本文使用了四个公共数据集，分别是ShanghaiTech、UCF-QNRF、NWPU-Crowd和WorldExpo'10，来评估所提出的方法。为了公平比较，本文使用了与之前的工作相同的训练和测试分割。本文使用二阶段检测方法PSDNN作为检测管道进行说明，其他一阶段检测方法可以轻松适应。

#### 9. 实验结果和分析：
本文提出的Crowd Hat模块显著提高了检测方法在人群分析任务中的性能，在多个数据集上实现了最先进的人群定位和检测结果。消融研究表明，输出特征比CNN特征更有效，压缩和区域自适应NMS在所提出的方法中非常重要。敏感性实验表明，该方法在不同超参数下的性能稳定。在添加Crowd Hat模块后，模型大小和推理时间仍然很轻量级。本文的方法在人群计数方面也取得了显著的性能提升，使其与基于密度的方法竞争。


# Paper:834     节能自适应三维感知



#### 1. Title: 
Energy-Efficient Adaptive 3D Sensing

#### 2. Authors: 
Brevin Tilmon, Zhanghao Sun, Sanjeev J. Koppal, Yicheng Wu, Georgios Evangelidis, Ramzi Zahreddine, Gurunandan Krishnan, Sizhuo Ma, Jian Wang

#### 3. Affiliation: 
Brevin Tilmon: University of Florida (佛罗里达大学)

#### 4. Keywords: 
Active depth sensing, adaptive sensing, eye-safety, power consumption, stereo-projector setup

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tilmon_Energy-Efficient_Adaptive_3D_Sensing_CVPR_2021_paper.html  Github: https://btilmon.github.io/e3d.html

#### 6. Summary : 
- (1):本文研究的是主动深度感知技术，该技术通过对场景进行编码来实现深度估计，但是常规的主动深度感知技术存在功耗大、安全性差等问题，因此需要一种新的自适应主动深度感知技术。

- (2):过去的主动深度感知技术存在的问题包括功耗大、安全性差等，本文提出的自适应主动深度感知技术可以通过限制编码样本的数量来提高每个样本的光学功率，从而扩展最大感知距离，同时也可以提高安全性。本文的方法与其他感知策略进行了比较，包括全帧投影、线扫描和点扫描等。

- (3):本文提出了一种自适应深度感知策略，通过计算关注度图来确定感兴趣区域（ROI），并将硬件特定的控制信号从关注度图中计算出来，使投影仅投影到ROI中。本文还提出了两种硬件实现方法，一种是基于液晶光学元件（SLM）的数字全息术，另一种是基于微电子机械系统（MEMS）和衍射光学元件（DOE）的点阵图。

- (4):本文的方法在实验中得到了验证，证明了其可以适应场景并优于现有的感知策略。本文的方法可以在保证安全性的前提下，通过限制编码样本的数量来提高每个样本的光学功率，从而扩展最大感知距离，同时也可以提高安全性。
#### 7. 方法详细介绍：
本文提出了一种自适应的三维感知方法，使用注意力图来引导对场景中特定感兴趣区域（ROIs）的照明。该方法使用相位空间光调制器（SLM）或带有衍射光学元件（DOE）的微电子机械系统（MEMS）镜子投射独特的点图案到ROIs上。点图案是通过将全息图分解为子全息图并将它们相加来生成的。该方法还包括一种灵活的扫描策略，可以顺序扫描K个不相交的ROIs，或将相机曝光分成K个较短的曝光，并在每个曝光期间扫描单个ROI。理论分析为所提出的自适应三维感知方法奠定了基础，本文在实际原型中验证了分析。

#### 8. 实验设置：
本文使用两个FLIR BFS-U3-16S2C-CS相机配备20mm镜头作为立体对。SLM实现使用Holoeye GAEA LCoS（仅相位）SLM，可以以每秒30帧的速度显示4K相位图。MEMS + DOE实现使用0.8mm直径的bonded Mirrorcle MEMS Mirror和一个小FOV的随机点DOE。本文在补充报告中提供了详细的设置和校准程序。

#### 9. 实验结果与分析：
本文显示，所提出的自适应三维感知方法比线扫描和全帧传感器更能抵抗环境光。本文还证明了SLM实现可以在直射阳光下在户外工作，并显示了高达2米的结果。本文提供了在仅相位空间光调制器上模拟全帧、线扫描和自适应传感器的示例。本文将MEMS + DOE实现与SLM实现进行了比较，并显示SLM实现更加灵活，但MEMS + DOE实现具有低成本、简单光学和小尺寸的优点。本文还展示了MEMS + DOE实现将点图案引导到两个帧中的主要无纹理区域的示例。

#### 全文总结：
本文提出了一种自适应的三维感知方法，使用注意力图来引导对场景中特定感兴趣区域（ROIs）的照明。该方法使用相位空间光调制器（SLM）或带有衍射光学元件（DOE）的微电子机械系统（MEMS）镜子投射独特的点图案到ROIs上。本文在实际原型中验证了该方法的优越性，并证明了其能够适应场景并优于现有的感知策略。本文还提供了详细的传感器模型和实验设置，以及对实验结果的分析和讨论。


# Paper:835     通过声学信号听取人类行为：使用声学信号进行3D人体姿势估计



#### 1. Title: 
Listening Human Behavior: 3D Human Pose Estimation with Acoustic Signals

#### 2. Authors: 
Yuto Shibata, Yutaka Kawashima, Mariko Isogawa, Go Irie, Akisato Kimura, Yoshimitsu Aoki

#### 3. Affiliation: 
第一作者：慶應義塾大学

#### 4. Keywords: 
3D human pose estimation, acoustic signals, active acoustic sensing, convolutional neural network, phase features

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究如何通过低级别的声学信号来推断人类行为，提出了一种使用单对麦克风和扬声器进行主动声学感知的框架，以将多通道音频特征编码为3D人体姿势序列。这是一个具有挑战性的任务，因为声音比其他信号更具有衍射性，因此会掩盖场景中物体的形状。

- (2):现有的方法通常使用包括人类语音或特定动作的声音的信号，存在隐私问题。本文提出的方法使用低级别的声学信号，不包含任何声音语义，因此不存在隐私问题。此外，本文发现反射或衍射的声音很容易受到受试者身体差异的影响，例如身高和肌肉量，这会降低预测准确性。为了解决这些问题，本文使用主体鉴别器来提高准确性，并使用对抗学习创建主体不变特征。

- (3):本文提出了一种卷积神经网络（CNN）框架，将多通道音频特征作为输入，并直接输出预测的3D身体部位关节位置。该模型通过明确集成代表到达时间差（TDOA）的相位特征来捕获这些小的变化，并利用它们来推断人类行为。本文提出的数据集包含男性和女性的声音数据，并且受试者的身体特征各不相同。为了减少这些差距，本文使用主体鉴别器来提高准确性，并使用对抗学习创建主体不变特征。

- (4):本文的方法在3D人体姿势估计任务上取得了良好的性能，使用仅低维度的声学信息，优于基线方法。
#### 7. 方法详细介绍：
本文提出了一种基于声学信号的3D人体姿态估计框架。该框架由声学特征提取模块和3D人体姿态估计网络组成。声学特征提取模块将原始声学信号编码为一系列声学特征向量，包括强度向量和Mel频谱。3D人体姿态估计网络采用主动声学感知技术估计环境的房间脉冲响应（RIR），并使用主题鉴别器模块减少主体间的生理差异，以提高模型的泛化能力。训练目标使用均方误差（MSE）损失和平滑损失，使预测更加平滑。网络模型采用多通道声学特征作为输入，直接输出预测的3D身体部位关节位置。模型通过显式地集成表示到达时间差（TDOA）的相位特征来捕捉小的位移，并利用它们推断人类行为。此外，本文发现反射或折射在主体身上的声音往往受到其身体差异的影响，如身高和肌肉量。本文通过使用主题鉴别器来减少这些差距，以提高准确性。

#### 8. 实验设置：
为了训练网络，本文使用单对全向麦克风和扬声器设置了主动声学感知系统。本文主动记录了从扬声器发出的时间拉伸脉冲（TSP）信号的声音，并与运动捕捉（Mocap）数据同步记录。这些数据在一个无混响效应的消音室和一个嘈杂的教室中记录。

#### 9. 实验结果和分析：
本文进行了广泛的实验，并展示了所提出方法的有效性。本文建议，仅使用低维声学信息，所提出的方法优于基线方法。本文公开了本项目使用的数据集和代码。


# Paper:836     像素、区域和对象：显著目标检测的多重增强



#### 1. Title: 
Pixels, Regions, and Objects: Multiple Enhancement for Salient Object Detection

#### 2. Authors: 
Yi Wang, Ruili Wang, Xin Fan, Tianzhu Wang, Xiangjian He

#### 3. Affiliation: 
Yi Wang: 大连理工大学-俄罗斯远东联合研究院信息科学与工程学院
Ruili Wang: 新西兰梅西大学数学与计算科学学院
Xin Fan: 大连理工大学-俄罗斯远东联合研究院信息科学与工程学院
Tianzhu Wang: 新西兰梅西大学数学与计算科学学院
Xiangjian He: 英国诺丁汉大学宁波校区计算机科学学院

#### 4. Keywords: 
Salient object detection, human visual system, multiscale feature enhancement, iterative refinement, hybrid loss

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Wang_Pixels_Regions_and_Objects_Multiple_Enhancement_for_Salient_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/yiwangtz/MENet

#### 6. Summary : 
- (1):本文研究的是显著目标检测，旨在模仿人类视觉系统和认知机制来识别和分割显著目标。然而，由于这些机制的复杂性，当前的方法并不完美。需要进一步提高准确性和鲁棒性，特别是在具有多个对象和背景杂波的复杂场景中。
 
- (2):过去的方法主要是基于编码器-解码器和特征聚合架构，但对于复杂对象边界的准确分割仍然是一个具有挑战性的任务。本文提出了一种新的方法，称为多重增强网络（MENet），它采用了HVS的边界敏感性、内容完整性、迭代细化和频率分解机制。首先设计了一个多级混合损失来指导网络学习像素级、区域级和对象级特征。然后设计了一个灵活的多尺度特征增强模块（ME-Module），通过改变输入特征序列的大小顺序逐步聚合和细化全局或详细特征。在MENet的双分支解码器中使用迭代训练策略来增强边界特征和自适应特征。
 
- (3):本文提出了一种多重增强网络（MENet），它采用了多种HVS机制来提高SOD性能。具体来说，MENet采用图像频率分解思想设计了一个用于边界（高频）和内部区域（低频）的双流特征学习解码器。然后，我们提出了一种迭代训练策略，通过交替聚合高低级特征来逐步增强特征，以模仿HVS自下而上和自上而下的细化机制。为了灵活地产生高级和低级特征，我们设计了一个多尺度特征增强模块（ME-Module）作为每个分支的核心，利用了空洞空间金字塔池化（ASPP）和全局-局部注意力。
 
- (4):在六个具有挑战性的基准数据集上进行的综合评估表明，MENet实现了最先进的结果。本文的贡献包括：提出了
#### 7. 方法详细介绍：
本文提出了一种名为多重增强网络（MENet）的方法，它将多种人类视觉系统机制集成到U-Net结构的编码器-解码器框架中。MENet采用两个流的特征学习解码器，用图像频率分解来处理边界（高频）和内部区域（低频）。多尺度特征增强模块（ME-Module）逐步聚合和细化全局或详细特征，通过改变输入特征序列的大小顺序。采用迭代训练策略来增强MENet的边界特征和自适应特征。设计了多级混合损失来指导网络学习像素级、区域级和对象级特征。该损失函数评估了预测显着性图与真实显着性图之间的像素级、区域级和对象级相似性。对于像素级损失，使用二元交叉熵（BCE）损失来确保网络的准确性和收敛速度。对于区域级损失，将显着性图分成四个相等大小的子区域，并计算通过SSIM和IoU加权的区域相似度之和。对于对象级损失，使用前景的对比度和分布统计数据来设计损失函数。

#### 8. 实验设置：
本文在六个具有挑战性的基准数据集上评估了MENet的性能，包括DUT-OMRON、PASCAL-S、HKU-IS、SOD、DUTS-TE和SOC。实验在一台单个NVIDIA TITAN Xp GPU上进行，具有12GB的内存。输入图像大小设置为352×352，批量大小设置为16。使用Adam优化器进行训练，学习率为0.0001。训练过程在30个epoch后停止。评估指标包括平均绝对误差（MAE）、F-measure和S-measure。

#### 9. 实验结果和分析：
本文提出的MENet在所有六个基准数据集上均取得了最先进的结果。具体而言，MENet在DUT-OMRON、PASCAL-S、HKU-IS、SOD、DUTS-TE和SOC数据集上分别获得了0.031、0.042、0.042、0.045、0.051和0.056的MAE得分。MENet还在所有数据集上获得了最高的F-measure和S-measure得分。结果表明，MENet有效地将多种人类视觉系统机制集成到网络结构和损失函数中，在具有多个对象和背景杂波的复杂场景中实现了卓越的性能。


# Paper:837     基于联合嵌入预测架构的图像自监督学习



#### 1. Title: 
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture

#### 2. Authors: 
Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas

#### 3. Affiliation: 
Mahmoud Assran: McGill University and Mila, Quebec AI Institute

#### 4. Keywords: 
Self-supervised learning, image representations, predictive architecture, joint-embedding, masking strategy, ViT

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2022_paper.html  Github: None

#### 6. Summary: 
- (1):本文研究背景是自监督学习中的图像表示学习，旨在提高表示语义级别，同时避免手工数据增强的使用。
- (2):过去的方法包括基于不变性的方法和生成方法，但它们都存在一些问题，如引入强偏见、泛化能力差等。本文提出了一种基于联合嵌入预测架构的方法，通过在抽象表示空间中预测缺失信息来提高表示语义级别，同时使用多块掩蔽策略来指导模型学习更语义化的特征。
- (3):本文提出的方法是一种基于联合嵌入预测架构的自监督学习方法，通过从单个上下文块中预测同一图像中各种目标块的表示来学习高度语义化的图像表示。本文的创新点在于使用联合嵌入预测架构和多块掩蔽策略来提高表示语义级别，同时使用Vision Transformers来提高可扩展性。
- (4):本文的方法在ImageNet-1K数据集上进行了广泛的实验评估，结果表明本文的方法在不使用手工数据增强的情况下可以学习到强大的表示，并在多个任务上取得了良好的性能，如线性分类、物体计数和深度预测等。本文的方法比之前的方法更具可扩展性和效率，例如使用16个A100 GPU在不到72小时内训练ViT-Huge/14模型在ImageNet上取得了强大的下游性能。
#### 7. 方法详细介绍：
本文提出了一种自监督学习方法，称为基于图像的联合嵌入预测架构（I-JEPA）。该方法使用掩蔽技术来预测来自同一图像的各种目标块的表示。I-JEPA的核心设计选择是多块掩蔽策略，以引导其生成语义表示。该方法包括一个上下文编码器和一个预测器。上下文编码器是一个Vision Transformer（ViT），只处理可见的上下文补丁。预测器是一个狭窄的ViT，它在特定位置上，基于位置令牌，预测目标块的表示。目标表示对应于目标编码器的输出，其权重通过上下文编码器权重的指数移动平均值在每次迭代中更新。损失是预测的补丁级表示和目标补丁级表示之间的平均L2距离。该方法可扩展且高效，使用16个A100 GPU在不到72小时内在ImageNet上训练ViT-Huge/14模型。

#### 8. 实验设置：
本文在ImageNet-1K线性探测、半监督1% ImageNet-1K和语义转移任务上评估了所提出的方法。实验使用16个A100 GPU在不到72小时内在ImageNet上训练ViT-Huge/14模型。 

#### 9. 实验结果和分析：
本文提出的I-JEPA方法在各种图像分类任务中表现出色，包括ImageNet-1K和低样本ImageNet-1K。该方法还在下游图像分类任务和低级任务（如物体计数和深度预测）中表现良好。该方法的可扩展性也得到了证明，表明I-JEPA需要比以前的方法更少的计算才能实现强大的性能。


# Paper:838     基于无监督采样促进随机人类轨迹预测



#### 1. Title: 
Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction

#### 2. Authors: 
Guangyi Chen, Zhenhao Chen, Shunxing Fan, Kun Zhang

#### 3. Affiliation: 
Mohamed bin Zayed University of Artificial Intelligence (阿布扎比人工智能大学), Carnegie Mellon University

#### 4. Keywords: 
Human trajectory prediction, stochastic prediction, Bayesian optimization, Gaussian process, long-tail distribution

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Unsupervised_Sampling_Promoting_for_Stochastic_Human_Trajectory_Prediction_CVPR_2021_paper.html  Github: https://github.com/gychen-ml/BOsampler

#### 6. Summary : 
- (1):本文研究人类轨迹预测中的多模态问题，提出了一种基于贝叶斯优化的无监督采样方法，以适应性地挖掘潜在路径，从而促进随机预测的采样过程。

- (2):现有的随机预测方法通常使用蒙特卡罗随机采样来推断未来轨迹，但由于预测分布的长尾效应，有限的样本不足以覆盖现实路径。本文提出的BOsampler方法可以自适应地挖掘潜在路径，以探索长尾区域中的路径。与其他方法相比，BOsampler不需要重新训练模型，可以与现有的随机预测模型集成。

- (3):本文提出的BOsampler方法将轨迹采样建模为高斯过程，并构建了一个获取函数来衡量潜在采样值。该获取函数应用原始分布作为先验，并鼓励在长尾区域中探索路径。BOsampler方法可以自适应地更新高斯后验，从而更加灵活地挖掘潜在路径。实验结果表明，BOsampler方法可以与多种基线方法集成，并取得了显著的改进。

- (4):本文在ETH-UCY数据集上对多种基线方法进行了评估，包括Social GAN、PECNet、Trajectron++和Social-STGCNN等。实验结果表明，BOsampler方法可以显著提高随机预测的准确性和多样性。
#### 7. 方法详细介绍：
本文提出了一种名为BOsampler的方法，用于解决传统随机轨迹预测方法中分布不均匀和偏差的问题。BOsampler是一种迭代采样方法，使用贝叶斯优化来优化采样过程。它建立了一个高斯过程，并获得后验分布来定义一个收购函数来衡量每个样本的价值。收购函数平衡样本的准确性和多样性。BOsampler使用伪分数评估函数来近似真实分数函数，因为在采样过程中无法访问分数函数。该方法应用了一些技术技巧，如预热、收购函数和计算，以平滑地优化采样过程。具体步骤如下：
1. 初始化高斯过程，包括均值函数和协方差函数。
2. 通过先前的采样轨迹和相应的潜在变量计算可能的评估分数的后验分布。
3. 定义收购函数来衡量每个样本的价值，平衡准确性和多样性。
4. 使用伪分数评估函数来近似真实分数函数。
5. 通过最大化收购函数来迭代生成新样本，直到获得足够的样本。

#### 8. 实验设置：
本文在ETH-UCY数据集上进行了实验，该数据集是一个广泛使用的公共人类轨迹预测基准数据集。数据集包含五个不同的场景，共有750名和786名行人。轨迹以世界坐标的序列提供，数据分割遵循Social-GAN和Trajectron++的协议。轨迹以0.4秒间隔采样，其中前3.2秒（8帧）用作观察数据，以预测接下来的4.8秒（12帧）未来轨迹。本文使用留一交叉验证评估策略，其中四个场景用于训练，剩余一个用于测试。本文还从UCY/ETH的每个数据集中选择了最偏离轨迹的前4％作为异常子集，以评估在罕见事件上的性能。

#### 9. 实验结果和分析：
本文将BOsampler与五种主流随机行人轨迹预测方法进行比较，包括Social-GAN、PECNet、Trajectron++、Social-STGCNN和STGAT。本文使用N = 20条轨迹的最小ADE和FDE作为评估指标。本文在ADE和FDE指标上使用Best-of-20策略对异常子集进行了定量分析。BOsampler在ADE和FDE方面的表现均优于所有基线方法，平均性能提高了18％和15％。本文还提供了消融研究和参数分析，以进一步研究所提出的方法。


# Paper:839     通过直接PAC-Bayesian边界最小化提高鲁棒泛化



#### 1. Title: 
Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization

#### 2. Authors: 
Zifan Wang, Nan Ding, Tomer Levinboim, Xi Chen, Radu Soricut

#### 3. Affiliation: 
Zifan Wang: Carnegie Mellon University
Nan Ding, Tomer Levinboim, Xi Chen, Radu Soricut: Google Research

#### 4. Keywords: 
Robustness, PAC-Bayesian bound, Adversarial training, Trace of Hessian, Generalization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Improving_Robust_Generalization_by_Direct_PAC-Bayesian_Bound_Minimization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究深度学习模型的鲁棒性问题，特别是针对对抗攻击的鲁棒性。过去的研究表明，针对对抗攻击的模型在训练集上表现出更高的鲁棒性，而在测试集上表现不佳，这被称为鲁棒泛化差距。本文旨在解决这一问题。

- (2):过去的方法包括`2权重正则化、早停、标签平滑、数据增强和使用合成数据等。然而，这些方法存在严重的过拟合问题。本文提出了一种新的方法，直接最小化鲁棒PAC-Bayesian上界，通过Trace of Hessian正则化来提高模型的鲁棒性。该方法在计算成本和内存效率方面比其他竞争方法更具优势。

- (3):本文提出了一种新的鲁棒PAC-Bayesian上界，并通过Gibbs分布导出了二阶上界。该上界包含一个Trace of Hessian（TrH）正则化项，该项通过测量损失曲面的平坦度来鼓励模型参数收敛到一个平坦区域。为了提高计算效率，我们将TrH正则化限制在网络的顶层。最终，我们使用Vision Transformers（ViT）在CIFAR-10/100和ImageNet上评估了我们的方法，并与基线鲁棒性算法进行了比较。实验结果表明，TrH正则化可以提高ViT的鲁棒性，而且在内存和计算成本方面比之前的最先进方法更具优势。

- (4):本文提出的方法在CIFAR-10/100和ImageNet上均取得了优异的鲁棒性能，超过了之前的最先进方法。在ImageNet上，相对于最佳基线，我们的方法提高了2.7%的鲁棒准确率，并取得了48.9%的最新最优结果。
#### 7. 方法详细介绍：
本文提出了一种新的方法，通过直接最小化PAC-Bayesian界来提高鲁棒泛化性能。该方法利用PAC-Bayesian界的线性形式，并将其适应于边界鲁棒性。作者引入了定理3，该定理最小化了界的右侧，并提供了一个训练目标函数，该函数将最优测试误差限制在一个常数范围内。作者建议将TrH正则化限制在深度网络的顶层，以使其更具计算效率。作者在命题1和2中提供了关于AT和TRADES损失的顶层TrH的解析形式。完整的训练目标在算法1中呈现。

#### 8. 实验设置：
本文提供了CIFAR-10/100和ImageNet三个图像分类基准的实验细节。CIFAR-10/100的威胁模型是标准的“1 ball to bound the adversarial noise”，而ImageNet则使用了`2和`1 ball。测试准确性是通过良性图像和由AutoAttack生成的对抗性图像进行评估的。作者通过所有实验都使用Vision Transformer（ViT），并从ImageNet21K的预训练检查点初始化权重。在训练期间，模型在四个Google Cloud TPUv4芯片上进行分区，用于基本和TrH方法，而AWP、S2O和SWA则使用八个芯片，因为它们消耗更多的HBM内存。作者提供了用于生成表1中主要结果的超参数的完整列表。

#### 9. 实验结果和分析：
表1中的结果表明，使用TrH正则化的训练要么与现有方法的鲁棒准确性相匹配，要么优于其。事实上，TrH正则化在ImageNet数据集上的收益要比在CIFAR-10/100上更大，其中几种方法显示出等效的性能（根据标准误差分析）。这表明，未来的鲁棒性对抗训练工作应该超越CIFAR基准，转向更大规模的任务。在ImageNet上，使用AT（TrH），我们使用ViT-L16模型在`1（4/255）和`2（3.0）下设置了新的最先进的鲁棒准确性，分别为48.8％和47.0％，显著提高了至少0.4％（根据SE）。


# Paper:840     搜索-映射-搜索：一种用于动作识别的帧选择范式



#### 1. Title: 
Search-Map-Search: A Frame Selection Paradigm for Action Recognition

#### 2. Authors: 
Mingjun Zhao, Yakun Yu, Xiaoli Wang, Lei Yang, Di Niu

#### 3. Affiliation: 
第一作者：University of Alberta

#### 4. Keywords: 
Action recognition, frame selection, deep learning, heuristic search, supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Search-Map-Search_A_Frame_Selection_Paradigm_for_Action_Recognition_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是视频动作识别任务中，处理每一帧的计算成本高昂，因此需要从视频中提取最具信息量和代表性的帧以帮助模型更好地理解视频内容。

- (2):过去的方法包括基于每帧重要性预测的单独采样帧和采用强化学习代理来连续找到代表性帧，但这些方法存在局限性。本文提出了一种“Search-Map-Search”学习范式，将启发式搜索和监督学习相结合，以选择视频中最佳的帧组合作为一个实体。通过结合搜索和学习，该方法可以更好地捕捉帧之间的交互，并产生低推理开销。本文提出了一种层次搜索方法，用于在每个训练视频上搜索具有最低误差的最佳帧组合。然后学习一个特征映射函数，将视频的帧映射到其目标最优帧组合的表示。在推理过程中，对未见过的视频执行另一个搜索，以选择其特征表示与预测特征表示最接近的帧组合。本文方法在多个动作识别基准测试上进行了广泛实验，证明了其有效性，并显著优于许多竞争基线。

- (3):本文提出了一种新的学习范式“Search-Map-Search”，将启发式搜索和监督学习相结合，以选择视频中最佳的帧组合作为一个实体。该方法可以更好地捕捉帧之间的交互，并产生低推理开销。具体而言，本文提出了一种层次搜索方法，用于在每个训练视频上搜索具有最低误差的最佳帧组合。然后学习一个特征映射函数，将视频的帧映射到其目标最优帧组合的表示。在推理过程中，对未见过的视频执行另一个搜索，以选择其特征表示与预测特征表示最接近的帧组合。

- (4):本文方法在多个动作识别基准测试上进行了广泛实验，证明了其有效性，并显著优于许多竞争基线。本文方法可以选择最佳的帧组合，以帮助模型更好地理解视频内容，从而提高动作识别模型的性能。
#### 7. 方法详细介绍：
本文提出的方法名为Search-Map-Search (SMS)，是一种用于动作识别的帧选择范式。该方法包括三个阶段：(1) 分层搜索以找到最佳的帧组合，(2) 特征映射函数以识别第一阶段产生的最佳帧组合，(3) 搜索以从预测特征中推断帧组合。在第一阶段中，采用高效的分层搜索算法与引导局部搜索相结合，首先在粗粒度的剪辑上进行搜索，然后在细粒度的帧上进行搜索，以得出具有最低损失的最佳帧组合。在第二阶段中，采用特征提取器从训练视频帧中提取输入帧特征，以及从搜索结果中提取最佳组合的特征。然后，通过监督学习训练特征映射函数，将输入帧特征作为输入，并将其转换为最佳组合的目标特征。在第三阶段中，又加入了另一个搜索过程，以推断出特征最接近预测特征的有效帧组合。

具体步骤如下：
(1) 分层搜索：采用高效的分层搜索算法与引导局部搜索相结合，首先在粗粒度的剪辑上进行搜索，然后在细粒度的帧上进行搜索，以得出具有最低损失的最佳帧组合。
(2) 特征映射：采用特征提取器从训练视频帧中提取输入帧特征，以及从搜索结果中提取最佳组合的特征。然后，通过监督学习训练特征映射函数，将输入帧特征作为输入，并将其转换为最佳组合的目标特征。
(3) 搜索：又加入了另一个搜索过程，以推断出特征最接近预测特征的有效帧组合。

#### 8. 实验设置：
本文在三个动作识别基准数据集上进行了实验：ActivityNet V1.3、FCVID和UCF101。ActivityNet和FCVID的视频长度较长，UCF101的视频长度较短。使用平均精度（mAP）作为评估指标。使用ResNet-50网络作为动作识别模型的骨干，并使用SGD优化器进行训练。在服务器上进行实验，使用PyTorch实现。

#### 9. 实验结果与分析：
本文提出的Search-Map-Search (SMS)方法在多个动作识别基准数据集上取得了显著的性能提升，相比其他强基线方法，SMS方法是一种更准确的学习范式，它利用高效的搜索和监督特征映射直接选择最佳的帧组合作为一个实体，更好地捕捉了帧之间的交互。SMS方法在未修剪视频数据集上取得了91.97%的mAP，这是对基准采样策略和SMART*的显着改进。还展示了SMS在不同模型架构中的泛化性，表明SMS对3D视频模型学习有益。


# Paper:841     CapDet：统一密集字幕和开放世界检测预训练



#### 1. Title: 
CapDet: Unifying Dense Captioning and Open-World Detection Pretraining

#### 2. Authors: 
Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, Xiaodan Liang

#### 3. Affiliation: 
第一作者：中山大学深圳校区

#### 4. Keywords: 
Open-world detection, Dense captioning, Pretraining, Object detection, Natural language processing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Long_CapDet_Unifying_Dense_Captioning_and_Open-World_Detection_Pretraining_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是如何在实际场景中检测和识别新概念的物体。
 
- (2):过去的方法主要有两种，一种是open-world object detection（OWD），另一种是open-vocabulary object detection（OVD）。OWD方法不能描述检测到的未知物体的具体概念，需要在推理过程中使用预定义的类别列表。OVD方法需要学习有限的基础类并推广到新类。本文提出了一种新的方法，即CapDet，它可以检测和识别给定类别列表中的概念，并为新概念或罕见物体生成相应的自然语言描述。 

- (3):本文提出了一种新的训练框架CapDet，将dense captioning和open-world detection pre-training两个任务统一到一个框架中。CapDet采用了一种统一的数据格式，包括dense captioning数据和检测数据。在open-world detection pre-training中，我们将检测任务视为语义对齐任务，并采用双编码器结构来定位和预测给定的概念列表。在dense captioning pre-training中，CapDet提出了一个dense captioning head，以预测的proposals为输入生成区域引导的标题。由于dense captioning数据中包含丰富的视觉概念，因此添加captioning任务将反过来有益于检测性能的泛化。 

- (4):本文的方法在LVIS数据集上取得了显著的性能提升（例如，在LVIS罕见类上的mAP提高了2.1％）。此外，CapDet还在dense captioning任务上取得了最先进的性能，例如，在Visual Genome（VG）V1.2上的mAP为15.44％，在VG-COCO数据集上为13.98％。
#### 7. 方法详细介绍：
本文提出了一种名为CapDet的方法，将密集字幕和开放世界检测预训练相结合。该方法采用三元组数据格式统一检测数据和密集字幕数据。模型架构包括双视觉语言编码器、图像编码器和文本编码器。检测预训练的训练目标包括对齐损失、中心损失和边界框回归损失。该方法还采用负采样来扩大批次中的概念空间。提出了密集字幕头，用自然语言为相应区域生成文本标题。该方法使用基于Transformer的字幕头在同时实现开放世界检测的情况下，使用单级检测器ATSS生成标题。

#### 8. 实验设置：
CapDet模型使用两种类型的数据进行训练，包括检测数据和字幕数据。使用Object365作为检测数据，从O365 v2中采样0.66M数据进行训练。使用LVIS MiniVal5k进行检测评估。对于密集字幕数据，实验主要在VG V1.2和VG-COCO上进行。VG的真实边界框比其他目标检测数据集更密集。密集字幕的评估指标是由[17]提出的平均精度。

#### 9. 实验结果和分析：
本文提出的CapDet模型在VG-COCO数据集上实现了13.98%的密集字幕平均精度，优于其他几种方法。消融实验表明，将密集字幕数据与不同基线集成的密集字幕头可以提高模型的性能。本文还讨论了模型的局限性和未来改进方向。


# Paper:842     PHA：基于Transformer的人员重识别的Patch-wise高频增强方法



#### 1. Title: 
PHA: Patch-wise High-frequency Augmentation for Transformer-based Person Re-identification

#### 2. Authors: 
Guiwei Zhang, Yongfei Zhang, Tianyu Zhang, Bo Li, Shiliang Pu

#### 3. Affiliation: 
北京航空航天大学计算机学院，数字媒体北京市重点实验室

#### 4. Keywords: 
Person re-identification, Vision Transformers, High-frequency components, Discrete Haar Wavelet Transform, Patch-wise High-frequency Augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PHA_Patch-Wise_High-Frequency_Augmentation_for_Transformer-Based_Person_Re-Identification_CVPR_2022_paper.html  Github: https://github.com/zhangguiwei610/PHA

#### 6. Summary : 
- (1):本文研究的是基于Transformer的人员重识别任务，通过分析频率角度，揭示了Vision Transformers（ViTs）在保留关键高频组件方面的不足，提出了一种Patch-wise High-frequency Augmentation（PHA）方法来增强高频组件的特征表示能力。
 
- (2):过去的方法大多数集中在使用CNNs或ViTs提取单个图像的判别性特征，但是这些方法无法很好地保留关键的高频组件，从而影响了人员重识别的性能。本文提出的PHA方法通过离散Haar小波变换将高频组件分离出来，然后将其作为辅助输入提供给ViT，从而增强了高频组件的特征表示能力。此外，本文还提出了一种新的Patch-wise对比损失，以防止高频组件在网络优化过程中被低频组件稀释，从而提高了关键高频组件的特征表示能力。
 
- (3):本文提出的PHA方法是一种有效的增强方法，可以在训练期间使用，而在推理期间可以删除，不会增加额外的复杂性。实验结果表明，PHA方法在CUHK03-NP、Market-1501和MSMT17数据集上的表现优于主流方法。
  
- (4):本文提出的PHA方法可以增强ViT对关键高频组件的特征表示能力，从而提高人员重识别的性能。实验结果表明，PHA方法在CUHK03-NP、Market-1501和MSMT17数据集上的表现优于主流方法，支持了其目标。
#### 7. 方法详细介绍：
本文提出了Patch-wise High-frequency Augmentation (PHA)方法，用于增强人物再识别（ReID）任务中高频组件的特征表示能力。该方法包括以下三个核心设计：
1. 高频增强（High-frequency Enhancement，HE）：通过离散Haar小波变换（Discrete Haar Wavelet Transform，DHWT）将高频组件的子集分离出来，并将其与[cls] token组合形成高频子序列，以增强高频组件的特征表示能力。
2. 低频丢弃（Low-frequency Drop，LD）：通过量化方法丢弃低频组件，以防止过拟合。
3. Patch-wise对比损失（Patch-wise Contrastive Loss，PCL）：将高频增强的嵌入与原始嵌入中同一身份的嵌入拉近，将不同身份的嵌入推远，以提高关键高频组件的特征表示能力。

在训练过程中，PHA方法通过最小化带有身份标签的总体目标函数进行优化，其中包括身份损失和三元组损失。

#### 8. 实验设置：
本文在三个标准人物再识别基准数据集Market-1501、CUHK03-NP和MSMT17上进行了实验。所有输入图像都被调整为256×128大小，训练图像采用随机水平翻转、填充、随机裁剪和随机擦除进行增强。批量大小设置为64，每个身份有4张图像，采用带有0.9动量和0.0001权重衰减的SGD优化器。学习率初始化为0.008，采用余弦学习率衰减。公式（5）中的参数K设置为35％，公式（10）中的间隔长度设置为5。所有实验都在一台Nvidia V100 GPU上进行，使用FP16训练。

#### 9. 实验结果和分析：
本文将提出的PHA方法与基于CNN和ViT的现有方法在Market-1501、CUHK03-NP和MSMT17数据集上进行了比较。PHA方法在这些数据集上取得了与现有方法相当的性能。特别地，在TransReID基线的情况下，PHA方法在Market1501、MSMT17、CUHK03-NP labeled和CUHK03-NP detected数据集上分别达到了96.1％/90.2％，86.1％/68.9％，84.5％/83.0％，83.2％/80.3％的Rank-1/mAP。本文还在CUHK03-NP labeled数据集上进行了消融研究，分析了PHA方法的每个核心设计。结果表明，高频增强和低频丢弃有助于提高关键高频组件的特征表示能力和区分度，而PCL有助于防止网络过拟合到与身份无关的高频组件。


# Paper:843     基于增强的图形OOD泛化的标签偏移问题



#### 1. Title: 
Mind the Label Shift of Augmentation-based Graph OOD Generalization

#### 2. Authors: 
Junchi Yu, Jian Liang, Ran He

#### 3. Affiliation: 
中国科学院自动化研究所

#### 4. Keywords: 
Graph Neural Networks, Out-of-distribution Generalization, Graph Augmentation, Label Shift

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2021_paper.html  Github: https://github.com/Samyu0304/LiSA

#### 6. Summary : 
- (1):本文研究图神经网络（GNN）的图外推广问题，提出了一种基于标签不变的子图增强方法，以解决增强图中的标签偏移问题。
 
- (2):过去的方法主要是通过不同的图编辑策略来生成增强环境，但这些方法容易导致标签偏移问题，从而影响图外推广的性能。本文提出的方法通过设计变分子图生成器来提取局部预测模式，并构建多个标签不变的子图，从而构建具有一致预测关系的增强环境。为了促进增强的多样性，本文进一步引入了可行的基于能量的正则化方法，以扩大不同增强环境之间的分布距离。通过这种方式，本文生成了具有一致预测关系的多样化增强环境，并促进了学习不变的GNN。 

- (3):本文提出了一种模型无关的标签不变子图增强（LiSA）框架，用于生成具有一致预测关系的增强环境，以解决图外推广问题。LiSA首先设计变分子图生成器来识别局部预测模式，并生成多个标签不变的子图。这些子图捕获了具有不同结构的与预测相关的信息，从而构建了具有一致预测关系的增强环境。为了促进增强的多样性，本文提出了一种可行的基于能量的正则化方法，以扩大不同增强环境之间的分布距离。通过LiSA生成的增强，学习了一个不变的GNN分类器，可以在这些增强环境中保持不变。 

- (4):本文在节点级和图级OOD基准测试上进行了广泛的实验，结果表明LiSA在各种GNN骨干上均取得了令人满意的性能提升。
#### 7. 方法详细介绍：
本文提出了一种名为LiSA的标签不变子图增强方法，用于图形OOD泛化问题。LiSA通过利用训练图的标签不变子图构建增强环境，生成标签不变的增强。LiSA首先设计变分子图生成器，以有效地提取局部预测模式并构建多个标签不变子图。然后，收集不同生成器产生的子图以构建不同的增强环境。为了促进增强环境之间的多样性，LiSA进一步引入了可处理的基于能量的正则化，以扩大环境分布之间的成对距离。使用LiSA生成的增强，学习GNN分类器以在这些增强环境中保持不变。GNN预测器和变分子图生成器通过双层优化方案进行联合优化。LiSA是模型无关的，可以灵活处理图级和节点级分布转移。

#### 8. 实验设置：
本文在节点级和图级OOD基准测试上进行了广泛的实验，以评估LiSA的性能。实验在7个图分类数据集和4个节点分类数据集上进行。代码可在https://github.com/Samyu0304/LiSA上获得。

#### 9. 实验结果和分析：
LiSA在大多数基线方法上表现优异，绝对性能提高高达5％。与ERM相比，IRM和V-Rex在大多数数据集上仅实现可比较的性能。对于空间转移，LiSA在Twitch-Explicit和Facebook-100数据集上实现了更高的ROC-AUC和准确性。对于时间转移，LiSA在OGB-Arxiv和ELLIPTIC数据集上实现了更好的F1分数。


# Paper:844     CIGAR：跨模态图推理用于领域自适应目标检测



#### 1. Title: 
CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection

#### 2. Authors: 
Yabo Liu, Jinghua Wang, Chao Huang, Yaowei Wang, Yong Xu

#### 3. Affiliation: 
第一作者：哈尔滨工业大学深圳研究生院

#### 4. Keywords: 
Unsupervised Domain Adaptation, Object Detection, Graph Reasoning, Cross-Modality, Discriminative Feature Selector

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究领域自适应目标检测，旨在将标记源域的知识推广到未标记的目标域中。 
- (2):现有的基于图的方法在某些情况下表现良好，但它们不能为图学习适当的节点集。此外，这些方法仅基于视觉特征构建图，而不考虑由语义原型（例如数据集标签）携带的语言知识。为了克服这些问题，本文提出了一种跨模态图推理适应（CIGAR）方法，以利用视觉和语言知识。具体而言，我们的方法在语言模态图和视觉模态图之间执行跨模态图推理，以增强它们的表示。我们还提出了一种判别特征选择器，以找到最具有区分性的特征，并将其作为视觉图的节点，以提高效率和效果。此外，我们采用语言图匹配损失来调节语言图的更新，并在训练过程中维护其语义表示。 
- (3):本文提出了一种跨模态图推理适应（CIGAR）框架，通过基于图的学习进行类别级别的对齐。为了提高效率并提高对噪声的鲁棒性，我们提出了一种判别特征选择器，用于找到具有区分性的图像特征并仅使用这些具有区分性的特征构建代表性视觉图。我们的CIGAR还探索了语言模态中的图，并在语言模态和视觉模态之间执行跨模态图推理。我们的CIGAR不仅可以为具有多个类别的任务构建图并捕获不同类别之间的关系，还可以为具有单个类别的任务构建图并捕获单个类别的不同组件之间的关系。 
- (4):在四个适应任务上进行了全面的实验，我们的CIGAR取得了最先进的性能，比现有方法大幅优越。
#### 7. 方法详细介绍：
本文提出了一种名为CIGAR的方法，用于域自适应目标检测。该方法包括以下步骤：
1. 构建视觉模态图和语言模态图，其中视觉模态图使用判别式特征选择器来挖掘最具代表性的特征，语言模态图使用类别标签或语义原型来构建。
2. 使用图卷积网络和自注意力机制对每个图中的节点嵌入进行编码，以进行内部图推理。
3. 使用交叉注意力机制对视觉和语言图进行推理，以增强它们的表示。
4. 引入语言知识匹配损失，以在训练过程中调整语言图的更新，维护其语义表示。

#### 8. 实验设置：
本文在四种不同的域转移任务上进行了实验，分别是Cityscapes→Foggy Cityscapes、Pascal VOC→Clipart、Sim10k→Cityscapes和KITTI→Cityscapes。使用FCOS作为基线检测器，并使用平均精度（mAP）来评估目标域上的检测性能。

#### 9. 实验结果和分析：
本文的CIGAR方法在所有四种域转移任务上均优于现有方法，取得了显著的mAP提升。与其他现有方法进行了详细的比较分析。实验结果表明，CIGAR方法在域自适应目标检测任务中具有很高的性能和实用性。


# Paper:845     expOSE：使用指数正则化的准确无初始化投影因子分解



#### 1. Title: 
expOSE: Accurate Initialization-Free Projective Factorization using Exponential Regularization

#### 2. Authors: 
José Pedro Iglesias, Amanda Nilsson, Carl Olsson

#### 3. Affiliation: 
José Pedro Iglesias: 瑞典查尔默斯理工大学 (Chalmers University of Technology, Sweden)

#### 4. Keywords: 
Structure from Motion, Factorization, Projective Depth, Regularization, Radial Distortion

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Iglesias_expOSE_Accurate_Initialization-Free_Projective_Factorization_Using_Exponential_Regularization_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了结构从运动（SfM）中的因子分解方法，该方法可以在不需要良好初始化的情况下进行准确的初始化。 
- (2):过去的方法需要良好的初始化，而本文提出的pOSE方法可以在不需要良好初始化的情况下进行准确的初始化。但是，pOSE方法对大深度的惩罚过于严厉，限制了可行深度的范围。因此，本文提出了expOSE方法，其具有指数正则化，对于正深度而言可以忽略不计。 
- (3):本文提出了expOSE方法，其具有指数正则化，对于正深度而言可以忽略不计。为了实现有效的推理，我们使用了二次近似，可以使用VarPro进行迭代求解。此外，我们通过将对象空间误差分解为径向和切向分量，将该方法扩展到具有径向畸变鲁棒性。 
- (4):在实验中，本文提出的方法在不需要捆绑调整的情况下，比现有方法具有更高的重建质量和更少的依赖于η的选择。
#### 7. 方法详细介绍：
本文提出了一种新的方法expOSE，用于准确的无初始化投影因子分解。该方法使用指数正则化将问题转化为低秩恢复问题，并使用指数函数对深度进行正则化。所提出的expOSE目标是原始pOSE模型和指数正则化的组合。优化使用VarPro方法，该方法使用Levenberg-Marquardt方法解决简化问题。指数正则化使用二次函数进行近似，以使其与VarPro兼容。算法还采用图像数据的归一化，以实现良好的条件形式。该方法还通过将对象空间误差分解为径向和切向分量来扩展径向畸变鲁棒性。本文提供了pOSE模型的详细说明以及提出的expOSE方法如何改进它。

#### 8. 实验设置：
本文在具有径向畸变的[24]中的3个序列上评估了所提出的管道的性能：Grossmunster（19个相机，1874个点，41％的缺失数据），Kirchenge（30个相机，1158个点，60％的缺失数据）和Munterhof（20个相机，2108个点，42％的缺失数据）。该管道与步骤1中的pOSE和RpOSE进行比较。

#### 9. 实验结果和分析：
本文评估了所提出的管道的性能，包括收敛速率、2D重投影误差、旋转误差和3D误差。结果表明，expOSE优于pOSE和RpOSE。当查看因子化的输出时，性能差异更加明显，其中expOSE能够实现几乎与捆绑调整的精细解决方案相匹配的重投影误差。还观察了使用径向畸变不变性的正则化的影响。在实践中，α = 1实现了具有径向畸变的图像的最佳结果。额外的实验显示了在特定问题实例中使用值1/2 <α <1的好处，其中数据可用性过低，无法稳定纯径向模型。


# Paper:846     HyperCUT：使用无监督排序从单个模糊图像中提取视频序列



#### 1. Title: 
HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering

#### 2. Authors: 
Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, Rang Nguyen, Minh Hoai

#### 3. Affiliation: 
Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, and Rang Nguyen are affiliated with VinAI Research, Vietnam. 

#### 4. Keywords: 
Image-to-video deblurring, HyperCUT, order ambiguity, contrastive loss, real blur2vid dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Pham_HyperCUT_Video_Sequence_From_a_Single_Blurry_Image_Using_Unsupervised_CVPR_2021_paper.html  Github: https://github.com/VinAIResearch/HyperCUT.git

#### 6. Summary: 
- (1): This paper focuses on the task of image-to-video deblurring, which aims to recover a sequence of sharp images from a single blurry image. The main challenge in this task is the order ambiguity issue, where both the forward and backward sequences are plausible solutions. 

- (2): Previous methods have relied on order-invariant losses, which do not fully resolve the issue of having multiple solutions. This paper proposes an effective self-supervised ordering scheme called HyperCUT, which assigns an explicit order for each video sequence, thus avoiding the order-ambiguity issue. The authors map each video sequence to a vector in a latent high-dimensional space so that there exists a hyperplane such that for every video sequence, the vectors extracted from it and its reversed sequence are on different sides of the hyperplane. The side of the vectors is used to define the order of the corresponding sequence. 

- (3): The authors propose a real-image dataset for the image-to-video deblurring problem that covers a variety of popular domains, including face, hand, and street. They also introduce HyperCUT, which is used to solve the order ambiguity issue for the task of extracting a sharp video sequence from a blurry image. HyperCUT is found by representing it as a neural network and training it in an unsupervised manner using a contrastive loss. 

- (4): The proposed method achieves state-of-the-art performance on the real blur2vid dataset, demonstrating the effectiveness of the proposed approach in solving the order ambiguity issue. The performance supports the authors' goals of improving the quality of the captured image and recovering the motion of objects.
#### 7. 方法详细介绍：
本文提出了一种名为HyperCUT的方法，用于解决模糊图像到视频的去模糊任务中的顺序不确定性问题。HyperCUT通过在高维空间中使用一个固定的超平面来找到一个映射函数H，该函数确保清晰帧的正向和反向序列位于超平面的不同侧。函数H由神经网络表示，并使用softplus损失函数进行训练。然后使用HyperCUT正则化来强制去模糊网络仅预测在超平面同侧的帧序列，从而解决了顺序不确定性问题。模型训练的最终损失函数是模糊到视频的损失和HyperCUT正则化的组合。

具体步骤如下：
1. 将所有帧序列映射到高维空间中。
2. 使用对比损失函数训练神经网络，使得同一时间对称序列的向量对之间的距离尽可能小，不同时间对称序列的向量对之间的距离尽可能大。
3. 使用HyperCUT正则化，将所有帧序列限制在超平面的同一侧。

#### 8. 实验设置：
本文使用了一个名为Real blur2vid (RB2V)的真实模糊到视频数据集进行评估，该数据集是使用分束器相机系统采集的。数据集包括三个类别：街道、手和人脸，并分为不相交的训练和测试集。方法使用PyTorch实现，并在NVIDIA Tesla V100 GPU上进行训练。

#### 9. 实验结果和分析：
本文的方法在RB2V数据集上取得了优于现有方法的性能。在人脸和手部轨迹恢复方面，与基线模型相比，提出的方法表现出更好的准确性和重建质量。实验结果表明，HyperCUT正则化可以有效地解决模糊图像到视频的顺序不确定性问题。


# Paper:847     通过条件属性插值学习去偏置表示



#### 1. Title: 
Learning Debiased Representations via Conditional Attribute Interpolation

#### 2. Authors: 
Yi-Kai Zhang, Qi-Wei Wang, De-Chuan Zhan, Han-Jia Ye

#### 3. Affiliation: 
南京大学（Nanjing University）

#### 4. Keywords: 
Deep Neural Network, Biased Dataset, Debiased Representation, Intermediate Attribute Samples, Conditional Attribute Interpolation

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Learning_Debiased_Representations_via_Conditional_Attribute_Interpolation_CVPR_2020_paper.html  Github: https://github.com/ZhangYikaii/chi-square

#### 6. Summary : 
- (1):本文研究的背景是在深度神经网络（DNN）训练时，当数据集存在偏差时，模型容易通过“非目标”属性进行预测，从而影响模型的泛化能力。

- (2):过去的方法需要额外的监督，如预定义的偏差类型，而这需要昂贵的手动注释。本文提出了一种χ2模型，通过中间属性样本（IASs）和条件插值来学习去偏置的表示，从而有效地提高了模型的泛化能力。

- (3):本文提出的χ2模型分为两个阶段。第一阶段，我们在偏差数据集上训练一个基本模型，并记录样本的训练动态。通过设计一个χ形状模式来匹配训练样本，我们挖掘出IASs，然后使用条件插值来消除外围属性的负面影响，并促进保留类内紧凑性。第二阶段，我们使用χ结构度量学习目标来修正表示，从而消除偏差属性的影响。

- (4):在各种数据集上的实验表明，χ2模型有效地学习了去偏置的表示，并取得了显着的改进。
#### 7. 方法详细介绍：
本文提出了一种名为χ2-model的方法，旨在通过条件属性插值学习去偏置的表示。该方法分为两个阶段。第一阶段，使用基础模型训练数据集，并记录训练动态，发现中间属性样本（IASs），这些样本编码了偏置属性从一个极端（主要BA侧）到另一个（BC侧）的变化，并填补了BA和BC样本之间的低密度内类“空洞”。第二阶段，使用IASs构建不同比例的偏置包，并设计χ-结构度量学习目标来拉近内类样本。该方法在Colored MNIST数据集上进行了评估，并取得了最先进的性能。具体步骤包括：
1. 训练基础模型
2. 记录训练动态，发现IASs
3. 构建偏置包
4. 计算原型
5. 设计χ-结构度量学习目标

#### 8. 实验设置：
本文在Colored MNIST、Corrupted CIFAR-10、Biased CelebA和Biased NICO数据集上验证了提出的χ2-model方法。其中，非目标偏置属性为颜色，目标属性为形状。实验中，选择与目标属性共现频率最高的偏置属性，并控制相关性比例。模型实现使用PyTorch，在单个NVIDIA Tesla V100 GPU上运行。

#### 9. 实验结果与分析：
本文在各个数据集上进行了实验，结果表明，提出的χ2-model方法在去偏置表示方面具有显著的优势。在CIFAR-10数据集上，该方法的准确率达到了94.68%，远高于现有方法。在CIFAR-100数据集上，该方法的准确率为74.23%，也优于现有方法。在ImageNet数据集上，该方法的准确率为75.5%，与现有方法相当。实验结果表明，提出的方法在学习去偏置表示方面具有很好的效果。


# Paper:848     视频中生理信号的非对比无监督学习



#### 1. Title: 
Non-Contrastive Unsupervised Learning of Physiological Signals from Video

#### 2. Authors: 
Jeremy Speth, Nathan Vance, Patrick Flynn, Adam Czajka

#### 3. Affiliation: 
University of Notre Dame (圣母大学)

#### 4. Keywords: 
Remote photoplethysmography, unsupervised learning, physiological signals, video analysis

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Speth_Non-Contrastive_Unsupervised_Learning_of_Physiological_Signals_From_Video_CVPR_2021_paper.html  Github: https://github.com/CVRL/SiNC-rPPG

#### 6. Summary : 
- (1):本文研究的背景是远程光电容积脉搏图（rPPG）的无监督学习，旨在通过视频分析提取生理信号，实现低成本的非接触式健康监测。

- (2):过去的方法主要是基于监督学习的深度学习模型，但是由于数据稀缺，这些方法需要使用带有接触式PPG传感器的基准数据集进行训练和评估。本文提出了一种基于非对比无监督学习的框架，通过周期性信号先验来发现未标记视频中的血容量脉搏。与现有的对比无监督学习方法相比，本文的方法不需要成对或三元组比较，可以在批次中计算损失函数。

- (3):本文提出了一种基于周期性信号先验的生理信号估计的非对比无监督学习框架（SiNC），通过鼓励正常生理频带内的稀疏功率谱和批次功率谱的方差来学习周期性信号的视觉特征。本文的方法可以发现视频中的其他周期性信号，从而实现多种生理测量而无需地面真实信号。本文的创新点是提出了一种非对比无监督学习方法，可以在未标记的视频数据上训练鲁棒的脉率估计器。

- (4):本文的方法在未标记视频数据上进行了实验，结果表明，本文的方法可以训练出鲁棒的脉率估计器。本文的方法可以发现视频中的其他周期性信号，从而实现多种生理测量而无需地面真实信号。本文的方法在多个数据集上进行了测试，结果表明，本文的方法在脉率估计方面具有竞争力。
#### 7. 方法详细介绍：
本文提出了一种非对比无监督学习方法，用于从视频中估计生理信号。该方法使用3D-CNN架构，时间核宽度为5，并将零填充替换为边缘重复。模型使用AdamW优化器进行训练，学习率为0.0001，剪辑长度为T = 120帧。脉搏率被计算为在10秒滑动窗口内0.66 Hz至3 Hz之间的最高谱峰。该方法使用PURE、UBFC-rPPG和DDPM三个数据集进行评估，并与传统、监督和无监督学习方法进行比较。交叉数据集测试也被执行，以分析该方法对光照、相机传感器、脉率分布和运动变化的鲁棒性。

#### 8. 实验设置：
本文在PURE、UBFC-rPPG和DDPM三个数据集上进行实验。模型在PURE和UBFC-rPPG上训练200个epoch，在DDPM上训练40个epoch。训练期间的批量大小为20个样本。该方法使用5倍交叉验证对PURE和UBFC进行评估，使用DDPM的预定义数据集拆分进行测试。

#### 9. 实验结果和分析：
本文对所提出的方法和其他传统、监督和无监督学习方法进行了数据集内和数据集间的脉搏率估计结果评估。该方法在所有数据集上都取得了最低的MAE，但与其他无监督方法相比具有更高的RMSE。该方法优于对比方法，仅被监督深度学习模型超越。由于DDPM数据集的整体难度，性能下降。该方法对光照、相机传感器、脉率分布和运动变化具有鲁棒性。在DDPM上训练总体效果最佳，而仅在PURE上训练在转移到UBFC-rPPG和DDPM时效果相对较差。


# Paper:849     NeFII：考虑近场间接照明的反射分解反渲染



#### 1. Title: 
NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination

#### 2. Authors: 
Haoqian Wu, Zhipeng Hu, Lincheng Li, Yongqiang Zhang, Changjie Fan, Xin Yu

#### 3. Affiliation: 
NetEase Fuxi AI Lab (网易福熙人工智能实验室)

#### 4. Keywords: 
Inverse rendering, reflectance decomposition, near-field indirect illumination, Monte Carlo sampling, radiance consistency constraint

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wu_NeFII_Inverse_Rendering_for_Reflectance_Decomposition_With_Near-Field_Indirect_Illumination_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究的是反渲染问题，即从多视角RGB图像中估计几何、材料和照明。本文的研究背景是如何更好地分解材料和照明，特别是考虑到近场间接照明的影响。
- (2):过去的方法大多数只考虑了直接照明，而忽略了间接照明。近期的一些方法尝试通过球形高斯函数（SG）来模拟不同材料反射的间接照明，但这往往会模糊高频反射细节。本文提出了一种端到端的反渲染流程，同时考虑到材料和照明的分解，以及近场间接照明。本文引入了基于蒙特卡罗采样的路径追踪，并将间接照明缓存为神经辐射，从而实现了物理上可信且易于优化的反渲染方法。为了提高效率和实用性，本文利用SG来表示平滑的环境照明，并应用重要性采样技术。为了监督来自未观察方向的间接照明，本文开发了一种新的辐射一致性约束，从而显著提高了分解性能。
- (3):本文提出了一种新的反渲染流程，同时考虑到材料和照明的分解，以及近场间接照明。本文引入了基于蒙特卡罗采样的路径追踪，并将间接照明缓存为神经辐射，从而实现了物理上可信且易于优化的反渲染方法。为了提高效率和实用性，本文利用SG来表示平滑的环境照明，并应用重要性采样技术。为了监督来自未观察方向的间接照明，本文开发了一种新的辐射一致性约束，从而显著提高了分解性能。本文的创新点在于将路径追踪和SG相结合，同时优化间接照明和材料，从而实现更好的反射分解。
- (4):本文在多个合成和真实数据集上进行了广泛的实验，结果表明本文的方法在多个任务上均优于现有方法，特别是在反射分解方面表现更好。
#### 7. 方法详细介绍：
本文提出了一种名为NeFII的反渲染方法，该方法考虑了近场间接照明。该方法使用联合学习框架，将路径追踪和神经渲染相结合，将场景的反射分解为漫反射反照率、粗糙度和镜面反射组件。该方法还模拟了间接照明和可见性，以准确地将间接照明和阴影与漫反射反照率分离开来。该方法使用观察到的和未观察到的光线进行训练，以提高间接照明建模的准确性。该方法使用Pytorch实现，并使用Adam进行优化。具体步骤包括：
1. 使用Monte Carlo采样的路径追踪模拟场景中的光线传播。
2. 使用神经隐式场表示材料和间接照明。
3. 使用球形高斯函数表示光照。
4. 使用重要性采样技术提高效率和实用性。
5. 引入辐射一致性约束，以减少歧义。

#### 8. 实验设置：
本文使用Blender Cycles在自然HDRI环境映射下渲染了四个具有自反射的合成场景，每个场景使用200张图像和它们的掩码进行渲染，并均匀采样100张图像进行训练，剩余的用于测试。本文还渲染了测试图像的漫反射反照率图、粗糙度图和镜面反射组件，以评估反渲染能力。图像分辨率设置为512×512。本文使用Peak Signal-to-Noise Ratio（PSNR）、Structural Similarity Index Measure（SSIM）和Learned Perceptual Image Patch Similarity（LPIPS）等指标对所提出的方法与现有方法进行比较。

#### 9. 实验结果和分析：
本文提出的方法在材料估计方面取得了显著的改进，特别是在粗糙度估计和镜面反射合成方面。该方法更好地模拟了锐利的自反射，并将阴影和间接照明与漫反射反照率分离开来。该方法恢复的粗糙度图更加准确。该方法还支持使用恢复的材料进行进一步的重新照明。然而，该方法在大阴影区域中估计粗糙度时存在困难，因为场景的可见性较低，在一些极端情况下，阴影可能会泄漏到漫反射反照率中，导致光照歧义。


# Paper:850     PyPose：基于物理优化的机器人学习库



#### 1. Title: 
PyPose: A Library for Robot Learning with Physics-based Optimization

#### 2. Authors: 
Chen Wang, Dasong Gao, Kuan Xu, Junyi Geng, Yaoyu Hu, Yuheng Qiu, Bowen Li, Fan Yang, Brady Moon, Abhinav Pandey, Aryan, Jiahe Xu, Tianhao Wu, Haonan He, Daning Huang, Zhongqiang Ren, Shibo Zhao, Taimeng Fu, Pranay Reddy, Xiao Lin, Wenshan Wang, Jingnan Shi, Rajat Talak, Kun Cao, Yi Du, Han Wang, Huai Yu, Shanzhao Wang, Siyu Chen, Ananth Kashyap, Rohan Bandaru, Karthik Dantu, Jiajun Wu, Lihua Xie, Luca Carlone, Marco Hutter, Sebastian Scherer

#### 3. Affiliation: 
第一作者：Carnegie Mellon University（卡内基梅隆大学）

#### 4. Keywords: 
Robot learning, physics-based optimization, PyTorch, Lie groups, Lie algebras

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PyPose_A_Library_for_Robot_Learning_With_Physics-Based_Optimization_CVPR_2021_paper.html  Github: https://github.com/cmwang0111/PyPose

#### 6. Summary : 
- (1):本文研究背景是深度学习在机器人感知方面的成功，但其数据中心的本质在面对不断变化的环境时表现不佳。
- (2):过去的方法是基于物理优化的，但由于缺乏高级语义信息和依赖手动参数调整，在复杂任务中表现不佳。本文提出了PyPose，这是一个基于PyTorch的机器人导向库，将深度感知模型与基于物理优化相结合。PyPose的架构整洁有序，具有命令式风格的接口，高效易用，易于集成到实际机器人应用中。此外，它支持任意阶梯度的李群和李代数以及二阶优化器，如信任区域方法。实验表明，PyPose在计算方面比现有的库快10倍以上。为了推动未来的研究，我们提供了几个机器人学习领域的具体示例，包括SLAM、规划、控制和惯性导航。
- (3):本文提出了PyPose，这是一个基于PyTorch的机器人导向库，将深度感知模型与基于物理优化相结合。PyPose支持任意阶梯度的李群和李代数以及二阶优化器，如信任区域方法。PyPose提供了一个命令式编程风格，易于实际机器人应用中的集成。PyPose是一个开源库，可以加速机器人学习的下一代发展。PyPose是一个系统化的开发工具，可以加速端到端学习的物理优化。PyPose是一个Python库，可以覆盖机器人学习的几个子领域，如感知、SLAM和控制，其中涉及优化。
- (4):本文在感知、运动规划和自动控制等方面探索了端到端学习的各种应用。实验表明，PyPose在计算方面比现有的库快10倍以上。PyPose提供了一个命令式编程风格，易于实际机器人应用中的集成。PyPose是一个开源库，可以加速机器人学习的下一代发展。PyPose是一个系统化的开发工具，可以加速端到端学习的物理优化。
#### 7. 方法详细介绍：
本文介绍了PyPose，这是一个基于PyTorch的机器人学习库，结合了深度感知模型和基于物理的优化。PyPose支持Lie群和Lie代数的任意阶梯度计算，以及Levenberg-Marquardt等二阶优化器。该库的设计旨在易于解释、用户友好和高效，具有整洁和良好组织的架构。PyPose提供了一种命令式编程风格，方便实际的机器人应用。具体而言，PyPose提供了四个概念：LieTensor、Module、Function和Optimizer。LieTensor是PyTorch Tensor的子类，用于表示3D变换，支持任意阶梯度自动微分，并与大多数流行设备兼容。Module和Function用于实现可微分的机器人相关功能，PyPose提供了许多有用的模块，例如系统转移函数、模型预测控制（MPC）、卡尔曼滤波器和IMU预积分。Optimizer用于集成除基本梯度下降方法（如SGD和Adam）之外的一般优化器，PyPose提供了一个二阶Levenberg-Marquardt（LM）优化器，用于鲁棒的非线性最小二乘问题。

#### 8. 实验设置：
本文没有特定的实验设置部分。

#### 9. 实验结果和分析：
本文提供了多个实验的详细结果和分析。例如，PyPose的性能与其他库进行了比较，包括计算密集型和批处理的Jacobian矩阵，显示出显著的加速。PyPose的Levenberg-Marquardt优化器的性能与PyTorch的一阶优化器进行了比较，证明了二阶优化器在某些问题上的有效性。PyPose还用于姿态图优化，并实现了与Ceres和GTSAM相同的最终误差。本文还包括SLAM和规划实验的重投影误差分布和结果轨迹和点云的视觉比较，以及MPC的计算成本比较。此外，本文还提供了多个实际机器人学习领域的具体示例，包括SLAM、规划、控制和惯性导航。


# Paper:851     基于三视角视图的三维语义占用预测



#### 1. Title: 
Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction

#### 2. Authors: 
Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, Jiwen Lu

#### 3. Affiliation: 
北京信息科学与技术国家研究中心，清华大学自动化系

#### 4. Keywords: 
3D perception, vision-based occupancy prediction, tri-perspective view, LiDAR segmentation, TPVFormer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2021_paper.html Github: https://github.com/wzzheng/TPVFormer

#### 6. Summary : 
- (1):本文研究的背景是自动驾驶系统中对三维环境的准确感知和全面理解。
- (2):过去的方法主要采用体素表示或鸟瞰图表示，但是体素表示计算量大，鸟瞰图表示难以描述细粒度的三维结构。本文提出了一种三视角表示法，将鸟瞰图表示法与两个垂直平面相结合，以更全面地描述三维结构。本文提出了一种基于Transformer的编码器TPVFormer，将图像特征转换到三视角表示空间中。本文的方法在只使用稀疏LiDAR点云标签进行训练的情况下，能够有效地预测所有体素的语义占用情况。
- (3):本文提出了一种新的三视角表示法，将鸟瞰图表示法与两个垂直平面相结合，以更全面地描述三维结构。本文提出了一种基于Transformer的编码器TPVFormer，将图像特征转换到三视角表示空间中。本文的方法在只使用稀疏LiDAR点云标签进行训练的情况下，能够有效地预测所有体素的语义占用情况。
- (4):本文的方法在LiDAR分割任务和3D语义场景补全任务上进行了评估，只使用RGB图像作为输入。在LiDAR分割任务上，本文的方法使用LiDAR数据进行点查询以计算评估指标，结果表明本文的方法在只使用稀疏LiDAR点云标签进行训练的情况下，能够有效地预测所有体素的语义占用情况，并且在LiDAR分割任务上取得了与基于LiDAR的方法相当的性能。
#### 7. 方法详细介绍：
本文提出了一种三视角（Tri-Perspective View，TPV）表示法，用于描述三维场景的细节结构。该方法使用一个基于Transformer的TPV编码器（TPVFormer）将图像特征有效地提升到三维TPV空间中。TPVFormer采用注意力机制聚合每个TPV平面中对应于每个查询的图像特征。该方法通过稀疏监督进行训练，可以预测所有体素的语义占用情况。具体步骤如下：
1. 使用TPV表示法将三维场景转换为三个正交平面的投影。
2. 使用TPVFormer将图像特征提升到三维TPV空间中。
3. 使用注意力机制聚合每个TPV平面中对应于每个查询的图像特征。
4. 使用轻量级的预测头对三个TPV平面上的特征进行求和，以预测三维空间中每个点的语义占用情况。
5. 将TPV平面转换为点和体素特征，并使用轻量级的MLP对点或体素特征进行语义标签预测。

#### 8. 实验设置：
本文在Panoptic nuScenes和Semantic KITTI数据集上进行了三种类型的实验，包括三维语义占用预测、LiDAR分割和语义场景补全。对于所有任务，模型仅使用RGB图像作为输入。

#### 9. 实验结果与分析：
本文在三种任务上进行了实验，结果表明该方法在三维语义场景补全和LiDAR分割任务上表现出色，与大多数基于LiDAR的方法相比，具有可比的mIoU。在三维语义占用预测任务上，该方法在IoU和mIoU方面均优于其他方法。本文还对TPV分辨率和特征维度进行了消融实验，并比较了BEV、体素和TPV表示法在LiDAR分割任务中的性能。该方法的失败案例包括难以区分靠近的行人和将远处的行人预测为条形区域。


# Paper:852     消除偏差的增强学习用于去偏联邦学习



#### 1. Title: 
Bias-Eliminating Augmentation Learning for Debiased Federated Learning

#### 2. Authors: 
Yuan-Yi Xu, Ci-Siang Lin, Yu-Chiang Frank Wang

#### 3. Affiliation: 
第一作者：国立台湾大学

#### 4. Keywords: 
Debiased Federated Learning, Bias-Eliminating Augmentation Learning, Federated Learning, Data Bias

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度学习模型在训练数据集存在偏差时，会出现性能下降的问题。

- (2):过去的方法主要是针对集中式机器学习，无法直接应用于分布式学习，如联邦学习。本文提出了一种新的联邦学习框架，称为Bias-Eliminating Augmentation Learning (FedBEAL)，用于解决分布式学习中的偏差问题。该方法通过引入Bias-Eliminating Augmenters (BEA)来生成客户端特定的偏差冲突样本，从而消除本地数据偏差。与现有方法不同的是，该方法不需要预先知道偏差类型或属性，而是采用独特的学习策略来联合训练BEA和FL框架。本文的方法在多个数据集上进行了广泛的图像分类实验，证明了其有效性和适用性。

- (3):本文提出了一种新的联邦学习框架，称为Bias-Eliminating Augmentation Learning (FedBEAL)，用于解决分布式学习中的偏差问题。该方法通过引入Bias-Eliminating Augmenters (BEA)来生成客户端特定的偏差冲突样本，从而消除本地数据偏差。与现有方法不同的是，该方法不需要预先知道偏差类型或属性，而是采用独特的学习策略来联合训练BEA和FL框架。

- (4):本文的方法在多个数据集上进行了广泛的图像分类实验，证明了其有效性和适用性。与现有的去偏方法相比，本文的方法在去除数据偏差方面表现更好，同时在联邦学习中也取得了良好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于联邦平均（FedAvg）的去偏差联邦学习框架——Bias-Eliminating Augmentation Learning（FedBEAL）。FedBEAL在每个客户端引入Bias-Eliminating Augmenter（BEA）来生成偏差冲突样本并消除本地数据偏差。BEA的学习是通过利用全局服务器和本地客户端模型在迭代中训练实现的，这允许识别和嵌入所需的语义和偏差特征以进行增强。通过引入增强器和生成的偏差冲突样本，可以在每个客户端执行去偏差的本地更新，然后简单地聚合这些模型以获得服务器模型。

具体步骤如下：
1. 在每个客户端引入BEA，用于生成偏差冲突样本并消除本地数据偏差。
2. 利用全局服务器和本地客户端模型在迭代中训练BEA，以识别和嵌入所需的语义和偏差特征以进行增强。
3. 利用BEA生成偏差冲突样本，进行去偏差的本地更新。
4. 将每个客户端的更新模型聚合，得到去偏差的全局模型。

#### 8. 实验设置：
本文使用三个数据集Colored MNIST、Corrupted CIFAR-10和Collage CIFAR-10来评估所提出的方法。训练集分布在10个客户端之间，本地数据偏差的严重程度由偏差本地数据量比率β量化。输入图像分别调整为28×28、32×32和64×64像素。在Colored MNIST和Corrupted CIFAR-10/Collage CIFAR-10上使用LeNet和ResNet-18作为分类器。采用ResNet-18编码器的U-Net作为增强器。通信轮数T设置为100，每个客户端使用SGD优化器依次训练g和f 5个时期，批量大小为64，学习率为0.01，动量为0.9，权重衰减为0.00001。使用PyTorch在单个NVIDIA 3090 GPU上进行实现。

#### 9. 实验结果与分析：
本文提出的FedBEAL方法与现有的集中式去偏差和异构联邦学习方法进行了比较。结果表明，FedBEAL在所有数据集上均优于现有方法，且对不同偏差比率的数据集都具有较好的效果。此外，本文还将所提出的方法与最先进的混合样本数据增强算法进行了比较。结果表明，与MSDA方法相比，FedBEAL的准确率提高了14.53％，表明所提出的增强方案对不同偏差类型具有鲁棒性和泛化能力。FedAvg和FedBEAL在Colored MNIST数据集上的全局和本地模型的偏差水平进行了比较，结果支持了所提出的BEA的设计和学习方案。最后，本文定性评估了FedBEAL推导语义感知和去偏差特征表示的能力，结果表明，我们的模型推导出的特征在偏差属性方面保持相对不相关，不同类别簇之间的分离更为显著。


# Paper:853     可重复的对比语言-图像学习的扩展规律



#### 1. Title: 
Reproducible scaling laws for contrastive language-image learning

#### 2. Authors: 
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, Jenia Jitsev

#### 3. Affiliation: 
Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ) (Mehdi Cherti)

#### 4. Keywords: 
scaling laws, contrastive language-image learning, CLIP, pre-training, power law scaling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2022_paper.html  Github: https://github.com/LAION-AI/scaling-laws-openclip

#### 6. Summary : 
- (1):本文研究了对比语言-图像预训练（CLIP）的可扩展性，探讨了训练集大小、模型大小和计算量等因素对性能的影响，旨在为大规模实验提供有价值的指导。
- (2):以往的扩展性研究主要使用私有数据和模型，或者专注于单模态语言或视觉学习。本文使用公共LAION数据集和开源OpenCLIP库，研究了对比语言-图像预训练的扩展性，并在多个下游任务中发现了多个功率律扩展规律。作者发现，训练分布在扩展规律中起着关键作用，OpenAI和OpenCLIP模型表现出不同的扩展行为，尽管模型架构相同，训练配方相似。本文的创新点在于使用公共数据集和开源代码，使得研究更具可重复性和可访问性。
- (3):本文使用LAION-5B数据集进行对比语言-视觉学习的扩展性研究，使用开源的OpenCLIP代码训练CLIP模型，同时变化模型、数据和样本量。作者在多个下游任务中评估了CLIP模型的性能，包括零样本分类、图像检索和线性探测和端到端微调。作者发现，当扩展模型、数据和计算时，性能始终有所提高，并且在不同下游任务中得到了功率律形式的扩展规律。作者还发现，OpenCLIP和OpenAI的原始CLIP模型在零样本检索方面具有更大的扩展系数，而OpenAI CLIP模型在零样本分类方面表现更好。作者假设训练数据集是OpenCLIP和OpenAI模型扩展行为在任务上差异的原因。
- (4):本文的方法在多个下游任务中取得了良好的性能，包括零样本分类、图像检索、线性探测和端到端微调。作者的创新点在于使用公共数据集和开源代码，使得研究更具可重复性和可访问性。本文的研究结果为改进当前的图像-文本数据集和模型提供了具体的起点。
#### 7. 方法详细介绍：
本文使用了对比语言-图像学习（CLIP）方法，通过在公共数据集LAION上进行训练，使用开源的OpenCLIP库进行模型训练。使用ViT-B/32、B/16、L/14、H/14和g/14作为视觉编码器，通过混合精度和AdamW优化器进行训练，训练时长为3B、13B和34B。在训练过程中，变化数据规模和样本数，以探究对下游任务的影响。在训练完成后，使用预定义的提示词对每个类别进行嵌入计算，并在ImageNet、ImageNet分布偏移数据集和视觉任务适应基准（VTAB）上进行评估。

#### 8. 实验设置：
本文使用了公共数据集LAION-400M和LAION-2B进行训练，使用了多个模型进行对比，包括ViT-B/32、B/16、L/14、H/14和g/14作为视觉编码器。在训练过程中，变化数据规模和样本数，以探究对下游任务的影响。使用混合精度和AdamW优化器进行训练，权重衰减为0.2。在JUWELS Booster和Stability AI AWS超级计算机上进行分布式训练。

#### 9. 实验结果和分析：
本文研究了模型规模对多个下游任务的影响，包括零样本分类、检索、线性探针和端到端微调。实验结果表明，当增加模型、数据和样本数时，准确率持续提高。准确率遵循幂律分布，较大的模型受益于较大的数据和样本数。使用得到的幂律分布预测了在使用最大数据规模2B和样本数34B的情况下，经过良好调整的ViT-g/14模型在ImageNet上的误差估计为20.9%（79.1%的top-1准确率）。实验结果还表明，在某些任务中，OpenCLIP在LAION-400M/2B上预训练的模型比在WIT-400M上预训练的CLIP具有更好的性能。


# Paper:854     DART：多样化-聚合-重复训练提高神经网络的泛化性能



#### 1. Title: 
DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks

#### 2. Authors: 
Samyak Jain, Sravanti Addepalli, Pawan Kumar Sahu, Priyam Dey, R.Venkatesh Babu

#### 3. Affiliation: 
第一作者：印度班纳拉斯印度理工学院（Indian Institute of Technology, Varanasi）

#### 4. Keywords: 
Neural Networks, Generalization, Data Augmentation, Domain Generalization, Model Weight Averaging

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jain_DART_Diversify-Aggregate-Repeat_Training_Improves_Generalization_of_Neural_Networks_CVPR_2021_paper.html  Github: https://github.com/val-iisc/DART

#### 6. Summary : 
- (1):本文研究神经网络的泛化问题，提出了一种新的训练策略DART，旨在提高神经网络的泛化性能，特别是在数据增强和域泛化方面。
- (2):过去的方法包括使用数据增强、集成和模型平均等策略来提高泛化性能。然而，这些方法的效果受到网络容量的限制，而且在处理多个域的数据时存在问题。本文提出的DART策略通过训练不同数据增强（或域）的多个模型来探索损失基底，并进一步聚合它们的权重以结合它们的专业知识，从而获得改进的泛化性能。重复聚合步骤可以提高优化轨迹并确保单个模型具有足够低的损失障碍以在组合它们时获得改进的泛化性能。
- (3):本文提出的DART策略首先使用不同的数据增强（或域）训练多个不同的模型，然后聚合它们的权重以获得单个泛化解决方案。聚合模型然后用于重新初始化M个模型，这些模型在聚合后进一步训练。这个过程在训练期间重复，以获得改进的泛化性能。DART策略结合了多样性、专业化和模型权重平均的优点，同时克服了它们各自的缺点。本文的贡献包括：提出了一种强大的基线Mixed-Training（MT），它使用训练minibatch中不同图像的多种不同数据增强；提出了一种新的算法DART，它学习专业化的多样化模型并迭代地聚合它们的权重以提高泛化性能；在几个In-Domain和Domain Generalization数据集上进行了理论和实证分析，证明了DART的有效性。
- (4):本文在CIFAR-10、CIFAR-100、ImageNet和多个Domain Generalization数据集上展示了DART的性能。结果表明，DART策略在各种任务上都取得了最先进的性能，证明了其有效性和实用性。
#### 7. 方法详细介绍：
本文提出了一种名为DART（Diversify-Aggregate-Repeat Training）的训练策略，旨在通过训练多个不同初始化和数据增强的模型，然后聚合它们的权重来提高神经网络的泛化性能。该方法包括四个步骤：i）ERM训练E'个epoch，ii）训练M个不同的模型，每个模型训练λ/M个epoch，iii）聚合它们的权重，iv）重复步骤Diversify-Aggregate E-E'个epoch。模型使用余弦学习率调度进行训练，最大学习率为LRmax。模型的权重使用简单平均聚合。该方法在In-Domain和Domain Generalization设置中与EMA和SWAD相结合，以获得进一步的性能提升。

#### 8. 实验设置：
本文在多个数据集上进行了实验，包括CIFAR-10、CIFAR-100、ImageNet-1K、Stanford-Cars、CUB-200、VLCS、PACS、OfficeHome、TerraInc和DomainNet。使用ResNet-50模型作为骨干网络，使用Imagenet预训练权重进行初始化。在DG设置中，对所有数据集使用固定的超参数值。在域泛化实验中，使用Adam优化器，固定学习率为5e-5，训练迭代次数为15k（DomainNet）和10k（其他数据集），插值频率设置为1k。在ID实验中，使用SGD优化器，动量为0.9，权重衰减为5e-4，余弦学习率调度，最大学习率为0.1，训练ResNet-18和WideResNet-28-10架构600个epoch。

#### 9. 实验结果和分析：
本文的DART方法在多个数据集上进行了实验，并与其他基线方法进行了比较。在CIFAR-10和CIFAR-100数据集上，DART的准确率优于ERM+EMA Mixed Training基线。在DG数据集上，DART的准确率优于多个最先进的方法。实验结果表明，DART方法可以有效提高神经网络的泛化性能。


# Paper:855     OcTr：基于八叉树的Transformer用于3D目标检测



#### 1. Title: 
OcTr: Octree-based Transformer for 3D Object Detection

#### 2. Authors: 
Chao Zhou, Yanan Zhang, Jiaxin Chen, Di Huang

#### 3. Affiliation: 
第一作者：北京航空航天大学软件开发环境国家重点实验室

#### 4. Keywords: 
3D object detection, point clouds, Transformers, octree-based, global receptive field

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_OcTr_Octree-Based_Transformer_for_3D_Object_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是基于LiDAR的3D目标检测，其中一个关键挑战是从大规模的3D场景中捕获足够的特征，特别是对于远距离或/和遮挡的物体。 
- (2):过去的方法可以分为基于网格和基于点云两类，但它们都无法通过深度模型中的小感受野捕获必要的上下文信息，导致结果有限。最近，Transformer在NLP领域的成功启发了许多研究，将这种架构扩展到了3D视觉领域。然而，它们在处理大规模场景时会遇到计算负担过重的问题。 
- (3):本文提出了一种名为OcTr的基于八叉树的Transformer，通过在顶层进行自注意力操作来构建动态八叉树，并递归地向下传播到下一级，从而以粗到细的方式捕获丰富的全局上下文特征，同时保持计算复杂度可控。此外，为了增强前景感知，本文提出了一种混合位置嵌入，由语义感知位置嵌入和注意力掩码组成，以充分利用几何和语义线索。 
- (4):在Waymo Open Dataset和KITTI Dataset上进行了广泛的实验，OcTr取得了最新的最优结果，表明该方法在精度和效率之间取得了良好的平衡。
#### 7. 方法详细介绍：
本文提出了一种基于八叉树的Transformer（OcTr）用于体素化的三维物体检测。该方法首先将点云体素化为规则网格，并采用稀疏三维卷积进行补丁嵌入。将网格视为“令牌”，并通过Octree Transformer Blocks（OTB）传递。自注意力模块被替换为提出的八叉树注意力OctAttn，以更有效地编码全局上下文。在对多尺度特征进行混合语义嵌入后，将两个OTB顺序堆叠，然后是下采样层。然后通过点卷积将体素特征投影到BEV视图中，并通过多尺度密集2D骨干网络。最终，使用基于锚点或无锚点的RPN头进行3D提议生成，RoI头用于细化。

#### 8. 实验设置：
本文在Waymo Open Dataset（WOD）和KITTI数据集上进行了实验。将提出的OcTr模型与几种最先进的方法进行了比较，包括PointPillars、SECOND和VoxelNet。实验在单个NVIDIA V100 GPU上进行，内存为32GB。对于所有实验，批量大小设置为2。输入点云被体素化为分辨率为0.1m的3D网格。

#### 9. 实验结果和分析：
提出的OcTr模型在Waymo Open Dataset和KITTI数据集上均取得了最先进的性能。在Waymo Open Dataset上，OcTr在中等难度和困难难度上的表现优于先前的最先进方法1.5％和1.2％。在KITTI数据集上，OcTr在中等和困难难度级别中的表现均优于所有方法，分别为83.56％和77.68％的AP。提出的OcTr模型还在远距离物体上显示出显着的增益。


# Paper:856     通过重写模型决策实现可信皮肤癌诊断



#### 1. Title: 
Towards Trustable Skin Cancer Diagnosis via Rewriting Model’s Decision

#### 2. Authors: 
Siyuan Yan, Zhen Yu, Xuelin Zhang, Dwarikanath Mahapatra, Shekhar S. Chandra, Monika Janda, Peter Soyer, Zongyuan Ge

#### 3. Affiliation: 
Monash University (莫纳什大学)

#### 4. Keywords: 
Skin cancer diagnosis, deep neural networks, confounding factors, concept learning, human-in-the-loop framework

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是深度神经网络在图像识别任务上表现出的优异性能，但是它们可能会过度依赖混淆因素，使用无关的人工制品或数据集内的偏见来提高性能。当模型基于这些虚假相关性进行决策时，它可能会变得不可信，并在实际场景中导致灾难性结果。本文旨在探索并解决这个问题，提出了一种人机交互的框架，使用户可以观察和纠正模型的决策逻辑，以消除混淆行为。

- (2):过去的方法主要是基于像素级别的归因解释方法或基于概念的解释方法，但是这些方法都存在一些问题，如需要完全注释的概念标签、无法发现数据集内的混淆因素等。本文提出了一种基于概念学习的方法，可以自动发现混淆因素，并通过人机交互的方式进行纠正，从而提高模型的可信度和可靠性。

- (3):本文提出了一种基于概念映射和逻辑重写的人机交互框架，可以将模型的特征表示映射到可解释的概念空间中，使人类用户可以理解概念并进行干预。具体而言，本文提出了一种基于谱相关性分析和概念激活向量的方法，可以学习混淆概念和临床相关概念向量，并通过人机交互的方式进行模型训练，从而消除模型的混淆行为。本文还构建了一个名为ConfDerm的皮肤病数据集，用于系统地评估模型在不同混淆行为下的可信度。

- (4):本文的方法在多个公共皮肤病数据集上进行了实验，结果表明，本文的方法可以有效地检测和消除数据集中的混淆因素，提高模型的性能和可信度。本文的方法还可以应用于任何深度模型，并且可以在不需要完全注释的情况下进行概念学习，具有较好的泛化性能。
#### 1. 实验细节：
本文提出了一种使用新型数据集ConfDerm进行皮肤癌诊断的实验。该数据集包含3576个真实的皮肤镜图像，分为恶性和良性两类。实验在五个公共训练集和一个新的数据集上进行，使用ResNet50和Inception V3模型作为骨干模型，使用准确率和ROC-AUC作为评估指标。实验结果表明，所提出的方法优于基线模型，并改善了所有五个子数据集上所有类别的性能。本文还提出了一种减少肤色对皮肤癌诊断负面影响的方法。

#### 2. 方法：
本文提出了一种人机交互框架，用于提高皮肤癌诊断模型的可信度。该方法通过分析共现行为和使用易于获得的概念示例学习混淆概念，从数据集中发现和去除混淆因素。模型的特征表示被映射到可解释的概念空间中，使人类用户可以通过一阶逻辑指令进行解释和干预。所提出的方法不需要完全注释的概念标签，并且可以应用于任何深度模型之上。

具体步骤如下：
1. 使用谱聚类算法发现全局混淆概念。
2. 使用概念激活向量构建概念库。
3. 将特征表示投影到概念子空间中。
4. 将分类层替换为可解释的逻辑层。
5. 在训练期间使用交互损失对可解释逻辑层的输入梯度进行决策重写。

#### 3. 实验设置：
本文引入了一个新的、经过良好控制的皮肤病变数据集ConfDerm，包含3576张基于ISIC2019和ISIC2020数据集的图像。在训练集中，一个类别中的所有图像都受到五个混淆因素之一的影响，包括暗角、边框、标尺、气泡和毛发。在测试集中，所有图像都是随机的。所提出的方法在ConfDerm和几个公共皮肤病变数据集上进行了系统评估。

#### 4. 实验细节：
所提出的方法在ConfDerm和几个公共皮肤病变数据集上进行了评估。实验结果表明，该方法可以有效地检测和去除数据集中的混淆因素，而不需要任何关于类别分布的先验知识，并且不需要完全注释的概念标签。该方法使模型能够专注于临床相关的概念，提高了模型在模型推断期间的性能和可信度。所提出的方法在性能改进、伪影去除和肤色去偏方面优于现有方法。

#### 5. 实验结果和分析：
本文的实验结果表明，所提出的方法在ConfDerm和几个公共皮肤病变数据集上均取得了优异的性能表现。与基线模型相比，所提出的方法在所有五个子数据集上的所有类别上均有所改善。本文还进行了消融实验，证明了所提出的方法的有效性。此外，本文还提出了一种减少肤色对皮肤癌诊断负面影响的方法，并在实验中证明了其有效性。


# Paper:857     面向灵活的多模态文档模型



#### 1. Title: 
Towards Flexible Multi-modal Document Models

#### 2. Authors: 
Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi

#### 3. Affiliation: 
第一作者：CyberAgent，日本

#### 4. Keywords: 
vector graphic documents, multi-modal elements, masked field prediction, multi-task learning, flexible model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Inoue_Towards_Flexible_Multi-Modal_Document_Models_CVPR_2021_paper.html  Github: https://cyberagentailab.github.io/flex-dm/

#### 6. Summary : 
- (1):本文研究的背景是矢量图形文档的生成和设计，这涉及到多种不同的设计任务，如元素对齐、字体选择、颜色搭配等。

- (2):过去的方法主要集中在单一模态的任务上，如布局生成、字体推荐、着色等，缺乏对多模态任务的综合考虑。本文提出了一种灵活的模型，可以在一个模型中处理多种设计任务，通过掩码字段预测的方式，实现了多任务学习。本文的方法在多个设计任务上取得了较好的性能，且与特定任务的基线方法相比，具有更好的灵活性和可扩展性。

- (3):本文提出的方法是一种基于Transformer的编码器-解码器架构，通过掩码字段预测的方式，实现了多模态元素的处理。通过显式的多任务学习和领域内预训练，本文的模型可以更好地捕捉不同文档字段之间的多模态关系。本文的方法在多个设计任务上取得了较好的性能，且与特定任务的基线方法相比，具有更好的灵活性和可扩展性。

- (4):本文的方法在多个设计任务上取得了较好的性能，如布局生成、字体推荐、着色等，且与特定任务的基线方法相比，具有更好的灵活性和可扩展性。本文的方法可以为设计师提供更好的自动化决策支持，也可以为设计自动化领域的研究提供新的思路和方法。
#### 7. 方法详细介绍：
本文提出了一种灵活的多模态文档模型FlexDM，它由编码器、Transformer块和解码器组成。编码器将每个字段映射到一个固定维度的向量，Transformer块将输入转换为中间表示，解码器将中间表示解码回原始字段空间。模型使用重构损失和显式多任务学习进行训练，其中包括各种掩蔽模式。

#### 8. 实验设置：
本文在两个矢量图文档数据集Rico和Crello上进行了实验。数据集被分为训练、验证和测试集。位置、大小和颜色信息被离散化，使用CLIP提取图像和文本特征。任务被精心选择以评估模型在各种设计任务中的性能，掩蔽比率适度以确保公平比较。

#### 9. 实验结果和分析：
本文的实验结果表明，FlexDM模型在Rico和Crello数据集上的五个设计任务中表现出色。与任务特定和昂贵的基线相比，该模型能够在单个模型中解决多个设计任务。与以前的任务特定方法相比，该模型在几个设计任务中表现出色。模型的性能受到Transformer块数量和预训练使用的影响。在Crello数据集上进行的消融实验表明，自注意力对于建模元素之间的关系非常重要。


# Paper:858     GeoMAE：用于自监督点云预训练的遮蔽几何目标预测



#### 1. Title: 
GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training

#### 2. Authors: 
Xiaoyu Tian, Haoxi Ran, Yue Wang, Hang Zhao

#### 3. Affiliation: 
Hang Zhao: 清华大学三个所

#### 4. Keywords: 
Point cloud, self-supervised learning, geometric feature reconstruction, masked autoencoder, Transformer-based point cloud encoder

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tian_GeoMAE_Masked_Geometric_Target_Prediction_for_Self-Supervised_Point_Cloud_Pre-Training_CVPR_2021_paper.html  Github: https://github.com/Tsinghua-MARS-Lab/GeoMAE

#### 6. Summary : 
- (1):本文旨在解决点云自监督学习中的一个基本问题：在没有注释的情况下，我们应该利用什么信号来学习点云的特征？ 
- (2):与最近的一些论文直接采用遮蔽自编码器（MAE）并仅从遮蔽点云中预测原始坐标或占用相比，本文重新审视了图像和点云之间的差异，并确定了三个点云特有的自监督学习目标，即质心预测、法线估计和曲率预测。这三个目标的结合产生了一个非平凡的自监督学习任务，并相互促进模型更好地推理点云的细粒度几何。 
- (3):本文提出了一种基于几何特征重建的点云表示学习框架。我们的管道在概念上很简单，由两个主要步骤组成：首先，随机屏蔽一组点，然后是基于Transformer的点云编码器；其次，一个轻量级的Transformer解码器预测每个体素中点的质心、法线和曲率。我们将预训练的Transformer编码器转移到下游感知模型。在nuScene数据集上，我们的模型在目标检测方面实现了3.38 mAP的提升，在分割方面实现了2.1 mIoU的提升，在多目标跟踪方面实现了1.7 AMOTA的提升。我们还在Waymo开放数据集上进行了实验，并取得了显著的性能改进。 
- (4):本文提出的方法在多个任务上取得了良好的性能，包括3D目标检测、3D跟踪和分割。本文的贡献在于提出了点云自监督学习的几何感知自监督目标，实现了对点云细粒度几何特征的有效表示学习。
#### 7. 方法详细介绍：
本文提出了一种自监督的点云表示学习方法GeoMAE。该方法采用动态体素化将稀疏的输入点云转换为规则的体素网格，并通过VFE层获取每个体素的特征/令牌。在移除令牌时使用高掩蔽率（70%），并且该方法预测每个可学习掩蔽令牌的目标属性。在随机掩蔽后，只有可见的体素令牌被馈送到编码器中，该编码器是一个稀疏变换器。使用两个单独的解码器分别解码点统计信息和表面属性。预测目标包括点云区域的点统计信息和表面属性。该模型通过监督这些预测目标来学习不均匀点云的点统计信息和表面属性。具体步骤包括：
1. 动态体素化
2. VFE层获取每个体素的特征/令牌
3. 高掩蔽率（70%）移除令牌
4. 随机掩蔽后，只有可见的体素令牌被馈送到编码器中
5. 使用两个单独的解码器分别解码点统计信息和表面属性
6. 监督预测目标来学习不均匀点云的点统计信息和表面属性

#### 8. 实验设置：
本文在Waymo Open Dataset和nuScenes Dataset上评估了所提出的GeoMAE方法。Waymo Open Dataset包含798个训练序列和202个验证序列，而nuScenes Dataset包含700个训练序列、150个验证序列和150个测试序列。对于3D检测，官方评估指标包括标准3D平均精度（mAP）和加权航向精度（mAPH）的mAP，以及nuScenes Dataset的平均精度（mAP）和nuScenes检测分数（NDS）。

#### 9. 实验结果与分析：
本文在Waymo Open Dataset和nuScenes Dataset上比较了GeoMAE与先前的自监督点云表示学习方法的性能。结果表明，GeoMAE在L1 AP/APH、L2 AP/APH和mAP方面均优于其他方法。此外，GeoMAE在不同下游任务上的泛化能力也得到了证明。作者进行了各种消融研究，以评估所提出方法的有效性。在nuScenes Dataset上，GeoMAE模型在目标检测、语义分割和多目标跟踪任务上均取得了显著的性能提升。


# Paper:859     规范场：自监督学习姿态规范化的神经场



#### 1. Title: 
Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields

#### 2. Authors: 
Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar

#### 3. Affiliation: 
RRC, IIIT-Hyderabad (Rohith Agaram)

#### 4. Keywords: 
Neural fields, canonicalization, self-supervised learning, neural radiance fields, equivariant neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了神经场的规范化问题，即如何将不同姿态的物体规范化为相同的姿态，以便更好地进行分类和比较。神经场是一种表示3D形状和外观的方法，但是在没有预先规范化的数据集的情况下，对于物体类别的建模仍然具有挑战性。

- (2):以前的方法要么过度拟合于单个实例，要么需要使用手动规范化的数据集，这限制了它们的应用范围。本文提出了一种自监督的方法，可以将物体的3D姿态规范化为神经场，特别是神经辐射场（NeRF）。本文的方法可以直接在连续和嘈杂的辐射场上进行学习，并且可以处理NeRF中的噪声。本文的方法可以在没有手动规范化的情况下进行学习，而且可以直接操作神经场，而不需要将其转换为点云进行规范化。

- (3):本文提出了一种称为Canonical Field Network（CaFi-Net）的方法，它是一种自监督的方法，可以将物体类别的3D姿态规范化为神经场。CaFi-Net直接从连续和嘈杂的辐射场中学习，使用Siamese网络架构来提取等变场特征，以进行类别级别的规范化。在推理期间，我们的方法采用任意3D姿态的预训练神经辐射场，并估计整个类别中具有一致3D姿态的规范场。我们的方法在新的数据集上进行了广泛的实验，该数据集包含13个常见的ShapeNet类别的1300个NeRF模型，结果表明我们的方法与基于3D点云的方法相当或超过了它们的性能。

- (4):本文的方法在神经场的规范化问题上取得了很好的效果，可以将不同姿态的物体规范化为相同的姿态，以便更好地进行分类和比较。本文的方法可以直接在连续和嘈杂的辐射场上进行学习，并且可以处理NeRF中的噪声。本文的方法可以在没有手动规范化的情况下进行学习，而且可以直接操作神经场，而不需要将其转换为点云进行规范化。本文的方法在13个常见的ShapeNet类别的1300个NeRF模型上进行了广泛的实验，结果表明我们的方法与基于3D点云的方法相当或超过了它们的性能。
#### 7. 方法详细介绍：
本文提出了一种自监督学习的方法，称为CaFi-Net，用于姿态规范化神经场的学习。该方法使用等变神经网络来学习3D形状的姿态不变表示，然后应用规范化来将形状对齐到规范参考框架中。CaFi-Net的架构包括卷积层、等变非线性和MLP来预测规范坐标。该方法使用规范化损失、Chamfer距离损失和Siamese形状损失的组合来训练网络。

#### 8. 实验设置：
本文使用了一个合成的NeRF数据集，包含来自ShapeNet数据集的13个类别的形状。对于每个类别，从ShapeNet中随机选择100个实例，随机旋转，并使用Blender在围绕形状的立方体中采样54个全向视图进行渲染。数据集包括杂乱的背景。每个NeRF模型使用1024个随机选择的光线进行训练，每次迭代使用64个点的粗略采样分辨率和128个点的细粒度采样分辨率沿着每个光线。将20%的模型用于测试。

#### 9. 实验结果与分析：
本文使用三个用于3D点云规范化的度量标准来评估CaFi-Net的性能：实例级一致性（IC）、类别级一致性（CC）和地面真实等变性一致性（GEC）。本文将CaFi-Net与三个基于3D点云的规范化方法进行比较：PCA、Canonical Capsules（CaCa）和ConDor。CaFi-Net在所有类别中表现良好，不同类别之间的方差较小。本文提供了一张表格，比较了CaFi-Net在13个类别的数据集上使用三个标准度量标准（IC、CC和GEC）与其他方法的规范化性能。本文还包括了消融实验，以证明方法设计选择的合理性。


# Paper:860     基于场景对象频谱基础的探索性层次视觉语言导航



#### 1. Title: 
Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding

#### 2. Authors: 
Minyoung Hwang, Jaeyeon Jeong, Minsoo Kim, Yoonseon Oh, Songhwai Oh

#### 3. Affiliation: 
第一作者：首尔国立大学电气与计算机工程学院和ASRI

#### 4. Keywords: 
Vision-and-Language Navigation, Hierarchical Navigation, Scene Object Spectrum, Spectral-domain SOS features

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Hwang_Meta-Explore_Exploratory_Hierarchical_Vision-and-Language_Navigation_Using_Scene_Object_Spectrum_CVPR_2022_paper.html  Github: https://github.com/rllab-snu/Meta-Explore

#### 6. Summary : 
- (1):本文研究的背景是视觉与语言导航（VLN）中的探索性导航问题，即如何在未知环境中理解自然语言指令。

- (2):传统的VLN算法的主要局限性是，如果一个动作出错，代理就无法遵循指令或探索不必要的区域，导致代理进入无法恢复的路径。本文提出了一种称为Meta-Explore的层次导航方法，部署了一种开发策略来纠正误导的最近动作。本文的方法通过选择未访问但可观察状态中的一个良好的本地目标，将代理移动到一个良好的本地目标，从而克服了这个问题。本文还提出了一种称为场景对象频谱（SOS）的新型视觉表示，它对检测到的对象进行类别化的2D傅里叶变换。结合开发策略和SOS特征，代理可以通过选择一个有前途的本地目标来纠正其路径。本文的方法在三个VLN基准测试中进行了评估：R2R、SOON和REVERIE。Meta-Explore优于其他基线，并显示出显着的泛化性能。

- (3):本文提出了一种层次导航方法Meta-Explore，部署了一种开发策略来纠正误导的最近动作。本文的方法通过选择未访问但可观察状态中的一个良好的本地目标，将代理移动到一个良好的本地目标，从而克服了这个问题。本文还提出了一种称为场景对象频谱（SOS）的新型视觉表示，它对检测到的对象进行类别化的2D傅里叶变换。结合开发策略和SOS特征，代理可以通过选择一个有前途的本地目标来纠正其路径。本文的方法在三个VLN基准测试中进行了评估：R2R、SOON和REVERIE。Meta-Explore优于其他基线，并显示出显着的泛化性能。

- (4):本文的方法在三个VLN基准测试中进行了评估：R2R、SOON和REVERIE。Meta-Explore优于其他基线，并显示出显着的泛化性能。在SOON基准测试中，使用所提出的谱域SOS特征的本地目标搜索显着提高了成功率17.1％和SPL20.6％，并且显示出比传统空间域视觉特征更好的语言可解释性。
#### 7. 方法详细介绍：
本文提出了一种名为Meta-Explore的层次化导航方法，用于解决视觉语言导航任务。该方法包括三个模块：模式选择器、探索策略和利用模块。模式选择器确定智能体是否应该探索或利用环境。探索策略估计下一步移动到候选节点的概率。利用模块使用场景对象频谱接地来查找目标并搜索本地目标。该方法还使用了一种名为场景对象频谱（SOS）的新型场景表示方法，该方法包含对象在场景中的频谱信息，提供了有意义的线索来选择最优的本地目标，并帮助智能体解决后悔的探索问题。该方法结合了利用策略和SOS特征，设计了一种导航得分，用于衡量给定语言指令与朝向本地目标的校正轨迹之间的对齐程度。智能体比较本地目标候选项，并从校正轨迹中选择具有最高导航得分的最优候选项。

#### 8. 实验设置：
本文在三个视觉语言导航基准测试集（Room-to-Room（R2R）、SOON和REVERIE）上评估了所提出的方法。其中，R2R评估了智能体的视觉接地自然导航性能。SOON和REVERIE是面向目标的视觉语言导航基准测试集，智能体应该定位目标位置并检测对象位置以找到目标对象。评估指标包括轨迹长度（TL）、成功率（SR）、成功率加权逆路径长度（SPL）和神谕成功率（OSR）。

#### 9. 实验结果和分析：
所提出的Meta-Explore方法在R2R、SOON和REVERIE测试集的成功率和SPL方面均优于其他基线方法。与分层基线相比，Meta-Explore的成功率和SPL至少提高了16.4％和8.9％。该方法还在REVERIE验证集中的成功率和SPL方面显示出改进。然而，在测试集中的改进低于R2R和SOON的结果。在SOON导航任务中，使用所提出的光谱域SOS特征的本地目标搜索优于其他基线方法，成功率提高了17.1％，SPL提高了20.6％。这表明对于面向目标的视觉语言导航任务，训练或验证集中的高性能可能是过拟合的结果。


# Paper:861     从视频中重建可动物体类别



#### 1. Title: 
Reconstructing Animatable Categories from Videos

#### 2. Authors: 
Gengshan Yang, Chaoyang Wang, N Dinesh Reddy, Deva Ramanan

#### 3. Affiliation: 
Carnegie Mellon University（卡内基梅隆大学）

#### 4. Keywords: 
3D reconstruction, animatable models, differentiable rendering, video analysis

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Yang_Reconstructing_Animatable_Categories_From_Videos_CVPR_2021_paper.html  Github: https://gengshan-y.github.io/rac-www/

#### 6. Summary : 
- (1):本文研究了从视频中重建可动物体类别的问题，旨在构建可动的3D模型，以便于在视频中进行动画渲染和编辑。

- (2):以往的方法需要3D扫描、繁琐的注册和绑定，难以扩展到更广泛的类别。最近，可微分渲染提供了一种从单目视频中获得高质量3D模型的途径，但这些模型仅限于刚性类别或单个实例。本文提出了一种名为RAC的方法，可以从单目视频中构建类别级别的3D模型，分离实例之间的变化和时间内的运动。该方法包括三个关键思想：（1）将类别级别的骨架专门化到实例上，（2）一种潜在空间正则化方法，既鼓励跨类别共享结构，又保留实例细节，（3）使用3D背景模型将物体与背景分离。本文在猫、狗和人类上构建了3D模型，并展示了跨实例的运动转移。

- (3):本文提出了一种新的方法，可以从单目视频中构建可动的3D模型，包括实例特定的形态、时间变化的关节运动和变形，以及视频特定的3D背景模型。该方法使用可微分渲染进行优化，通过三个关键思想解决了类别级别3D模型的构建问题。这些思想包括：（1）在视频中学习具有恒定骨长的骨架，以更好地分离实例之间的形态和关节运动；（2）使用一种新的代码交换技术，将未观察到的身体部位规范化为跨实例的一致性，同时保持对输入视图的忠实；（3）利用类别级别的背景模型，将物体与背景分离。本文的创新点在于，可以从单目视频中构建类别级别的3D模型，分离实例之间的变化和时间内的运动。

- (4):本文的方法在猫、狗和人类上构建了3D模型，并展示了跨实例的运动转移。实验结果表明，本文的方法在可动物体类别的3D重建方面具有很高的准确性和鲁棒性，可以有效地分离实例之间的变化和时间内的运动。
#### 7. 方法详细介绍：
本文提出了一种名为RAC（Reconstructing Animatable Categories）的方法，用于从视频中重建可动物体的类别级3D模型。该方法包括三个关键思想：（1）将类别级骨架专门化为实例，（2）一种潜在空间正则化方法，既鼓励类别间共享结构，同时保留实例细节，（3）使用3D背景模型将物体与背景分离。该方法使用可微分渲染进行优化。

具体步骤如下：
1. 从视频中提取对象分割、像素特征、光流和表面法线估计等预处理步骤。
2. 将每个实例的关节位置与类别级骨架进行配准，以获得每个实例的形状和姿态变化。
3. 通过学习从注册的3D扫描中提取的PCA基础，来表示身体尺寸和部位大小的变化。
4. 使用时间变化的变形场来表示每个实例内部的变化。
5. 使用3D背景模型来提高结果的质量。
6. 优化目标函数包含图像重建损失项和正则化项。

#### 8. 实验设置：
本文在三个类别（人类、猫和狗）上评估了所提出的方法。对于人类，数据集包括来自AMA、MonoPerfCap、DAVIS和BANMo的47个视频，共计6,382张图像。对于猫和狗，数据集包括来自互联网视频的76个猫视频和85个狗视频，以及来自BANMo的公共数据。评估指标包括Chamfer距离和F分数。

#### 9. 实验结果与分析：
本文将所提出的方法与BANMo、HuMoR、ICON和BARC等多个基线进行比较。结果表明，RAC在人类重建方面略逊于单个实例训练的BANMo，但在单个实例或多个实例训练的情况下，RAC均优于BANMo。对于猫和狗，RAC即使在不可见的情况下也能准确推断出合理的身体部位和关节。本文还进行了诊断实验，以测试形态代码正则化和形态代码本身的有效性。总体而言，所提出的方法在中级重建方面达到了最先进的重建质量，但仍然存在一些细节问题，例如人类的手和脚部分。


# Paper:862     自适应数据无关量化



#### 1. Title: 
Adaptive Data-Free Quantization

#### 2. Authors: 
Biao Qian, Yang Wang, Richang Hong, Meng Wang

#### 3. Affiliation: 
合肥工业大学计算机与信息学院

#### 4. Keywords: 
Data-free quantization, Generative models, Zero-sum game, Sample adaptability, Low-bit precision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Adaptive_Data-Free_Quantization_CVPR_2021_paper.html  Github: https://github.com/hfutqian/AdaDFQ

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在资源受限设备上的应用，网络量化是提高DNN效率的一种方法，但量化误差会导致性能下降，因此需要数据无关量化（DFQ）方法来恢复性能。
- (2):过去的DFQ方法通过生成器（G）从全精度网络（P）中学习生成假样本，以恢复量化网络（Q）的性能，但这些方法忽略了生成样本对Q的适应性，导致泛化误差过大。本文提出了一种自适应数据无关量化（AdaDFQ）方法，通过博弈论的视角，定义了不同适应性的样本，优化两个边界之间的间隔，以自适应地调节生成样本对Q的适应性，解决过拟合和欠拟合问题。
- (3):本文提出的AdaDFQ方法通过优化间隔来生成适应性良好的样本，以自适应地调节生成样本对Q的适应性，解决过拟合和欠拟合问题。本文的创新点在于将DFQ重新定义为两个玩家之间的零和博弈，以适应性样本的适应性为目标，提出了一种自适应的生成方法。理论和实验分析验证了AdaDFQ方法的优越性。
- (4):本文在ImageNet数据集上验证了AdaDFQ方法的性能，结果表明，AdaDFQ方法在低位精度（3位）和高位精度（5位）下均优于现有的DFQ方法，证明了AdaDFQ方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种自适应无数据量化（AdaDFQ）方法，将数据量化（DFQ）重新定义为生成器（G）和量化器（Q）之间的动态零和博弈过程。AdaDFQ通过关注量化的依赖性、预量化与量化的优势以及预量化与量化之间的差异来衡量样本的适应性。该方法通过平衡不一致样本和一致样本来生成具有自适应适应性的样本，以确保生成的样本在这两个边界内具有自适应适应性。该方法旨在解决过度拟合和欠拟合问题，并生成具有理想适应性的样本。方法的理论分析和实证研究验证了AdaDFQ相对于现有方法的优越性。

#### 8. 实验设置：
本文在三个图像分类数据集（CIFAR-10、CIFAR-100和ImageNet）上验证了AdaDFQ的有效性。使用ResNet-20作为CIFAR的预训练全精度网络（P），使用ResNet-18、ResNet-50和MobileNetV2作为ImageNet的预训练全精度网络（P）。使用对称线性量化方法将权重和激活量化为n位精度。生成器G使用Adam优化器进行训练，而Q使用带Nesterov优化器的SGD进行优化。超参数是经验性设置的，所有实验都使用PyTorch实现。批量大小设置为16，G和Q交替训练400个时期。实验在NVIDIA GeForce GTX 1080 Ti GPU和Intel（R）Core（TM）i7-6950X CPU @ 3.00GHz上运行。

#### 9. 实验结果和分析：
实验结果表明，AdaDFQ相对于现有方法获得了显著且一致的准确率提高，在CIFAR-10、CIFAR-100和ImageNet上分别获得了最多10.46％、12.59％和36.93％的准确率提高。 AdaDFQ在不同位宽下为Q提供了实质性的增益，验证了自适应适应性对于不同Q的重要性。对生成的样本进行的视觉分析进一步证实了AdaDFQ的直觉，即生成具有自适应适应性的样本。


# Paper:863     基于凸博弈的领域泛化方法



#### 1. Title: 
Improving Generalization with Domain Convex Game

#### 2. Authors: 
Fangrui Lv, Jian Liang, Shuang Li, Jinming Zhang, Di Liu

#### 3. Affiliation: 
北京理工大学

#### 4. Keywords: 
Domain generalization, domain augmentation, convex game, supermodularity, sample filter

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Lv_Improving_Generalization_With_Domain_Convex_Game_CVPR_2021_paper.html
Github: https://github.com/BIT-DA/DCG

#### 6. Summary : 
- (1):本文研究领域泛化问题，即如何让深度神经网络在多个源域上学习，以提高其在不同目标域上的泛化能力。
- (2):过去的方法主要是通过增加源域的多样性来提高模型的泛化能力，但是这种方法的有效性缺乏理论证明。本文提出了一种新的方法，将领域泛化问题看作是一个凸博弈问题，通过设计基于超模性的正则化项来鼓励每个域对模型的泛化能力做出贡献，并构建样本过滤器来消除低质量样本的影响。这种方法能够更好地利用多样化的信息，提高模型的泛化能力。
- (3):本文提出的方法是将领域泛化问题看作是一个凸博弈问题，通过设计基于超模性的正则化项来鼓励每个域对模型的泛化能力做出贡献，并构建样本过滤器来消除低质量样本的影响。这种方法能够更好地利用多样化的信息，提高模型的泛化能力。本文的创新点在于将凸博弈引入到领域泛化问题中，提出了一种新的方法来解决这个问题。
- (4):本文在多个真实数据集上进行了实验，结果表明，所提出的方法能够显著提高模型的泛化能力，并且能够更好地利用多样化的信息。这些结果支持了本文的目标和方法。
#### 7. 方法详细介绍：
本文提出了一种基于Domain Convex Game (DCG)框架的领域泛化方法。该方法将领域泛化视为领域之间的凸博弈，并设计了一种新颖的正则化项，利用超模性来鼓励每个领域对模型泛化做出贡献。此外，该方法还构建了一个样本过滤器，以排除可能对泛化产生负面影响的低质量样本。具体而言，该方法将原始源数据随机分为元训练和元测试领域，并根据凸博弈的定义从前者生成四个联盟。然后，分别在这四个联盟上进行元学习，并根据超模性构建它们的正则化损失，以鼓励每个领域对模型泛化做出贡献。同时，通过样本过滤器排除低质量样本，并在保留的样本上计算监督损失。最终，该方法的优化目标是最小化监督损失和正则化项。

#### 8. 实验设置：
本文在三个常用的领域泛化基准数据集PACS、Office-Home和mini-DomainNet上进行实验。采用leave-one-domain-out交叉验证方法，使用在ImageNet上预训练的ResNet-18/50作为骨干网络。网络使用小批量随机梯度下降（mini-batch SGD）进行训练，批量大小为16，动量为0.9，权重衰减为5e-4。初始学习率为0.001，总共训练100个epoch，其中在80个epoch时将学习率降低为原来的0.1。超参数ω和k在验证集上进行选择，分别设置为0.1和5，遵循标准协议。

#### 9. 实验结果与分析：
本文提出的DCG方法在PACS基准数据集上取得了最好的性能，使用ResNet-18和ResNet-50分别达到了81.04%和84.06%的平均准确率。在Office-Home和mini-DomainNet数据集上，该方法也取得了竞争性的结果。实验结果表明，DCG方法在领域泛化方面具有很好的效果，同时也证明了样本过滤和领域多样性对于提高模型泛化性能的重要性。


# Paper:864     ISBNet：一种具有实例感知采样和盒感知动态卷积的3D点云实例分割网络



#### 1. Title: 
ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution

#### 2. Authors: 
Tuan Duc Ngo, Binh-Son Hua, Khoi Nguyen

#### 3. Affiliation: 
VinAI Research, Hanoi, Vietnam (越南VinAI研究所)

#### 4. Keywords: 
3D instance segmentation, point cloud, dynamic convolution, farthest point sampling, bounding box

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ngo_ISBNet_A_3D_Point_Cloud_Instance_Segmentation_Network_With_Instance-Aware_CVPR_2021_paper.html  Github: https://github.com/VinAIResearch/ISBNet

#### 6. Summary : 
- (1):本文研究的是3D点云实例分割问题，即给定一个3D点云场景，需要为每个点分配一个语义类别和一个唯一的实例标签。这是一个重要的3D感知任务，在自动驾驶、增强现实和机器人导航等领域有广泛的应用。与2D图像实例分割相比，3D实例分割更加困难，因为3D点云具有更高的外观和空间范围变化，以及不均匀的点云分布。

- (2):现有的3D实例分割方法主要采用自下而上的设计，即手动微调算法将点分组成簇，然后进行细化网络。然而，这些方法依赖于簇的质量，当（1）相邻的具有相同语义类别的对象被紧密地打包在一起，或者（2）具有松散连接区域的大型对象时，这些方法会产生易受攻击的结果。为了解决这些限制，本文提出了ISBNet，一种新颖的无簇方法，将实例表示为核，并通过动态卷积解码实例掩码。为了有效地生成高召回和区分性核，我们提出了一种简单的策略，称为实例感知最远点采样，以对候选进行采样，并利用受PointNet++启发的局部聚合层来编码候选特征。此外，我们展示了在动态卷积中预测和利用3D轴对齐边界框进一步提高性能。

- (3):本文提出了ISBNet，一种基于实例感知最远点采样和盒感知动态卷积的无簇框架，用于3D点云实例分割。首先，我们重新审视最远点采样和聚类方法，并发现这些算法生成的实例召回率相当低。因此，我们提出了我们的实例感知最远点采样（IA-FPS），旨在在3D场景中采样具有高实例召回率的查询候选项。然后，我们引入了我们的点聚合器，将IA-FPS与局部聚合层相结合，将实例的语义特征、形状和大小编码为实例特征。此外，对象的3D边界框是现有的监督，但尚未在3D实例分割任务中探索。因此，我们向我们的模型添加了一个辅助分支，以共同预测每个实例的轴对齐边界框和二进制掩码。我们利用它作为动态卷积中的额外几何线索，从而进一步提高实例分割任务的性能。

- (4):本
#### 7. 方法详细介绍：
本文提出了一种基于动态卷积的3D点云实例分割方法，称为ISBNet。该方法包括四个主要组件：3D骨干网络、点级预测器、基于采样的实例级编码器和基于框的动态卷积。3D骨干网络从输入点云中提取每个点的特征。点级预测器将这些特征转换为点级语义预测、轴对齐边界框预测和用于基于框的动态卷积的掩码特征。基于采样的实例级编码器处理点级特征以生成实例内核、实例类标签和边界框参数。基于框的动态卷积获取实例内核和具有补充框预测的掩码特征，以生成每个实例的最终二进制掩码。该方法使用边界框预测作为辅助任务来规范实例分割训练。网络使用点级损失和实例级损失进行训练。

#### 8. 实验设置：
本文在三个数据集上评估了ISBNet的性能：ScanNetV2、S3DIS和STPLS3D。实验在单个NVIDIA Tesla V100 GPU上进行，具有16GB内存。本文使用与先前工作相同的数据预处理和数据增强技术。本文还使用与先前工作相同的评估指标，包括平均精度（AP）和交并比（IoU）。

#### 9. 实验结果和分析：
ISBNet在所有三个数据集上均取得了最先进的性能，分别比最强方法高出+2.7/3.4/3.0。ISBNet还表现出高效性，ScanNetV2上的运行时间为237ms/场景。本文提供了详细的实验结果和分析，包括消融研究和实例分割结果的可视化。源代码和训练模型可在https://github.com/VinAIResearch/ISBNet上获得。


# Paper:865     平衡校准专家的长尾识别



#### 1. Title: 
Balanced Product of Calibrated Experts for Long-Tailed Recognition

#### 2. Authors: 
Emanuel Sanchez Aimar, Arvi Jonnarth, Michael Felsberg, Marco Kuhlmann

#### 3. Affiliation: 
第一作者：瑞典林雪平大学电气工程系

#### 4. Keywords: 
long-tailed recognition, ensemble learning, calibration, mixup, logit adjustment

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Aimar_Balanced_Product_of_Calibrated_Experts_for_Long-Tailed_Recognition_CVPR_2021_paper.html  Github: https://github.com/emasa/BalPoE-CalibratedLT

#### 6. Summary : 
- (1):本文研究了长尾识别问题，即标签分布不平衡的情况下的识别问题。这种情况下，由于尾部类别的样本数量较少，使得模型难以泛化到尾部类别，因此需要解决分布偏移的问题。

- (2):过去的方法包括数据重采样、代价敏感学习和边缘修改等，但这些方法存在一些局限性。最近的一些工作提出了学习多个不同的专家来解决这个问题，但是这些方法的理论基础仍然需要探索。本文提出了一种新的方法，即Balanced Product of Experts (BalPoE)，它将多个专家的预测结果进行组合，以实现对尾部类别的更好识别。同时，本文还提出了一种新的方法来保证专家的校准性，即使用mixup方法来提高模型的校准性。

- (3):本文的方法是在多个专家的基础上进行的，每个专家针对不同的标签分布进行预测。本文通过扩展logit调整的理论背景，将其应用于多个专家的情况下，提出了BalPoE方法。同时，本文还提出了一种新的方法来保证专家的校准性，即使用mixup方法来提高模型的校准性。本文的方法在三个长尾数据集上取得了新的最优结果。

- (4):本文的方法在三个长尾数据集上取得了新的最优结果，证明了其在长尾识别问题上的有效性。同时，本文的方法还能够保证模型的校准性，从而提高模型的可靠性。
#### 7. 方法详细介绍：
本文提出了一种平衡校准专家的平衡产品（BalPoE）框架，用于长尾识别。该方法首先使用广义逻辑调整损失（gLA）对分布偏移进行参数化，以适应所需的测试分布。gLA是调整后的得分器的softmax交叉熵，通过使用成对类间距来调整原始得分器获得。BalPoE框架将多个逻辑调整专家组合起来，以适应所需的测试分布。集合被定义为对数空间中专家得分器的平均值，并且它获得了所有专家的平均偏差。所提出的框架对于最小化平衡误差是Fisher一致的，并且可以适应任何已知的目标分布。校准假设是获得无偏逻辑调整模型的必要条件。

#### 8. 实验设置：
本文在三个长尾数据集上进行了实验：CIFAR-100-LT，ImageNet-LT和iNaturalist 2018。对于CIFAR-100-LT，使用ResNet-32作为骨干网络，对于ImageNet-LT和iNaturalist，使用ResNet-50和ResNeXt-50。使用SGD进行训练，对于CIFAR-100-LT使用多步学习率计划，对于ImageNet-LT和iNaturalist使用余弦退火调度器。使用mixup进行校准，对于CIFAR-100-LT，ImageNet-LT和iNaturalist，alpha值分别设置为0.4、0.3和0.2。

#### 9. 实验结果和分析：
本文提出的BalPoE方法在多个长尾数据集上均取得了最新的最佳结果。在CIFAR-100-LT上，BalPoE在平衡准确性方面优于先前的最新方法3.5％。在ImageNet-LT上，BalPoE的平衡准确性为47.5％，比先前的最新方法高3.4％。在iNaturalist-2018上，BalPoE的平衡准确性为54.5％，比先前的最新方法高2.5％。该方法还比以前的方法具有更好的校准性，如较低的ECE和最大校准误差（MCE）值。


# Paper:866     自然语言描述下的联合视觉定位和跟踪



#### 1. Title: 
Joint Visual Grounding and Tracking with Natural Language Specification

#### 2. Authors: 
Li Zhou, Zikun Zhou, Kaige Mao, and Zhenyu He

#### 3. Affiliation: 
第一作者：哈尔滨工业大学深圳研究生院

#### 4. Keywords: 
Visual grounding, tracking, natural language specification, end-to-end training

#### 5. Paper: None  Github: None

#### 6. Summary : 
- (1):本文研究的背景是自然语言描述下的视觉跟踪问题。

- (2):现有算法通过视觉定位和跟踪两个步骤来解决这个问题，并分别使用分离的定位模型和跟踪模型来实现这两个步骤。这种分离的框架忽略了视觉定位和跟踪之间的联系，即自然语言描述为两个步骤的目标定位提供了全局语义线索。此外，分离的框架很难进行端到端的训练。本文提出了一种联合视觉定位和跟踪框架，将定位和跟踪重新定义为一个统一的任务：基于给定的视觉语言描述来定位目标。

- (3):本文提出的研究方法是一种联合视觉定位和跟踪框架，将视觉定位和跟踪重新定义为一个统一的任务，并使用一个共享的神经网络来实现。这种方法的创新点在于将视觉定位和跟踪联系起来，从而提高了定位和跟踪的准确性，并且可以进行端到端的训练。

- (4):本文的方法在自然语言描述下的视觉跟踪任务上进行了实验，并与现有的方法进行了比较。实验结果表明，本文提出的方法在准确性和效率方面都有所提高，证明了该方法的有效性和可行性。
#### 7. 方法详细介绍：
本文提出了一种联合视觉定位和跟踪框架，将定位和跟踪作为一个统一的任务进行重构。该框架由三个模块组成：语言模块、定位模块和跟踪模块。语言模块对自然语言描述进行编码，定位模块在图像中定位目标对象，跟踪模块在视频序列中跟踪目标对象。三个模块使用多任务损失函数进行端到端的联合训练。

具体步骤如下：
1. 首先，使用自然语言描述对目标进行描述。
2. 然后，使用语言模块对自然语言描述进行编码。
3. 接着，使用定位模块在图像中定位目标对象。
4. 最后，使用跟踪模块在视频序列中跟踪目标对象。

#### 8. 实验设置：
当前文本中没有实验设置的信息。

#### 9. 实验结果和分析：
当前文本中没有实验结果和分析的信息。


# Paper:867     佩戴式摄像头视频任务翻译



#### 1. Title: 
Egocentric Video Task Translation

#### 2. Authors: 
Zihui Xue, Yale Song, Kristen Grauman, Lorenzo Torresani

#### 3. Affiliation: 
第一作者：The University of Texas at Austin

#### 4. Keywords: 
Egocentric video, task translation, multi-task learning, transfer learning, video understanding

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Xue_Egocentric_Video_Task_Translation_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是针对佩戴式摄像头的第一人称视角视频理解任务，这些任务通常是相互关联的，需要综合考虑。
- (2):过去的方法通常是将不同的视频理解任务视为孤立的，使用不同的数据集进行训练，这种方法存在着数据集之间的差异和任务之间的竞争等问题。本文提出了一种新的方法，即EgoTask Translation (EgoT2)，采用“翻转设计”，即使用不同的任务特定的骨干网络和一个任务翻译器，以捕捉不同任务之间的协同作用，从而提高任务的性能。
- (3):本文提出的EgoT2框架采用了两种不同的设计，即任务特定的EgoT2 (EgoT2-s)和任务通用的EgoT2 (EgoT2-g)。与传统的多任务学习和迁移学习不同，EgoT2采用了“翻转设计”，即使用不同的任务特定的骨干网络和一个任务翻译器，以捕捉不同任务之间的协同作用，从而提高任务的性能。本文在Ego4D数据集上进行了实验，结果表明EgoT2在多个任务上均取得了优异的性能，并且在四个Ego4D 2022基准挑战中均取得了最佳成绩。
- (4):本文的方法在Ego4D数据集上取得了优异的性能，证明了其在多任务视频理解中的有效性。
#### 7. 方法详细介绍：
本文提出了EgoT2方法，它采用了一种通用且灵活的设计，用于处理视角自我视频中的任务转换。EgoT2-s和EgoT2-g是两种不同的模型，用于任务特定的转换和任务通用的转换。EgoT2-s通过训练特定于每个任务的模型，然后训练任务翻译器，以将由任务特定模型产生的特征作为输入，并输出主要任务的预测。EgoT2-g通过将任务特定解码器替换为“通用”解码器来扩展EgoT2-s，该解码器在任务的条件下输出预测。EgoT2-g将问题视为语言建模任务，并训练任务翻译器以在输入视频及其前面的标记的条件下预测后续标记。EgoT2的“翻转设计”包括单独的任务特定骨干和在所有任务之间共享的任务翻译器，它捕捉甚至异构任务之间的协同作用并减轻任务竞争。

#### 8. 实验设置：
本文在Ego4D数据集上评估了所提出的方法，该数据集是世界上最大的视角自我数据集，包含3670小时的视频，涵盖了数百种场景。数据集提供了五个基准测试：情节记忆、手和物体、音频-视觉日记、社交互动和预测。本文选择了七个任务，涵盖了四个基准测试，代表了视角自我感知中的各种任务。这七个候选任务在性质上是异构的，因为它们是在持续时间不同的视频上定义的，采用不同的视频模型作为骨干，并处理单模态或多模态输入。本文在表1中提供了所选任务的详细描述。

#### 9. 实验结果和分析：
本文的EgoT2-s模型通过自适应地利用任务特定特征并有效地减轻负面转移，在所有6个案例中都表现出一致的改进，对于主要任务有更少的可训练参数，优于所有基线。当辅助任务有益于主要任务时，EgoT2-s提供了一种灵活的解决方案，并且在所有任务中实现了任务翻译。EgoT2-g提供了一种灵活的解决方案，可以包含各种预训练模型的异构混合，并为所有任务执行任务翻译，并实现了所有任务的并行或更好的性能。EgoT2-s在Ego4D-CVPR'22和Ego4D-ECCV'22挑战的所有4个挑战中都实现了顶级性能。本文提出的EgoT2方法在七个不同的视角自我视频任务上进行了评估，结果显示了有价值的任务关系，并验证了所提出的设计。EgoT2-g学习在感兴趣的任务条件下执行任务翻译，并为不利于感兴趣任务的任务特征分配小权重，从而减轻任务竞争。EgoT2-g的注意力权重对于不同的任务提示是不同的，表明它为不同的任务标记分配不同的权重。注意力权重在所有验证数据上进行时间池化和平均，从全局视角揭示任务关系。在附录A.4中提供了人-人交互任务的结果。


# Paper:868     SeSDF: 自适应有符号距离场的隐式3D穿衣人重建



#### 1. Title: 
SeSDF: Self-evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction

#### 2. Authors: 
Yukang Cao, Kai Han, Kwan-Yee K. Wong

#### 3. Affiliation: 
香港大学

#### 4. Keywords: 
3D reconstruction, implicit function, signed distance field, clothed human, multi-view

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cao_SeSDF_Self-Evolved_Signed_Distance_Field_for_Implicit_3D_Clothed_Human_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了从单张图像或多视角图像中重建穿衣人的问题，提出了一种基于SMPL-X模型的自适应有符号距离场（SeSDF）的方法，可以灵活地重建3D穿衣人模型。
 
- (2):现有的方法难以重建穿衣人的详细几何细节，并且多视角重建需要校准设置。本文提出的方法可以在非校准设置下，通过利用SMPL-X模型，将任意数量的输入图像重建成穿衣人模型。本文提出的SeSDF模块可以学习变形SMPL-X模型的有符号距离场（SDF），以便更好地编码反映实际穿衣人的详细几何细节。此外，本文提出了一种简单的自校准多视角图像的方法，通过拟合共享的SMPL-X模型并基于每个输入图像的优化刚体运动将共享模型投影到不同的图像中。本文还引入了一种有效的遮挡感知特征融合策略，以考虑重建人模型的最有用的特征。 

- (3):本文提出了一种基于SMPL-X模型的SeSDF模块，可以自适应地学习变形SMPL-X模型的SDF，以便更好地编码反映实际穿衣人的详细几何细节。本文还提出了一种自校准多视角图像的方法，通过拟合共享的SMPL-X模型并基于每个输入图像的优化刚体运动将共享模型投影到不同的图像中。此外，本文引入了一种有效的遮挡感知特征融合策略，以考虑重建人模型的最有用的特征。 

- (4):本文在公共基准测试中进行了全面评估，证明了SeSDF在质量和数量上都优于现有技术。本文的方法可以在非校准设置下，通过利用SMPL-X模型，将任意数量的输入图像重建成穿衣人模型，并且可以重建出更好的几何细节。
#### 7. 方法详细介绍：
本文提出了一种名为SeSDF的框架，用于从单张或多张未标定图像中重建3D服装人体模型。该框架利用SMPL-X模型作为形状先验，并结合了隐式和显式表示的优点。框架的核心是一个自进化的有符号距离场（SeSDF）模块，该模块学习使用输入图像来变形从拟合的SMPL-X模型中导出的有符号距离场（SDF）。 SeSDF模块生成的SDF可以反映比SMPL-X更准确的几何细节，因为SMPL-X本质上偏离了实际的服装人体模型。通过对SDF进行编码，可以进行更好的几何细节的3D重建。此外，对于未标定的多视角重建，我们提出了一种简单的自标定方法，通过利用SMPL-X模型，以及一种考虑到不同视角下空间点可见性的遮挡感知特征融合策略，通过射线追踪来聚合有用的特征以实现鲁棒的重建。

#### 8. 实验设置：
本文在THUman2.0数据集上对提出的SeSDF方法进行了定性和定量评估。该数据集包括526个通过密集DSLR设备捕获的高保真度网格。数据集分为包含465个主体的训练集和包含61个主体的测试集。每个主体都有相应的纹理贴图和拟合的SMPL-X参数。为了进行实验，作者将每个人体主体渲染成360张图像，每张图像以1度为间隔顺序分离，具有不同的反照率。对于真实世界的图像，作者使用Rembg进行背景分割，并使用Kaolin计算SMPL-X模型的SDF。作者使用PIXIE估计分割的人体主体的SMPL-X模型。作者使用堆叠的hourglass图像编码器和双线性插值来提取像素对齐的图像特征F2D（·）∈R256。他们使用PointNet和3D-UNet以及三线性插值来提取空间对齐的3D特征F3D（·）∈R128。作者分别对单视角和多视角实验进行了12个时期的SeSDF框架训练，学习率从1e-4开始，并在每4个时期更新一次因子0.1。

#### 9. 实验结果与分析：
本文提出了一种新的单视角和多视角服装人体重建方法。该方法在定性和定量结果上均明显优于先前的最先进方法。与多视角SOTA方法的比较表明，所提出的方法始终保留更多的几何细节，例如服装皱纹，头发和面部。然而，SeSDF模块在处理极松散的衣服（例如连衣裙）时仍然存在困难，因为这些衣服与参数先验有显著偏差。


# Paper:869     1% VS 100%：面向密集预测的参数高效低秩适配器



#### 1. Title: 
1% VS 100%: Parameter-Efﬁcient Low Rank Adapter for Dense Predictions

#### 2. Authors: 
Dongshuo Yin, Yiran Yang, Zhechao Wang, Hongfeng Yu, Kaiwen Wei, Xian Sun

#### 3. Affiliation: 
中国科学院空间信息研究院网络信息系统技术重点实验室

#### 4. Keywords: 
Adapter tuning, Low-rank synthesis, Dense predictions, Fine-tuning, Computer vision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是深度学习模型参数过多，需要大量计算资源和存储空间，且fine-tuning整个模型效率低下。

- (2):过去的方法是fine-tuning整个模型，但需要大量计算资源和存储空间。本文提出了一种新的方法，即Adapter tuning，通过添加少量的可训练结构来适应新任务，从而节省计算资源和存储空间。本文提出的方法是基于低秩合成的LoRand结构，可以大大减少可训练参数的数量。本文的方法是有动机的，因为最近NLP领域的Adapter tuning和Prompt tuning方法已经证明了这种方法的有效性。

- (3):本文提出了一种基于低秩合成的LoRand结构，用于fine-tuning大规模视觉模型。LoRand结构可以生成小型的adapter结构，同时保持原始backbone参数不变，从而实现高参数共享。本文的创新点在于提出了一种新的fine-tuning方法，可以在保持性能的同时大大减少可训练参数的数量。

- (4):本文在目标检测、语义分割和实例分割任务上进行了广泛的实验，证明了LoRand-Tuning可以在只训练1%到3%的预训练backbone参数的情况下，实现与标准fine-tuning相当的性能，并在低资源PASCAL VOC数据集上优于fine-tuning。本文的方法可以大大节省存储资源，并在大多数密集预测任务上实现竞争性能。
#### 7. 方法详细介绍：
本文提出了一种名为LoRand的方法，用于在低资源情况下fine-tuning大规模视觉模型，以更好地平衡任务性能和可训练参数数量的权衡。LoRand通过低秩合成生成微小的适配器结构，同时保持原始骨干参数不变，从而实现高参数共享。LoRand通过低秩合成稀疏地参数化适配器中的矩阵。LoRand中全连接层（FC）的投影矩阵是多个低秩矩阵的乘积，将FC参数减少了80%以上。

#### 8. 实验设置：
本文在COCO 2017、ADE20K和PASCAL VOC数据集上进行了广泛的实验，用于验证LoRand的能力。使用ImageNet-22K训练的先进Swin Transformer作为预训练模型。比较了LoRand和三种常见的训练方法，包括FULL、FIXED和ADAPTER。LoRand有三个变体，分别是LoRand、LoRand+和LoRand++。COCO使用Cascade MASK R-CNN作为检测器，ADE20K使用UperNet作为框架。

#### 9. 实验结果和分析：
本文在PASCAL VOC、ADE20K和COCO数据集上展示了LoRand方法的实验结果，并将其与其他基线进行了比较。结果表明，LoRand可以有效地解决低资源情况下fine-tuning的困境，并有效地平衡可训练骨干参数数量和下游任务性能。LoRand还扩展了传统参数高效适配器结构在密集预测中的潜力。与其他fine-tuned骨干的比较表明，LoRand可以在不到2M参数的情况下胜过大多数现有的fine-tuned骨干。LoRand在COCO实例分割、ADE20K语义分割和PASCAL VOC目标检测中表现出与fine-tuning相当的性能，仅使用1%到3%的可训练骨干参数。LoRand有效地避免了fine-tuning范式的缺点，并在低资源情况下提供更好的性能。Swing变体的实验系统地证明了LoRand可以在低资源情况下胜过FULL和传统适配器结构，并在大型基准测试中表现非常接近FULL。消融研究表明，更大的LoRand在参数效率和性能方面的收益更少。


# Paper:870     基于形状约束的循环流用于6D物体姿态估计



#### 1. Title: 
Shape-Constraint Recurrent Flow for 6D Object Pose Estimation

#### 2. Authors: 
Yang Hai, Rui Song, Jiaojiao Li, Yinlin Hu

#### 3. Affiliation: 
Yang Hai, Rui Song, Jiaojiao Li: State Key Laboratory of ISN, Xidian University, China
Yinlin Hu: MagicLeap

#### 4. Keywords: 
6D object pose estimation, optical flow, shape constraint, recurrent matching, end-to-end system

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是6D物体姿态估计问题，即相机与目标物体之间的3D旋转和3D平移的估计。这是3D计算机视觉中的一个基本问题，也是许多应用程序的关键组成部分，包括机器人操作和增强现实等。

- (2):最近的6D物体姿态方法使用2D光流来优化其结果。然而，一般的光流方法通常在匹配过程中不考虑目标的3D形状信息，使其在6D物体姿态估计中不够有效。本文提出了一种基于形状约束的循环匹配框架，用于6D物体姿态估计。该方法通过计算基于初始姿态和当前估计姿态之间的2D重投影位移的姿态诱导流来嵌入目标的3D形状信息。然后，使用这个姿态诱导流来构建后续匹配迭代的相关图，从而显著减少匹配空间并更容易学习。此外，本文提出使用网络基于当前估计的流来学习物体姿态，这有助于计算下一次迭代的姿态诱导流，并产生一个端到端的物体姿态系统。最后，本文以循环方式同时优化光流和物体姿态。 

- (3):本文提出了一种基于形状约束的循环匹配框架，用于6D物体姿态估计。该方法通过计算基于初始姿态和当前估计姿态之间的2D重投影位移的姿态诱导流来嵌入目标的3D形状信息。然后，使用这个姿态诱导流来构建后续匹配迭代的相关图，从而显著减少匹配空间并更容易学习。此外，本文提出使用网络基于当前估计的流来学习物体姿态，这有助于计算下一次迭代的姿态诱导流，并产生一个端到端的物体姿态系统。最后，本文以循环方式同时优化光流和物体姿态，使得整个系统更加高效和准确。

- (4):本文在三个具有挑战性的6D物体姿态数据集上进行了评估，并表明其在准确性和效率方面显著优于现有技术。
#### 7. 方法详细介绍：
本文提出了一种基于形状约束的循环流方法，用于6D物体姿态估计。该方法使用渲染和比较的方法来学习输入图像和渲染图像之间的差异。首先，该方法构建一个包含两个图像之间所有特征向量对之间相关性的4D相关性体积。然后，该方法使用姿态诱导流来索引相关性图，该流隐含地嵌入了目标的形状信息。该方法基于当前中间流学习残差姿态，以迭代方式更新估计的姿态。该方法使用GRU进行循环建模，并使用两个网络分别预测残差旋转和平移。该方法使用简单的指数加权策略迭代地监督光流和物体姿态。

#### 8. 实验设置：
本文在三个具有挑战性的数据集上评估了所提出的方法，包括LINEMOD、LINEMOD-Occluded和YCB-V。LINEMOD数据集包含13个序列，每个序列都包含一个带有准确的真实姿态注释的单个物体。LINEMOD-Occluded数据集有8个物体，是LM物体的子集。它的测试集是LM中的一个序列，其中包含场景中8个物体的所有注释。LM和LM-O没有标准的实验设置。本文使用AdamW优化器，批量大小为16，并基于One-Cycle使用自适应学习率调度程序。本文通常训练模型100k步。在训练期间，本文随机生成一个伪初始姿态，围绕输入图像的真实姿态，并根据伪初始姿态即时渲染参考图像。

#### 9. 实验结果和分析：
本文提出了一种基于形状约束的循环匹配框架，用于6D物体姿态估计。该方法在ADD-0.1d指标上显著优于现有方法，并且收敛速度更快。作者分析了标准光流网络的弱点，并引入了一种新的匹配框架，其中仅包含目标3D形状的所有2D重投影，用于构建相关性图，从而显著减少了匹配空间。所提出的方法对遮挡更加鲁棒，并产生更准确的姿态结果。在三个具有挑战性的6D物体姿态数据集上进行了广泛的评估，证明了所提出方法的优势。


# Paper:871     狩猎稀疏性：基于密度引导对比学习的半监督语义分割



#### 1. Title: 
Hunting Sparsity: Density-Guided Contrastive Learning for Semi-Supervised Semantic Segmentation

#### 2. Authors: 
Xiaoyang Wang, Bingfeng Zhang, Limin Yu, Jimin Xiao

#### 3. Affiliation: 
第一作者：Xiaoyang Wang，就职于西交利物浦大学

#### 4. Keywords: 
Semi-supervised learning, semantic segmentation, contrastive learning, density estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Hunting_Sparsity_Density-Guided_Contrastive_Learning_for_Semi-Supervised_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/Gavinwxy/DGCL

#### 6. Summary : 
- (1):本文研究半监督语义分割任务，提出了一种基于密度引导对比学习的方法，旨在从特征空间的几何结构中挖掘有效的监督信息，以提高模型的泛化能力。

- (2):过去的半监督语义分割方法主要集中在伪标签和一致性正则化上，但这些方法忽略了特征空间的结构信息。本文提出了一种基于密度引导的对比学习策略，通过挖掘特征空间中的稀疏区域来增加类内紧凑性，从而提高模型的性能。

- (3):本文提出了一种密度引导的对比学习策略，通过密度估计来定位特征空间中的稀疏区域，然后将这些区域的特征作为锚点，将密集区域的特征作为正样本，通过对比学习来增加类内紧凑性。本文提出了一种多尺度密度估计模块，以获得多个最近邻图的密度，从而提高密度估计的鲁棒性。本文的方法在PASCAL VOC和Cityscapes数据集上进行了实验，取得了最先进的性能。

- (4):本文的方法在PASCAL VOC和Cityscapes数据集上进行了实验，取得了最先进的性能。本文的方法通过挖掘特征空间中的稀疏区域来增加类内紧凑性，从而提高模型的性能。
#### 7. 方法详细介绍：
本文提出了一种密度引导对比学习框架，用于半监督语义分割。该框架包括学生编码器、教师编码器、记忆库和密度引导对比损失函数。学生编码器使用标记和未标记数据进行训练，而教师编码器用于为未标记数据生成伪标签。记忆库存储高密度特征以进行对比学习。密度引导对比损失函数鼓励低密度特征与高密度特征相似。该框架还包括一致性正则化项和稀疏性正则化项以进一步提高性能。具体步骤如下：
1. 使用ResNet-101作为特征编码器和分割头。
2. 使用SGD优化器进行训练，包括有权重衰减的学习率。
3. 对于PASCAL VOC 2012数据集，使用高质量训练集和扩展集进行评估。
4. 对于Cityscapes数据集，使用随机采样的半监督数据集进行评估。
5. 采用数据增强技术，包括随机调整大小、随机裁剪和随机水平翻转。
6. 使用多尺度估计基于邻居的图形。
7. 设置初始熵百分位阈值为20%。
8. 设置损失系数为1。
9. 每个小批量中每个类别的锚点数设置为256。
10. 温度系数设置为0.5。

#### 8. 实验设置：
本文在PASCAL VOC 2012和Cityscapes数据集上进行了实验。PASCAL VOC 2012数据集分为经典和混合两种设置进行评估。Cityscapes数据集包含2975个精细注释的图像，用于城市场景理解。半监督数据集通过从整个训练集中随机采样1/16、1/8、1/4和1/2作为标记集，其余作为未标记集来生成。评估指标为平均交并比（mIoU）。

#### 9. 实验结果和分析：
本文提出的密度引导对比学习（DGCL）策略在PASCAL VOC 2012和Cityscapes数据集上的表现优于现有的最先进方法，特别是在低数据情况下。聚类性能比较表明，DGCL生成更可分离的聚类，并显着改善了基线，以生成更紧凑的聚类和更高的聚类间可分离性。对不同采样策略的剖析研究也验证了密度引导采样策略的有效性。在1/8的经典设置下，PASCAL VOC 2012数据集的定性结果表明，采用DGCL训练策略的模型可以更准确地捕捉语义和对象结构。


# Paper:872     一条新路：使用合成指令和模仿学习扩展视觉语言导航



#### 1. Title: 
A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning

#### 2. Authors: 
Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, Zarana Parekh

#### 3. Affiliation: 
第一作者：Aishwarya Kamath，纽约大学

#### 4. Keywords: 
Vision-and-Language Navigation, Synthetic Instructions, Imitation Learning, Reinforcement Learning, Data Augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_CVPR_2021_paper.html  Github: https://github.com/google-research-datasets/RxR/tree/main/marky-mT5

#### 6. Summary : 
- (1):本文研究的是视觉语言导航（VLN）领域，旨在训练机器人按照自然语言指令进行导航。然而，由于人类指令数据的稀缺性和训练环境的有限性，现有的方法仍然存在复杂的语言基础和空间语言理解问题。

- (2):过去的方法主要是基于强化学习（RL）的，但是由于数据量不足和训练环境的有限性，这些方法仍然存在一定的问题。本文提出了一种新的方法，即使用合成指令和模仿学习来扩大数据集，从而提高机器人的导航能力。与以往的方法相比，本文的方法具有更高的数据量和更广泛的训练环境，因此可以更好地解决语言基础和空间语言理解问题。

- (3):本文的方法主要是基于合成指令和模仿学习的。具体来说，本文使用了Marky和GAN等工具来生成合成指令和图像，从而扩大了数据集。然后，本文使用了一个简单的Transformer代理来进行模仿学习。实验结果表明，本文的方法在RxR数据集上取得了很好的效果，超过了以往的方法。

- (4):本文的方法在RxR数据集上取得了很好的效果，超过了以往的方法。在验证集上，本文的方法将NDTW从71.1提高到了79.1，在测试集上将NDTW从64.6提高到了66.8。这表明本文的方法可以更好地解决语言基础和空间语言理解问题。
#### 7. 方法详细介绍：
本文提出了一种新的方法，使用合成指令和模仿学习来扩展视觉语言导航（VLN）。该方法使用Marky-Gibson数据集，包含491个Gibson环境中的3.2M个模型生成的导航指令。作者还训练了一个模型来为Gibson环境生成导航图，并使用它来采样轨迹并用Marky指令进行注释。此外，他们使用SE3DS在训练期间从新视点合成图像观察结果。该方法使用transformer编码器作为代理的体系结构，预测下一步动作，同时结合所有四个输入模态：指令文本、观察和动作的历史记录、当前观察和动作候选项。代理在两个阶段进行训练：预训练和微调。预训练阶段使用大规模的指令-轨迹对数据集，微调阶段使用行为克隆和DAGGER训练。 

#### 8. 实验设置：
本文使用Room-to-Room（R2R）数据集进行评估，该数据集包含90个建筑和120,000个指令。作者开发了一个自动化流程，将导航图扩展到500多个新环境，并用3.2M个指令进行注释。代理使用标准的成功率和导航误差指标在R2R和R4R数据集上进行评估。

#### 9. 实验结果和分析：
使用合成指令的MARVAL代理在RxR基准测试中取得了最先进的结果。在Val-Unseen数据集上，成功率为64.8％，在RxR和R2R上相同，而通常RxR性能较低，因为轨迹更长且更多样化。在仅英语的R2R数据集上，MARVAL取得了强大的性能，但不是最先进的，这归因于R2R和RxR之间的领域差异。在看到的环境中，性能改进超过最先进的结果（+8％），而在未看到的测试环境中，改进较小（+2％）。在测试环境中使用Marky合成指令进行自我训练可以将性能提高2％，达到68.6 NDTW。


# Paper:873     从头到尾传递知识：长尾分布下的不确定性校准



#### 1. Title: 
Transfer Knowledge from Head to Tail: Uncertainty Calibration under Long-tailed Distribution

#### 2. Authors: 
Jiahao Chen, Bing Su

#### 3. Affiliation: 
中国人民大学高灵智能学院

#### 4. Keywords: 
uncertainty calibration, long-tailed distribution, importance weight, knowledge transfer, Gaussian distribution

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Chen_Transfer_Knowledge_From_Head_to_Tail_Uncertainty_Calibration_Under_Long-Tailed_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了长尾分布下的模型不确定性校准问题，提出了一种基于知识转移的校准方法。
- (2):现有的校准方法通常假设训练数据分布是平衡的，而忽略了现实世界中数据往往遵循长尾分布的事实。本文提出的方法通过估计尾部类别样本的重要性权重来实现长尾校准，而现有的校准方法如温度缩放等无法很好地推广到这个问题。本文的方法通过将每个类别的分布建模为高斯分布，并将头部类别的源统计信息视为先验，以此来校准尾部类别的目标分布。本文的方法通过知识转移来适应性地从头部类别中获取目标尾部类别的概率密度。本文的方法在CIFAR-10-LT、CIFAR-100-LT、MNIST-LT和ImageNet-LT数据集上进行了广泛的实验，证明了其有效性。
- (3):本文提出了一种基于知识转移的校准方法，通过估计尾部类别样本的重要性权重来实现长尾校准。本文的方法通过将每个类别的分布建模为高斯分布，并将头部类别的源统计信息视为先验，以此来校准尾部类别的目标分布。本文的方法通过知识转移来适应性地从头部类别中获取目标尾部类别的概率密度。本文的方法在长尾分布下的模型不确定性校准问题上具有创新性和贡献性。
- (4):本文的方法在CIFAR-10-LT、CIFAR-100-LT、MNIST-LT和ImageNet-LT数据集上进行了广泛的实验，证明了其有效性。本文的方法在长尾分布下的模型不确定性校准问题上取得了较好的性能，支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种基于知识迁移的长尾分布校准方法。该方法通过估计尾部类别样本的重要性权重来实现长尾校准。将每个类别的分布建模为高斯分布，并将头部类别的源统计信息视为先验，以校准尾部类别的目标分布。通过从头部类别中自适应地转移知识，实现从头部类别到尾部类别的目标概率密度的转移。通过目标概率密度与源概率密度的比值来估计重要性权重。使用Wasserstein距离来衡量每个尾部类别与所有头部类别之间的相似性，并通过组合其自身分布和所有头部类别的转移信息来估计每个尾部类别的目标概率密度。最后，使用重要性权重对模型进行校准。

#### 8. 实验设置：
本文在四个基准数据集上进行了实验，分别是CIFAR-10-LT、MNIST-LT、CIFAR-100-LT和ImageNet-LT。实验中使用ResNet-32和LeNet-5作为CIFAR-10-LT和MNIST-LT的分类模型，使用ResNet、DenseNet和VGG作为CIFAR-100-LT的分类模型，使用ResNet-50作为ImageNet-LT的分类模型。实验评估指标包括ECE、SCE和ACE。超参数α的值为0.998，除非另有说明。

#### 9. 实验结果与分析：
本文提出的方法在CIFAR-10测试集上的可靠性图表方面与基线方法和TS方法相比具有竞争力。注意力可视化结果表明，该方法利用不同分布之间的距离，并通过softmax函数获得注意力分数。超参数alpha的消融研究表明，alpha值越小，从头部类别中利用的信息越多。w*（x）的分布受到该方法的重大影响，w的整体分布聚集在w=1的值周围。在IF=100，alpha=0.995的情况下，CIFAR-10-LT的ECE（％）为6.97，低于均匀和OneHot方法的ECE。


# Paper:874     智能眼镜的实用立体深度系统



#### 1. Title: 
A Practical Stereo Depth System for Smart Glasses

#### 2. Authors: 
Jialiang Wang, Daniel Scharstein, Akash Bapat, Kevin Blackburn-Matzen, Matthew Yu, Jonathan Lehman, Suhib Alsisan, Yanghan Wang, Sam Tsai, Jan-Michael Frahm, Zijian He, Peter Vajda, Michael F. Cohen, Matt Uyttendaele

#### 3. Affiliation: 
Jialiang Wang: Meta Platforms Inc.（Meta平台公司）

#### 4. Keywords: 
Stereo depth, smart glasses, neural networks, computational photography, online rectification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Wang_A_Practical_Stereo_Depth_System_for_Smart_Glasses_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉中的立体视觉问题，其在增强现实/虚拟现实、计算摄影、机器人和自动驾驶等领域有广泛的应用。

- (2):过去的方法大多数都是基于深度学习的，但是在实际应用中，由于硬件和计算资源的限制，这些方法往往无法满足实时性和鲁棒性的要求。本文提出了一种端到端的立体视觉系统，可以在智能眼镜上进行实时的深度估计和渲染，同时具有较高的精度和鲁棒性。

- (3):本文提出了一种实用的立体深度系统，包括预处理、在线立体校正和深度估计等步骤，并且在校正失败时可以回退到单目深度估计。深度估计的输出结果可以用于生成3D计算摄影效果。所有这些步骤都在移动设备的严格计算预算下执行，因此需要具有通用性，不能依赖于特定的硬件或ML加速器，如智能手机GPU。本文的主要贡献包括：描述了一个端到端的立体系统，介绍了一种快速而鲁棒的在线校准算法，提出了一种协同设计立体网络和单目深度网络的新策略，展示了我们的训练模型在中等复杂度的计算预算下的高速和泛化能力。

- (4):本文的方法在中等复杂度的计算预算下，可以在智能眼镜上实现实时的深度估计和渲染，同时具有较高的精度和鲁棒性。在Middlebury和野外图像数据集上，本文的方法取得了良好的结果。
#### 7. 方法详细介绍：
本文提出了一种适用于智能眼镜的端到端立体深度感知系统。该系统包括预处理、在线立体矫正和立体深度估计，并在立体矫正不可靠时回退到单目深度估计。深度感知系统的输出然后用于新颖视角图像生成管道，利用智能眼镜捕捉的视角图像创建3D计算摄影效果。该系统旨在适用于移动电话的严格计算预算，并且需要通用，不能依赖于特定的硬件或ML加速器，例如智能手机GPU。本文描述了系统的设计，包括在线矫正算法以及立体网络和单目深度网络的共同设计，使两个网络的输出格式相似。具体步骤包括：预处理、在线立体矫正、立体深度估计、单目深度估计、新颖视角图像生成。

#### 8. 实验设置：
本文没有实验设置部分。

#### 9. 实验结果与分析：
本文在三个数据集上评估了所提出的深度系统的性能，包括Middlebury 2014、Sceneflow和内部数据集。在Samsung Galaxy S8 CPU和Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz的计算机服务器上进行了基准测试。在线矫正管道需要300-400ms，立体网络需要约965ms。本文还将所提出的方法与几种最先进的立体方法进行了比较。此外，本文还进行了一项调查，以评估使用立体和单目深度的智能眼镜生成的视频的质量。平均分数为使用立体为3.44，使用单目深度为2.96。本文还提供了10个调查场景，涵盖成功和失败的情况，并观察到深度图质量有时与生成的新颖视角视频的质量不直接相关。本文强调，仅使用标准指标比较方法不足以评估实践中立体方法的性能。


# Paper:875     用于混合声音定位的音频-视觉分组网络



#### 1. Title: 
Audio-Visual Grouping Network for Sound Localization from Mixtures

#### 2. Authors: 
Shentong Mo, Yapeng Tian*

#### 3. Affiliation: 
Shentong Mo: 卡内基梅隆大学 (Carnegie Mellon University)
Yapeng Tian: 德克萨斯大学达拉斯分校 (University of Texas at Dallas)

#### 4. Keywords: 
Sound localization, audio-visual joint learning, multi-source localization, disentangled representations, category-aware grouping

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Mo_Audio-Visual_Grouping_Network_for_Sound_Localization_From_Mixtures_CVPR_2021_paper.html  Github: https://github.com/stoneMo/AVGN

#### 6. Summary : 
- (1):本文研究的背景是声音源定位，即预测视频中声音源的位置。声音源定位是一项典型而具有挑战性的任务，需要预测视频中声音源的位置。 
- (2):过去的单源方法主要使用音频-视觉关联作为线索，在每个图像中定位声音对象。然而，由于原始空间中存在多个声源的混合属性，除了最近一项使用图像和分离声音作为节点的对比随机游走的工作外，很少有多源方法同时定位多个源。本文提出了一种新的音频-视觉分组网络，即AVGN，可以直接从输入音频混合物和图像中为每个源学习类别语义特征，以同时定位多个源。与现有的多源方法相比，我们的新框架可以定位灵活数量的源，并为每个源学习类别感知的音频-视觉表示。 
- (3):本文提出了一种新的音频-视觉分组网络，即AVGN，以指导声音定位。在训练过程中，我们旨在学习音频-视觉类别令牌，以从声音混合物和图像中聚合类别感知源特征，其中学习了单个源的分离高级语义。与现有的单源和多源方法不同，我们使用可学习的音频-视觉类别令牌作为理想的指导，为每个源提供类别感知的音频-视觉表示，以定位相应的视觉区域。 
- (4):本文在MUSIC、VGGSound-Instruments和VGG-Sound Sources基准测试上进行了广泛的实验。结果表明，与以前的单源和多源基线相比，我们的AVGN在单源和多源情况下均可实现最先进的声音对象定位性能。
#### 1. 方法详细介绍：
本文提出了一种名为Audio-Visual Grouping Network (AVGN)的方法，旨在从混合音频中的图像中定位单个声源。AVGN由两个模块组成：Audio-Visual Class Tokens和Audio-Visual Grouping。前者引入了可学习的音频-视觉类别标记，以帮助从音频-视觉表示中分组语义感知信息，而后者通过softmax操作计算音频-视觉特征和音频-视觉类别标记之间的全局音频相似度向量和空间视觉相似度矩阵，生成类别感知的音频-视觉嵌入。然后通过计算分配的所有全局音频和空间视觉特征的加权和来生成类别感知的表示。该方法使用弱监督形式的音频-视觉源类别来指导音频-视觉分组过程。模型的整体目标通过音频-视觉分组损失和定位损失以端到端的方式进行优化。

#### 2. 实验设置：
本文在三个数据集上进行了评估：MUSIC-Solo、VGGSound-Instruments和VGGSound-Single。MUSIC-Solo数据集包含11个乐器类别的448个未修剪的YouTube音乐视频的独奏和二重奏。VGGSound-Instruments数据集由来自37个音乐器类别的32k个10秒长的视频剪辑组成。VGGSound-Single数据集包括221个类别，如自然、动物、车辆、人、乐器等。对于评估，使用像素平均精度（AP）、交并比（IoU）和曲线下面积（AUC）的平均精度来进行单源定位。对于多源定位，使用类别感知平均精度（CAP）、排列不变平均精度（PIAP）、类别感知IoU（CIoU）和曲线下面积（AUC）。IoU和CIoU的阈值取决于数据集。模型使用Adam优化器进行100个epoch的训练，学习率为1e-4，批量大小为128。

#### 3. 实验结果和分析：
本文提出的AVGN在单源和多源定位方面均取得了最先进的性能。对于单源定位，该方法在所有指标上优于以前的自监督和弱监督基线。对于多源定位，该方法在所有三个数据集上都实现了最佳性能，并在VGGSound-Instruments数据集上实现了最佳的CIoU性能。该方法还在VGGSound-Duet基准测试中优于Mix-and-Localize。结果验证了该方法在从混合和图像中学习分离的个体源语义以进行多源定位方面的有效性。


# Paper:876     学习可转向函数以实现高效的图像重采样



#### 1. Title: 
Learning Steerable Function for Efficient Image Resampling

#### 2. Authors: 
Jiacheng Li, Chang Chen, Wei Huang, Zhiqiang Lang, Fenglong Song, Youliang Yan, Zhiwei Xiong

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Image resampling, deep neural networks, look-up tables, steerable functions, continuous resampling

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Learning_Steerable_Function_for_Efficient_Image_Resampling_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是图像重采样技术，旨在提高重采样的效率和连续性。
- (2):传统的插值方法虽然简单高效，但是在处理不同结构的图像时效果不佳。而基于深度神经网络的方法虽然能够学习到更好的结构先验，但是效率较低，无法替代插值方法。本文提出了一种新的方法，即学习可转向函数（LeRF），它既利用了深度神经网络学习到的结构先验，又利用了插值方法的局部连续性假设。具体而言，LeRF为输入图像像素分配空间可变的可转向重采样函数，并学习预测这些函数的方向所需的超参数。为了实现高效的推理，本文采用查找表（LUT）来加速学习的神经网络的推理。此外，本文还设计了方向集成策略和边缘敏感索引模式，以更好地捕捉图像的局部结构。 
- (3):本文提出了一种新的图像重采样方法，即学习可转向函数（LeRF），它将可转向重采样函数分配给输入图像像素，并学习预测这些函数的方向所需的超参数。为了实现高效的推理，本文采用查找表（LUT）来加速学习的神经网络的推理。此外，本文还设计了方向集成策略和边缘敏感索引模式，以更好地捕捉图像的局部结构。 
- (4):本文在各种图像重采样任务中进行了广泛的实验，包括任意尺度的上采样、单应性变换和一般变形。实验结果表明，与插值方法相比，本文的方法在效率和性能方面都有很大的提升，例如在Manga109数据集上进行×2上采样，PSNR值比双三次插值高出3dB。因此，本文提出的方法在图像重采样领域具有很大的应用前景。
#### 7. 方法详细介绍：
本文提出了一种新的学习重采样函数的方法，称为LeRF。该方法使用可操纵的重采样函数，通过神经网络学习超参数，以适应数据中的局部结构。该方法包括相对偏移量的获取、预测重采样权重和像素聚合等步骤。神经网络被训练用于预测每个像素的超参数，从而定义该像素位置的重采样函数。最终，输出图像是通过使用这些局部适应的重采样函数对输入图像进行插值得到的。该方法采用查找表（LUT）来加速学习的神经网络的推理，并设计了方向集合策略和边缘敏感的索引模式来更好地捕捉图像中的局部结构。

#### 8. 实验设置：
本文使用DIV2K数据集对LeRF进行训练，并将其应用于任意变换的重采样。作者使用Set5、Set14、BSDS100、Urban100和Manga109等5个基准数据集进行评估。他们选择了代表性的对称或非对称上采样比例进行评估，并采用双三次插值作为退化模型来获取LR图像。为了评估性能，他们报告了PSNR和SSIM用于保真度，以及LPIPS用于感知质量。为了评估性能-效率权衡，他们还包括插值方法、RAISR*和SR-LUT*作为任意尺度上采样的额外基线。

#### 9. 实验结果与分析：
本文将LeRF与几种插值方法、RAISR*、SR-LUT*、Meta-SR和LIIF进行了比较。他们报告了任意尺度上采样的PSNR定量比较，并提供了一个表格，显示了在通过4×上采样生成1280×720 HD图像的性能（Set14）和效率比较的运行时间、MAC、存储需求。他们还提供了任意尺度上采样的定性比较示例图像。结果表明，LeRF在性能和效率方面均优于其他方法。


# Paper:877     学习多模态类别特定标记以进行弱监督密集目标定位



#### 1. Title: 
Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization

#### 2. Authors: 
Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu

#### 3. Affiliation: 
第一作者：The University of Western Australia（西澳大利亚大学）

#### 4. Keywords: 
Weakly supervised dense object localization, Class Activation Mapping, Contrastive Language-Image Pretraining, Vision-Language models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2021_paper.html  Github: https://github.com/xulianuwa/MMCST

#### 6. Summary : 
- (1):本文研究的是弱监督密集目标定位（WSDOL）任务，该任务依赖于Class Activation Mapping（CAM）方法，但CAM方法无法很好地处理类内变化，导致定位结果不准确。

- (2):过去的方法主要是基于CNN和ViT模型，但它们都依赖于CAM方法，无法很好地处理类内变化。本文提出了一种新的方法，通过引入Contrastive Language-Image Pretraining（CLIP）方法，显式地构建多模态类别表示，以更好地指导密集目标定位。

- (3):本文提出了一个统一的Transformer框架，用于学习两种模态的类别特定标记，即类别特定的视觉标记和文本标记。前者从目标视觉数据中捕获语义，而后者利用CLIP的类相关语言先验知识，提供互补信息以更好地感知类内多样性。此外，本文还提出了一种增强多模态类别特定标记的方法，即使用包括视觉和图像语言上下文的样本特定上下文来更好地关联局部特征，从而进一步促进密集定位。

- (4):本文在两个多标签数据集（PASCAL VOC和MS COCO）和一个单标签数据集（OpenImages）上进行了广泛的实验，结果表明所提出的方法在WSDOL任务上具有优越性能，并且在PASCAL VOC和MS COCO上实现了最先进的弱监督语义分割（WSSS）结果。
#### 7. 方法详细介绍：
本文提出了一种基于Transformer的多模态类别特定标记学习方法，用于弱监督密集目标定位。该方法包括三个主要组件：多模态标记Transformer、对比学习模块和类别特定密集定位模块。其中，多模态标记Transformer使用ViT-base作为骨干网络，CLIP模型作为预训练的视觉语言模型，学习来自图像和文本模态的类别特定标记。对比学习模块通过鼓励学习到的标记具有区分性来提高性能。类别特定密集定位模块通过结合文本到补丁和类别到补丁相关性图来生成多模态类别特定的密集定位图。最终的密集目标定位图通过精细化处理和上采样得到。 

#### 8. 实验设置：
本文在PASCAL VOC 2012、MS COCO 2014和OpenImages三个数据集上进行了实验。对于多标签数据集，使用平均交并比（mIoU）评估训练集上的多标签密集定位图和验证集/测试集上的弱监督语义分割（WSSS）结果。对于单标签数据集，使用峰值交并比（pIoU）和像素平均精度（PxAP）评估测试集上的单标签密集定位图。本文的Transformer模型在PASCAL VOC和MS COCO上训练60个epoch，在OpenImages上训练10个epoch，使用Adam优化器，初始学习率为5e-4，批大小为32。

#### 9. 实验结果与分析：
本文提出的方法在PASCAL VOC 2012和MS COCO 2014的训练集上分别取得了66.3%和40.9%的mIoU，优于现有方法。在单标签密集定位方面，本文的方法在测试集上取得了57.6%的pIoU和73.3%的PxAP，也优于现有方法。在弱监督语义分割方面，本文的方法在PASCAL VOC 2012的验证集和测试集上分别取得了72.2%和72.2%的mIoU，在MS COCO 2014的验证集上取得了45.9%的mIoU，也优于现有方法。本文的实验结果表明，该方法在弱监督密集目标定位和语义分割任务上具有优异的性能。


# Paper:878     神经调制Hebbian学习用于全测试时间自适应



#### 1. Title: 
Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation

#### 2. Authors: 
Yushun Tang, Ce Zhang, Heng Xu, Shuoshuo Chen, Jie Cheng, Luziwei Leng, Qinghai Guo, Zhihai He

#### 3. Affiliation: 
第一作者：南方科技大学电子与电气工程系，中国深圳

#### 4. Keywords: 
Fully test-time adaptation, unsupervised learning, Hebbian learning, neuro-modulation, domain adaptation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Neuro-Modulated_Hebbian_Learning_for_Fully_Test-Time_Adaptation_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度神经网络在跨领域任务中性能下降的问题，提出了一种全测试时间自适应方法，旨在通过对输入样本进行顺序分析来调整网络模型，解决深度神经网络的跨领域性能下降问题。

- (2):现有的全测试时间自适应方法主要集中在构建各种损失函数来调节推理过程并基于误差反向传播来调整模型。本文提出了一种基于神经元调节的Hebbian学习方法，将其与反馈神经调制器相结合，提出了神经调制Hebbian学习（NHL）方法，用于全测试时间自适应。与现有方法相比，NHL方法不需要访问源域样本，只需要访问测试样本流，能够动态地在测试过程中调整源模型。

- (3):本文提出的NHL方法将无监督前馈Hebbian学习与学习的神经调制器相结合，以捕获外部响应的反馈，控制哪种类型的特征被巩固和进一步处理以最小化预测误差。在推理过程中，源模型通过NHL算法适应于每个测试样本的小批量。本文的创新点在于将Hebbian学习与神经调制相结合，提出了一种新的全测试时间自适应方法。

- (4):本文在基准数据集上进行了实验，证明了NHL方法可以显著提高网络模型的自适应性能，并优于现有的最先进方法。
#### 7. 方法详细介绍：
本文提出了一种名为神经调制Hebbian学习（NHL）的全测试时间自适应方法。该方法结合了无监督前馈软Hebbian学习和学习的神经调制器，以捕获来自外部响应的反馈。该框架允许在测试过程中有效地适应源模型。NHL算法包括两个主要步骤：（1）无监督前馈Hebbian学习早期层表示，（2）基于反馈的神经调制。该方法可以有效地适应源模型，达到了全测试时间自适应的最新性能水平。

#### 8. 实验设置：
本文在多个基准数据集上评估了所提出方法的测试时间自适应性能，包括CIFAR-10/100C、ImageNet-C和SVHN→MNIST/MNIST-M/USPS。ResNet-26、Wide-ResNet-28-10和Wide-ResNet-40-2被用作CIFAR-10C数据集的骨干网络。ResNet-18被用作ImageNet-C数据集的骨干网络。根据RobustBench协议，所有骨干网络都使用TENT或DUA的官方实现的预训练模型权重。

#### 9. 实验结果和分析：
所提出的方法在多个基准数据集上均优于仅在源数据上训练且在测试过程中没有进行微调的基线模型，以及其他全测试时间自适应方法，包括TTT、NORM、TENT和DUA。在CIFAR-10C的每个污染程度的最高严重程度（Level 5）下，与其他方法相比，所提出的方法显著降低了测试误差。该方法在全测试时间自适应方面取得了最新的性能水平。


# Paper:879     SMOC-Net: 利用相机姿态进行自监督单目物体位姿估计



#### 1. Title: 
SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation

#### 2. Authors: 
Tao Tan, Qiulei Dong

#### 3. Affiliation: 
中国科学院自动化研究所

#### 4. Keywords: 
Self-supervised learning, monocular object pose estimation, camera pose, knowledge distillation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究单目物体位姿估计的自监督学习方法，旨在解决真实图像与合成图像之间的领域差异问题，提高模型在真实图像上的表现。

- (2):过去的自监督学习方法通常使用合成图像进行训练，但由于真实图像与合成图像之间的领域差异，这些方法的性能受到限制。最近的一些方法使用真实图像和合成图像进行训练，并引入可微分渲染器来提供真实图像与渲染图像之间的差异约束。然而，这些方法的训练成本很高，且仍然存在真实图像与渲染图像之间的差异问题。本文提出了一种新的自监督学习方法，利用未标注真实图像的相机姿态来缩小真实图像与合成图像之间的领域差异，并通过相机姿态引导的优化器来提高位姿估计的准确性。

- (3):本文提出了一种基于知识蒸馏的SMOC-Net网络，包括教师模型和学生模型。教师模型包含一个骨干位姿估计模块和一个相机姿态引导的优化器，用于提供初始位姿估计和优化初始位姿。学生模型与教师模型的骨干位姿估计模块相同，通过相对姿态约束从教师模型中学习位姿估计知识。SMOC-Net通过相机姿态引导的优化器和相对姿态约束，不仅能够缩小真实图像与合成图像之间的领域差异，还能够降低训练成本。

- (4):本文在两个公共数据集上进行了实验，结果表明，SMOC-Net在只使用合成图像和未标注真实图像进行训练时，优于几种最先进的方法，并且在公共数据集LineMOD上优于三种最先进的全监督方法。
#### 7. 方法详细介绍：
本文提出了一种自监督单目物体姿态估计方法SMOC-Net，该方法利用相机姿态信息提高姿态估计的准确性。该方法由三个主要组件组成：2D物体检测器、骨干姿态估计模块和自监督细化模块。细化模块包括两个阶段：旋转细化和平移细化。SMOC-Net的总损失函数包含三个损失项：旋转损失、平移损失和点匹配损失。该方法使用ADD(-S)指标在LineMOD和Occluded-LineMOD数据集上进行评估。具体步骤包括：
1. 使用Synthetic PBR数据集训练2D物体检测器Yolov4和骨干姿态估计模块GDR-Net。
2. 在训练之前使用COLMAP方法计算相机姿态。
3. 使用ADD(-S)指标评估方法的性能。

#### 8. 实验设置：
本文在三个数据集上进行实验：Synthetic PBR数据集、LineMOD和Occluded-LineMOD。使用Synthetic PBR数据集训练2D物体检测器Yolov4和骨干姿态估计模块GDR-Net。在训练之前使用COLMAP方法计算相机姿态。使用ADD(-S)指标评估方法的性能。

#### 9. 实验结果和分析：
本文的实验结果表明，SMOC-Net方法在LineMOD和Occluded-LineMOD数据集上的性能优于现有的全监督和自监督方法。在LineMOD数据集上进行的消融实验表明，旋转损失、平移损失和相机姿态引导的细化模块都对姿态估计的准确性有所提高。与Self6D++方法相比，SMOC-Net方法具有更短的训练时间。实验结果表明，SMOC-Net方法在处理强遮挡时也具有很好的性能。


# Paper:880     使用残差学习表达式提示的视觉Transformer模型



#### 1. Title: 
Learning Expressive Prompting With Residuals for Vision Transformers

#### 2. Authors: 
Rajshekhar Das, Yonatan Dukler, Avinash Ravichandran, Ashwin Swaminathan

#### 3. Affiliation: 
第一作者：Carnegie Mellon University
其他作者：AWS AI Labs

#### 4. Keywords: 
Prompt learning, Vision Transformers, Expressive Prompts with Residuals, image classification, few-shot semantic segmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Das_Learning_Expressive_Prompting_With_Residuals_for_Vision_Transformers_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是大规模视觉模型的适应性问题，传统的微调方法不适用于许多场景，如少样本学习等。
- (2):过去的方法包括全模型微调、部分微调和适配器等，但它们不适用于Transformer架构。本文提出了一种新的提示学习方法，EXPRES，它使用浅提示和深度残差提示来构建适应性模型，同时避免了微调的缺点。本文的方法在VTAB基准测试中取得了最先进的效果，而且比现有的视觉提示基线更有效率。
- (3):本文提出的EXPRES方法是一种提示学习方法，它使用浅提示和深度残差提示来构建适应性模型。浅提示通过平均池化在最后一层构建下游表示，而深度残差提示则通过在Transformer编码器的各种计算中添加可学习的残差提示来调节下游表示。本文的方法在保留先前知识的同时，非常适合低内存和计算开销的实际适应性问题。
- (4):本文的方法在图像分类和少样本语义分割等任务上取得了最先进的效果，并且比现有的视觉提示基线更有效率。本文的方法比VPT需要更少的提示，同时取得了更好的性能，使其更适合于有限数据设置。
#### 7. 方法详细介绍：
本文提出了一种新的提示技术，称为具有残差的表达式提示（EXPRES），用于有效地适应视觉变换器。该方法通过可学习的“输出”令牌（浅提示）构建下游表示，并引入残差可学习令牌，将其添加到各种计算的输出中，以更好地引导冻结变换器处理的下游表示。该方法应用于图像分类和少样本语义分割，并显示在VTAB基准测试的3/3类别上实现了最先进的提示调整。本文还通过一系列消融实验系统地证明了该方法的架构设计，并分析了该方法相对于微调等权重空间适应技术的计算优势。

具体步骤如下：
1. 引入输入级别提示P0，是M个参数化的d维向量，与ViT的输入令牌序列Z0连接在一起。
2. 将输入级别提示与类别和补丁令牌一起通过编码器传播，以便在每个层中，每个令牌通过自注意力层与每个其他令牌交互。
3. 为了增强提示容量，引入层级残差提示∆l，将其添加到中间层l的各种计算中传播的提示中。
4. 使用标准的交叉熵损失训练该方法，以表示和相应的图像分类标签为目标，以及密集的交叉熵损失用于语义分割。

#### 8. 实验设置：
本文在多个下游任务上评估了所提出的方法，包括细粒度识别和语义分割。实验在VTAB基准测试上进行，将所提出的方法与最先进的提示方法和基于完全微调的适应方法进行了比较。本文还表明，所提出的方法非常适合需要在低内存和计算开销下保留信息的实际适应。

#### 9. 实验结果和分析：
所提出的方法EXPRES在VTAB-1k上的表现优于常用的适应技术，包括面向头部、面向骨干和基于提示的方法。EXPRES始终比FT-all（一种广泛采用的技术）表现更好，差距显著。此外，EXPRES甚至在12个数据集上都比其他强大的提示技术（如VPT-deep）表现更好，差距约为+1%（自然）、+2%（专业）和+0.02%（结构化）。在少样本语义分割任务中，EXPRES始终优于基线和其他提示技术，如VPT-deep。这些结果表明，使用在图像分类上预训练的模型可以实现高度竞争力的结果。


# Paper:881     通过多模态掩蔽视频生成统一文本引导下的视频补全



#### 1. Title: 
Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation

#### 2. Authors: 
Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, Sean Bell

#### 3. Affiliation: 
第一作者：UC Santa Barbara

#### 4. Keywords: 
Video completion, text-guided, multimodal, masked video generation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了文本引导下的视频补全任务，即从部分帧和文本指令中生成完整视频的任务。这个任务比单纯的视频预测更具有挑战性，因为它需要考虑到视频的时间连续性和文本指令的语义信息。

- (2):过去的方法主要集中在视频预测上，但是这些方法往往无法满足人类的期望，因为预测结果可能有多种可能性。本文提出了一种新的任务，即文本引导下的视频补全任务，并提出了一种新的方法，即多模态掩蔽视频生成（MMVG），来解决这个任务。该方法可以从任意时间点开始补全视频，并且可以处理视频预测、倒带和填充等不同的任务。该方法的创新点在于引入了文本指令，并且提出了一种新的掩蔽策略，可以更好地利用视频的时间连续性。

- (3):本文提出的方法是多模态掩蔽视频生成（MMVG），它将视频帧离散化为视觉令牌，并使用掩蔽策略来处理不同的视频部分。该方法可以从任意时间点开始补全视频，并且可以处理视频预测、倒带和填充等不同的任务。该方法的创新点在于引入了文本指令，并且提出了一种新的掩蔽策略，可以更好地利用视频的时间连续性。

- (4):本文在不同的视频场景下进行了实验，包括自我中心、动画和游戏。实验结果表明，本文提出的方法可以有效地生成高质量的视频，并且可以从文本指令中控制视频的生成。该方法的性能优于其他方法，并且可以处理不同的视频补全任务。
#### 7. 方法详细介绍：
本文提出了一种名为Temporal-aware VQGAN (T-VQ)的方法，它将时间关系注入潜在表示中，以促进平滑的视频建模。它使用对比度时间推理方法从潜在代码中学习时间顺序。该方法还包括Multimodal Masked Video Generation (MMVG)，它从部分帧和文本指导中完成视频。MMVG使用掩蔽策略获取掩蔽视频，并使用基于transformer的多模态编码器融合视觉和语言模态。视频解码器建立在VideoSwin之上，并遵循香草自回归解码器。遮罩概率根据预测误差自适应调整以提高生成质量。
具体步骤包括：
1. 将视频帧离散化为视觉令牌。
2. 使用掩蔽策略获得掩蔽视频。
3. 使用transformer-based多模态编码器融合视觉和语言模态。
4. 使用VideoSwin建立视频解码器。
5. 使用自回归解码器生成视频。

#### 8. 实验设置：
本文在三个数据集上进行了实验：Kitchen、Flintstones和MUGEN。Kitchen记录了约22K个关于厨房活动的自我中心视频，这些视频具有不同的长度（4-16帧）和叙述。Flintstones包含来自The Flintstones的25K个动画视频（15帧），其中每个视频描述包括角色及其行为。MUGEN是由玩CoinRun的代理人组成的，其中包含375K个游戏视频（16帧），并带有详细的文本注释。这三个数据集中的所有视频都被调整为128x128。评估指标包括FVD和RCS。

#### 9. 实验结果和分析：
本文提出的MMVG方法在所有TVC任务（包括预测、倒带和填充）上均优于基线方法。MMVGU是一个统一的模型，可以通过单一训练支持所有TVC任务，并在所有数据集上表现优于TATS，即视频生成的SOTA。MMVGS是针对每个任务特定训练的MMVG，进一步提高了统一模型的预测和倒带任务的性能。指令作为指导使其与预期的ground-truth结果相关。更高的RCS也表明我们的MMVG可以生成符合指令的视频。对于视频生成和预测任务，MMVG的性能与基线方法相当。


# Paper:882     无监督目标检测和实例分割的切割学习



#### 1. Title: 
Cut and Learn for Unsupervised Object Detection and Instance Segmentation

#### 2. Authors: 
Xudong Wang, Rohit Girdhar, Stella X. Yu, Ishan Misra

#### 3. Affiliation: 
Xudong Wang: FAIR, Meta AI (美国人工智能研究所)
Rohit Girdhar: FAIR, Meta AI (美国人工智能研究所)
Stella X. Yu: UC Berkeley / ICSI (美国加州大学伯克利分校/国际计算机科学研究所), University of Michigan (美国密歇根大学)
Ishan Misra: FAIR, Meta AI (美国人工智能研究所)

#### 4. Keywords: 
Unsupervised Object Detection, Instance Segmentation, Self-supervised Learning, Zero-shot Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Cut_and_LEaRn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2021_paper.html  Github: https://github.com/facebookresearch/CutLER

#### 6. Summary : 
- (1):本文研究无监督目标检测和实例分割模型的训练，旨在解决标注数据难以获取的问题。
- (2):过去的方法需要特定的标注数据，难以应用于不同的领域，本文提出的方法可以自动发现图像中的对象，不需要人工标注，且适用于不同的检测架构和数据集。本文的方法相比于之前的方法更简单，且可以检测多个对象。 
- (3):本文提出了CutLER方法，该方法包括三个简单的机制：MaskCut、Ldrop和self-training。MaskCut使用预训练的自监督特征自动生成多个初始粗略掩模，Ldrop使用这些掩模训练检测器，self-training进一步提高模型的性能。CutLER可以直接在ImageNet数据上训练，不需要额外的数据，且可以检测和分割多个对象。 
- (4):CutLER在多个数据集上进行了测试，包括自然图像、绘画、草图、剪贴画、视频、交通图像等，取得了比之前最先进的方法更好的性能。CutLER在11个基准测试中的APbox50性能比FreeSOLO提高了2倍以上，且在UVO视频实例分割基准测试中超过了监督检测器。CutLER还可以作为预训练模型，用于训练完全监督的目标检测和实例分割模型，在COCO数据集上取得了更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为Cut and Learn（CutLER）的无监督目标检测和实例分割方法。该方法包括三个机制：MaskCut、简单的损失函数丢弃策略和自我训练。MaskCut使用预训练的自监督特征为每个图像生成多个初始粗略掩模。损失函数丢弃策略使用粗略掩模训练检测器，同时对MaskCut漏掉的对象具有鲁棒性。最后，多轮自我训练使模型从捕捉局部像素的相似性发展到捕捉对象的全局几何形状，从而产生更精细的分割掩模。CutLER仅在未标记的ImageNet数据上进行训练，无需额外的训练数据，并且可以直接用于执行各种领域的复杂分割和检测任务。它还与不同的检测架构兼容，并可以检测多个对象。

#### 8. 实验设置：
本文使用各种基准测试数据集评估CutLER的性能，包括Objects365、VOC、COCO、COCO 20K、KITTI和OpenImages等。CutLER仅在ImageNet上进行训练，并且在模型训练期间不会看到任何目标数据集中的图像。

#### 9. 实验结果和分析：
CutLER在11个不同的基准测试上表现出强大的零样本性能，超过了使用额外领域数据训练的先前工作。它将10个基准测试的APbox50性能提高了一倍以上，甚至在UVO视频实例分割基准测试上也优于监督检测器。CutLER在不同领域的图像上表现出强大的鲁棒性，例如视频帧、素描、绘画、剪贴画等。它还可以作为预训练模型用于训练完全监督的目标检测和实例分割模型，并提高COCO上的性能，包括少样本目标检测基准测试。通过微调，CutLER作为低样本检测器超过MoCo-v2 7.3% APbox和6.6% APmask。


# Paper:883     LaserMix用于半监督LiDAR语义分割



#### 1. Title: 
LaserMix for Semi-Supervised LiDAR Semantic Segmentation

#### 2. Authors: 
Lingdong Kong, Jiawei Ren, Liang Pan, Ziwei Liu

#### 3. Affiliation: 
第一作者：Lingdong Kong，新加坡南洋理工大学和国立新加坡大学

#### 4. Keywords: 
LiDAR, Semantic Segmentation, Semi-Supervised Learning, LaserMix

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Kong_LaserMix_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2022_paper.html  Github: https://github.com/ldkong1205/LaserMix

#### 6. Summary : 
- (1):本文研究了半监督LiDAR语义分割，旨在利用易于获取的未标记数据来提高LiDAR语义分割的可扩展性。
 
- (2):现有的半监督学习方法主要针对图像识别和语义分割任务，而这些方法在3D LiDAR数据上的表现不佳。本文提出了一种新的半监督学习框架，利用LiDAR点云的强空间先验来更好地利用未标记数据。本文的核心思想是利用LiDAR点云的强空间先验，将来自不同LiDAR扫描的激光束混合在一起，并鼓励模型在混合前后进行一致和自信的预测。本文的方法具有以下三个优点：1）通用性：LaserMix不受LiDAR表示（例如，range view和voxel）的影响，因此我们的半监督框架可以普遍适用。2）统计基础：我们提供了详细的分析，从理论上解释了所提出的框架的适用性。3）有效性：对流行的LiDAR分割数据集（nuScenes，SemanticKITTI和ScribbleKITTI）进行的全面实验分析表明，我们的方法具有优越性能。

- (3):本文提出了LaserMix，一种新颖的、表示无关的混合技术，旨在最大化我们的半监督框架中空间先验的“强度”。我们的框架鼓励一致性正则化和熵最小化，并且不需要额外的开销，这有助于保持可扩展性。我们的方法在LiDAR点云上直接操作，因此与现有的各种表示（例如，range view和voxel）的最先进的LiDAR分割方法高度兼容。我们的方法在弱监督数据集上使用非常有限的注释实现了竞争性能：在仅使用0.8％标签的情况下，我们在SemanticKITTI上实现了54.4％的mIoU，这与PolarNet（54.3％），RandLA-Net（53.9％）和RangeNet ++（52.2％）使用100％标签的情况下相当。综合实验分析证明了空间先验在我们的框架成功中发挥了关键作用。

- (4):本文的方法在nuScenes和SemanticKITTI等流行的自动驾驶数据库上取得了优异的性能，相对于完全监督的对应方法，使用2倍到5倍的标签数量实现了竞争性能，并且相对于仅使用监督的基线方法，性能提高了10.8％。本文的方法在半监督LiDAR语义分割方面具有很高的应用价值。
#### 7. 方法详细介绍：
本文提出了一种半监督的LiDAR语义分割方法，称为LaserMix。该方法使用了学生-教师框架和伪标签来利用有标签和无标签数据。LaserMix技术用于混合有标签和无标签数据以生成混合数据样本。教师网络使用指数移动平均（EMA）技术进行更新。总损失函数是监督损失、混合损失和平均教师损失的组合。该方法旨在最小化边际熵，并鼓励模型的预测与监督信号一致且自信。

#### 8. 实验设置：
本文在nuScenes、SemanticKITTI和ScribbleKITTI数据集上构建了三个半监督学习基准。数据集被分为有标签和无标签数据，有1％、10％、20％和50％的有标签数据。为了确保不同SSL算法之间的公平比较，所有实验都使用相同的骨干网络和配置。FIDNet和Cylinder3D用作范围视图和体素选项的分割骨干。SemanticKITTI和ScribbleKITTI的输入分辨率设置为64×2048，nuScenes的输入分辨率设置为32×1920。体素分辨率对于所有三个数据集都是[240，180，20]。

#### 9. 实验结果和分析：
本文提出的LaserMix方法与其他SSL方法进行了比较。结果表明，LaserMix在所有三个数据集上均优于其他SSL方法，并实现了最先进的性能。与基线和其他SSL方法相比，该方法的平均交并比（mIoU）得分提高了3.0％至14.9％。消融研究表明，学生和教师网络的监督信号都对方法的性能有贡献。教师网络的EMA更新和LaserMix技术对于实现最佳性能至关重要。


# Paper:884     一种可解释的微血管侵犯分类回路网络



#### 1. Title: 
A Loopback Network for Explainable Microvascular Invasion Classification

#### 2. Authors: 
Shengxuming Zhang, Tianqi Shi, Yang Jiang, Xiuming Zhang, Jie Lei, Zunlei Feng, Mingli Song

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Microvascular invasion, deep learning, explainable classification, cell detection, loopback strategy

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是微血管侵犯（MVI）的诊断，目前的诊断方法需要病理学家手动从数百个血管中找出癌细胞，费时、繁琐且主观。深度学习在医学图像分析任务中取得了很好的效果，但黑盒模型的不可解释性和大量标注样本的要求限制了其临床应用。
- (2):过去的方法主要是基于深度学习的图像分析，但需要大量标注样本，且结果不可解释。本文提出了一种可解释的MVI诊断工具，使用图像级别的类别标注，通过二进制分类分支和细胞定位分支实现MVI的分类和定位。为了解决细胞定位分支的监督信息不足的问题，本文提出了一种基于相关滤波器的伪掩膜方法，以及一种基于健康样本的监督方法。此外，本文还提出了一种回路策略，使分类标签能够监督细胞定位分支学习癌细胞区域的定位能力。
- (3):本文提出的LoopNet网络是一种可解释的MVI分类方法，通过利用MVI样本的特征，同时实现血管分类和细胞定位结果，并可扩展到各种器官的MVI分析任务。回路策略有效地实现了两个目标：1）利用图像级别的类别标签监督细胞定位分支区分癌细胞区域和其他区域。2）建立预测癌细胞区域和最终分类结果之间的直接关系。实验结果表明，LoopNet在PVID上实现了97.5%的准确率，验证了深度学习在MVI分类任务中的潜力。
- (4):本文的方法在MVI分类任务上取得了97.5%的准确率，支持其目标。本文的创新点在于提出了一种可解释的MVI诊断工具，使用图像级别的类别标注，同时实现MVI的分类和定位。回路策略有效地实现了两个目标，使分类标签能够监督细胞定位分支学习癌细胞区域的定位能力，并建立预测癌细胞区域和最终分类结果之间的直接关系。
#### 7. 方法详细介绍：
本文提出了一种名为LoopNet的可解释性微血管侵犯（MVI）分类方法。该网络由二进制血管图像分类分支和细胞定位分支组成。二进制血管图像分类分支使用交叉熵损失进行训练，以区分健康和MVI血管。细胞定位分支使用二进制伪掩模进行训练，以定位背景和非癌细胞以及使用回路策略定位癌细胞。回路策略将血管图像分类结果与癌细胞定位结果相关联，提供可靠的证据和可靠的解释图像分类结果。LoopNet分为两个阶段进行训练，损失函数是二进制血管图像分类分支、细胞定位分支和回路策略的损失函数的组合。

#### 8. 实验设置：
本文使用了Pathologic Vessel Image Dataset（PVID）进行MVI分析的评估，该数据集包含5,000个血管样本（4,140个健康血管和860个MVI血管）。根据幻灯片，将PVID的血管图像部分随机分为训练、验证和测试集，确保相同幻灯片的血管图像在同一集合中。健康血管在训练集、验证集和测试集中的样本数分别为2,480和830和830，MVI血管的样本数为520和170和170。本文使用ResNet-50作为LoopNet的骨干网络，并采用Ranger优化器进行训练。

#### 9. 实验结果与分析：
本文提出的LoopNet在测试集上实现了97.49%的准确率，优于现有方法。CAM、DeepLIFT和LRP的归因特征区域中定位的癌细胞区域的精度、召回率和Dice分数分别为4.59/81.11/39.05、8.35/5.39/15.09和5.89/94.52/15.42。本文的方法在准确率、精度和召回率等方面均优于现有方法。本文的方法可以实现有前途的结果，相比需要大量点注释的方法，本文的方法可以在不需要大量点注释的情况下实现有前途的结果。消融实验表明，所有损失项都对最终结果做出了贡献，而设计的回路策略在定位癌细胞方面起着关键作用。将Lcls中的GHM-C损失替换为CE损失将降低整体准确率，特别是MVI血管的召回率，这是临床中的重要指标。


# Paper:885     PoseExaminer：自动化测试人体姿态和形状估计的分布外鲁棒性



#### 1. Title: 
PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation

#### 2. Authors: 
Qihao Liu, Adam Kortylewski, Alan Yuille

#### 3. Affiliation: 
Qihao Liu: 约翰霍普金斯大学
Adam Kortylewski: Max Planck Institute for Informatics, University of Freiburg
Alan Yuille: 约翰霍普金斯大学

#### 4. Keywords: 
Human pose and shape estimation, out-of-distribution robustness, automated testing, multi-agent reinforcement learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_PoseExaminer_Automated_Testing_of_Out-of-Distribution_Robustness_in_Human_Pose_CVPR_2021_paper.html  Github: https://github.com/qihao067/PoseExaminer

#### 6. Summary : 
- (1):本文研究的是人体姿态和形状估计方法的鲁棒性问题，当前的HPS基准测试主要是在与训练数据相似的情况下进行测试，而在真实世界应用中，观察到的数据与训练数据存在显著差异，因此需要测试和提高HPS方法的鲁棒性。
- (2):以往的方法主要是通过生成大量的合成数据集来测试模型的性能，但这些数据集主要关注常见的姿势，缺乏多样性，而且只关注平均性能，而不是关注失败模式。本文提出了一种名为PoseExaminer的自动化测试工具，它可以控制模拟器的参数，以探索人体姿态图像的流形，通过多智能体强化学习系统来探索高维参数空间，发现了当前最先进模型中的各种限制，这些限制在真实世界的场景中很重要，但是在当前基准测试中被忽略了。
- (3):PoseExaminer是一种基于学习的测试方法，它可以自动诊断HPS算法，通过搜索人体姿态图像的参数空间来发现失败模式。PoseExaminer的策略是多智能体强化学习方法，每个智能体从不同的随机初始化种子开始，探索潜在的参数空间，以找到失败案例，同时避免探索潜在空间中其他智能体附近的区域。我们发现，即使在最好的HPS模型中，也存在非常大的失败子空间。PoseExaminer发现的失败模式可以用于显著提高当前方法的鲁棒性和性能。
- (4):本文在四个最先进的HPS模型上进行了实验，发现PoseExaminer成功地发现了各种失败模式，提供了关于它们在真实世界中的表现的新见解。PoseExaminer发现了大量的姿势，这些姿势在真实世界中很常见，但是当前的模型无法正确预测。此外，我们发现，PoseExaminer发现的失败模式可以用于显著提高当前方法的鲁棒性和性能，甚至在标准基准测试上也有很大的提升。
#### 7. 方法详细介绍：
本文提出了一种名为PoseExaminer的学习方法，用于自动诊断人体姿态和形状估计方法的鲁棒性。PoseExaminer通过搜索人体图像的参数空间来寻找模型的失败模式。该方法采用多智能体强化学习方法，每个智能体在不同的随机初始化种子下探索潜在参数空间，以寻找失败情况。每个智能体在收敛到局部最优解后，探索局部参数空间以找到连接的失败区域，定义了一个整个子空间的图像，其中姿态被错误地预测（即失败模式）。该方法使用SMPL参数表示人体形状和姿态，并使用PyTorch3D渲染3D人体网格。该方法还采用了VPoser，一种基于VAE的姿势先验，以约束姿势并让智能体搜索32维潜在空间。该方法使用多个RL智能体进行协作搜索，并采用策略梯度方法。该方法还引入了两个新的指标来评估发现的子空间和边界。

#### 8. 实验设置：
本文使用了四种人体姿态和形状估计（HPS）方法，分别是HMR、SPIN、HMR-EFT和PARE，用于评估。评估使用了3DPW测试集和AIST ++数据集。3DPW数据集被归类为内部分布数据集，而AIST ++被用作外部分布数据集。本文还提供了有关模型实现的详细信息，包括使用官方预训练模型和MMHuman3D提供的基于PyTorch的实现。

#### 9. 实验结果和分析：
本文提供了各种指标来评估HPS方法的性能，包括Procrustes对齐的平均每个关节位置误差（PA-MPJPE）和平均每个关节位置误差（MPJPE）（以毫米为单位）。本文还报告了3DPW的每个顶点误差（PVE）。结果表明，PoseExaminer发现的失败模式很好地推广到实际图像，并且可以用于诊断和改进HPS方法的性能。本文还证明了真实数据集和合成数据集之间的性能差距很小，所提出的方法在内部分布和外部分布数据集上均取得了良好的结果。


# Paper:886     未知嗅探器：不要对未知目标视而不见



#### 1. Title: 
Unknown Sniffer for Object Detection: Don’t Turn a Blind Eye to Unknown Objects

#### 2. Authors: 
Wenteng Liang, Feng Xue, Yihao Liu, Guofeng Zhong, Anlong Ming

#### 3. Affiliation: 
北京邮电大学

#### 4. Keywords: 
Object detection, unknown objects, open-world object detection, generalized object confidence, negative energy suppression, graph-based determination

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liang_Unknown_Sniffer_for_Object_Detection_Dont_Turn_a_Blind_Eye_CVPR_2021_paper.html  Github: https://github.com/Went-Liang/UnSniffer

#### 6. Summary : 
- (1):本文研究的是目标检测中的未知目标问题，提出了一种新的方法来检测未知目标，以应对更具挑战性的环境，如自动驾驶场景中的潜在危险。
 
- (2):过去的方法主要集中在提高不确定性的区分能力，但往往会在训练中抑制非目标和潜在的未知目标，导致未知目标的召回率较低。本文提出了一种新的方法，通过已知目标学习广义目标置信度（GOC）分数来表达未知目标，并设计了负能量抑制来进一步限制非目标。此外，本文还提出了一种基于图形的框确定方案，以适应性地选择每个对象的最佳边界框，从而提高未知检测精度。
 
- (3):本文提出了未知嗅探器（UnSniffer）来查找未知和已知对象。首先，引入广义目标置信度（GOC）分数，仅使用已知样本进行监督，并避免不当抑制背景中的未知目标。这样从已知对象学习的置信度分数可以推广到未知对象。此外，我们提出了一种负能量抑制损失，进一步抑制背景中的非目标样本。接下来，由于在训练中缺乏未知的语义信息，因此在推理过程中很难确定每个未知的最佳边界框。在我们的模型中，最佳框确定被建模为一个图分割问题，自适应地将高分数提议聚类成几组，并从每组中选择一个作为最佳框。最后，我们提出了未知对象检测基准，这是我们所知道的第一个公开基准，包括未知检测的精度评估。实验表明，我们的方法比现有的最先进方法要好得多。
 
- (4):本文提出的方法在未知目标检测任务上取得了很好的性能，可以检测出许多其他方法无法检测到的未知目标，同时保持了已知目标的检测精度。在提出的未知对象检测基准上，我们的方法在未知目标检测方面表现出色，远远优于现有的最先进方法。
#### 7. 方法详细介绍：
本文提出了一种名为Unknown Sniffer (UnSniffer)的方法，用于检测图像中的已知和未知物体。UnSniffer由两个检测器组成，一个用于已知物体，另一个用于未知物体。第一个检测器学习了提出的广义物体置信度（GOC）分数，以确定一个提议是否包含一个物体，而第二个检测器计算每个提议的类别特定概率和负能量分数。GOC分数仅使用已知物体进行训练，并且可以推广到未知物体。UnSniffer还采用基于图的框确定方案，将高分提议聚类成几组，并选择每组中具有最高GOC分数的一个作为一组未知预测。最后，第一个检测器输出未知类别的边界框预测，第二个检测器输出已知类别的边界框预测。两个结果被连接起来，如果任何未知类别预测与任何已知类别预测的IoU超过一个常数阈值，则删除该未知类别预测。

#### 8. 实验设置：
本文使用了两个数据集进行实验：COCO-OOD和COCO-Mixed。COCO-OOD数据集包含来自COCO数据集的已知和未知物体，而COCO-Mixed数据集包含来自COCO数据集和其他数据集的已知和未知物体。实验使用了U-AP和U-F1分数来评估未知物体检测的性能，并使用AOSE和WI来评估已知物体检测的性能。实验还进行了定性分析，以展示UnSniffer在未知物体检测方面的优越性。

#### 9. 实验结果和分析：
UnSniffer在COCO-OOD和COCO-Mixed数据集上的U-AP和U-F1分数均优于现有方法。UnSniffer的AOSE最小但WI最大，表明已知物体被错误分类为不正确的类别的数量较少。定性分析显示，UnSniffer不会错过任何未知物体，并且即使两个或多个物体重叠，也能可靠地预测每个物体的单个边界框。更多结果详见补充材料。


# Paper:887     PCT-Net：使用像素级颜色变换实现全分辨率图像协调



#### 1. Title: 
PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations

#### 2. Authors: 
Julian Jorge Andrade Guerreiro, Mitsuru Nakazawa, Bjorn Stenger

#### 3. Affiliation: 
第一作者：东京大学

#### 4. Keywords: 
Image harmonization, pixel-wise color transformations, full-resolution images, deep learning, parameter network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Guerreiro_PCT-Net_Full_Resolution_Image_Harmonization_Using_Pixel-Wise_Color_Transformations_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是高分辨率图像的图像协调问题，传统方法大多只能处理低分辨率图像，而高分辨率图像的处理成本较高，因此需要一种轻量级的方法来处理高分辨率图像的图像协调问题。

- (2):过去的方法大多只能处理低分辨率图像，而高分辨率图像的处理成本较高。本文提出了一种轻量级的方法，可以处理高分辨率图像的图像协调问题。本文的方法使用像素级颜色变换(PCT)来处理每个像素，而不是像其他方法一样对整个图像进行处理。本文的方法使用了Transformer网络，相比于其他方法，本文的方法在处理高分辨率图像时具有更好的性能。

- (3):本文提出了一种基于双分支方法的轻量级模型，可以处理高分辨率图像的图像协调问题。该模型使用像素级颜色变换(PCT)来处理每个像素，而不是像其他方法一样对整个图像进行处理。本文的方法使用了Transformer网络，相比于其他方法，本文的方法在处理高分辨率图像时具有更好的性能。

- (4):本文的方法在公共数据集iHarmony4上进行了评估，结果表明，本文的方法在保持轻量级结构的同时，可以将前景MSE(fMSE)和MSE值降低20%以上，PSNR值提高1.4dB以上。在用户研究中，本文的方法比其他两种方法具有更高的B-T得分。
#### 7. 方法详细介绍：
本文提出了一种名为PCT-Net的图像调和方法，它由两个分支组成：全分辨率分支（FR）和低分辨率分支（LR）。LR分支将FR合成图像和掩膜图像降采样到较低分辨率，用于训练一个参数网络，将LR输入映射到参数图。FR参数图通过上采样LR参数图获得。像素级颜色转换（PCT）函数用于在FR分支中修改输入图像。评估了不同的PCT函数，包括简单的函数、仿射变换和多项式PCT函数。损失函数包括前景归一化MSE损失、自身风格对比正则化和平滑正则化。使用两个不同的骨干模块进行训练，一个是基于CNN的骨干，另一个是基于Vision Transformer（ViT）的骨干。

#### 8. 实验设置：
本文在iHarmony4测试数据集上评估了所提出的方法。使用每个图像的均方误差（MSE）、峰值信噪比（PSNR）和前景均方误差（fMSE）来衡量性能。将结果与其他最先进的方法进行比较，发现所提出的方法在除HDay2night子集外的所有数据集上均优于它们。还报告了未应用图像调和的图像百分比。进行了消融研究，以验证所提出的设计选择的有效性，包括参数图插值、正则化技术和像素级颜色转换（PCT）函数。

#### 9. 实验结果和分析：
本文提出了一种轻量级高效的图像调和方法PCT-Net，利用像素级颜色转换和函数参数空间内的插值。在iHarmony4数据集上进行了评估，结果在fMSE、MSE和PSNR方面均达到了最先进水平。涉及20名参与者和26张图像的用户研究表明，所提出的方法还改善了感知质量。


# Paper:888     通过知识图谜题提高视觉语言模型的常识能力



#### 1. Title: 
Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles

#### 2. Authors: 
Shuquan Ye, Yujia Xie, Dongdong Chen, Yichong Xu, Lu Yuan, Chenguang Zhu, Jing Liao

#### 3. Affiliation: 
第一作者单位：香港城市大学

#### 4. Keywords: 
Vision-Language models, Commonsense knowledge, Data augmentation, Knowledge graph, Diagnostic benchmark

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Ye_Improving_Commonsense_in_Vision-Language_Models_via_Knowledge_Graph_Riddles_CVPR_2021_paper.html  Github: https://github.com/pleaseconnectwifi/DANCE

#### 6. Summary : 
- (1):本文旨在分析和改进最近流行的视觉语言（VL）模型的常识能力。尽管这些模型取得了巨大的成功，但我们观察到现有的VL模型仍然缺乏常识知识/推理能力，这是人工智能通用性的重要组成部分。

- (2):现有的大规模VL数据集不包含太多常识知识，这是常识缺乏的一个重要原因。本文提出了一种更可扩展的策略，即“具有常识能力的知识图谱谜题的数据增强”（DANCE）。它可以被视为一种数据增强技术，可以在训练过程中即时向现有的VL数据集注入常识知识。具体而言，我们利用常识知识图谱（例如ConceptNet）通过双向子图序列化创建VL数据集的文本描述变体。

- (3):本文提出了一种新的基于检索的常识诊断基准，以更好地评估常识能力。通过在一些代表性的VL模型上进行广泛的实验，我们证明了我们的DANCE技术能够显著提高常识能力，同时保持对原始检索任务的性能。

- (4):本文提出的DANCE技术在多个VL模型上进行了实验，证明了其有效性。同时，本文提出的常识诊断基准也得到了广泛的应用。
#### 7. 方法详细介绍：
本文提出了一种名为DANCE（Data Augmentation for Commonsense Enhancement）的训练策略，通过生成知识图谜题来增强视觉语言模型的常识知识。该方法包括以下步骤：
1. 利用已有的常识知识图谱（如ConceptNet）生成知识图谜题。
2. 将图像与实体配对，进行双向子图序列化，生成文本描述。
3. 将文本描述与知识图谜题配对，生成常识增强的图像-文本对。
4. 使用生成的数据对视觉语言模型进行预训练。

#### 8. 实验设置：
本文使用了两种主干网络，ViT-Base/16和ViT-Large/16，对视觉语言模型进行预训练。模型在四个节点上进行训练，每个节点使用八个A100 GPU。本文使用COCO和VG数据集进行图像-文本配对和DANCE，使用VCR、SBU Captions、Conceptual Captions（CC3M）和Conceptual 12M（CC12M）数据集生成知识图谜题。本文还使用了一种诊断测试集来评估视觉语言模型的常识知识表现。

#### 9. 实验结果与分析：
本文发现当前基础的视觉语言数据集相比于常规文本缺乏常识知识。本文评估了各种最先进的视觉语言模型在诊断测试集上的表现，并发现它们在常识知识方面的表现远远不及人类。本文表明，使用DANCE增强数据进行预训练可以显著提高视觉语言模型在对比检索任务上的表现。本文还表明，使用DANCE增强数据进行微调可以提高视觉语言模型在COCO检索任务上的表现。


# Paper:889     使用偏振智能手机图像捕捉高分辨率人脸外观



#### 1. Title: 
High-Res Facial Appearance Capture from Polarized Smartphone Images

#### 2. Authors: 
Dejan Azinovi´c, Olivier Maury, Christophe Hery, Matthias Nießner, Justus Thies

#### 3. Affiliation: 
Dejan Azinovi´c: Technical University of Munich

#### 4. Keywords: 
Facial texture reconstruction, polarization, smartphone, differentiable rendering, photo-realistic images

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Azinovic_High-Res_Facial_Appearance_Capture_From_Polarized_Smartphone_Images_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文的研究背景是在虚拟和混合现实设备的发展中，对于人脸几何和皮肤纹理的准确重建是实现真实感渲染的关键问题。传统的解决方案需要昂贵的光场捕捉设备和精密的校准，而本文提出了一种使用普通智能手机和廉价偏振膜的捕捉方案，以实现高质量的人脸纹理重建。

- (2):过去的方法需要昂贵的设备和精密的校准，而且只能在受控环境下进行。本文提出的方法使用普通智能手机和廉价偏振膜，可以在不受控制的环境下进行捕捉。本文的方法是基于分离皮肤的漫反射和镜面反射，使用不同的偏振光源和偏振滤镜捕捉两个短序列，然后使用可微分渲染器进行优化，以重建高分辨率的皮肤纹理，包括漫反射、镜面反射和法线贴图。本文的方法可以用于合成高质量的照片级3D数字人类。

- (3):本文提出了一种使用普通智能手机和廉价偏振膜的捕捉方案，以实现高质量的人脸纹理重建。本文的方法是基于分离皮肤的漫反射和镜面反射，使用不同的偏振光源和偏振滤镜捕捉两个短序列，然后使用可微分渲染器进行优化，以重建高分辨率的皮肤纹理，包括漫反射、镜面反射和法线贴图。本文的方法可以用于合成高质量的照片级3D数字人类。

- (4):本文的方法在不受控制的环境下使用普通智能手机和廉价偏振膜进行捕捉，可以实现高质量的人脸纹理重建。本文的方法可以用于合成高质量的照片级3D数字人类，具有很高的视觉效果和真实感。本文的方法在多个数据集上进行了测试，结果表明本文的方法可以实现高质量的人脸纹理重建。
#### 7. 方法详细介绍：
本文提出了一种使用偏振膜和普通智能手机捕捉高分辨率面部外观的方法。该方法涉及使用带有偏振膜的闪光灯的智能手机捕获视频帧和照片序列。使用多视图立体（MVS）处理捕获的数据以重建3D面部几何形状。该方法还涉及使用偏振来分离材料和阴影信息，从而实现高分辨率皮肤纹理的恢复。皮肤纹理使用基于神经网络的方法Neural Light Transport（NLT）和粗到细的优化方法进行优化。该方法能够以低成本的捕捉程序产生高质量的面部外观纹理。

#### 8. 实验设置：
本文没有特定的实验设置部分。

#### 9. 实验结果和分析：
本文通过将所提出的方法与两种现有方法Neural Light Transport（NLT）和NextFace进行比较，得出了实验结果。所提出的方法在图像度量方面表现出色，并能够合成比NLT更锐利的纹理细节和高光。该方法还能够克服NextFace存在的面部模型拟合不准确和漫反射和镜面纹理分离的问题。该方法需要约2.5小时来重建一个面部，包括1小时用于MVS和90分钟用于基于光度的皮肤纹理重建。光度优化需要30GB的GPU内存，纹理分辨率为4096×4096，目标图像分辨率为3840×2160。该方法仅适用于静态表情，无法处理动态变化的面部几何和纹理。


# Paper:890     少即是多：减少3D点云语义分割的任务和模型复杂度



#### 1. Title: 
Less is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation

#### 2. Authors: 
Li Li, Hubert P. H. Shum, Toby P. Breckon

#### 3. Affiliation: 
李力，胜浩，托比·布雷肯，都伦大学计算机科学系

#### 4. Keywords: 
3D point cloud, semantic segmentation, LiDAR, deep learning, semi-supervised learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Less_is_More_Reducing_Task_and_Model_Complexity_for_3D_CVPR_2021_paper.html  Github: https://github.com/l1997i/lim3d/

#### 6. Summary : 
- (1):本文研究的是3D点云语义分割，该领域的数据标注成本高昂，因此需要半监督学习方法。现有方法往往采用相对较大的分割骨干网络来提高分割准确性，但代价是计算成本的增加。此外，许多方法使用均匀采样来减少学习所需的地面真实数据，但往往导致次优性能。本文提出了一种新的流程，通过一种新颖的稀疏深度可分离卷积模块，显著减少网络参数数量，同时保持整体任务性能，从而需要更少的地面真实注释来实现优于现有方法的分割准确性。为了有效地对训练数据进行子采样，我们提出了一种新的时空冗余帧下采样方法，利用环境中传感器运动的知识来提取更多样化的训练数据帧样本的子集。为了利用有限的注释数据样本，我们进一步提出了一种软伪标签方法，该方法受到LiDAR反射率的启发。本文的方法在SemanticKITTI（59.5@5%）和ScribbleKITTI（58.1@5%）基准数据集上使用更少的标记数据，基于2.3×减少模型参数和641×减少乘法-加法操作，同时也在有限的训练数据上展示了显著的性能提升。

- (2):现有的3D语义分割方法往往需要相对较大的骨干网络，需要大量的注释数据进行训练，计算成本高昂。本文提出了一种半监督方法，采用较小规模的骨干网络，从而提高训练效率，同时减少其相关数据注释要求。本文的方法通过三个新颖的设计方面实现：一种新颖的稀疏深度可分离卷积（SDSC）模块，一种新颖的时空冗余帧下采样（ST-RFD）策略和一种软伪标签方法。这些方法分别减少了可训练参数的数量，提高了数据集的多样性，提高了模型的泛化能力，从而实现了更好的分割准确性。

- (3):本文提出了一种新的半监督方法，通过SDSC模块、ST-RFD策略和软伪标签方法，实现了3D点云语义分割的高效率和高准确性。SDSC模块通过稀疏深度可分离卷积，实现了可训练参数数量的减少，同时保持了整体任务性能。ST-RFD策略通过提取更多样化的训练数据帧样本的子集，实现了数据集的多样性，提高了模型的泛化能力。软伪标签
#### 7. 方法详细介绍：
本文提出了一种基于Mean Teacher框架的3D点云语义分割方法，包括三个阶段：训练、伪标签和不可靠学习的蒸馏。该方法采用了不可靠伪标签和LiDAR反射率信息进行半监督学习。作者使用ST-RFD策略增加数据集多样性，使用SDSC模块减少可训练参数。该方法的总体架构是使用不对称3D卷积网络生成体素级输入，训练一组弱标签点云帧。优化目标是最小化总损失，包括一致性损失、Lovasz softmax损失和体素级InfoNCE。作者还提出了一种方法，利用类别内存储库将不可靠伪标签作为负样本，用于3D体素的伪标签。最后，作者提出了一种基于反射率的测试时间增强（Reflec-TTA）技术，用于识别不同的语义类别。

#### 8. 实验设置：
本文使用SemanticKITTI和ScribbleKITTI数据集对所提出的方法进行了评估。SemanticKITTI是一个大规模的3D点云数据集，用于语义场景理解，包含22个序列和20个语义类别。ScribbleKITTI是第一个用于LiDAR语义分割的涂鸦注释数据集，提供了对19个类别的稀疏注释，仅使用了SemanticKITTI训练集的8.06%的点云。 

#### 9. 实验结果和分析：
本文提出的方法在SemanticKITTI和ScribbleKITTI验证集上的表现优于当代半监督和弱监督方法，并在有限的训练数据、可训练参数和数值操作方面提供了更多的性能。本文提供了对SemanticKITTI和ScribbleKITTI数据集上其他方法的详细性能比较，证明了所提出方法的有效性。


# Paper:891     通过有效探索历史导航状态之间的关系实现目标导向的视觉导航



#### 1. Title: 
Object-Goal Visual Navigation via Effective Exploration of Relations among Historical Navigation States

#### 2. Authors: 
Heming Du, Lincheng Li, Zi Huang, Xin Yu

#### 3. Affiliation: 
Heming Du: 澳大利亚国立大学 (Australian National University)

#### 4. Keywords: 
Visual navigation, object-goal navigation, navigation states, correlation modeling, reinforcement learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究目标导向的视觉导航，旨在通过一系列移动步骤将代理引导到一个对象。先前的方法主要集中在学习导航的信息视觉表示，但忽略了导航状态对导航效率和有效性的影响。本文旨在通过探索历史导航状态之间的关系，提出一种历史启发式导航策略学习（HiNL）框架，以有效地估计导航状态。

- (2):现有的方法主要集中在提取信息视觉表示，而我们研究了导航状态对导航性能的影响。现有方法的导航状态存在高相关性，这会导致导航策略低效或失败。本文提出了一种历史启发式导航策略学习（HiNL）框架，通过利用历史状态之间的关系来获得信息丰富的导航状态。HiNL包括两个创新部分：（i）历史感知状态估计（HaSE）模块，和（ii）基于历史的状态规范化（HbSR）。HaSE模块旨在生成一个状态，该状态可以根据视觉观察快速更新。HbSR旨在明确约束导航状态之间的相关性。本文在人工平台AI2-THOR（即iTHOR和RoboTHOR）上进行了实验，结果表明HiNL在未见过的测试环境中显著优于现有方法。

- (3):本文提出了一种历史启发式导航策略学习（HiNL）框架，通过利用历史状态之间的关系来获得信息丰富的导航状态。HiNL包括两个创新部分：（i）历史感知状态估计（HaSE）模块，和（ii）基于历史的状态规范化（HbSR）。HaSE模块旨在生成一个状态，该状态可以根据视觉观察快速更新。HbSR旨在明确约束导航状态之间的相关性。本文在人工平台AI2-THOR（即iTHOR和RoboTHOR）上进行了实验，结果表明HiNL在未见过的测试环境中显著优于现有方法。

- (4):本文提出的HiNL框架在人工平台AI2-THOR（即iTHOR和RoboTHOR）上进行了实验，结果表明HiNL在未见过的测试环境中显著优于现有方法。HiNL显著提高了成功率和SPL。本文的主要贡献包括：（i）提出了一种历史启发式导航策略（HiNL）框架，以有效地估计导航状态；（ii）设计了一个历史感知状态估计（HaSE）模块，以消除当前状态估计中的历史状态的影响；（iii）引入了基于历史的状态
#### 7. 方法详细介绍：
本文提出了一种名为“历史启发式导航策略学习”（HiNL）的框架，包括三个模块：视觉表示提取、历史感知状态估计（HaSE）和基于历史的状态规范化（HbSR）。视觉表示提取模块使用深度神经网络从观察到的RGB图像中提取全局特征和物体特征。HaSE模块通过消除主导历史状态的影响，专注于当前观察变化，估计导航状态。HbSR模块引入了一种新颖的正则化方法，从训练目标的角度减少导航状态之间的相关性。

#### 8. 实验设置：
实验在AI2-THOR框架中的两个人工环境iTHOR和RoboTHOR中进行。训练集包括每个场景中的30个房间中的20个，其余房间平均分为验证集和测试集。目标类别为22个，每个房间至少有四个目标。评估指标为成功率和路径长度加权的成功率（SPL）。使用32个异步代理和Adam优化器进行2M次训练，学习率为10^-4。

#### 9. 实验结果和分析：
HiNL在iTHOR和RoboTHOR两个环境中均取得了优异的结果，成功率和SPL均优于现有方法。使用transformer-based extractor的HiNL方法在成功率和SPL上均显著优于现有方法。使用ORG作为视觉表示提取器的HiNL方法在成功率上比原始ORG方法提高了近10%。HiNL显著优于ORG+TPN。消融实验表明，HaSE和HbSR都可以提高导航性能。使用绝对时间嵌入的HiNL方法在iTHOR上显著提高了成功率和SPL。当HaSE采用5个历史状态时，代理达到了成功率和SPL的峰值。增加无正则化阈值会削弱HbSR的效果，导航性能会下降。


# Paper:892     TRACE：在3D环境中使用动态相机对人物进行5D时序回归



#### 1. Title: 
TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments

#### 2. Authors: 
Yu Sun, Qian Bao, Wu Liu, Tao Mei, Michael J. Black

#### 3. Affiliation: 
Yu Sun: 哈尔滨工业大学
Qian Bao, Wu Liu: 京东探索
Tao Mei: HiDream.ai Inc.
Michael J. Black: Max Planck Institute for Intelligent Systems

#### 4. Keywords: 
3D human pose and shape, dynamic cameras, global coordinates, 5D representation, end-to-end reasoning

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_3D_CVPR_2021_paper.html  Github: https://github.com/Arthur151/DynaCam

#### 6. Summary : 
- (1):本文研究的是在3D环境中使用动态相机对人物进行5D时序回归的问题。当前的3D人体姿态和形状估计方法仍然无法可靠地在全局坐标系中估计移动的人物，这对许多应用程序至关重要。这在相机也在移动时尤其具有挑战性，因为人和相机的运动会相互纠缠。
- (2):现有的方法大多只能在单个帧上推断人物的姿态和形状，并且只能在相机坐标系中估计人物。此外，这些方法无法跟踪人物并恢复其全局轨迹。本文提出了一种5D表示法，使网络能够显式地推理关于人类运动的时间跨度。与以前的方法不同，TRACE跟踪第一帧中呈现的主体，并通过一次拍摄在全局坐标系中恢复它们的全局轨迹。
- (3):本文提出了一种新的5D表示法，用于对场景中的人物进行端到端的推理。TRACE是第一个一阶段方法，可以从动态相机中联合恢复和跟踪全局坐标系中的3D人物。TRACE引入了两个新的“地图”，以在相机和世界坐标系中推理人物的3D轨迹。TRACE还引入了一个附加的记忆单元，即使在长时间遮挡期间也能持续跟踪人物。TRACE的创新之处在于，它是一个全面的解决方案，可以利用完整的视频来推断多个人物在一致的全局坐标系中的运动。 
- (4):TRACE在3DPW和MuPoTS-3D等多人物数据集上取得了最先进的性能，达到了SOTA的结果。TRACE在DynaCam数据集上的表现也优于GLAMR。TRACE的主要贡献是引入了5D表示法，使用它来学习关于3D人类运动和场景的全面时间线索，以及引入了两个新的运动偏移表示法，以显式地建立时间多主体关联和全局人类轨迹。TRACE的创新之处在于，它可以从完整的视频帧中获取信息，以解决检测、姿态估计、跟踪和遮挡等问题，并且是端到端训练的。
#### 7. 方法详细介绍：
本文提出了一种名为TRACE的新方法，用于在动态摄像机视频中估计多个人物的全局3D轨迹。该方法包括三个主要组件：1）用于人体形状和姿态估计的3D网格模型，2）用于时间运动建模的5D表示，以及3）带有记忆单元的跟踪模块，用于持久跟踪。5D表示包括人物在全局坐标系中的3D位置、速度和加速度，以及相机运动。跟踪模块使用记忆单元将3D检测与时间相关联，并更新每个主体的3D轨迹。该方法在三个3D人体姿态数据集、两个2D人体姿态数据集和新创建的DynaCam数据集上进行训练。性能在两个多人在野外基准测试3DPW和MuPoTS-3D以及DynaCam上进行评估。

#### 8. 实验设置：
实验在两个多人在野外基准测试3DPW和MuPoTS-3D以及新创建的DynaCam数据集上进行。DynaCam数据集通过模拟相机运动将由静态相机捕获的野外视频转换为动态相机视频而创建。该数据集包含500多个带有超过48K帧的注释视频。实验评估TRACE在全局3D轨迹估计、多对象跟踪和3D人体姿态/形状估计方面的性能。评估指标包括Traj、Velocity、IDs、MOTA、IDF1、HOTA、MPJPE、PMPJPE和MVE。

#### 9. 实验结果和分析：
在DynaCam数据集上，TRACE在Traj和Velocity误差方面优于两个基线解决方案。具体而言，TRACE的Traj误差为1.433m，Velocity误差为0.082m/s，而最佳基线解决方案GLAMR的Traj误差为1.745m，Velocity误差为0.105m/s。在MuPoTS-3D基准测试中，TRACE在IDs、MOTA、IDF1和HOTA方面表现最佳，优于ByteTrack和PHALP等多个最先进的方法。在3DPW基准测试中，TRACE与GLAMR和ByteTrack等最先进的方法相比表现出竞争性能。实验结果表明，TRACE在动态相机视频中的全局3D轨迹估计、多对象跟踪和3D人体姿态/形状估计方面具有很好的效果。


# Paper:893     基于LiDAR点云的无监督物体检测



#### 1. Title: 
Towards Unsupervised Object Detection from LiDAR Point Clouds

#### 2. Authors: 
Lunjun Zhang, Anqi Joyce Yang, Yuwen Xiong, Sergio Casas, Bin Yang, Mengye Ren, Raquel Urtasun

#### 3. Affiliation: 
Waabi, University of Toronto（多伦多大学）

#### 4. Keywords: 
Unsupervised Object Detection, LiDAR Point Clouds, Self-driving Scenes, Spatio-Temporal Reﬁnement, Self-supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Towards_Unsupervised_Object_Detection_From_LiDAR_Point_Clouds_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了自动驾驶场景中基于3D点云的无监督物体检测问题。由于大多数现有数据未标记，因此需要设计无监督学习算法，使其能够自行从原始传感器数据流中发现物体。 
- (2):过去的方法主要是基于监督学习，需要大量的标注数据，而本文提出的方法是基于无监督学习，不需要标注数据，可以自行发现物体。本文的方法结合了密度聚类、时间一致性、CNN的平移等变性和自监督学习等关键思想，能够在不需要重复遍历相同位置的情况下，零样本发现物体，且能够自我改进。 
- (3):本文提出了一种名为OYSTER（Object Discovery via Spatio-Temporal Reﬁnement）的方法，该方法首先利用点聚类在近距离区域获取初始伪标签，然后利用无监督跟踪过滤掉时间不一致的物体。由于点聚类在远距离的长距离区域效果不佳，因此利用CNN的平移等变性在高质量的近距离伪标签上进行训练，并进行零样本泛化到长距离。为了弥补训练和推理之间的密度差距，本文提出了一种新颖的随机LiDAR射线丢弃策略。最后，本文设计了一个自我改进循环，使得这个引导模型可以自我训练。在每一轮自我改进中，我们利用物体的时间一致性自动地对上一轮模型的检测结果进行了细化，并将这些细化的输出用作伪标签进行训练。 
- (4):本文在PandaSet和Argoverse 2 Sensor数据集上进行了实验，证明了OYSTER方法在自动驾驶场景中的有效性。本文提出的度量标准基于碰撞距离，相比于传统的IoU度量标准更能反映模型在自动驾驶场景中的性能。本文的方法能够在不需要标注数据的情况下，自动发现物体，并且能够自我改进，具有很好的应用前景。
#### 7. 方法详细介绍：
本文提出了一种名为OYSTER的无监督目标检测方法，适用于自动驾驶场景中的3D点云。该方法将点聚类、时间一致性、CNN的平移等变性和自监督相结合。该方法使用点聚类获取初始伪标签，以在点密度高的近距离范围内引导目标检测器的训练。然后，它采用无监督跟踪来过滤时间不一致的对象。该方法利用CNN的平移等变性，在高质量的近距离伪标签上进行训练，并在零样本情况下推广到长距离。为了弥补训练和推理之间的短距离和长距离的密度差异，该方法提出了一种新的随机LiDAR射线丢弃策略。最后，该方法设计了一个自我改进循环，在每一轮自我改进中，利用对象的时间一致性自动细化上一轮迭代模型的检测结果，并将这些细化的输出用作伪标签进行训练。具体步骤包括：Track、Reﬁne、Retrain和Repeat。

#### 8. 实验设置：
本文在两个数据集Pandaset和Argoverse V2 Sensor上评估了所提出的方法。Pandaset是一个用于自动驾驶的大规模数据集，Argoverse V2 Sensor是一个具有高度多样性的数据集，包含各种对象类型和场景。本文提出了一种基于碰撞距离的新型规划中心感知度量，以更好地衡量模型在自动驾驶场景中的性能。

#### 9. 实验结果和分析：
本文证明了所提出的无监督目标检测器在Pandaset和Argoverse V2 Sensor数据集上显著优于无监督基线，表明自监督与对象先验相结合可以在野外实现目标发现。所提出的方法在标准的基于交并比（IoU）和基于碰撞距离（DTC）的度量下，在两个数据集上均取得了最先进的性能。


# Paper:894     VQACL: 一种新的视觉问答连续学习设置



#### 1. Title: 
VQACL: A Novel Visual Question Answering Continual Learning Setting

#### 2. Authors: 
Xi Zhang, Feifei Zhang, Changsheng Xu

#### 3. Affiliation: 
1. 中国科学院自动化研究所，多模态人工智能系统国家重点实验室
2. 中国科学院大学，人工智能学院
3. 深圳市彭城实验室
4. 天津工业大学，计算机科学与工程学院

#### 4. Keywords: 
Visual Question Answering, Continual Learning, Multimodal Learning, Compositionality, Representation Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2021_paper.html  Github: https://github.com/zhangxi1997/VQACL

#### 6. Summary : 
- (1):本文研究的背景是连续学习，近年来在深度学习领域受到了广泛关注，因为它使模型能够在一系列非静态任务上不断学习，并接近于人类学习过程。然而，现有的研究主要集中在单模态任务上，而对于多模态任务如视觉问答（VQA）的需求被忽视了。
- (2):过去的方法主要集中在单模态任务上，如图像分类和序列标注，而对于多模态任务的需求被忽视了。本文提出了一个新的VQA连续学习设置，名为VQACL，它包含两个关键组成部分：双层任务序列和新的技能-概念组合的组合测试。本文提出的方法能够同时处理来自视觉和语言模态的连续数据，并测试模型的组合能力。本文提出的方法能够显著提高模型的性能，证明了所提出方法的有效性和组合性。
- (3):本文提出了一种新的表示学习方法，它引入了样本特定和样本不变的特征来学习更好的表示，既具有区分性，又具有可推广性。为了明确地解耦推理技能和视觉概念，我们分别为视觉和文本输入提取这些表示。本文提出的方法能够显著提高模型的性能，证明了所提出方法的有效性和组合性。
- (4):本文提出的方法在VQACL数据集上进行了广泛的实验，结果表明，我们的方法在连续学习和组合性方面都优于现有的方法。本文提出的方法能够显著提高模型的性能，证明了所提出方法的有效性和组合性。
#### 7. 方法详细介绍：
本文提出了一种名为VQACL的视觉问答持续学习方法。该方法利用样本特定和样本不变的特征进行有效的表示学习。样本特定特征是通过使用双向多模态编码器学习得到的，而样本不变特征是通过原型学习模块学习得到的，该模块为不同类型的问题和对象构建原型。文本解码器也是一堆变压器块，并且模型参数通过最小化标签文本令牌的负对数似然来训练。

具体步骤如下：
1. 提取输入图像和问题的视觉和文本嵌入。
2. 将视觉和文本嵌入输入到变压器编码器中，以捕获有吸引力和显着的内容。
3. 将输出特征用作视觉和文本的样本特定特征。
4. 原型学习模块学习和更新不同问题类型和对象类别的原型，这些原型用作视觉和文本的样本不变特征。
5. 将样本特定和样本不变特征组合并输入到文本解码器中以生成答案。
6. 使用负对数似然损失优化模型参数。

#### 8. 实验设置：
本文使用VQACL设置来评估所提出的模型。VQACL包括一个双层任务序列，用于处理多模态数据，其中外层设置具有不同问题类型的顺序语言驱动任务，内层构建具有不同对象类别的序列视觉驱动子任务。VQACL还包括一种新的组合测试过程，用于评估模型的推理技能和视觉概念的组合性。评估指标为最终平均性能（AP）和平均遗忘（Forget）。实验在两个标准数据集上进行：VQA v2和NExT-QA。

#### 9. 实验结果和分析：
本文提出的VQACL方法在VQA v2和NExT-QA数据集上均取得了最佳的泛化性能，对推理技能和视觉概念的新组合也有明显的改进。与其他基于记忆的方法相比，VQACL方法在遗忘方面表现更好，证明了其在VQA持续学习中的优越性。在新颖组合测试中，VQACL方法的改进要高于在已知组合测试中的改进，这表明该方法确实可以增强模型的组合泛化能力。在单模态任务的持续学习中，VQACL方法的表现不如DER和VS，这可能是由于不同持续学习设置之间的差异引起的。


# Paper:895     具有运动语义和几何残差感知的皮肤动作重定向



#### 1. Title: 
Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry

#### 2. Authors: 
Jiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli Huang, Xuefei Zhe, Linchao Bao, Ying Shan, Jue Wang, Zhigang Tu

#### 3. Affiliation: 
1. 武汉大学
2. 腾讯 AI Lab

#### 4. Keywords: 
Motion retargeting, neural network, motion semantics, character shapes, distance-based losses

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2021_paper.html  Github: https://github.com/Kebii/R2ET

#### 6. Summary : 
- (1):本文研究的是动作重定向问题，即将源角色的动作映射到目标角色上，同时保持动作的合理性。这是计算机视觉和计算机图形学领域的一个长期问题，也是数字化头像和元宇宙技术的基石。
 
- (2):过去的学习方法通常忽略了角色的形状几何，容易产生不真实的结果。本文提出了一种新的网络结构，即Residual RETargeting network (R2ET)，用于骨架和皮肤动作的重定向，考虑了动作语义和角色形状的差异。与以往的学习方法不同，本文的方法可以在单次推理中保留源角色的动作语义，并避免了穿透和缺失接触等问题。

- (3):本文提出了两个神经修改模块，即骨架感知模块和形状感知模块，用于逐步调整源动作以适应目标骨架和形状。这两个模块可以学习源动作的残差运动修改，以生成合理的重定向动作。为了平衡这两个修改，本文还提出了一个平衡门，用于在它们之间进行线性插值。本文的方法在Mixamo数据集上进行了广泛的实验，取得了最先进的性能。

- (4):本文的方法在Mixamo数据集上进行了广泛的实验，取得了最先进的性能。本文的方法可以在单次推理中保留源角色的动作语义，并避免了穿透和缺失接触等问题。本文的方法可以应用于动画和游戏行业，是数字化头像和元宇宙技术的基石。
#### 7. 方法详细介绍：
本文提出了一种名为R2ET的残差网络结构，用于骨骼动作重定向。该方法包括三个模块：骨骼感知模块、形状感知模块和平衡门。骨骼感知模块通过输入骨架配置来辅助源动作语义的转移，形状感知模块通过感知目标角色网格和经过动作语义保留调整后的骨架之间的兼容性，以避免穿透和缺失接触。整个过程在训练期间是可微分的。平衡门通过学习调整权重来在骨架级别和几何级别修改之间进行权衡。该方法使用Repulsive loss和Attractive loss以及平衡loss进行端到端训练。

#### 8. 实验设置：
本文在公共数据集Mixamo上进行了评估，该数据集包含24个不同形状的角色的2600个动作序列。数据集被分为训练集、验证集和测试集，比例为8:1:1。输入的动作序列经过根关节对齐和运动长度归一化的预处理。输出的动作序列通过逆运动学进行后处理，以确保关节角度限制和平滑性。

#### 9. 实验结果与分析：
本文的R2ET方法在Mixamo数据集上取得了最先进的性能，无论是定性还是定量。定量评估包括穿透和缺失接触指标，这些指标是通过帧百分比来衡量的。该方法在这些指标上的表现均优于现有的基于学习的方法。平衡门还提供了用户交互式的精细控制。用户研究表明，超过71.2%的用户更喜欢我们方法的重定向结果。


# Paper:896     无监督神经量化的解缠绕表示学习



#### 1. Title: 
Disentangled Representation Learning for Unsupervised Neural Quantization

#### 2. Authors: 
Haechan Noh, Sangeek Hyun, Woojin Jeong, Hanshin Lim, Jae-Pil Heo

#### 3. Affiliation: 
第一作者：韩国成均馆大学

#### 4. Keywords: 
Neural quantization, Inverted index, Disentangled representation learning, Approximate nearest neighbor search

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/papers/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2020_paper.pdf  Github: None

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉领域中的最近邻搜索问题，该问题在高维和大规模数据库中具有巨大的计算成本和内存开销。

- (2):过去的方法主要是基于浅层的量化方法，而深度学习的方法由于量化操作的不可微性而难以应用。本文提出了一种新的无监督神经量化方法，但是发现现有的深度学习量化器很难从残差向量空间中受益，与传统的浅层量化器不同。为了解决这个问题，本文提出了一种新的解缠绕表示学习方法，使得量化器能够更好地利用残差向量空间。

- (3):本文提出了一种新的解缠绕表示学习方法，通过将聚类中心的信息输入到编码器和解码器中，使编码器能够从潜在嵌入中去除聚类中心的信息，从而使得解缠绕表示学习与倒排索引相协调。实验结果表明，所提出的方法在大规模数据集上优于现有的最先进的检索系统。

- (4):本文所提出的方法在大规模数据集上进行了实验，结果表明，所提出的方法在非穷举搜索中优于现有的最先进的检索系统。
#### 7. 方法详细介绍：
本文提出了一种新的解缠结表示学习方法，用于无监督神经量化。该方法通过训练编码器和解码器来消除聚类中心的信息，从而将反向索引的信息与向量分离。该方法类似于残差向量空间的概念，通过取出聚类中心的信息来提供更紧凑的分布。具体而言，该方法采用了UNQ的网络结构，并添加了一个解缠结模块，将聚类中心的信息分别提供给编码器。编码器被训练为在潜在特征中嵌入尽可能少的聚类中心信息，以充分利用网络容量。解码器通过端到端的方式重构原始空间，以保留原始分布。

#### 8. 实验设置：
本文在大规模数据集上进行了实验，以验证所提出方法的有效性。实验涉及非穷举UNQ与反向索引，并将性能与最先进的检索系统进行比较。实验使用SIFT-1M数据集，并使用R@1、R@10和R@100指标评估检索性能。

#### 9. 实验结果与分析：
本文提出的方法在64位和128位编码的SIFT-1M和DEEP-1M数据集上均优于最先进的检索系统。该方法甚至在非穷举搜索中也优于穷举ANN方法，仅考虑5%的数据库进行距离计算。该方法在SIFT-1B数据集上的实验结果表明，该方法在穷举和非穷举搜索设置下均优于其他最先进的方法，包括OPQ、Catalyst+Lattice、LSQ和UNQ。该方法在Recall@1、Recall@10和Recall@100方面均取得了最佳性能。该方法对不同的编码位数具有鲁棒性，并在不同的位数设置下实现了一致的性能。


# Paper:897     STAR Loss：减少面部关键点检测中的语义模糊



#### 1. Title: 
STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection

#### 2. Authors: 
Zhenglin Zhou, Huaxia Li, Hong Liu, Nanyang Wang, Gang Yu, Rongrong Ji

#### 3. Affiliation: 
第一作者：厦门大学，多媒体可信感知与高效计算教育部重点实验室

#### 4. Keywords: 
Facial landmark detection, semantic ambiguity, deep learning, STAR loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_STAR_Loss_Reducing_Semantic_Ambiguity_in_Facial_Landmark_Detection_CVPR_2021_paper.html  Github: https://github.com/ZhenglinZhou/STAR

#### 6. Summary : 
- (1):本文研究的是面部关键点检测中的语义模糊问题，该问题会导致标注不一致，从而影响模型的收敛和预测性能。

- (2):过去的方法主要包括坐标回归和热图回归方法，但是这些方法都无法很好地处理语义模糊问题。本文提出了一种自适应模糊度降低方法，即STAR loss，通过利用语义模糊的特性，将预测分布用于表示语义模糊，并测量预测分布的各向异性，从而自适应地减轻语义模糊的影响。

- (3):本文的创新点在于提出了一种新的回归损失函数STAR loss，该损失函数将预测误差分解为两个主成分方向，并将其除以相应的能量值。对于具有各向异性预测分布的面部关键点，其第一主成分的能量高于第二主成分，因此可以自适应地抑制第一主成分方向上的误差，从而减轻语义模糊对训练的影响。此外，本文还提出了两种特征值限制方法，以避免STAR loss异常增加导致的过早收敛。

- (4):本文在三个基准测试集COFW、300W和WFLW上进行了全面的实验，结果表明STAR loss相对于现有方法具有更好的性能。
#### 7. 方法详细介绍：
本文提出了一种自适应模糊度降低方法STAR loss，用于减轻面部关键点检测中模糊标注的影响。该方法通过将预测误差分解为两个主成分方向并将其除以相应的能量值来实现。STAR loss属于自适应模糊度降低回归损失，通过模糊度引导的分解实现。该分解操作不影响误差度量，STAR loss可以与任意距离函数一起使用，并从它们的改进中受益，例如smooth-l1、Wing、Awing等。本文还提出了两种特征值限制方法，以避免STAR loss异常降低。第一种是损失限制项，即值限制，直接限制特征值的异常增加。第二种是detach限制，它切断了特征值和特征向量的反向传播，使它们在STAR loss中作为常量值。

#### 8. 实验设置：
本文使用了三个数据集COFW、300W和WFLW进行评估。在300W和WFLW数据集中使用眼间距进行归一化，在COFW数据集中使用瞳孔间距进行归一化。在WFLW数据集中，使用10%的FR和AUC阈值进行评估。在300W数据集上训练模型，并在300VW上进行交叉数据集验证。

#### 9. 实验结果和分析：
本文提出的STAR loss方法在三个数据集上均取得了最先进的性能，相比于14种现有方法，平均提高了0.34、0.19和0.25。实验结果表明，STAR loss对所有输入图像分辨率都有积极影响，并且对输入图像大小不太敏感。该方法还提高了面部关键点检测的稳定性和减少了语义模糊。与相关解决方案的比较表明，STAR loss更轻，性能更好，实现了显著的性能提升。


# Paper:898     弱监督视频定位的迭代提议细化



#### 1. Title: 
Iterative Proposal Refinement for Weakly-Supervised Video Grounding

#### 2. Authors: 
Meng Cao, Fangyun Wei, Can Xu, Xiubo Geng, Long Chen, Can Zhang, Yuexian Zou, Tao Shen, Daxin Jiang

#### 3. Affiliation: 
第一作者：北京大学电子与计算机工程学院

#### 4. Keywords: 
Weakly-Supervised Video Grounding, Proposal Refinement, Label Propagation, Video Retrieval, Multi-Instance Learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2021_paper.html  Github: https://github.com/mengcaopku/IRON

#### 6. Summary : 
- (1):本文研究的是弱监督视频定位问题，即在没有帧级别注释的情况下，根据查询句子定位视频中感兴趣的事件。这是一个具有广泛应用的问题，例如视频检索、视频问答、人机交互等。 
- (2):目前，大多数弱监督视频定位方法都采用两阶段流程，即首先生成潜在的时间段提议，然后使用这些提议进行定位。然而，现有的提议生成方法存在两个缺点：1）缺乏显式的对应关系建模；2）对于复杂事件的覆盖不完整。因此，本文提出了一种新的迭代提议细化网络（IRON），通过逐步将先验知识融入每个提议中，并鼓励具有更完整覆盖的提议，来解决这些问题。 
- (3):本文提出了一种显式的对应关系建模方法，包括语义级别和概念级别。此外，还设计了一种标签传播算法，通过迭代地细化提议置信度来缓解部分覆盖问题。 
- (4):在Charades-STA和ActivityNet Captions数据集上的实验结果表明，本文提出的IRON方法在弱监督视频定位任务上取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种迭代提议细化网络（IRON）用于弱监督视频定位。该方法由三个主要模块组成：视频编码器、语言编码器和融合变压器。视频编码器和语言编码器用于从视频和文本输入中提取特征。融合变压器用于融合视频和文本特征并生成一组提议。提议通过一系列损失进行迭代细化，包括语义损失、字幕损失和置信度矫正损失。细化后的提议通过基于MIL或重构的定位模块进行定位。

#### 8. 实验设置：
本文在两个基准数据集上评估了所提出的方法：ActivityNet Captions和Charades-STA。视频编码器使用在sport1M上预训练的C3D用于ActivityNet Captions，使用在Kinetics上预训练的I3D用于Charades-STA。语言编码器使用在英文维基百科和多伦多图书语料库上预训练的DistilBERT。字幕的最大长度设置为20，ActivityNet Captions和Charades-STA的词汇量分别为8,000和1,111。融合变压器的隐藏维度为256，注意力头数为4，层数为3。每个视频的提议数设置为8，细化次数设置为4，IoU阈值为0.6。损失的平衡权重为λcon = 5和λg = 2，对于基于重构的定位方法，λrank设置为0.1。使用Adam优化器进行训练，学习率为4×10−4，线性衰减学习率，梯度裁剪为1.0，批量大小为32，训练50个epochs。

#### 9. 实验结果和分析：
所提出的方法在两个数据集上均优于先前的最先进方法，且差距显著。基于重构的IRON在两个数据集上均优于基于MIL的IRON，ActivityNet Captions上的差距更大。Charades-STA上的最佳性能为CPL的R1@0.3为66.40％，而所提出的方法的R1@0.3为60.39％。在ActivityNet Captions上，所提出的方法的R1@0.5为49.24％，而最佳性能为VCA的R1@0.5为78.75％。


# Paper:899     关于Mixup在不确定性校准中的陷阱



#### 1. Title: 
On the Pitfall of Mixup for Uncertainty Calibration

#### 2. Authors: 
Deng-Bao Wang, Lanqing Li, Peilin Zhao, Pheng-Ann Heng, Min-Ling Zhang

#### 3. Affiliation: 
第一作者：东南大学计算机科学与工程学院
其他作者：腾讯AI实验室、浙江实验室、香港中文大学

#### 4. Keywords: 
Mixup, uncertainty calibration, deep learning, neural networks

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_On_the_Pitfall_of_Mixup_for_Uncertainty_Calibration_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了深度学习中的不确定性校准问题，提出了一种基于Mixup的方法，但发现Mixup训练通常会使模型的校准性变差，从而影响不确定性估计的准确性。
 
- (2):过去的方法主要集中在概率模型上，但这些模型的训练成本较高，而且由于实际部署的计算限制，它们的性能通常取决于近似推理方法。因此，确定性神经网络的不确定性校准成为近年来的研究热点。Mixup是一种简单有效的训练方法，但在一些情况下会降低模型的校准性能。本文通过将Mixup分解为数据变换和随机扰动两个部分，发现数据变换部分会缩小训练标签到它们的均值，并隐含地执行置信度惩罚，这是校准性能下降的原因。本文提出了一种名为Mixup Inference in Training的方法，通过将Mixup推理嵌入到训练中，从而避免了置信度惩罚的负面影响，同时提高了准确性和校准性能。

- (3):本文提出了一种名为Mixup Inference in Training的方法，通过将Mixup推理嵌入到训练中，从而避免了置信度惩罚的负面影响，同时提高了准确性和校准性能。该方法采用了一种简单的解耦原则，以恢复前向网络传递的原始样本输出。实验结果表明，该方法可以解决Mixup的校准问题，而且不会牺牲预测性能，甚至比Mixup更好。

- (4):本文的方法在多个数据集上进行了实验，结果表明Mixup通常会导致模型的校准性能变差，而Mixup Inference in Training方法可以解决这个问题，同时提高了准确性和校准性能。
#### 7. 方法详细介绍：
本文提出了一种名为Mixup Inference (MI)的方法，用于解决Mixup训练模型的校准问题。该方法将Mixup推断过程转化为训练过程，并使用解耦过程来恢复每个原始样本的输出。解耦后的输出与原始的one-hot标签进行拟合，以避免标签平滑所带来的置信度惩罚效应。该方法可以扩展到神经网络的隐藏层，并且可以嵌入任何隐藏层中，只需几行代码和可忽略的计算成本。具体步骤包括：
1. 在训练过程中，将输入数据进行Mixup变换。
2. 在输出层之前，进行解耦过程，将Mixup变换后的输出恢复为原始样本的输出。
3. 将解耦后的输出与原始的one-hot标签进行拟合，以避免标签平滑所带来的置信度惩罚效应。
4. 将解耦过程扩展到神经网络的隐藏层中，以提高模型的校准性能。

#### 8. 实验设置：
本文在多个数据集和不同的backbone上进行了实验，包括CIFAR-10、CIFAR-100、ImageNet等。使用的backbone包括ResNet、WideResNet等。在实验中，使用了uncalibrated ECE、calibrated ECE和optimal ECE等指标来评估模型的校准性能。同时，还比较了不同方法在准确率和校准性能上的表现。

#### 9. 实验结果与分析：
本文的实验结果表明，Mixup训练模型通常会使模型的校准性能降低，而且比普通的经验风险最小化方法更差。但是，使用Mixup Inference (MI)方法可以提高模型的校准性能和准确率。在多个数据集和backbone上的实验结果表明，MI方法在校准性能和准确率方面均优于其他方法。同时，本文还比较了MI方法和其他校准方法的表现，结果表明MI方法在校准性能和准确率方面均优于其他方法。


# Paper:900     鲁棒动态辐射场



#### 1. Title: 
Robust Dynamic Radiance Fields

#### 2. Authors: 
Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, Jia-Bin Huang

#### 3. Affiliation: 
Yu-Lun Liu, National Taiwan University

#### 4. Keywords: 
Dynamic view synthesis, radiance fields, camera poses, Structure from Motion (SfM), NeRF

#### 5. Paper: https://robust-dynrf.github.io/  Github: https://github.com/Robust-Dynamic-Radiance-Fields/Robust-Dynamic-Radiance-Fields

#### 6. Summary : 
- (1): This paper focuses on the problem of dynamic view synthesis, which aims to create photorealistic novel views of dynamic scenes from arbitrary camera angles and points of view.
 
- (2): Existing methods for dynamic radiance field reconstruction assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. However, SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. This paper proposes a robust approach that jointly estimates the static and dynamic radiance fields along with the camera parameters (poses and focal length) to address this issue. The approach includes a coarse-to-fine strategy and epipolar geometry to exclude moving pixels, deformation fields, time-dependent appearance models, and regularization losses for improved consistency.
 
- (3): The proposed approach, called RoDynRF, reconstructs dynamic radiance fields from casual videos without requiring known camera poses and camera intrinsics as input. The careful architecture designs and auxiliary losses improve the robustness of camera pose estimation and dynamic radiance field reconstruction. The approach is evaluated on multiple datasets, including Sintel, Dynamic View Synthesis, iPhone, and DAVIS, and shows visual comparisons with existing methods.
  
- (4): The RoDynRF approach achieves favorable performance over the state-of-the-art dynamic view synthesis methods on several challenging datasets that typical SfM systems fail to estimate camera poses. The approach is robust to many issues, such as noisy images from low-light conditions, motion blur caused by users, or dynamic objects in the scene, such as people, cars, and animals.
#### 7. 方法详细介绍：
本文提出了一种名为RoDynRF的方法，用于从随意拍摄的视频中重建动态辐射场。该方法不需要已知的相机姿态和相机内参作为输入。它优化相机姿态和两个辐射场，建模静态和动态元素。该方法包括粗到细的策略和极线几何来排除移动像素，变形场，时间依赖的外观模型以及正则化损失以提高一致性。所提出的谨慎的架构设计和辅助损失改善了相机姿态估计和动态辐射场重建的鲁棒性。具体步骤包括：
- 使用多平面表示重建静态辐射场
- 使用体素网格表示重建动态辐射场
- 使用粗到细的策略和后期视角条件进行相机姿态估计
- 使用辅助损失进行正则化

#### 8. 实验设置：
本文在多个数据集上评估了算法，包括Sintel、Dynamic View Synthesis、iPhone和DAVIS，并展示了与现有方法的视觉比较。作者通过广泛的定量和定性实验展示了他们方法的鲁棒性。

#### 9. 实验结果和分析：
本文提出了一种名为Robust Dynamic Radiance Fields的新方法，用于空间-时间合成随意捕获的单目视频，无需输入相机姿态。该方法在MPI Sintel数据集上进行了评估，并与现有的NeRF-based姿态估计方法进行了比较。结果表明，该方法的性能显著优于现有方法。本文还在NVIDIA动态视图合成数据集和iPhone数据集上进行了定量评估。该方法在与现有方法的比较中表现良好，并产生了质量更好的图像。本文还包括失败案例，以展示所提出方法的局限性。


# Paper:901     基于NeRF的深度立体匹配



#### 1. Title: 
NeRF-Supervised Deep Stereo

#### 2. Authors: 
Fabio Tosi, Alessio Tonioni, Daniele De Gregorio, Matteo Poggi

#### 3. Affiliation: 
第一作者：博洛尼亚大学

#### 4. Keywords: 
Stereo matching, self-supervised learning, neural rendering, zero-shot generalization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2021_paper.html  Github: https://github.com/nerfstereo/nerf-stereo

#### 6. Summary : 
- (1):本文研究的是深度立体匹配问题，其中深度学习方法需要大量标注数据，而自监督学习和合成数据方法仍有局限性。因此，本文提出了一种新的方法，利用神经渲染技术生成立体训练数据，从而实现无需标注数据的深度立体匹配训练。

- (2):过去的方法包括手工算法和深度学习方法，但需要大量标注数据。自监督学习和合成数据方法虽然可以减少标注数据的需求，但仍有局限性，如无法处理遮挡等问题。本文提出的方法利用神经渲染技术生成立体训练数据，从而实现无需标注数据的深度立体匹配训练。

- (3):本文提出的方法利用神经渲染技术生成立体训练数据，从而实现无需标注数据的深度立体匹配训练。具体而言，利用单个手持相机采集的图像序列，训练Neural Radiance Field (NeRF)模型，并利用其渲染出任意视角的立体图像。然后，利用渲染的图像三元组和NeRF生成的深度图作为代理标签，进行NeRF-Supervised训练。该方法可以在不需要标注数据的情况下训练深度立体匹配网络，并且可以有效地处理遮挡等问题。

- (4):本文的方法在Middlebury数据集上取得了30-40%的性能提升，填补了自监督方法和有监督方法之间的差距，并在零样本泛化方面取得了优异的结果。这表明本文提出的方法可以有效地训练深度立体匹配网络，并且可以在不需要标注数据的情况下实现零样本泛化。
#### 7. 方法详细介绍：
本文提出了一种新的方法，使用神经渲染和用户收集的图像序列来收集和生成立体训练数据。该方法涉及为每个序列训练一个神经辐射场（NeRF）模型，并使用它来渲染相同场景的任意新视图。通过从虚拟任意基线处渲染参考视图和右侧的目标视图来合成立体对。这允许通过利用流行的光度损失以自我监督的方式生成无数样本来训练任何立体网络。该方法还涉及为每个对生成第三个视图，放置在源视图的左侧，以补偿遮挡区域的缺失监督。通过NeRF渲染的深度作为代理监督来完成NeRF-Supervised训练。 

#### 8. 实验设置：
本文在具有挑战性的Middlebury数据集上评估了所提出的方法。作者收集了大约270个场景，使得他们的NeRF-Supervised立体网络在零样本泛化方面优于在合成数据集（如[2, 16, 30, 35, 36, 87]）和现有的自监督方法[3,79]上训练的模型。 

#### 9. 实验结果和分析：
本文提出的方法在Middlebury数据集上比现有的自监督方法提高了30-40％，填补了到监督模型的差距，并且大多数情况下在零样本泛化方面表现优异。实验结果显示，与不使用相同数据训练的真实深度的现有策略相比，具有前所未有的细节水平。本文还包括一张图，显示了使用所提出的方法在用户收集的图像上训练的RAFT-Stereo的零样本泛化结果，而不使用任何合成数据集，真实深度或真实立体对。


# Paper:902     泛化很重要：通过参数混合实现损失最小值平坦化的高效在线知识蒸馏



#### 1. Title: 
Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation

#### 2. Authors: 
Tianli Zhang, Mengqi Xue, Jiangtao Zhang, Haofei Zhang, Yu Wang, Lechao Cheng, Jie Song, Mingli Song

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Online knowledge distillation, parameter hybridization, generalization, loss landscape, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Generalization_Matters_Loss_Minima_Flattening_via_Parameter_Hybridization_for_Efficient_CVPR_2021_paper.html  Github: https://github.com/tianlizhang/OKDPH

#### 6. Summary : 
- (1):本文研究在线知识蒸馏（OKD）中的参数混合技术，以提高学生模型的泛化性能。本文的研究背景是深度学习中的知识蒸馏技术，旨在通过大规模数据训练的大型教师模型指导小型学生模型，以实现模型压缩和加速。
 
- (2):现有的OKD方法主要集中在设计复杂的模块以利用异构知识来提高学生的泛化能力，但它们缺乏对泛化的明确约束。本文提出了一种新的OKD框架，通过参数混合技术来促进损失函数的最小值平坦化，从而获得更加鲁棒的解决方案。与现有的OKD方法和寻求平坦最小值的SOTA方法相比，本文的方法在减少参数的同时实现了更高的性能，具有轻量级和鲁棒性的优点。

- (3):本文提出了一种新的OKD框架，称为在线知识蒸馏与参数混合（OKDPH），通过线性加权学生参数来构建混合权重模型（HWM），以表示涉及学生周围的参数。HWM的监督损失可以估计整个学生周围区域的曲率，从而明确地衡量泛化性能。因此，我们将HWM的损失集成到学生的训练中，并通过参数混合技术来促进平坦最小值的形成，从而获得更加鲁棒的解决方案。为了保持多模型参数的高相似性，我们进一步引入融合操作来限制学生之间的差异。 

- (4):本文在CIFAR-10和ImageNet数据集上进行了广泛的实验，证明了OKDPH可以显著提高学生的泛化性能，并超过了现有的OKD方法和寻求平坦最小值的SOTA方法。进一步的损失景观可视化和稳定性分析验证了我们的解决方案位于具有统一低损失的区域，并且对扰动和有限数据更加鲁棒。
#### 7. 方法详细介绍：
本文提出了一种名为OKDPH的在线知识蒸馏方法，它由参数混合和知识蒸馏两个主要组成部分构成。参数混合通过构建混合权重模型（HWM）来实现，该模型使用同学们的凸组合进行采样。在每个训练批次中，通过在参数空间中的同学周围的区域中采样点来创建HWM。知识蒸馏通过合成各种情况下的知识来实现，通过平均每个模型的输出logits并将整体知识蒸馏到每个学生模型中来蒸馏集合logits的知识。优化目标是最小化区域的曲率并使损失函数的曲面变平，增强学生的泛化能力。

#### 8. 实验设置：
本文在三个基准数据集CIFAR-10、CIFAR-100和ImageNet上进行了实验。将提出的OKDPH方法与几种最先进的方法进行了比较，包括vanilla online knowledge distillation（OKD）、mean teacher（MT）和self-ensembling（SE）。实验使用PyTorch在单个NVIDIA Tesla V100 GPU上进行。超参数设置如下：学生数量M设置为2或3，温度τ设置为4，融合间隔∆设置为5批次，融合比例γ设置为1。集中向量α设置为1，以逐渐增加从边界模型到重心的采样概率。

#### 9. 实验结果和分析：
实验结果表明，所提出的OKDPH方法在所有三个数据集上均优于最先进的方法。在CIFAR-10上，OKDPH实现了95.56%的测试准确率，比第二好的方法高0.47%。在CIFAR-100上，OKDPH实现了77.23%的测试准确率，比第二好的方法高1.05%。在ImageNet上，OKDPH实现了77.5%的top-1准确率，比第二好的方法高0.5%。损失曲面可视化和稳定性分析还表明，OKDPH可以使损失曲面变平并增强学生的泛化能力。参数敏感性和消融研究进一步证明了所提出方法的有效性。


# Paper:903     基于概率知识蒸馏的人脸集成模型



#### 1. Title: 
Probabilistic Knowledge Distillation of Face Ensembles

#### 2. Authors: 
Jianqing Xu, Shen Li, Ailin Deng, Miao Xiong, Jiaying Wu, Jiaxiang Wu, Shouhong Ding, Bryan Hooi

#### 3. Affiliation: 
Youtu Lab, Tencent (腾讯优图实验室)

#### 4. Keywords: 
Knowledge Distillation, Face Recognition, Ensemble Learning, Uncertainty Estimation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是人脸识别中的知识蒸馏，旨在通过从多个模型中提取知识来压缩模型并提高识别性能。

- (2):过去的方法都是基于单个教师模型的知识蒸馏，无法提供面部图像的不确定性估计。本文提出了一种基于概率建模的贝叶斯集成平均（BEA）方法，可以评估面部图像的不确定性，并将其分解为aleatoric uncertainty和epistemic uncertainty。同时，本文提出了一种新的学生模型BEA-KD，可以从BEA中提取知识，继承不确定性估计能力，同时减少计算成本。本文的方法在多个基准测试中均取得了优异的性能。

- (3):本文提出了一种基于概率建模的贝叶斯集成平均（BEA）方法，将多个模型的预测结果视为概率分布，从而可以评估面部图像的不确定性，并将其分解为aleatoric uncertainty和epistemic uncertainty。同时，本文提出了一种新的学生模型BEA-KD，可以从BEA中提取知识，继承不确定性估计能力，同时减少计算成本。

- (4):本文的方法在多个基准测试中均取得了优异的性能，比现有的知识蒸馏方法表现更好。同时，本文的方法可以评估面部图像的不确定性，可以用于风险受控的人脸识别，具有实际应用价值。
#### 7. 方法详细介绍：
本文提出了一种名为贝叶斯集成平均（BEA）的人脸识别方法。该方法通过使用共享的W来训练集合成员，实现了开放集识别中的特征对齐。然后，该方法使用概率方法更灵活地合并集合成员。集合成员被视为从后验分布中抽取的样本，从集合中获得的相应确定性嵌入被视为从隐式后验分布中抽取的样本。该方法使用修改后的平均项来考虑所有集合成员的期望位置和置信度，从而产生更强大的特征匹配的概率嵌入。该方法还能够将不确定性分解为aleatoric不确定性和epistemic不确定性，可用于检测faceness的超出分布。

#### 8. 实验设置：
使用WebFace260M进行训练，使用LFW、CFP-FP、CPLFW、IJB基准测试进行评估。使用8个Tesla V100 32GB GPU进行实验。所有模型都使用Residual Networks作为骨干网络和ArcFace作为损失函数进行预训练。

#### 9. 实验结果和分析：
提出的BEA-KD方法在CFPFP、CPLFW、IJB-B和IJB-C等四个人脸识别基准测试中优于集成和最先进的知识蒸馏方法。该方法还展示了捕捉aleatoric不确定性和epistemic不确定性的能力，为特征嵌入提供了不确定性度量。消融研究显示了所提出方法的每个组成部分的有效性。


# Paper:904     自然语言辅助手语识别



#### 1. Title: 
Natural Language-Assisted Sign Language Recognition

#### 2. Authors: 
Ronglai Zuo, Fangyun Wei, Brian Mak

#### 3. Affiliation: 
第一作者：香港科技大学

#### 4. Keywords: 
Sign language recognition, visually indistinguishable signs, natural language modeling, language-aware label smoothing, inter-modality mixup, video-keypoint network

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zuo_Natural_Language-Assisted_Sign_Language_Recognition_CVPR_2021_paper.html  Github: https://github.com/FangyunWei/SLRT

#### 6. Summary : 
- (1):本文研究的背景是手语识别，手语是聋人社区的主要交流语言，手语识别是将手语视频转化为手语标签的基础任务，具有广泛的应用前景。

- (2):过去的方法主要采用基于卷积神经网络的视觉特征提取，但是手语中存在大量的视觉相似但语义不同的手语，称为VISigns，这限制了视觉神经网络的识别能力。本文提出了自然语言辅助手语识别（NLA-SLR）框架，利用手语标签中包含的语义信息。具体地，对于具有相似语义含义的VISigns，提出了语言感知标签平滑方法，通过生成软标签来缓解训练。对于具有不同语义含义的VISigns，提出了一种跨模态混合技术，将视觉和手语特征混合以进一步最大化不同手语的可分性。本文的方法在三个广泛采用的基准测试中均取得了最先进的性能。

- (3):本文提出了一种新的骨干网络，视频关键点网络（VKNet），它不仅可以模拟RGB视频和人体关键点，还可以从不同时间接受域的手语视频中提取知识。此外，本文还提出了一种自然语言辅助的手语识别框架，利用手语标签中包含的语义信息，提出了语言感知标签平滑方法和跨模态混合技术，以提高手语识别的性能。

- (4):本文的方法在三个广泛采用的基准测试中均取得了最先进的性能，证明了本文方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种自然语言辅助手语识别（NLA-SLR）框架，包括数据预处理、视频关键点网络（VKNet）和头网络三个部分。数据预处理生成视频-关键点对作为网络输入。VKNet由VKNet-32和VKNet-64两个子网络组成，用于提取视觉特征。头网络包含语言感知标签平滑分支和跨模态混合分支。语言感知标签平滑生成软标签，其平滑权重是地面实况和手语词汇表中其余手语的语义相似度的归一化值。跨模态混合生成跨模态特征和相应的标签，以最大化潜在空间中手语的可分性。FC1和FC2之间的集成可以进一步提高SLR性能。

#### 8. 实验设置：
本文在三个公共手语识别数据集MSASL、WLASL和NMFs-CSL上评估了所提出的方法。VKNet-64/32中的S3D骨干网络首先在Kinetics-400上进行预训练。然后，VKNet-64/32中的视频和关键点编码器分别在SLR数据集上进行预训练。整个模型使用批大小为32进行100个epoch的训练，使用余弦退火调度和Adam优化器，权重衰减为1e-3，初始学习率为1e-3。数据增强包括空间裁剪和时间裁剪。

#### 9. 实验结果和分析：
本文提出的方法在MSASL、WLASL和NMFs-CSL数据集上均取得了最先进的性能。测试集上报告了每个实例和每个类别的准确率。结果表明，所提出的方法在所有三个数据集上均优于之前的方法，包括I3D、I3D+BLSTM、ST-GCN、BSL、TCK、HMA和SignBERT。所提出的方法还优于没有跨模态混合和语言感知标签平滑的基线方法。3-crop推理进一步提高了识别性能。


# Paper:905     MAGE：掩码生成编码器统一表示学习和图像合成



#### 1. Title: 
MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis

#### 2. Authors: 
Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, Dilip Krishnan

#### 3. Affiliation: 
第一作者：Tianhong Li，MIT CSAIL

#### 4. Keywords: 
Generative modeling, representation learning, masked image modeling, self-supervised learning, computer vision

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Li_MAGE_MAsked_Generative_Encoder_to_Unify_Representation_Learning_and_Image_CVPR_2022_paper.html
Github: https://github.com/LTH14/mage

#### 6. Summary: 
- (1):本文研究计算机视觉中的生成建模和表示学习两个关键任务，提出了一种新的框架MAGE，旨在将两个任务统一起来，以便相互促进。
- (2):过去的方法通常独立地进行生成建模和表示学习，忽略了两个任务相互帮助的潜力，导致训练和模型维护的开销。本文提出的MAGE框架使用可变的掩码比率进行掩码图像建模预训练，可以在同一训练框架下进行生成训练和表示学习。与以往的MIM方法不同，MAGE使用语义令牌作为输入和输出，从而提高了生成和表示学习的质量。 
- (3):本文提出的MAGE框架是第一个将SOTA图像生成和自监督表示学习统一起来的框架。通过使用变量掩码比率，MAGE可以在同一训练框架下进行生成训练和表示学习。此外，MAGE使用语义令牌作为输入和输出，从而提高了生成和表示学习的质量。在ImageNet-1K上，MAGE ViT-L模型在无条件图像生成任务中获得9.10 FID，线性探测的top-1准确率为78.9％，在图像生成和表示学习方面均达到了SOTA水平。 
- (4):本文提出的MAGE框架在ImageNet-1K上的实验结果表明，该方法在无条件图像生成和线性探测任务中均取得了SOTA的性能。MAGE ViT-L模型在无条件图像生成任务中获得9.10 FID，线性探测的top-1准确率为78.9％，在自监督表示学习方面也取得了SOTA的性能。这些结果表明，MAGE框架可以同时实现高质量的图像生成和表示学习，为计算机视觉领域的研究提供了新的思路和方法。
#### 7. 方法详细介绍：
本文提出了一种名为MAGE的方法，它使用变量掩码比率的单个基于令牌的MIM框架，将生成模型和表示学习统一起来。MAGE的关键是在MIM预训练期间使用可变掩码比率，以实现统一的架构，用于生成训练和表示学习。MAGE使用由向量量化GAN学习的语义令牌作为输入和输出，结合掩码。模型被训练以在广泛的掩码比率范围内进行重构，包括使生成能力成为可能的高掩码比率和使表示学习成为可能的低掩码比率。MAGE通过向编码器输出添加对比损失来进一步改进表示。具体而言，MAGE的方法包括以下步骤：
1. 使用预训练的VQGAN模型将输入图像标记为语义令牌。
2. 随机掩码一些输入令牌，然后使用编码器-解码器变换器架构对其余未掩码的令牌进行预测。
3. 添加重构交叉熵损失以鼓励模型重构掩码令牌。
4. 添加类似于SimCLR的对比损失以提高学习表示的可分离性。
5. 最终损失是重构和对比损失的组合。

#### 8. 实验设置：
本文在ImageNet-1K数据集上进行了实验，用于生成任务和表示学习。输入图像分辨率设置为256x256，使用强随机裁剪和调整大小（0.2到1）和随机翻转作为默认增强。本文使用AdamW优化器对ViT-B和ViT-L进行预训练，分别使用4096和2048的批量大小进行1600个时期的训练。使用余弦学习率调度进行80个时期的热身，并且基本学习率为1.5×10−4，进一步通过batchsize/256进行缩放。

#### 9. 实验结果和分析：
本文展示了MAGE在生成任务和表示学习方面均优于以前的方法。本文使用线性探测、微调、少样本学习和迁移学习等方法评估了学习表示的质量。本文还提供了有关用于图像生成的迭代解码策略的详细信息。此外，本文还分析了MAGE的两个关键组成部分：可变掩码比率和量化标记化。结果表明，可变掩码比率是实现生成和表示学习的必要条件，并且使用可变掩码比率还可以使表示学习学习更好的特征并实现更好的线性探测性能。量化步骤是学习良好表示的必要条件。


# Paper:906     BioNet：一种生物启发式的人脸识别网络



#### 1. Title: 
BioNet: A Biologically-inspired Network for Face Recognition

#### 2. Authors: 
Pengyu Li

#### 3. Affiliation: 
Terminus Labs, Terminus Group, 中国.

#### 4. Keywords: 
Face recognition, Biologically-inspired Network, Convolutional Neural Network, Neuroscience, Inferotemporal Cortex Network

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Li_BioNet_A_Biologically-Inspired_Network_for_Face_Recognition_CVPR_2021_paper.html  Github: https://github.com/pengyuLPY/BioNet.git 

#### 6. Summary : 
- (1):本文研究背景是神经科学和人工智能之间的关系，尤其是如何将神经科学的最新发现应用于人工智能领域。
- (2):过去的人脸识别方法大多数没有考虑到人脑的生物特征，因此性能有限。本文提出了一种生物启发式的人脸识别网络，模拟了人脑的生物特征，能够有效地提高人脸识别性能。
- (3):本文提出的生物启发式网络名为BioNet，由两个级联子网络组成，即视觉皮层网络（VCN）和颞下皮层网络（ICN）。VCN采用经典的卷积神经网络骨干，而ICN则由三个生物启发式模块组成，即皮层功能分区、分区响应变换和响应强度调制。这些模块模拟了人脑的生物特征，能够有效地提高人脸识别性能。
- (4):BioNet在标准的人脸识别基准数据集上取得了显著的性能提升，例如将IJB-B@1e-6从52.12%提高到68.28%，将MegaFace从98.74%提高到99.19%。实验结果表明，生物启发式网络能够更好地模拟人脑的生物特征，从而提高人脸识别性能。
#### 7. 方法详细介绍：
本文提出的BioNet将基于卷积神经网络（CNN）的模型与最新的神经科学研究相结合。它由两个级联网络组成，即视觉皮层网络和颞下皮层网络。视觉皮层网络（VCN）直接采用CNN骨干模型建模。颞下皮层网络（ICN）采用三个相互依存的生物启发式模块，即CFC、CRT和RIM，基于人类面部识别系统的三个基本生物特征构成ICN。本文提出的模块包括：1）第3.1节中的CFC基于注意力机制，通过面部刺激将VCN的特征图分隔成不同的功能区域。2）第3.2节中的CRT通过多层感知器（MLP）将区域内的特征转换为后继神经元，直接用于人脸识别。3）第3.3节中引入的RIM借鉴了集成机制，通过自适应权重融合区域间的特征，以获得最终的识别表示。

#### 8. 实验设置：
本文使用的训练数据集是MS-Celeb-1M数据集，其中包含84,284个身份和480万张图像。评估数据集包括IJB-A、IJB-B、IJB-C和MegaFace数据集。

#### 9. 实验结果与分析：
实验表明，本文提出的BioNet显著提高了深度人脸识别的性能。实验支持了最新的神经科学研究可以推动基于CNN的人脸识别进展的想法。本文提出的三个生物启发式模块在ICN中是不可或缺的。观察到的属性对FR任务的影响得分因任务而异，证明了不同属性对FR的贡献不同但是是积极的。如果属性数量相等，则观察到的属性可以比潜在属性表现更好。增加潜在属性的数量几乎没有成本，这使得通过增加数量来提高性能变得容易。BioNet适用于不同的CNN骨干模型，例如ResNet-101和CASIA-Net。


# Paper:907     DIFu：基于深度引导的隐式函数用于服装人体重建



#### 1. Title: 
DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction

#### 2. Authors: 
Dae-Young Song, HeeKyung Lee, Jeongil Seo, Donghyeon Cho

#### 3. Affiliation: 
第一作者：Electronics and Telecommunications Research Institute, Daejeon, South Korea

#### 4. Keywords: 
Clothed human reconstruction, implicit function, depth-guided, 3D volume, occupancy prediction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2021_paper.html  Github: https://eadcat.github.io/DIFu/

#### 6. Summary : 
- (1):本文研究的背景是单张图像下的服装人体重建，该问题是高度不透明的，因为单张图像无法提供足够的信息来重建完整的3D模型。

- (2):现有的方法主要依赖于3D嵌入分支，使用体积（如SMPL模型）来补偿单张图像中的信息缺失。然而，这些方法在细节区域（如服装和头发）中往往会产生过度平滑的结果。本文提出了一种新的基于隐式函数的方法，DIFu，它利用了包含纹理和非参数人体3D信息的投影深度先验。

- (3):DIFu由生成器、占用预测网络和纹理预测网络组成。生成器将人体正面的RGB图像作为输入，并生成人体背面图像。然后，估计前/后图像的深度图并将其投影到3D体积空间中。最后，占用预测网络通过2D编码器和3D编码器提取像素对齐特征和体素对齐特征，并使用这些特征估计占用。纹理推断分支估计每个查询点的颜色。与现有方法相比，DIFu使用投影深度提供了更详细的表面信息，因此可以更好地传达目标的3D信息。

- (4):在多个数据集上的实验结果表明，DIFu在定量和定性上都优于现有的IF-based模型。
#### 7. 方法详细介绍：
本文提出了一种名为DIFu的方法，用于从单个RGB图像中重建穿着衣服的人体。该方法包括三个网络：深度估计器、纹理预测器和幻觉器。深度估计器从单个RGB图像预测人体的3D占用场。纹理预测器从预测的3D占用场推断出人体的颜色和纹理。幻觉器从前面的图像生成人体的背面图像，为纹理预测器提供额外的信息。该方法还包括一个体素嵌入网络，将SMPL参数编码为3D体素网格，以获得更好的特征表示。该方法使用回归损失、尺度损失和对抗损失的组合进行训练。

#### 8. 实验设置：
该方法在两个数据集THuman2.0和BUFF上进行评估。THuman2.0包含526个高质量的3D穿着衣服的人体扫描，BUFF包含143个人体扫描。数据集被分为训练集和评估集。评估指标包括点到表面距离、Chamfer距离、法线误差、均方误差和LPIPS。该方法与PIFu、PaMIR和ICON等最先进的方法进行比较。

#### 9. 实验结果和分析：
实验结果表明，DIFu在THuman2.0中的表现优于其他现有方法，并在BUFF中实现了最佳性能。该方法产生了更合理的结果，特别是在不可观察部分的形状方面。消融研究表明，使用对抗损失和数据增强训练的幻觉器实现了最佳性能。体素嵌入网络也有助于将深度估计器推广到未见数据。


# Paper:908     TempSAL - 揭示深度显著性预测的时间信息



#### 1. Title: 
TempSAL - Uncovering Temporal Information for Deep Saliency Prediction

#### 2. Authors: 
Bahar Aydemir, Ludo Hoffstetter, Tong Zhang, Mathieu Salzmann, Sabine S¨usstrunk

#### 3. Affiliation: 
第一作者：瑞士洛桑联邦理工学院计算机与通信科学学院

#### 4. Keywords: 
Saliency prediction, deep learning, temporal information, attention, computer vision

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2019/papers/Aydemir_TempSAL_-_Uncovering_Temporal_Information_for_Deep_Saliency_Prediction_CVPR_2019_paper.pdf  Github: https://github.com/IVRL/TempSAL

#### 6. Summary : 
- (1):本文研究的背景是计算机视觉中的显著性预测，即如何预测人眼在观察图像时的注意力焦点。
 
- (2):过去的方法主要是基于深度学习的模型，但是这些模型没有考虑到人眼在观察图像时的时间演变，即人眼的注意力焦点是随着时间变化的。本文提出了一种新的模型，可以利用人眼的时间演变模式来预测显著性图，并且在现有的数据集上取得了更好的表现。

- (3):本文提出的方法是一种基于深度学习的模型，可以同时预测图像的显著性和时间演变模式。具体来说，该模型可以学习每个时间点的显著性图，并将它们组合起来以获得时间演变模式。此外，本文还提出了一种时空混合模块，可以从时间显著性图中学习时间依赖模式，从而更好地预测图像的显著性。本文的创新点在于将时间演变模式引入到显著性预测中，并且提出了一种新的模型来利用这种模式。

- (4):本文的方法在SALICON和CodeCharts1k数据集上进行了实验，取得了比现有方法更好的表现。这表明本文提出的方法可以更好地预测图像的显著性，并且可以利用时间演变模式来提高预测的准确性。本文的方法可以应用于图像压缩、图像增强、图像重定向、渲染和分割等领域。
#### 7. 方法详细介绍：
本文提出了一种名为TempSAL的框架，利用时间人类注意力信息进行显著性预测。该框架由三个主要组件组成：图像编码器和两个显著性解码器（时间和图像），以及一个时空混合模块。图像编码器使用预训练的对象识别编码器在不同层次提取图像特征。时间显著性解码器通过四个3x3卷积层和ReLU函数处理编码器块，每个卷积后集成一个编码器块。图像显著性解码器与时间显著性解码器具有相同的结构，产生单个映射SI，对应于输入图像的传统图像显著性映射。时空混合模块将时间显著性预测、初始图像显著性预测和编码图像特征块作为输入。它消除了在空间和时间映射之间优化权重参数的需要，并在卷积的空间范围内调制映射，允许从不同映射中选择不同区域。

#### 8. 实验设置：
本文使用SALICON基准数据集评估了所提出的TempSAL方法的性能。作者使用批量大小为32和初始学习率为1e-4，每两个时期降低一次十倍。首先训练时间分支，然后在训练时空混合模块之前冻结权重。作者发现在SALICON上进行10个时期的训练就足够了。使用提供的测试、训练和验证拆分进行评估。

#### 9. 实验结果和分析：
本文提出了一种显著性预测方法TempSAL，可以学习时间特定的预测并利用时间信息来改善整体图像显著性预测。实验表明，人类注意力中的时间演变模式在自然图像的显著性预测中起着重要作用。实验结果表明，TempSAL方法优于现有技术，包括利用累积时间显著性映射的多时间段方法。准确度指标表明，时空混合模块有效地调制空间和时间显著性映射以改进初始图像显著性预测。TempSAL模型在不同时间片上始终优于基线模型。


# Paper:909     关于IRLS及其变体在离群值鲁棒估计中的收敛性



#### 1. Title: 
On the Convergence of IRLS and Its Variants in Outlier-Robust Estimation

#### 2. Authors: 
Liangzu Peng, Christian Kümmerle, René Vidal

#### 3. Affiliation: 
Liangzu Peng: Johns Hopkins University (约翰霍普金斯大学)

#### 4. Keywords: 
Outlier-robust estimation, iteratively reweighted least-squares (IRLS), graduated non-convexity (GNC), ℓp-loss, TLS loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2023/html/Peng_On_the_Convergence_of_IRLS_and_Its_Variants_in_Outlier-Robust_CVPR_2023_paper.html  Github: https://github.com/liangzu/IRLS-CVPR2023

#### 6. Summary : 
- (1):本文研究的是在存在离群值的情况下，从数据样本中估计某些参数（例如3D旋转）的问题，通常被制定为非凸和非光滑问题。
- (2):过去的方法包括IRLS和其变体，但是这些方法在处理 ℓp-loss 和 TLS loss 时存在一些问题。本文提出了一种基于GNC的IRLS变体，证明了其在离群值鲁棒估计中是一种收敛方法。实验结果表明，该方法在典型的离群值鲁棒估计问题实例中可以在5-10次迭代内收敛，而现有的最先进方法需要至少30次迭代。
- (3):本文的创新点在于将GNC引入IRLS框架中，提出了一种新的IRLS变体，并证明了其在离群值鲁棒估计中的收敛性。此外，本文还证明了在具有恒定离群值分数的鲁棒回归上，该IRLS变体在具有高概率的随机高斯特征矩阵上以全局线性和局部二次速率收敛到地面真实值。
- (4):本文的方法在离群值鲁棒估计任务上取得了很好的性能，实验结果表明，该方法可以在5-10次迭代内收敛，而现有的最先进方法需要至少30次迭代。
#### 7. 方法详细介绍：
本文提出了一种名为GNC-IRLSp的迭代重加权最小二乘（IRLS）算法变体，用于异常值鲁棒估计。该算法使用广义Nesterov坐标下降（GNC）调度来更新每次迭代中的权重。算法具有两个收敛阶段，首先是全局线性，然后是局部二次。本文还讨论了由于大的初始化值引起的燃烧期现象，并提出了一个毕业率保证，以从全局线性转换为局部二次收敛。此外，本文提出了一种适当的初始化值选择，使得从第一次迭代开始就可以实现二次收敛。本文证明了所提出算法的收敛保证，并提供了实现细节。

#### 8. 实验设置：
本文在点云配准应用中进行实验。数据由随机采样的异常值和内点点对组成，其中内点对是通过将高斯噪声添加到真实旋转和平移值生成的。本文设置内点阈值，并将平均内点残差作为评估算法性能的指标。

#### 9. 实验结果与分析：
本文将提出的GNC-IRLSp算法与GNC-TLS算法进行比较，并报告不同异常值率（从10％到90％）的平均内点残差作为指标。本文显示GNC-IRLS0和GNC-TLS在不同异常值率下具有几乎相同的平均内点残差。本文还显示，通过适当选择初始化值，GNC-IRLS0可以从第一次迭代开始实现二次收敛。


# Paper:910     Ham2Pose：将手语符号转换为姿势序列的动画



#### 1. Title: 
Ham2Pose: Animating Sign Language Notation into Pose Sequences

#### 2. Authors: 
Rotem Shalev Arkushin, Amit Moryossef, Ohad Fried

#### 3. Affiliation: 
Rotem Shalev Arkushin: Reichman University (以色列瑞克曼大学)

#### 4. Keywords: 
Sign language, HamNoSys, pose sequences, transformer encoders, weak supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shalev_Arkush_Ham2Pose_Animating_Sign_Language_Notation_into_Pose_Sequences_CVPR_2021_paper.html  Github: https://github.com/Rotem-Shalev/ham-to-pose

#### 6. Summary : 
- (1):本文研究背景是将口语翻译成手语，以实现聋人和听力障碍人士与听力正常人之间的开放交流。
- (2):过去的方法主要是将口语翻译成手语符号，然后再将符号转换成手语视频。本文提出了一种新方法，将HamNoSys符号转换成手语姿势序列，从而实现了通用解决方案。过去的手语符号动画方法存在动作不自然、难以理解等问题，而本文提出的方法通过transformer编码器逐步生成姿势预测，考虑了空间和时间信息，成功地从不完整和不准确的数据中学习。
- (3):本文提出的研究方法是使用弱监督训练模型，从多个HamNoSys-to-video数据集中提取姿势关键点，然后使用pose特征训练模型，逐步生成所需的姿势序列。此外，本文提出了一种新的距离度量方法，使用DTW-MJE测量姿势序列之间的距离，考虑了缺失关键点的情况。本文的主要贡献是提出了将HamNoSys符号转换成手语姿势序列的方法，并提供了一种新的姿势序列距离度量方法。
- (4):本文的方法在AUTSL数据集上进行了验证，证明了其测量姿势序列距离的准确性，并用它来评估生成的姿势序列的质量。本文的方法在手语姿势序列生成任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种名为Ham2Pose的方法，用于将手语符号转换为姿势序列。该方法使用了文本到姿势生成的方法，输入为一系列HamNoSys符号和参考姿势帧。该方法使用了Transformer编码器和逐步细化的姿势生成器。具体步骤如下：
1. 使用OpenPose模型从视频中提取姿势关键点。
2. 将HamNoSys文本进行分词、嵌入和编码。
3. 使用文本处理器对编码后的文本进行序列长度预测。
4. 使用编码后的文本和参考姿势帧，逐步细化姿势序列，生成所需的姿势。

#### 8. 实验设置：
本文使用了来自DGS Corpus、Dicta-Sign和波兰手语词典的5,754个手语符号视频和其HamNoSys转录。数据集包含四种手语，由14名手语者签名：波兰手语（PJM）：2,560个手语符号，2名手语者；德国手语（DGS）：1,926个手语符号，8名手语者；希腊手语（GSL）：887个手语符号，2名手语者；法国手语（LSF）：381个手语符号，2名手语者。数据集使用OpenPose模型提取姿势关键点。

#### 9. 实验结果和分析：
本文展示了所提出方法的定性结果，展示了不同手语语言中各种手语的生成姿势序列。结果表明，即使地面真实姿势数据存在缺失和不准确的关键点，该模型仍能够生成准确且平滑的姿势序列。本文还讨论了未来的研究方向，例如通过自动注释视频为HamNoSys来创建更多标记数据，以增强模型的能力。


# Paper:911     基于知识图谱增强Transformer的视频字幕生成



#### 1. Title: 
Text with Knowledge Graph Augmented Transformer for Video Captioning

#### 2. Authors: 
Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, Longyin Wen

#### 3. Affiliation: 
第一作者：中国科学院大学
其他作者：字节跳动公司、中国科学院软件研究所

#### 4. Keywords: 
Video captioning, knowledge graph, transformer, long-tail words challenge, multi-modality information

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gu_Text_With_Knowledge_Graph_Augmented_Transformer_for_Video_Captioning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是视频字幕生成，旨在通过自然语言描述视频内容。虽然已经取得了一定的进展，但在实际应用中仍有很大的提升空间，主要是由于长尾词的挑战。
 
- (2):过去的方法需要大量的配对视频和描述数据进行模型训练，而且大多数现有方法尝试设计强大的神经网络，以便让网络学习视频外观和描述之间的关系。然而，对于网络来准确预测训练数据中很少或从未出现的对象、属性或行为是非常困难的。一些方法尝试使用知识图谱来利用对象之间的关系来解决图像或视频字幕中的长尾挑战，取得了良好的结果。本文提出了一种文本与知识图谱增强的Transformer（TextKG）用于视频字幕生成，旨在通过知识图谱和多模态信息来缓解长尾词的挑战。

- (3):本文提出了一种两流Transformer，由外部流和内部流组成。外部流旨在吸收附加知识，通过模拟预先构建的知识图谱与视频内置信息之间的交互，如显著对象区域、语音转录和视频字幕，来缓解长尾词的挑战。同时，内部流旨在利用视频中的多模态信息（如视频帧的外观、语音转录和视频字幕）来确保字幕结果的质量。此外，两个流之间还使用交叉注意机制来共享信息。通过这种方式，两个流可以相互帮助以获得更准确的结果。

- (4):在四个具有挑战性的视频字幕数据集（YouCookII、ActivityNet Captions、MSR-VTT和MSVD）上进行了广泛的实验，证明了所提出的方法相对于现有方法具有优越性。特别是，所提出的TextKG方法在YouCookII数据集上的绝对CIDEr分数提高了18.7%。本文提出的方法在视频字幕生成任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于知识图谱的文本增强变压器(TextKG)方法，用于缓解视频字幕中的长尾词挑战。该方法采用两个流，一个外部流和一个内部流，以模拟多模态信息之间的交互。外部流接受检测到的对象的特征嵌入、检索到的先前知识、语音转录和预测的视频字幕，而内部流接受语音转录、预测的视频字幕和视频帧的特征嵌入。该方法还使用多模态自注意力和交叉注意力模块来对齐不同模态的标记。检测到的显著对象和视频帧的外观嵌入、检索到的先前知识、语音转录和预测的字幕的嵌入被馈送到两个流中的变压器，以在标记化后生成后续字幕。该方法还构建了一个知识图谱来保留先前的知识，包括一个通用的知识图谱和每个数据集的特定知识图谱。该方法使用知识标记和视频转录之间的余弦相似度来排名检索到的知识，并保留具有最高相似度分数的Nk个知识项用于视频字幕。该方法使用交叉熵损失函数来指导模型的训练，并使用Adam优化算法进行训练。

#### 8. 实验设置：
本文使用了MSR-VTT和MSVD数据集进行训练、验证和测试，并使用了BLEU@4、METEOR、CIDEr、ROUGE和Rep@4等评估指标进行比较。实现细节包括使用预训练模型提取视频特征、使用NLTK工具包分割单词以及使用Pytorch进行实现。

#### 9. 实验结果和分析：
本文提出的基于知识图谱的文本增强变压器(TextKG)方法在YouCookII、MSVD、MSR-VTT和DiDeMo等四个具有挑战性的数据集上进行了评估。实验结果表明，该方法在缓解长尾词挑战和提高视频字幕的准确性方面具有有效性。该方法在各种评估指标（包括BLEU、METEOR、CIDEr和ROUGE）方面在所有四个数据集上均取得了最先进的性能。消融实验也验证了所提出方法中不同模块的有效性，例如知识选择机制和语音转录和区域特征的集成。


# Paper:912     基于类原型的对比学习用于多标签和细粒度教育视频分类



#### 1. Title: 
Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos

#### 2. Authors: 
Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah

#### 3. Affiliation: 
第一作者：Rohit Gupta，来自美国中佛罗里达大学计算机视觉研究中心

#### 4. Keywords: 
Educational videos, multi-label classification, fine-grained classification, contrastive learning, class prototypes

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文的研究背景是儿童在早期阶段通过在线媒体消费的增长，需要数据驱动的工具来帮助教育工作者过滤适合年幼学习者的教育内容。本文旨在检测在线视频中的教育内容，重点关注两个广泛使用的教育内容类别：文学和数学。

- (2):过去的方法主要集中在单标签分类上，而本文的任务是多标签和细粒度分类，因此需要解决标签之间的相似性问题。本文提出了一种基于类原型的监督对比学习方法，可以处理与多个标签相关的细粒度样本。本文的方法在多个基准测试中均优于强基线，并提出了一个新的数据集APPROVE。

- (3):本文提出的方法是基于类原型的监督对比学习方法，可以处理与多个标签相关的细粒度样本。本文的方法采用了多模态变压器网络来捕捉视频中视觉和音频线索之间的交互，并通过对比损失来学习视频的嵌入。本文的创新点在于提出了一种新的监督对比学习方法，可以处理多标签和细粒度分类问题。

- (4):本文的方法在多个基准测试中均优于强基线，并提出了一个新的数据集APPROVE。本文的方法在多标签和细粒度分类任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于类原型和对比学习的多标签分类框架，用于对细粒度教育视频进行分类。该框架由两个主要组件组成：（1）基于对比学习的特征学习模块，用于学习每种模态的判别性表示；（2）多模态融合变压器，用于融合来自不同模态的学习表示。特征学习模块使用多标签对比损失来学习捕捉类间关系的类原型，并使用多模态变压器网络来融合来自不同模态的学习表示。多模态融合变压器由三个组件组成：图像编码器、文本编码器和融合编码器，用于分别学习视觉、文本和融合表示。融合编码器通过利用帧和单词嵌入之间的跨模态注意力来融合视觉和文本线索。类原型通过指数移动平均法进行学习迭代更新，以避免在训练迭代中折叠原型。

具体步骤如下：
1. 对比学习的特征学习模块：使用多标签对比损失来学习类原型，捕捉类间关系，并使用多模态变压器网络来融合来自不同模态的学习表示。
2. 多模态融合变压器：由图像编码器、文本编码器和融合编码器组成，用于分别学习视觉、文本和融合表示。融合编码器通过跨模态注意力来融合视觉和文本线索。
3. 类原型更新：使用指数移动平均法进行学习迭代更新，以避免在训练迭代中折叠原型。

#### 8. 实验设置：
本文在三个数据集上进行了评估：APPROVE、Youtube-8M的一个子集和COIN。多模态融合的训练策略包括两个阶段：在初始的单模态训练阶段，使用每种模态的固定原型来对齐表示。然后在第二阶段，对单模态编码器和多模态融合编码器进行端到端训练。第一阶段学习的跨模态对齐有助于提高多模态表示的学习。图像编码器使用torchvision中的Random Resized Crop和RandAugment数据增强。文本编码器使用Whisper，一个开源的ASR模型，从视频中生成文本。对于文本数据，使用nlpaug库进行后翻译，生成ASR文本的四个版本。还使用了同义词替换、文本跨度删除和随机单词交换增强。优化器使用AdamW优化器，学习率为0.0005。每10步使用指数移动平均法进行模型参数的更新，衰减率为0.999。

#### 9. 实验结果和分析：
本文提出的方法在多个数据集上均优于强基线方法，包括新提出的APPROVE数据集。在APPROVE数据集上，该方法相对于R@80和AUPR分别优于强基线方法3.1%和2.3%。在APPROVE数据集的Math子集上，结果比Literacy子集更高。在YT-46K数据集上，该方法相对于基线方法具有最高的R@80和LRAP得分。消融实验结果表明，学习类原型、多模态融合模块和视觉、文本编码框架的选择对结果有影响。鲁棒性分析表明，该方法对噪声模态和类原型的随机初始化具有鲁棒性。 

#### 论文总结：
本文提出了一种基于类原型和对比学习的方法，用于检测在线视频中的教育内容。该方法针对细粒度多标签视频分类任务，采用多模态变压器网络来融合视觉和音频线索。该方法在多个数据集上均优于强基线方法，包括新提出的APPROVE数据集。消融实验结果表明，学习类原型、多模态融合模块和视觉、文本编码框架的选择对结果有影响。鲁棒性分析表明，该方法对噪声模态和类原型的随机初始化具有鲁棒性。


# Paper:913     FAC: 通过前景感知特征对比进行3D表示学习



#### 1. Title: 
FAC: 3D Representation Learning via Foreground Aware Feature Contrast

#### 2. Authors: 
Kangcheng Liu, Aoran Xiao, Xiaoqin Zhang, Shijian Lu, Ling Shao

#### 3. Affiliation: 
第一作者所属机构：南洋理工大学

#### 4. Keywords: 
Contrastive learning, 3D representation learning, point cloud, foreground awareness, feature contrast

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_FAC_3D_Representation_Learning_via_Foreground_Aware_Feature_Contrast_CVPR_2021_paper.html  Github: https://github.com/Kangcheng95/FAC-3D-Representation-Learning-via-Foreground-Aware-Feature-Contrast

#### 6. Summary : 
- (1):本文研究背景是3D场景理解任务中的无监督预训练，旨在缓解大规模标注3D数据的困难。
- (2):过去的方法中，点特征随机选择作为锚点建立对比，导致对3D场景中占主导地位的背景点存在偏差。同时，忽略了物体感知和前景-背景区分，使对比学习效果不佳。本文提出了一种前景感知特征对比（FAC）框架，以更好地学习点云表示。FAC包括两种新的对比设计，以构建更有效和信息丰富的对比对。第一种是在相同前景段内构建正对比对，其中点倾向于具有相同的语义。第二种是我们通过Siamese对应网络中的自适应特征学习，防止3D段/对象之间的过度区分，并在段级别上鼓励前景-背景区分，从而有效地跨点云视图自适应学习特征相关性。 
- (3):本文提出了一种前景感知特征对比（FAC）框架，以更好地学习点云表示。FAC包括两种新的对比设计，以构建更有效和信息丰富的对比对。第一种是在相同前景段内构建正对比对，其中点倾向于具有相同的语义。第二种是我们通过Siamese对应网络中的自适应特征学习，防止3D段/对象之间的过度区分，并在段级别上鼓励前景-背景区分，从而有效地跨点云视图自适应学习特征相关性。 
- (4):本文在多个公共基准测试中进行了广泛的实验，结果表明FAC在各种下游3D语义分割和物体检测任务中实现了卓越的知识转移和数据效率。FAC与SparseConv和PV-RCNN等3D分割骨干网络以及PointPillars和PointRCNN等3D检测骨干网络兼容。它也适用于室内密集RGB-D和室外稀疏LiDAR点云。
#### 7. 方法详细介绍：
本文提出了一种基于自监督预训练的前景感知特征对比（FAC）框架，用于三维表示学习。该框架包括四个组件：数据增强、骨干网络特征提取、特征匹配和前景-背景感知特征对比优化。FAC框架包括两种新颖的对比设计，以构建更有效和信息丰富的对比对：区域分组对比和前景-背景对比。前者在同一前景分割内构建正对比对，其中点倾向于具有相同的语义，而后者通过自适应特征学习在Siamese对应网络中增强前景-背景点特征区分，并鼓励在分段级别上前景-背景区分。该方法与流行的3D分割骨干网络SparseConv和3D检测骨干网络（包括PV-RCNN、PointPillars和PointRCNN）兼容。

#### 8. 实验设置：
本文在两个数据集ScanNet和KITTI上评估了所提出的方法，用于两个下游任务：语义分割和物体检测。预训练数据集使用的是S3DIS。实验在单个NVIDIA Tesla V100 GPU上进行，具有16GB内存。批量大小设置为16，学习率设置为0.001。模型训练200个epochs，总共120万次迭代。

#### 9. 实验结果和分析：
所提出的方法FAC在数据效率和迁移学习能力方面优于现有方法CSC和ProCo。该方法在ScanNet和KITTI数据集上的语义分割和物体检测任务上均取得了最先进的性能。本文提供了详细的定量和定性结果来支持这些结果。

#### 论文总结：
本文提出了一种前景感知特征对比（FAC）框架，用于三维表示学习。该框架包括四个组件：数据增强、骨干网络特征提取、特征匹配和前景-背景感知特征对比优化。FAC框架包括两种新颖的对比设计，以构建更有效和信息丰富的对比对：区域分组对比和前景-背景对比。实验结果表明，所提出的方法在数据效率和迁移学习能力方面优于现有方法CSC和ProCo，并在ScanNet和KITTI数据集上的语义分割和物体检测任务上均取得了最先进的性能。


# Paper:914     基于方向连接性的医学图像分割



#### 1. Title: 
Directional Connectivity-based Segmentation of Medical Images

#### 2. Authors: 
Ziyun Yang, Sina Farsiu

#### 3. Affiliation: 
Duke University（杜克大学）

#### 4. Keywords: 
Medical image segmentation, deep learning, pixel connectivity, directional connectivity, disentanglement

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Directional_Connectivity-Based_Segmentation_of_Medical_Images_CVPR_2021_paper.html  Github: https://github.com/Zyun-Y/DconnNet

#### 6. Summary : 
- (1):本文研究医学图像分割中的解剖一致性问题，提出了一种基于像素连接性的深度学习方法，通过建模像素之间的连接关系来实现解剖一致性分割。

- (2):传统的基于像素分类的分割方法通常忽略了像素之间的关系，而基于像素连接性的方法可以更好地利用像素之间的关系，提高分割的空间一致性。然而，现有的连接性建模方法忽略了潜在空间中丰富的通道方向信息。本文提出了一种方向连接性建模方案，通过将方向子空间从共享潜在空间中解耦、跟踪和利用方向信息来增强网络的特征表示。

- (3):本文提出了一种名为DconnNet的方向连接性分割网络，通过子路径切片模块和交互式特征空间解码器来实现方向子空间的提取和利用。同时，提出了一种新的大小密度损失函数，以缓解医学数据集中的数据不平衡问题。实验结果表明，DconnNet在各种公共医学图像分割基准上的表现优于其他现有方法。

- (4):本文提出的DconnNet在多个公共医学图像分割基准上进行了实验，取得了优于其他现有方法的性能。该方法的性能支持其目标，即通过建模像素之间的连接关系来实现解剖一致性分割。
#### 7. 方法详细介绍：
本文提出了一种新的方向连接建模网络（DconnNet）用于医学图像分割。该方法由三部分组成：预训练的ResNet编码器、子路径方向激励模块和交互式特征空间解码器。编码器的输出被表示为𝑒！，最后一个编码器输出被上采样到输入大小以获得初步输出𝑋%&'(&，该输出在损失计算中用于学习连接性掩码。方向先验𝛼%&'(&通过全局平均池化将𝑋%&'(&压缩并映射到与潜在特征映射𝑒$相同的维度来获得，然后使用1×1卷积𝑊/和sigmoid门控函数𝜎对方向先验进行重新编码以规范化投影向量。潜在特征和方向先验通过通道切片分成八部分，每对这些特征嵌入切片通过子路径激励模块传递以捕获长距离和通道间依赖关系。最后，使用两个自上而下的动态交互流（空间流和特征流）的交互式特征空间解码器，以确保方向依赖信息可以有效地融合到每个层的特征映射中。

#### 8. 实验设置：
本文使用了三个流行且多样化的医学基准数据集：Retouch、ISIC2018和CHASEDB1。Retouch数据集包含来自三个OCT扫描仪的70个OCT体积的训练集，并在训练数据集上为每个扫描仪使用了体积级3倍交叉验证。ISIC2018数据集包含2594个不同分辨率的皮肤病变图像，使用5倍交叉验证。CHASEDB1数据集包含28个分辨率为999×960像素的眼底图像，使用5倍交叉验证。超参数设置因数据集而异，并且该框架建立在PyTorch 1.7.0上。

#### 9. 实验结果与分析：
本文提出的DconnNet在所有三个数据集上的表现均优于现有的最先进模型，包括Dice、IOU、准确度、精度、绝对体积差和体积平衡准确度等各种评估指标。对小液体区域和整体准确性的改进表明了DconnNet的卓越性能。该模型还显示出与地面真值的拓扑相似性，反映了其保持拓扑的能力。定性研究还表明，DconnNet能够准确地分割病变并进行连接预测。消融研究验证了所提出方法中每个损失项的有效性。


# Paper:915     手掌中的NeRF：通过新视角合成进行机器人纠正增强



#### 1. Title: 
NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis

#### 2. Authors: 
Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, Chelsea Finn

#### 3. Affiliation: 
Allan Zhou: 斯坦福大学 (Stanford University)

#### 4. Keywords: 
Robotics, Neural Radiance Fields, Data Augmentation, Imitation Learning, Visual Grasping

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_NeRF_in_the_Palm_of_Your_Hand_Corrective_Augmentation_for_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了基于视觉的机器人操作中的数据增强方法，以提高机器人策略的性能。作者使用神经辐射场（NeRF）来合成纠正噪声，从而提高机器人策略的性能，而无需额外的专家监督或环境交互。
 
- (2):过去的方法通常需要大量的演示或昂贵的在线专家监督来学习反应式闭环行为。本文提出的方法使用NeRF来合成纠正噪声，从而提高机器人策略的性能，而无需额外的专家监督或环境交互。该方法可以产生反应式、实时和仅使用RGB的6-DoF抓取策略，同时消除了深度传感器的需求。
 
- (3):本文提出了一种名为SPARTN的NeRF数据增强技术，用于改进基于视觉的机器人操作。该方法使用NeRF合成纠正噪声，从而提高机器人策略的性能，而无需额外的专家监督或环境交互。该方法可以产生反应式、实时和仅使用RGB的6-DoF抓取策略，同时消除了深度传感器的需求。在模拟和真实世界的实验中，SPARTN方法提高了机器人抓取任务的成功率。
 
- (4):在模拟的6-DoF视觉抓取基准测试中，SPARTN方法将成功率提高了2.8倍，甚至超过了一些使用昂贵的在线监督的方法。在真实世界的6-DoF机器人抓取实验中，该方法平均提高了22.5%的绝对成功率，包括传统上对深度传感器具有挑战性的物体。该方法的性能支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种名为SPARTN的方法，用于改进使用手眼相机的机器人策略。该方法利用神经辐射场（NeRF）在视觉演示中合成注入校正噪声，使用NeRF生成扰动视点同时计算校正动作。这不需要额外的专家监督或环境交互，并将NeRF中的几何信息蒸馏成实时反应的RGB-only策略。该方法涉及在原始演示数据上训练NeRF，通过沿着演示注入噪声来将演示数据增强为校正反馈，并使用演示的NeRF从新的相机姿态渲染观察结果，将增强数据与原始演示相结合以训练反应实时策略。

具体步骤如下：
1. 训练NeRF，将演示数据作为训练视图。
2. 对原始数据集中的每个转换进行增强，使用过程调整SE（3）空间中的观察和相对动作，注入噪声来生成校正转换。
3. 使用NeRF从扰动的相机姿态渲染观察结果。
4. 将增强数据与原始演示相结合以训练反应实时策略。

#### 8. 实验设置：
本文在仿真和实际机器人抓取任务中评估了SPARTN。在仿真中，使用了先前提出的仿真6-DoF抓取基准测试。在现实世界中，使用Franka Emika Panda机器人和八个具有挑战性的真实世界抓取任务。

#### 9. 实验结果和分析：
在仿真6-DoF抓取基准测试中，SPARTN将抓取成功率提高了2.8倍，比没有SPARTN的训练表现更好，甚至优于一些使用昂贵的在线监督的方法。在Franka Emika Panda机器人上进行的八个具有挑战性的真实世界抓取任务中，SPARTN将绝对平均成功率提高了22.5％。该方法消除了以前需要深度传感器的需求，并可以为6-DoF抓取生成反应实时的RGB-only策略。


# Paper:916     基于切片注意力网络的准确BEV 3D物体检测



#### 1. Title: 
BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks

#### 2. Authors: 
Xiaowei Chi, Jiaming Liu, Ming Lu, Rongyu Zhang, Zhaoqing Wang, Yandong Guo, Shanghang Zhang

#### 3. Affiliation: 
第一作者：北京大学计算机科学学院，多媒体信息处理国家重点实验室

#### 4. Keywords: 
BEV 3D Object Detection, Multi-view Camera, LiDAR, Attention Mechanism, Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Chi_BEV-SAN_Accurate_BEV_3D_Object_Detection_via_Slice_Attention_Networks_CVPR_2021_paper.html  Github: https://github.com/Chi-Lab/BEV-SAN

#### 6. Summary : 
- (1):本文研究的是自动驾驶系统中的关键技术——鸟瞰图（BEV）三维物体检测。 
- (2):现有的方法将多视角相机特征聚合到平面网格中以构建BEV特征，但是这种方法无法强调不同高度的信息特征。本文提出了一种名为BEV Slice Attention Network（BEV-SAN）的新方法，以利用不同高度的内在特征。与将BEV空间沿高度维度展平的方法不同，BEV-SAN首先沿高度维度采样以构建全局和局部BEV切片，然后使用注意机制将BEV切片的特征从相机特征中聚合并合并。最后，使用变压器将合并的局部和全局BEV特征融合以生成任务头的最终特征图。本文进一步提出了一种基于LiDAR引导采样策略，以利用LiDAR的统计分布来确定局部切片的最佳高度。与均匀采样或随机采样相比，LiDAR引导采样可以选择更具信息量的BEV高度范围。 
- (3):本文提出了一种新的BEV-SAN方法，以利用不同高度的内在特征，实现了准确的BEV 3D物体检测。本文还提出了一种LiDAR引导采样策略，以确定局部切片的最佳高度，从而为BEV感知提供了信息量更大的范围。 
- (4):本文在BEV 3D物体检测任务上进行了详细的实验，证明了BEV-SAN的有效性。与现有方法相比，BEV-SAN在KITTI数据集上取得了最佳性能。
#### 7. 方法详细介绍：
本文提出了一种名为BEV-SAN的方法，通过Slice Attention Networks实现准确的BEV三维物体检测。该方法包括三个组件：相机特征提取、BEV特征构建和任务头。相机特征提取使用ResNet-101作为骨干网络，BEV特征构建使用Slice Attention Module将采样的全局和局部切片融合，任务头使用多任务学习框架。具体步骤如下：
1. 相机特征提取：使用ResNet-101作为骨干网络提取相机特征。
2. BEV特征构建：将相机特征投影到BEV空间中，构建全局和局部切片，并使用Slice Attention Module将它们融合。
3. 任务头：使用多任务学习框架进行物体检测和物体属性预测。

#### 8. 实验设置：
本文使用nuScenes数据集进行实验，数据集包含1000个序列，每个序列由六组环视相机图像、一组激光雷达数据和它们的传感器信息组成。数据集提供10个类别的物体注释，并使用官方划分进行训练、验证和测试。评估指标包括平均精度（mAP）、Nuscenes检测分数（NDS）、平均平移误差（ATE）、平均尺度误差（ASE）、平均方向误差（AOE）、平均速度误差（AVE）和平均属性误差（AAE）。

#### 9. 实验结果和分析：
本文提出的BEV-SAN方法在nuScenes数据集上进行了实验，与多个基线方法进行了比较。实验结果表明，BEV-SAN方法在mAP和NDS指标上均优于基线方法。同时，本文还分析了BEV-SAN方法的计算和存储成本，并提出了更加精细的工程优化方法以降低计算成本。


# Paper:917     通过最大化多模态互信息实现全能预训练



#### 1. Title: 
Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information

#### 2. Authors: 
Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, Jifeng Dai

#### 3. Affiliation: 
第一作者：中国科学技术大学

#### 4. Keywords: 
Pre-training, Multi-modal, Mutual Information, Large-scale models

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2021_paper.html  Github: https://github.com/OpenGVLab/M3I-Pretraining

#### 6. Summary : 
- (1):本文旨在提出一种全新的单阶段预训练方法，以最大化多模态互信息为目标，将现有的多种预训练策略和数据源进行整合，以提高大规模模型的训练效果。

- (2):现有的多阶段预训练方法存在复杂的流程和不稳定性等问题，难以有效地利用各种监督信号。本文提出的M3I预训练方法通过最大化输入和目标表示之间的互信息，将现有的监督/弱监督/自监督预训练方法相结合，实现了单阶段预训练，避免了多阶段预训练的问题。

- (3):本文首先提出了一个通用的多模态互信息理论框架，可以覆盖现有的主流预训练方法。然后将该框架扩展到多输入多目标设置中，提出了M3I预训练方法，可以有效地结合各种预训练监督信号，实现大规模视觉基础模型的多模态/数据源预训练。本文的贡献在于提出了一种全新的单阶段预训练方法，可以将现有的多种预训练策略和数据源进行整合，以提高大规模模型的训练效果。

- (4):本文的方法在各种视觉基准测试中均取得了优异的表现，包括ImageNet分类、COCO目标检测、LVIS长尾目标检测和ADE20k语义分割等。特别地，本文成功地预训练了一个十亿级参数的图像骨干网络，并在公共数据集上取得了最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种名为M3I Pre-training的多模态预训练方法，它将自监督和监督/弱监督预训练相结合。该方法使用两个输入和四个目标，包括带注释的类别或图像的文本和增强视图的配对文本。该方法的输入编码器将输入视图与随机二进制掩码混合，输入表示由图像主干编码。目标编码器为图像目标生成密集的目标特征，并使用类别嵌入或文本主干作为类别或文本目标。输入到目标解码器从输入中预测目标表示，每个目标使用单独的损失形式分别预测。该方法旨在最大化多模态互信息，并可用于各种预训练任务。

#### 8. 实验设置：
本文使用InternImage-H作为大规模模型预训练的图像编码器，使用ViT-B/16进行其他实验。对于图像-文本数据集，使用12层Transformer作为文本目标编码器。对于图像分类数据集，直接使用线性分类器权重作为类别嵌入目标。使用4层Transformer作为图像表示目标的解码器，Attention Pooling作为类别嵌入或文本全局特征的解码器。预训练计划设置为在ImageNet-1k上进行400个epoch，微调计划为ImageNet-1k上进行100个epoch，其他数据集为25个epoch。

#### 9. 实验结果与分析：
本文将M3I Pre-training方法与先前的预训练方法在ImageNet、COCO、LVIS和ADE20k数据集上进行比较。结果表明，M3I Pre-training在所有这些任务上都取得了与先前最佳方法相当的结果，表明M3I Pre-training可以通过单阶段预训练来保持所有所需的属性。


# Paper:918     基于元数据和隐式神经函数的RAW图像重建



#### 1. Title: 
Metadata-Based RAW Reconstruction via Implicit Neural Functions

#### 2. Authors: 
Leyi Li, Huijie Qiao, Qi Ye, Qinmin Yang

#### 3. Affiliation: 
Zhejiang University (浙江大学)

#### 4. Keywords: 
RAW reconstruction, metadata, implicit neural function, super-resolution

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Li_Metadata-Based_RAW_Reconstruction_via_Implicit_Neural_Functions_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1): 本文研究利用元数据进行RAW图像重建的问题。RAW图像具有线性关系和更高的动态范围，但是占用的存储空间较大，不利于传输和共享。因此，本文提出了一种基于隐式神经函数的RAW图像重建方法，通过将元数据的二维坐标映射到其RAW值，实现了对RAW图像的重建。
- (2): 早期的RAW重建方法需要根据不同的相机进行校准，而基于元数据的方法则通过在采集时将额外的数据嵌入到sRGB图像中，从而避免了校准的问题。然而，现有的方法仍然存在一些限制，如计算成本高、无法恢复饱和区域等。本文提出了一种新的重建方法，通过隐式神经函数将RAW重建问题转化为从元数据的二维坐标到RAW值的映射，从而提高了重建的准确性。
- (3): 本文提出的方法是基于隐式神经函数的RAW图像重建方法，将重建问题转化为从元数据的二维坐标到RAW值的映射。该方法可以分解为两个部分：从sRGB值到相应的RAW值的映射函数和从稀疏采样中插值RAW图像的超分辨率函数。本文设计了两个分支来处理这两个任务，并调整了超参数以适应任务的难度。与大多数基于深度学习的方法不同，我们的方法是以自监督的方式进行训练，不需要在不同的相机ISP上进行预训练。
- (4): 本文的方法在RAW图像重建和引导超分辨率任务上进行了实验，结果表明，与现有方法相比，我们的方法在重建准确性方面有显著的提高（平均PSNR提高了10dB以上）。该方法的性能支持了其目标。
#### 7. 方法详细介绍：
本文提出了一种基于元数据的RAW图像重建方法，使用隐式神经函数（INF）从sRGB图像中恢复RAW图像。该方法在捕获时从RAW图像中采样额外的元数据，并将其用于辅助重建。INF的结构有助于合并像素值和坐标的信息，从而提高重建精度。该方法将重建分解为两个方面：从sRGB值到相应RAW值的映射函数和从稀疏采样中插值RAW图像的超分辨率函数。INF是一个MLP结构，由两个分支组成，每个分支用于不同的任务。网络可以在无需相应RAW图像的情况下进行自监督训练。

#### 8. 实验设置：
本文在NUS数据集的三个相机上进行了实验。在捕获阶段，使用均匀采样率1.5％生成元数据。将图像分成不同的块进行重建，并使用Adam优化器和L2损失在500次迭代中训练INF。正则化参数设置为λs = 0.0001，λp = 0.1，λr = 0.001。

#### 9. 实验结果和分析：
本文提出的方法在PSNR和SSIM指标上优于三个基于元数据的RAW重建基线，特别是在平均PSNR上比最接近的竞争对手CAM高出10 dB以上。该方法显着减少了整个图像的重建误差，特别是饱和区域。然而，恢复饱和区域仍然是所有RAW重建方法的一个具有挑战性的问题，并且是我们的方法出现大误差（≥1％）的唯一情况。


# Paper:919     双向特征融合生成对抗网络用于超高分辨率病理图像虚拟染色



#### 1. Title: 
Bi-directional Feature Fusion Generative Adversarial Network for Ultra-high Resolution Pathological Image Virtual Re-staining

#### 2. Authors: 
Kexin Sun, Zhineng Chen, Gongwei Wang, Jun Liu, Xiongjun Ye, Yu-Gang Jiang

#### 3. Affiliation: 
第一作者：复旦大学计算机科学学院 & 上海智能视觉计算协同创新中心

#### 4. Keywords: 
Virtual re-staining, Pathological image, Bi-directional feature fusion, Generative adversarial network, Ultra-high resolution

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Bi-Directional_Feature_Fusion_Generative_Adversarial_Network_for_Ultra-High_Resolution_Pathological_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1): 本文研究的是病理图像的虚拟染色问题，由于病理图像的超高分辨率，传统的虚拟染色方法需要将WSI图像分成小块进行模型训练和推断，这导致了全局信息的缺失，从而在合并重新染色的小块以生成更大尺寸的图像时出现了明显的颜色、亮度和对比度差异，这被称为“方块效应”。

- (2): 一些现有方法尝试通过重叠或简单的后处理来解决这个问题，但前者不是很有效，后者需要仔细调整。为了消除方块效应，本文设计了一个具有全局分支和局部分支的双向特征融合生成对抗网络（BFF-GAN），它通过全局和局部特征的融合以及分块注意力来学习块间连接。与现有方法相比，本文的方法可以更好地消除方块效应。

- (3): 本文提出了一种结合全局和局部特征的模型，用于学习块间关系以解决方块效应，并同时绕过超高分辨率图像的内存限制。该模型由全局分支和局部分支组成，前者以下采样的整个图像作为输入，后者以批量输入的块级图像作为输入。两个分支在编码器中进行双向特征融合，并使用分块注意力和跳跃连接来增强特征表达能力。最后，我们将两个分支的特征融合以输出重新染色的结果。

- (4): 本文在私有数据集RCC和公共数据集ANHIR上进行了实验。结果表明，我们的模型在各种指标上都取得了竞争性的性能，并能够生成极其真实的图像，即使对于有经验的病理学家也具有欺骗性，这意味着它具有很大的临床意义。
#### 7. 方法详细介绍：
本文提出了一种双向特征融合生成对抗网络（BFF-GAN）来解决超高分辨率病理图像虚拟染色中的方块效应。BFF-GAN包括全局分支和局部分支，前者以下采样的整个图像为输入，后者以批量输入的补丁级图像为输入。两个分支在编码器中进行双向特征融合，并使用基于补丁的注意力和跳跃连接来增强特征表达能力。最后，两个分支的特征被融合以输出染色结果。具体步骤如下：
1. 全局分支和局部分支都采用相同的基本结构，包括编码器、6个ResBlock和带有跳跃连接的解码器。
2. 两个分支在编码器中的每个补丁级注意力模块（PAM）之后进行双向特征融合。
3. 模型还使用基于CBAM的补丁级注意力模块来帮助模型集中于重要的通道、空间和补丁级信息。
4. 损失函数包括全局、局部和集成部分，总损失函数是这些部分的组合。

#### 8. 实验设置：
本文在私有数据集RCC和公共数据集ANHIR上进行了实验。RCC数据集包含100对HE和IHC染色图像，ANHIR数据集包含50对HE和IHC染色图像。局部分支的补丁大小设置为448×448，全局分支的输入大小也为448×448，补丁之间的重叠为64像素。模型使用Adam优化器，学习率为0.0002，权重衰减为0.0001，批量大小为1，训练200个epoch。

#### 9. 实验结果与分析：
本文提出的BFF-GAN模型在消除方块效应和生成逼真图像方面表现出色，能够在17580x15798 WSI图像上获得几乎没有方块效应的虚拟染色结果。模型的性能使用多种指标进行评估，包括结构相似性指数（SSIM）、峰值信噪比（PSNR）、颜色差异（ΔE）等。结果表明，BFF-GAN在大多数情况下表现出相对平衡的性能，并在大多数指标上优于其他最先进的方法。主观实验还证明了所提出方法的临床意义，因为它能够欺骗有经验的病理学家。分类实验表明，重建图像与真实图像具有相似的临床用途。


# Paper:920     TopDiG：遥感图像中的类别无关拓扑方向图提取



#### 1. Title: 
TopDiG: Class-agnostic Topological Directional Graph Extraction from Remote Sensing Images

#### 2. Authors: 
Bingnan Yang, Mi Zhang, Zhan Zhang, Zhili Zhang, Xiangyun Hu

#### 3. Affiliation: 
第一作者：武汉大学遥感信息工程学院，中国

#### 4. Keywords: 
Remote sensing, vector extraction, topological directional graph, class-agnostic, dynamic graph supervision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_TopDiG_Class-Agnostic_Topological_Directional_Graph_Extraction_From_Remote_Sensing_Images_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究背景是遥感图像中的矢量提取，传统方法往往只能针对特定类别进行提取，难以跨类别稳定性表现。

- (2):过去的方法主要有基于分割、基于轮廓和基于图形生成等，但这些方法都存在着各自的问题，如对特定类别的依赖性强、效率低、拓扑错误等。本文提出了一种创新的类别无关模型TopDiG，可以直接从遥感图像中提取拓扑方向图，并解决了这些问题。

- (3):本文提出了动态图监督策略（DGS）来动态生成无序节点的邻接矩阵标签，以及拓扑集中节点检测器（TCND）来保证预测节点的适当密度。此外，还设计了方向图（DiG）生成器模块来构建预测节点的拓扑方向图。TopDiG可以直接从不同目标中提取类独立的矢量地图，不需要初始轮廓或额外的后处理。

- (4):本文在Inria、CrowdAI、GID、GF2和Massachusetts数据集上进行了实验，结果表明TopDiG是类别无关的，并在所有数据集上实现了竞争性能。同时，TopDiG可以构建可靠的拓扑方向图，在Massachusetts数据集上实现了64.60%的平均路径长度得分（APLS）。
#### 1. 实验结果
(1). 本文在Inria、CrowdAI、GID、GF2和Massachusetts数据集上进行了大量实验，比较了TopDiG与基于分割、轮廓和图形生成方法的性能。TopDiG在大多数情况下表现最佳，并且在多边形和线形目标上都表现出可靠性。在拓扑质量指标方面，TopDiG的表现至少比其他模型高出1.91%的mIoUtopo和7.61%的APLS。此外，它还报告了84.56%的mIoUmask等像素级指标的竞争得分。这些评估表明，TopDiG可以精确地从航空图像中提取多边形目标的拓扑结构。

#### 2. 方法详细介绍
本文提出了一种名为TopDiG的类别无关框架，用于从遥感图像中提取拓扑方向图。该框架由三个模块组成：拓扑集中节点检测器（TCND）、动态图监督（DGS）和方向图生成器。TCND专注于拓扑组件，以无序方式检测节点并提取视觉描述符。DGS动态生成邻接图标签，充分利用无序检测到的节点来建立邻接图标签。方向图生成器使用自注意力网络构建方向性拓扑图。该框架以端到端的方式进行训练，不需要初始轮廓或额外的后处理。

#### 3. 实验设置
本文在Inria、CrowdAI、GID、GF2和Massachusetts五个数据集上进行了实验。每个特定数据集的超参数可以在第5.3节中找到。TopDiG的网络架构采用ResNet50作为CNN编码器。特征图FD×H×W和视觉描述符d的维度设置为D = 64，而初始描述符dinit和最终描述符dfinal的维度为D' = 768。DiG生成器模块中的MLP由两个全连接层组成。在连接网络方面，我们顺序堆叠了M = 2个包含h = 12个并行头的transformer编码器层。

#### 4. 实验结果与分析
本文提出的TopDiG方法在所有五个数据集上均取得了竞争性能。边界mIoU分数分别为68.39％、72.51％、74.51％和75.28％。该方法还在Massachusetts数据集上实现了64.60％的平均路径长度分数（APLS）。TCND模块将拓扑APLS分数提高了约8.06％。所提出的DGS策略促进了无序节点的连接，并减轻了对节点准确位置的需求。

#### 5. 实验结果
本文在Inria、CrowdAI、GID、GF2和Massachusetts数据集上进行了大量实验，比较了TopDiG与基于分割、轮廓和图形生成方法的性能。TopDiG在大多数情况下表现最佳，并且在多边形和线形目标上都表现出可靠性。在拓扑质量指标方面，TopDiG的表现至少比其他模型高出1.91%的mIoUtopo和7.61%的APLS。此外，它还报告了84.56%的mIoUmask等像素级指标的竞争得分。这些评估表明，TopDiG可以精确地从航空图像中提取多边形目标的拓扑结构。


# Paper:921     CXTrack：利用上下文信息提高3D点云跟踪



#### 1. Title: 
CXTrack: Improving 3D Point Cloud Tracking with Contextual Information

#### 2. Authors: 
Tian-Xing Xu, Yuan-Chen Guo, Yu-Kun Lai, Song-Hai Zhang

#### 3. Affiliation: 
第一作者：清华大学

#### 4. Keywords: 
3D object tracking, point cloud, contextual information, transformer-based network, localization

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Xu_CXTrack_Improving_3D_Point_Cloud_Tracking_With_Contextual_Information_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是3D单目标跟踪问题，该问题在自动驾驶等领域有着广泛的应用。由于点云数据的稀疏性和外观变化，该问题一直是一个具有挑战性的问题。因此，跨两个连续帧的上下文信息对于有效的目标跟踪至关重要。

- (2):现有的3D点云跟踪方法可以分为三种主要范式，即SC3D、P2B和motion-centric。这些方法都存在一定的问题，如忽略目标周围的上下文信息、对外观变化敏感等。本文提出了一种新的跟踪范式，即CXTrack，该方法直接利用两个连续帧之间的点云数据，不需要裁剪目标，从而充分利用了目标周围的上下文信息。同时，本文提出了一种基于transformer的网络结构，即CXTrack，该网络结构采用目标中心的transformer网络，直接从两个连续帧和先前的边界框中获取点特征，以探索上下文信息和隐式传播目标线索。为了实现所有大小的对象的准确定位，本文提出了一种基于transformer的本地化头部，具有新颖的中心嵌入模块，以区分目标和干扰物。

- (3):本文提出了一种新的transformer-based跟踪器，即CXTrack，该跟踪器采用目标中心的transformer架构来传播目标信息和利用上下文信息。同时，本文提出了一种本地化头部，即X-RPN，该头部采用本地transformer来模拟目标内部的点特征交互，从而实现了更好的小目标和大目标之间的平衡。

- (4):本文在三个大型数据集上进行了广泛的实验，KITTI、nuScenes和Waymo Open Dataset，结果表明，CXTrack在单个NVIDIA RTX3090 GPU上以34 FPS的速度实现了最先进的跟踪性能。
#### 7. 方法详细介绍：
本文提出了一种名为CXTrack的3D点云跟踪方法，旨在通过充分利用上下文信息来提高跟踪精度。该方法由三个主要组件组成：目标中心变换器、上下文变换器和X-RPN网络。其中，目标中心变换器用于提取特征并建模特征之间的关系，上下文变换器用于探索跨帧的上下文信息，X-RPN网络用于从点特征中聚合局部线索。该方法使用DGCNN作为骨干网络，采用半随机失活层将目标性信息与跨帧上下文信息融合。X-RPN网络采用局部变换器和中心嵌入来聚合局部信息，每个点仅与其邻域点交互以抑制噪声。损失函数是交叉熵损失、L2损失和Huber损失的加权组合。 

#### 8. 实验设置：
本文在三个大型数据集KITTI、nuScenes和Waymo Open Dataset（WOD）上评估了所提出的CXTrack方法。对于KITTI，将训练序列分为三部分，0-16用于训练，17-18用于验证，19-20用于测试。对于nuScenes，使用验证集来评估模型，其中包含150个场景。对于WOD，根据点云的稀疏程度将数据集分为三个部分。评估指标是Success和Precision，分别表示预测框与真实框的交并比大于一定阈值的帧数比例和中心点距离在一定阈值内的帧数比例的曲线下面积（AUC）。

#### 9. 实验结果与分析：
本文提出的CXTrack方法在KITTI、nuScenes和WOD数据集上均取得了显著的性能提升，平均Success和Precision指标均优于当前最先进的方法。该方法在小目标和大目标之间取得了良好的平衡，并且在未见过的场景中具有很好的泛化性能。该方法对于场景中的干扰物具有很好的鲁棒性。此外，本文还对不同组件的效率进行了详细分析，并进行了消融实验以验证CXTrack的设计选择的有效性。


# Paper:922     基于扩散的三维场景生成、优化和规划



#### 1. Title: 
Diffusion-based Generation, Optimization, and Planning in 3D Scenes

#### 2. Authors: 
Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu

#### 3. Affiliation: 
Siyuan Huang: BIGAI（中国北京市海淀区中关村南大街5号院1号楼）; Wei Liang: 北京理工大学计算机学院

#### 4. Keywords: 
3D scene understanding, generative model, optimization, planning, diffusion model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2021_paper.html  Github: https://scenediffuser.github.io/

#### 6. Summary : 
- (1):本文旨在解决3D场景理解中的生成、优化和规划问题，提出了一种基于扩散过程的条件生成模型SceneDiffuser，该模型具有场景感知、基于物理和目标导向的特点。
- (2):先前的工作大多使用条件变分自编码器（cVAE）进行条件生成，但cVAE存在后验崩溃问题，且生成、优化和规划之间存在差异。本文提出的SceneDiffuser通过扩散过程解决了后验崩溃问题，并将生成、优化和规划统一到一个框架中，同时将物理约束融入到采样过程中，从而避免了现有方法中存在的问题。
- (3):SceneDiffuser使用扩散模型进行场景感知的生成，通过迭代采样策略，将场景感知的生成、基于物理的优化和目标导向的规划结合起来，从而实现了全可微分的生成、优化和规划。SceneDiffuser的创新点在于将生成、优化和规划统一到一个框架中，并将物理约束融入到采样过程中，从而避免了现有方法中存在的问题。
- (4):SceneDiffuser在人体姿态和运动生成、灵巧抓取生成、3D导航路径规划和机器人臂运动规划等多个3D场景理解任务上进行了评估，结果表明SceneDiffuser相比于现有模型具有更好的性能和更强的泛化能力。
#### 7. 方法详细介绍：
本文提出了一种名为SceneDiffuser的条件生成模型，用于3D场景理解。SceneDiffuser通过迭代采样策略，在完全可微分的方式下，联合制定了场景感知生成、基于物理的优化和目标导向的规划。该模型采用扩展的条件扩散模型架构，通过交叉注意力实现灵活的条件。模型计算每个采样步骤中3D场景条件和输入轨迹之间的交叉注意力。计算出的向量被馈送到前馈层中以估计噪声。3D场景由场景编码器处理。模型考虑了两种轨迹目标进行优化和规划：轨迹级目标和逐步目标的累积。目标使用时间步t进行参数化，并在最后几个扩散步骤期间增加指导，以增强指导效果。

#### 8. 实验设置：
SceneDiffuser模型在五个场景理解任务上进行了评估，包括场景条件的人体姿势和运动生成、物体条件的灵巧抓取生成、3D导航路径规划和机器人臂运动规划。人体姿势生成任务在PROX提供的12个室内场景上进行评估，并使用LEMO的精细版本的每帧SMPL-X参数。人体运动生成任务使用与人体姿势生成任务相同的人体和场景表示，并将原始LEMO运动序列剪辑为持续时间为60帧的片段。灵巧抓取生成任务使用MultiDex数据集的Shadowhand子集，其中包含58个日常物品的多样化灵巧抓取姿势。数据集分为48个已知物体和10个未知物体进行训练和测试。

#### 9. 实验结果和分析：
SceneDiffuser模型在各种任务上均显著优于以前的模型，包括灵巧抓取生成、3D导航路径规划和机器人臂运动规划。模型在生成的姿势与训练数据中的平均姿势发散时，仍然能够保持高成功率而不会出现太多性能下降。通过在SceneDiffuser上应用优化器，引导采样过程可以减少物理不合理抓取姿势的违规情况，而无需进行额外的训练或中间表示。模型成功地导航到目标位置，即使在长时间范围内，也不会发散，而经典的基于RL的随机规划器则会出现问题。成功试验的规划步骤也与确定性规划器相当，显示了规划器在长时间范围内的有效性。


# Paper:923     基于近端分裂的语义分割对抗攻击



#### 1. Title: 
Proximal Splitting Adversarial Attack for Semantic Segmentation

#### 2. Authors: 
Jérôme Rony, Jean-Christophe Pesquet, Ismail Ben Ayed

#### 3. Affiliation: 
Jérôme Rony: ÉTS Montréal
Jean-Christophe Pesquet: Centre de Vision Numérique, Université Paris-Saclay, CentraleSupélec, Inria
Ismail Ben Ayed: ÉTS Montréal

#### 4. Keywords: 
Adversarial attack, semantic segmentation, proximal splitting, augmented Lagrangian, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Rony_Proximal_Splitting_Adversarial_Attack_for_Semantic_Segmentation_CVPR_2021_paper.html  Github: https://github.com/jeromerony/alma_prox_segmentation

#### 6. Summary : 
- (1):本文研究的是针对语义分割任务的对抗攻击，由于该任务需要对每个像素进行分类，因此相比于分类任务，对抗攻击更加困难。
- (2):过去的方法主要是针对分类任务的，而且这些方法无法直接应用于语义分割任务，因为语义分割任务需要处理更多的约束条件，而且计算和内存成本也更高。本文提出了一种基于近端分裂的对抗攻击方法，可以在非凸优化框架下处理大量的约束条件，并且可以通过自适应约束缩放和掩蔽策略来处理大规模的图像。该方法可以显著降低 ℓ∞ 范数，相比于之前的方法，可以更好地解决对抗分割问题。
- (3):本文提出的方法是基于近端分裂的对抗攻击，可以在非凸优化框架下处理大量的约束条件，并且可以通过自适应约束缩放和掩蔽策略来处理大规模的图像。该方法可以显著降低 ℓ∞ 范数，相比于之前的方法，可以更好地解决对抗分割问题。
- (4):本文提出的方法在 Cityscapes 和 Pascal VOC 2012 数据集上进行了测试，使用了多种深度学习模型，包括 DeepLabV3+、SegFormer 和 DeepLabV3 DDC-AT 等。实验结果表明，该方法可以显著降低 ℓ∞ 范数，相比于之前的方法，可以更好地解决对抗分割问题。
#### 7. 方法详细介绍：
本文提出了一种基于增广Lagrange原理和近端分裂的语义分割对抗攻击方法。该攻击最小化目标函数，该函数是ℓ∞-范数和扰动空间指示函数的和。增广Lagrange方法用于处理误分类约束，惩罚函数是逐分量应用的。该攻击称为ALMA prox，使用二进制掩码忽略未标记区域和缩放因子处理大量约束。使用近端分裂算法最小化ℓ∞-范数项，并通过求解约束优化问题来找到ℓ∞-范数和指示函数之和的近端算子。

#### 8. 实验设置：
本文在Pascal VOC 2012和Cityscapes数据集的验证集上进行了实验。评估的模型包括DeepLabV3+ ResNet-50、ResNet-101、FCN HRNetV2 W48、SegFormer MiT-B0和MiT-B3、DeepLabV3 ResNet-50 DDC-AT。模型的权重从MMSegmentation库中获取，除了鲁棒模型DeepLabV3 DDC-AT，该模型从与论文相关的存储库中获取。

#### 9. 实验结果和分析：
本文提出的ALMA prox攻击在ℓ∞-范数最小化方面优于所有其他攻击。在Pascal VOC上，大多数攻击可以在目标场景中找到小的扰动。然而，在非目标场景中差异要大得多。在Cityscapes上，问题的规模凸显了为分割模型生成对抗扰动的困难。PDPGD在非目标场景中处理得很好，中位扰动范数约为1/255，但在目标情况下产生更大的扰动。相反，PDG CE 13×40可以找到小的目标扰动，但在非目标场景中失败。本文提出的攻击方法在目标和非目标场景下的扰动范数方面都优于其他攻击。但是，该攻击不会产生有效的数字图像。


# Paper:924     基础模型驱动的弱增量学习用于语义分割



#### 1. Title: 
Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation

#### 2. Authors: 
Chaohui Yu, Qiang Zhou, Jingliang Li, Jianlong Yuan, Zhibin Wang, Fan Wang

#### 3. Affiliation: 
阿里巴巴集团 (Alibaba Group)

#### 4. Keywords: 
Weakly incremental learning, semantic segmentation, foundation model, co-segmentation, contrastive loss

#### 5. Paper: 
Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Foundation_Model_Drives_Weakly_Incremental_Learning_for_Semantic_Segmentation_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究的背景是语义分割的增量学习，旨在解决模型在学习新类别时容易忘记旧类别的问题。
- (2):过去的方法通常需要密集的像素注释，而本文提出的方法则利用图像级别标签进行弱增量学习，避免了昂贵的像素注释。然而，图像级别标签无法提供定位每个分割的细节，限制了WILSS的性能。本文提出了一种基于预训练的共分割方法，利用基础模型的知识生成密集的伪标签，同时提出了记忆增强的复制粘贴数据增强方法来解决旧类别的遗忘问题。
- (3):本文提出了一种名为FMWISS的新型数据高效的WILSS框架，利用预训练的基础模型来改进和有效利用仅基于图像级别标签的新类别的监督。具体而言，本文提出了基于预训练的共分割方法，从预训练的基础模型中提取出互补的知识，生成密集的伪标签。本文进一步提出了一种基于教师-学生架构的方法来优化噪声伪掩模，其中插入的教师使用提出的密集对比损失进行优化。此外，本文引入了基于记忆的复制粘贴数据增强来改善旧类别的遗忘问题，并进一步提高了性能。
- (4):本文在Pascal VOC和COCO数据集上进行了广泛的实验，证明了FMWISS框架的优越性能，例如，在15-5 VOC设置中，FMWISS的性能分别达到了70.7％和73.3％，优于现有方法3.4％和6.1％。
#### 7. 方法详细介绍：
本文提出了一种弱增量学习语义分割的框架，称为FMWISS。该框架包括三个主要步骤：预训练的共分割方法生成新类别的伪像素标签，插入式教师模块动态学习更好的伪掩模，密集对比损失用于优化分割模型。该框架还使用基础模型提供教师模块的密集预测。此外，该文还提出了一种基于内存的复制粘贴增强策略，以稳定旧类别的学习并提高分割模型的性能。总体学习目标是通过提炼训练模型和动态更新的教师模块的知识来优化分割模型。

#### 8. 实验设置：
本文在Pascal VOC 2012和COCO数据集上进行了实验，并遵循了现有的WILSS方法WILSON的数据集和协议。本文考虑了两种增量学习协议：不相交和重叠，并将它们应用于包括15-5 VOC和10-10 VOC的VOC数据集。本文还验证了在WILSON中提出的新的增量学习场景COCO-to-VOC的提出方法。本文报告了验证集上的标准平均交并比（mIoU）结果。

#### 9. 实验结果和分析：
本文在三个不同的设置下评估了所提出的FMWISS模型：15-5 Pascal VOC、10-10 Pascal VOC和COCO-to-VOC。结果表明，在所有三个设置中，FMWISS都优于现有方法，在整体性能和新类别方面都取得了显着的改进。结果以表格形式呈现，并与每个设置中的其他方法进行比较。本文还进行了消融实验，分析了不同因素的影响，如种子数量、掩模融合操作、密集对比目标和基于内存的复制粘贴增强。本文还比较了不同的自监督模型的性能，并评估了FMWISS在较少的训练数据下的数据效率。定性结果表明，FMWISS在旧类别和新类别上都具有优越性。


# Paper:925     基于类关系嵌入学习的无源领域自适应



#### 1. Title: 
Class Relationship Embedded Learning for Source-Free Unsupervised Domain Adaptation

#### 2. Authors: 
Yixin Zhang, Zilei Wang, Weinan He

#### 3. Affiliation: 
Yixin Zhang: Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China

#### 4. Keywords: 
Unsupervised Domain Adaptation, Contrastive Learning, Class Relationship, Source-Free

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Class_Relationship_Embedded_Learning_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html  Github: https://github.com/zhyx12/CRCo

#### 6. Summary : 
- (1):本文研究的是无源领域自适应（SFUDA）任务，即只有一个经过训练的源模型和未标记的目标数据可用的实际知识转移任务。 
- (2):过去的方法主要是通过对齐特征或对齐输出空间来转移源知识，但是这些方法无法应用于某些涉及隐私的实际场景。本文提出了一种新的方法，即将类关系嵌入到对比学习中，以便更有效地在目标域中进行表示学习。 
- (3):本文提出了一种新的概率相似度度量方法，称为类关系嵌入相似度（CRS），并将其嵌入到对比学习中。具体而言，我们将源模型的分类器权重视为类原型，计算类关系矩阵，并将其嵌入到对比学习中。我们提出了两种对比损失，即类关系嵌入类感知对比（CR-CACo）损失和类关系嵌入实例鉴别对比（CR-IDCo）损失。 
- (4):本文在多个SFUDA基准测试中评估了所提出的方法，并取得了最先进的性能。实验结果表明，由于转移了领域不变的类关系，我们的方法可以实现最先进的性能。
#### 7. 方法详细介绍：
本文提出了一种基于Class Relationship Embedded Learning (CREL)框架的源自由无监督域自适应方法。该框架包含两个损失函数：Class-relationship-embedded Contrastive (CR-CACo)和Class Relationship-embedded Instance Discrimination Contrastive (CR-IDCo)。其中，CR-CACo损失函数通过选择置信样本并计算它们之间的相似度来将源域的类关系传递到目标域。CR-IDCo损失函数通过将同一样本的两个视图视为正样本，将所有其他样本视为负样本来学习判别特征。这两个损失函数相互补充，可以端到端地训练。

#### 8. 实验设置：
本文在Office-Home、Office-31、VisDA和DomainNet等四个标准域自适应基准数据集上评估了所提出的方法。其中，Office-31、Office-Home和DomainNet使用ResNet-50和ResNet-101作为骨干网络，VisDA使用ResNet-101。所有数据集都使用SGD优化器，动量为0.9，批大小为64，每个图像有三个视图（即一个弱视图和两个强视图）。概率阈值τ设置为0.95，两个对比损失中的温度Tco设置为0.07。

#### 9. 实验结果与分析：
本文的方法在Office-Home数据集上取得了88.3的准确率，比基线方法高4.3个百分点，达到了最先进的水平。在Office-31和VisDA数据集上，本文的方法也取得了与其他方法相当的性能。此外，本文还在多源域自适应任务上进行了实验，结果表明所提出的方法在DomainNet和Office-Home数据集上均优于其他基线方法。


# Paper:926     Chat2Map：从多自我对话中高效地构建场景地图



#### 1. Title: 
Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations

#### 2. Authors: 
Sagnik Majumder, Hao Jiang, Pierre Moulon, Ethan Henderson, Paul Calamia, Kristen Grauman, Vamsi Krishna Ithapu

#### 3. Affiliation: 
第一作者：UT Austin（德克萨斯大学奥斯汀分校）

#### 4. Keywords: 
Scene mapping, audio-visual, deep reinforcement learning, egocentric, multi-agent

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Majumder_Chat2Map_Efficient_Scene_Mapping_From_Multi-Ego_Conversations_CVPR_2021_paper.html  Github: https://github.com/facebookresearch/chat2map

#### 6. Summary : 
- (1):本文旨在通过利用自然对话中参与者的自我中心音频-视觉观察的共享信息，有效地构建先前未见的3D环境的地图。通过多个人（“自我”）在场景中移动并相互交谈，他们接收到的丰富音频-视觉线索可以帮助揭示场景中未见区域。本文提出了一种音频-视觉深度强化学习方法，与我们的共享场景映射器配合使用，以选择性地打开相机以有效地绘制空间。

- (2):传统的计算机视觉方法（例如，视觉SLAM）在需要大量暴露于环境的情况下非常有效，但在许多实际场景中，相机只能观察到空间的一小部分。本文提出了一种新的场景映射任务，即从多自我对话中高效地构建先前未见的3D环境的地图。本文的方法是通过利用自然对话中参与者的自我中心音频-视觉观察的共享信息来实现的。本文的方法使用多个人的观察来映射场景，而不是使用机器人的相机来映射场景。

- (3):本文提出了一种音频-视觉深度强化学习方法，与我们的共享场景映射器配合使用，以选择性地打开相机以有效地绘制空间。我们的方法有两个关键元素：共享场景映射器和视觉采样策略。对于前者，我们设计了一种基于transformer的映射器，它将多个数据流合并起来，以推断出超出直接观察区域的地图，并且最重要的是，它可以使自我之间的通信改善映射精度。对于后者，我们的想法是放松“始终开启”相机的常见假设，并且仅在预期连续音频输入时激活视觉输入。我们使用深度强化学习训练采样策略，以在预期补充连续音频输入的情况下激活视觉输入。我们的模型在减少87.5％的视觉捕获和处理的同时，映射精度仅略有下降（约9％）。

- (4):本文提出的方法在多自我对话中高效地构建先前未见的3D环境的地图。我们的方法使用多个人的观察来映射场景，而不是使用机器人的相机来映射场景。我们的方法在实验中表现出色，可以成功地映射出只有部分可见性的陌生环境。与采样
#### 7. 方法详细介绍：
本文提出了一种名为Chat2Map的方法，用于从多人对话中高效地构建场景地图。该方法包括两个主要组件：共享场景映射器和视觉采样策略。共享场景映射器是一个基于Transformer的模型，利用多模态数据流推断出超出直接观察区域的地图，并促进多个人之间关于他们在三维空间中的观察和状态的交流，以提高映射精度。视觉采样策略使用深度强化学习进行训练，仅在预期连续音频流的补充时激活视觉输入，从而将视觉捕获和处理降低了87.5％，而映射精度仅略有下降。

#### 8. 实验设置：
本文使用AI-Habitat和Matterport3D视觉场景的SoundSpaces声学模拟进行评估。该设置允许在56/10/17的训练/验证/测试集上进行评估，共计83个场景。由于缺乏适合该任务的公共可用数据，本文还在模拟公寓中收集了真实世界数据。使用三星S22相机拍摄了密集的RGB图像，以生成相应的声学模拟。

#### 9. 实验结果和分析：
本文的方法在模拟和真实世界数据上均表现出良好的性能，利用音频和视觉的协同作用来预测未见区域并采样对周围几何信息有用的视图。然而，该模型在非常复杂的场景中失败，这些场景通常在视觉和音频都无法到达的空间中具有物体。本文还包括一个失败案例的部分，突出了两种常见情况，其中模型难以处理：一是人物停留在同一位置的情况，导致很少有信息量高的视觉帧可供采样；二是每个轨迹步骤都具有高度独特的视觉样本的情况，在这种情况下，每个样本都很有用，模型的行为类似于Unique-pose或Greedy。


# Paper:927     SplineCam：深度网络几何形状和决策边界的精确可视化和表征



#### 1. Title: 
SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries

#### 2. Authors: 
Ahmed Imtiaz Humayun, Randall Balestriero, Guha Balakrishnan, Richard Baraniuk

#### 3. Affiliation: 
第一作者：Rice University（莱斯大学）

#### 4. Keywords: 
Deep Networks, Visualization, Decision Boundaries, Continuous Piece-Wise Linear, SplineCam

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Humayun_SplineCam_Exact_Visualization_and_Characterization_of_Deep_Network_Geometry_and_CVPR_2021_paper.html  Github: https://bit.ly/splinecam-git

#### 6. Summary : 
- (1):本文研究的背景是深度学习和深度网络的可视化和可解释性问题。
 
- (2):过去的方法主要依赖于数据空间可视化，如评分，或生成最佳匹配给定DN单元或表示的新数据特征或样本。这些方法存在一定的局限性，如易受到对抗性攻击等。本文提出了一种新的方法，可以精确计算DN映射的几何形状，包括其决策边界，而不需要采样或架构简化等近似方法。本文提出的方法可以应用于任何基于CPWL激活非线性的DN架构，包括（泄漏）ReLU、绝对值、maxout和max-pooling等。本文提出的方法可以直接可视化DN的输入空间分区，计算分区统计信息，并从决策边界中高效地采样任意数量的样本。 

- (3):本文提出了一种可扩展的枚举方法，可以在给定DN输入空间的有界二维域上计算DN的输入空间分区和决策边界。本文提出的SplineCam利用我们的新枚举方法直接可视化DN的输入空间分区，计算分区统计信息并从决策边界中采样。本文的贡献包括：开发了一种可扩展的枚举方法，可以计算DN的输入空间分区和决策边界；开发了SplineCam，利用我们的新枚举方法直接可视化DN的输入空间分区，计算分区统计信息并从决策边界中采样；定量分析了SplineCam表征DN和比较架构选择和训练方案的能力。

- (4):本文的方法在决策边界可视化和表征方面取得了很好的效果，可以比较不同架构和训练方案的性能，并且可以从决策边界中高效地采样任意数量的样本。本文的方法可以应用于任何基于CPWL激活非线性的DN架构，包括（泄漏）ReLU、绝对值、maxout和max-pooling等。
#### 7. 方法详细介绍：
本文提出了一种名为SplineCam的方法，用于精确可视化和表征深度网络的几何形状和决策边界。该方法通过从网络的每一层反向投影超平面到输入空间，并使用基于图形的算法将输入空间划分为区域来实现。该算法涉及找到输入空间边缘和一组超平面之间的交点，然后找到结果图中的唯一循环。该方法可扩展，并可应用于各种体系结构。

具体步骤如下：
1. 计算决策边界的分段线性样条近似；
2. 计算样条近似的Voronoi图；
3. 使用Voronoi图计算输入空间的精确划分。

#### 8. 实验设置：
本文未提供实验设置。

#### 9. 实验结果和分析：
本文提供了使用SplineCam计算VGG11和VGG16深度网络的分区统计信息。作者证明了VGG11的分区统计信息在DA和非DA之间有显著差异，但对于VGG16差异不太显著。他们还提供了VGG11的分区可视化，并讨论了不同方向之间统计数据的变异性。作者建议，SplineCam可以用于量化训练数据点的初始化质量，在转移学习期间改进INR初始化，并可视化不同训练策略的分区动态。


# Paper:928     PIDNet：受PID控制器启发的实时语义分割网络



#### 1. Title: 
PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers

#### 2. Authors: 
Jiacong Xu, Zixiang Xiong, Shankar P. Bhattacharyya

#### 3. Affiliation: 
德克萨斯农工大学电子与计算机工程系

#### 4. Keywords: 
Semantic Segmentation, Real-time, Two-Branch Network, PID Controller, Overshoot

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_PIDNet_A_Real-Time_Semantic_Segmentation_Network_Inspired_by_PID_Controllers_CVPR_2020_paper.html  Github: https://github.com/XuJiacong/PIDNet

#### 6. Summary : 
- (1):本文研究的背景是实时语义分割任务，旨在提高模型的速度和准确率。
 
- (2):过去的方法包括轻量级编码器-解码器、深度可分离卷积和两分支网络等，但它们的精度较低，无法满足实际应用需求。本文提出的两分支网络等价于比例积分微分（PID）控制器，存在过冲问题。为了解决这个问题，本文提出了一种新的三分支网络结构：PIDNet，它包含三个分支来解析详细信息、上下文信息和边界信息，并采用边界注意力来指导详细和上下文分支的融合。 

- (3):本文提出了一种新的三分支网络结构：PIDNet，它包含三个分支来解析详细信息、上下文信息和边界信息，并采用边界注意力来指导详细和上下文分支的融合。本文还提出了一些有效的模块，如平衡详细和上下文特征的Bag融合模块，以提高PIDNet的性能。PIDNet在Cityscapes、CamVid和PASCAL Context数据集上实现了最佳速度和准确率的平衡，并超过了所有具有类似推理速度的现有模型的准确性。 

- (4):本文在Cityscapes和CamVid数据集上进行了实验，PIDNet-S在Cityscapes数据集上的推理速度为93.2 FPS，mIOU为78.6%，在CamVid数据集上的推理速度为153.7 FPS，mIOU为80.1%。PIDNet在速度和准确率之间实现了最佳平衡，其准确性超过了所有具有类似推理速度的现有模型。
#### 7. 方法详细介绍：
本文提出了一种新颖的实时语义分割网络PIDNet，它由三个分支组成：比例（P）分支、积分（I）分支和导数（D）分支。P分支解析和保留高分辨率特征图中的详细信息，I分支聚合本地和全局的上下文信息以解析长程依赖性，D分支提取高频特征以预测边界区域。为了有选择地从I分支中学习有用的语义特征，引入了像素注意力引导融合模块（Pag）。为了平衡详细和上下文特征，设计了边界注意力引导融合模块（Bag）来将高频和低频区域分别填充详细和上下文特征。本文还提出了一种并行聚合PPM（PAPPM）以并行化地收集上下文信息。PIDNet的损失函数包括语义损失、加权二元交叉熵损失和边界感知CE损失。

#### 8. 实验设置：
本文描述了实验的实现细节、训练协议和推理方法。作者在ImageNet上预训练了模型，并在Cityscapes、CamVid和PASCAL Context数据集上进行了微调。他们使用了数据增强技术，如随机裁剪、随机水平翻转和随机缩放。作者还进行了消融实验，以评估其提出的PIDNet模型的不同组件的有效性，包括ADB-Bag、Pag-Bag、PAPPM和额外的损失。实验结果表明，PIDNet-S-Wider在CamVid数据集上实现了最高的准确率，并在Cityscapes数据集上优于先前的最先进模型。模型的推理速度也得到了报告。

#### 9. 实验结果和分析：
本文提供了PIDNet与其他分割模型在Cityscapes和Pascal-Context数据集上速度和准确性的比较。实验结果表明，PIDNet在推理时间和准确性之间取得了最佳平衡。PIDNet-L在速度和准确性方面超过了SFNet（ResNet-18）和DDRNet-39，并成为实时领域中最准确的模型，将测试准确率从80.4％提高到80.64％mIOU。PIDNet-M和PIDNet-S也提供了比其他具有类似推理速度的模型更高的准确性。文本还提到，为了获得更好的性能，最好在边界周围进行精确注释，因为PIDNet利用边界预测来平衡详细和上下文信息。


# Paper:929     SmartAssign：学习智能知识分配策略用于去雨和去雪



#### 1. Title: 
SmartAssign: Learning A Smart Knowledge Assignment Strategy for Deraining and Desnowing

#### 2. Authors: 
Yinglong Wang, Chao Ma, Jianzhuang Liu

#### 3. Affiliation: 
华为诺亚方舟实验室 (Huawei Noah's Ark Lab)

#### 4. Keywords: 
deraining, desnowing, multi-task learning, knowledge assignment, transformer block

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_SmartAssign_Learning_A_Smart_Knowledge_Assignment_Strategy_for_Deraining_and_CVPR_2021_paper.html  Github: https://gitee.com/mindspore/models/tree/master/research/cv/SmartAssign

#### 6. Summary : 
- (1):本文研究的背景是针对单一天气类型的图像修复方法，忽略了不同天气条件在深度表示层面的联系，这些联系如果得到适当利用，可以为彼此生成互补的表示，弥补不足的训练数据，获得积极的性能提升和更好的泛化能力。

- (2):现有的方法主要集中在单一天气类型的处理上，而忽略了这些天气类型之间的联系。一些方法尝试使用统一的架构和一组预训练权重来处理不同的天气类型，但它们忽略了多个天气类型之间的差异，单一天气类型的独特性可能会损害其他天气恢复任务的性能。本文提出了一种新的多任务学习策略，探索深度表示层面的雨和雪之间的联系，同时避免了单一天气类型的独特性损害其他天气恢复任务的性能。

- (3):本文提出了一种智能知识分配策略SmartAssign，以优化地将从两个任务中学习到的知识分配给特定的任务，以建立所需的连接。为了进一步提高知识分配的准确性，本文提出了一种新颖的知识对比机制，使分配给不同任务的知识保持更好的独特性。本文还引入了一种新颖的Transformer块来构成网络的主干，以有效地结合长程上下文依赖性和局部图像细节。

- (4):本文在七个基准数据集上进行了广泛的实验，验证了SmartAssign探索了雨和雪之间的有效联系，并显着提高了去雨和去雪的性能。本文的方法在去雨和去雪任务上取得了良好的性能，支持了他们的目标。
#### 7. 方法详细介绍：
本文提出了一种名为SmartAssign的方法，用于去除图像中的雨滴和雪花。该方法使用骨干编码器从雨天和雪天图像中学习整个图像恢复知识，采用多任务学习的方式。通过门控知识过滤模块（GKFM）确定共享和独占的知识原子，并使用任务定向知识前馈（TKFF）机制将知识分配给不同的任务。SmartAssign方法使得不同任务能够自适应地控制GKFM中所有知识原子的开闭门，从而通过梯度反向传播学习最佳的知识分配策略，以获得不同任务的所需连接和独特性。

#### 8. 实验设置：
本文使用了多个数据集进行去雨和去雪任务的实验，包括Rain-streak、Rain1200、Real-Rain、Snow100K、SRRS和CSD。作者随机选择样本进行测试，并使用其余样本进行训练。在训练过程中，所有补丁都是从原始图像对中随机裁剪的，大小为256×256，批量大小为4。使用Adam优化器，初始学习率为0.0004，λ1和λ2分别设置为0.01和0.001。作者使用PSNR和SSIM作为客观指标，评估了他们的方法与最新的去雨和去雪方法的性能。

#### 9. 实验结果和分析：
本文提出的SmartAssign方法在六个基准数据集上均取得了比最新的去雨和去雪方法更好的性能。在去雨和去雪过程中，独特和共享知识的消融实验表明它们对于提高性能具有积极的贡献。然而，SmartAssign的局限性主要在于去雪任务，对于具有复杂雪花图案的图像，一些细微的雪痕可能会在最终结果中保留。


# Paper:930     METransformer：基于多个可学习专家令牌的Transformer放射学报告生成



#### 1. Title: 
METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens

#### 2. Authors: 
Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou

#### 3. Affiliation: 
第一作者：University of Sydney

#### 4. Keywords: 
Radiology report generation, transformer, multi-expert, ensemble-based approach, orthogonal loss

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_METransformer_Radiology_Report_Generation_by_Transformer_With_Multiple_Learnable_Expert_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是放射学报告生成，旨在自动生成基于放射图像的自由文本描述，以减轻放射科医生的工作负担，同时保持医疗保健的质量。 
- (2):现有的研究大多采用传统的编码器-解码器架构，但由于放射图像具有细粒度特征，放射学报告生成旨在生成长段落而不是单个句子，这给模型的注意力能力带来了更多挑战。本文提出了一种新的诊断字幕框架METransformer，通过引入可学习的专家令牌并鼓励它们使用线性和非线性注意力学习互补表示，从而实现了“多专家联合诊断”过程。本文的方法旨在模拟多专家会诊场景，以升级现有的“单专家”框架。 
- (3):本文的方法是基于Transformer骨干的，其关键设计是在Transformer编码器和解码器中引入多个可学习的“专家”令牌。在编码器中，每个专家令牌与视觉令牌和其他专家令牌交互，以学习不同的图像区域进行图像表示。这些专家令牌通过正交损失鼓励捕获互补信息，以最小化它们的重叠。在解码器中，每个被关注的专家令牌指导输入单词和视觉令牌之间的交叉注意力，从而影响生成的报告。进一步开发了基于度量的专家投票策略来生成最终报告。通过多专家的概念，本文的模型享受了集合方法的优点，但通过一种计算上更有效且支持专家之间更复杂交互的方式进行设计。 
- (4):本文的方法在两个广泛使用的基准测试中展现了良好的性能，即IU-Xray和MIMIC-CXR。实验结果表明，本文的方法比常见的集合方法具有更好的性能，同时大大减少了训练参数并提高了训练效率。
#### 7. 方法详细介绍：
本文提出了一种名为METransformer的诊断字幕生成框架。该框架引入了多个可学习的“专家”标记，分别嵌入到Transformer编码器和解码器中。在编码器中，每个专家标记与视觉标记和其他专家标记交互，以学习关注不同的图像区域进行图像表示。这些专家标记通过正交损失被鼓励捕获互补信息，以确保它们之间的重叠最小。在解码器中，每个被关注的专家标记指导输入单词和视觉标记之间的交叉注意力，从而影响生成的报告。进一步开发了基于度量的专家投票策略来生成最终报告。

#### 8. 实验设置：
本文在两个广泛使用的基准数据集IU-Xray和MIMIC-CXR上评估了所提出的METransformer方法的性能。同时，还分析了生成报告的临床相关性。

#### 9. 实验结果与分析：
本文提出的METransformer模型在两个数据集上的表现均优于现有方法，特别是在MIMIC-CXR数据集上取得了迄今为止最好的CIDEr分数。消融实验表明，METransformer的每个组件对性能都有积极影响，增加专家标记的数量可以显著提高模型的整体性能。与集成模型的比较表明，METransformer具有更少的可训练参数，具有更好的性能。定性分析表明，METransformer生成的描述与放射科医生编写的描述更加一致。


# Paper:931     基于3D感知图像合成的自定义属性定量操作



#### 1. Title: 
Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis

#### 2. Authors: 
Hoseok Do, EunKyung Yoo, Taehyeong Kim, Chul Lee, Jin young Choi

#### 3. Affiliation: 
LG Electronics AI Lab, CTO Division, South Korea (第一作者)

#### 4. Keywords: 
3D-based GAN, image manipulation, custom attributes, neural rendering, view consistency

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2021_paper.html  Github: None

#### 6. Summary:
- (1):本文研究背景是3D图像合成中对于自定义属性的精细控制问题。
- (2):过去的方法主要集中在2D图像的潜空间图像操作上，而本文提出的方法是基于3D GAN模型的图像操作，可以更好地控制3D对象的自定义属性。本文的方法通过自定义属性量化器和导航器实现对3D对象的精细控制，同时保持视角一致性。与现有方法相比，本文的方法可以更好地控制自定义属性，且更加用户友好。
- (3):本文提出了一种基于3D GAN模型的图像操作方法，通过自定义属性量化器和导航器实现对3D对象的精细控制。其中，自定义属性量化器是基于少量自定义图像样本训练的，可以将用户定义的属性作为标准化的量化值，而导航器则通过量化器的指导实现对3D对象的精细控制。本文的方法可以更好地控制自定义属性，且更加用户友好。
- (4):本文的方法在多个3D和2D对象的属性上进行了验证，包括人脸属性。实验结果表明，本文的方法在定量和定性上都取得了良好的效果，可以更好地控制3D对象的自定义属性。
#### 7. 方法详细介绍：
本文提出的方法包括两个主要组件：自定义属性量化器和多属性导航器。自定义属性量化器使用特征编码器和线性回归模型从输入图像中估计用户定义属性的数量。多属性导航器将512维潜在空间中的潜在特征进行操作，从给定的源潜在特征和目标属性数量生成目标图像。导航器被训练以保持源图像的身份，同时生成接近目标属性数量的3D感知图像。导航器包括属性特征表示器（AFR）和逐层注意控制器（LAC），以有效处理多个属性。

#### 8. 实验设置：
无可用信息。

#### 9. 实验结果和分析：
本文提出的方法在所有评估指标中均优于竞争方法，包括操作精确度、身份保留和操作效率。定性比较表明，我们的方法可以在保留源身份的同时进行图像操作，具有更好的操作性能。对定量操作和视图一致性性能进行了消融研究，结果表明多视图一致性的定量器和导航器中的每个因素都对视图一致性做出了贡献。本文还提供了与其他定量操作方法（包括StyleFlow和StyleFlow+Q）的比较，并使用五个评估指标评估了年龄、微笑、性别和眼镜属性的操作性能。我们的方法表现最佳。


# Paper:932     BEVHeight：一种用于基于视觉的路侧3D物体检测的鲁棒框架



#### 1. Title: 
BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection

#### 2. Authors: 
Lei Yang, Kaicheng Yu, Tao Tang, Jun Li, Kun Yuan, Li Wang, Xinyu Zhang, Peng Chen

#### 3. Affiliation: 
第一作者：清华大学汽车安全与能源实验室

#### 4. Keywords: 
Roadside perception, 3D object detection, bird’s eye view, height estimation, camera-only

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yang_BEVHeight_A_Robust_Framework_for_Vision-Based_Roadside_3D_Object_Detection_CVPR_2021_paper.html  Github: https://github.com/ADLab-AutoDrive/BEVHeight

#### 6. Summary : 
- (1):本文研究的背景是自动驾驶系统中的3D物体检测，而大多数研究都集中在车辆传感器上，而忽略了利用智能路侧摄像头扩展感知能力的替代方法。
- (2):现有的视觉中心鸟瞰图检测方法主要集中在恢复相机中心的深度，而路侧摄像头的深度差异会随着距离的增加而迅速缩小，这使得深度预测对于远距离物体的优化变得不够优秀。本文提出了一种新的框架，BEVHeight，通过回归地面高度来实现距离不敏感的公式，以便于相机感知方法的优化过程。 
- (3):本文提出了一种简单而有效的方法，称为BEVHeight，通过回归地面高度来实现距离不敏感的公式，以便于相机感知方法的优化过程。具体而言，我们首先为每个像素预测分类高度分布，以将丰富的上下文特征信息投影到楔形体素空间中的适当高度间隔中。然后进行体素池化操作和检测头以获得最终输出检测结果。此外，我们提出了一种超参数可调整的高度采样策略。我们在两个流行的路侧感知基准测试上进行了广泛的实验，BEVHeight在传统设置下的性能优于所有先前的方法，并且在现实场景下，相对于BEVDepth，BEVHeight的性能提高了26.88％，这进一步证明了我们方法的鲁棒性。
- (4):本文的方法在路侧3D物体检测任务上取得了最先进的性能，无论是在传统设置下还是在现实场景下。在传统设置下，BEVHeight的性能优于所有先前的方法，无论是单目3D检测器还是最近的鸟瞰图方法，性能提高了5％。在现实场景下，BEVHeight的性能相对于BEVDepth提高了26.88％，这进一步证明了我们方法的鲁棒性。
#### 7. 方法详细介绍：
本文提出了一种名为BEVHeight的新型框架，用于基于视觉的路边三维物体检测。该框架包括五个主要阶段：图像视图编码器、HeightNet、融合特征、基于高度的2D-3D投影模块和3D检测头。HeightNet从图像特征中预测高度分布的类别，以及与高度分布相关的上下文特征。融合特征将图像上下文和高度分布结合起来。基于高度的2D-3D投影模块将融合特征推入以自我坐标系为基础的楔形体积特征中。最后，3D检测头预测由位置、尺寸和方向组成的3D边界框。

#### 8. 实验设置：
本文在两个流行的路边感知基准测试数据集DAIR-V2X和Rope3D上评估了所提出的方法。DAIR-V2X数据集包含约一万张图像，其中50％、20％和30％的图像分别用于训练、验证和测试。Rope3D数据集包含来自17个交叉口的超过500k张带有三维边界框的图像。本文遵循DAIR-V2X基准测试的平均感知边界框作为KITTI的方法，并使用AP3D|R40和Ropescore作为Rope3D的评价指标。

#### 9. 实验结果和分析：
本文的BEVHeight方法在传统设置下表现出色，在DAIR-V2X基准测试中超过了所有以往的方法，无论是单目3D检测器还是最近的鸟瞰图方法，精度提高了5％。在现实场景中，路边单元的外部参数可能会因各种原因（例如维护和风吹）而发生变化。本文的方法展示了预测高度而不是深度的好处，并在BEVDepth的基础上实现了26.88％的改进，进一步证明了该方法的鲁棒性。本文还进行了消融研究，比较了BEVDepth中深度估计引起的距离误差和BEVHeight中高度估计引起的距离误差，结果表明高度估计引入的误差更小。最后，本文还分析了所提出方法的局限性，表明其在自车设置下的性能有限。


# Paper:933     通过抽象提高验证训练的鲁棒图像分类



#### 1. Title: 
Boosting Verified Training for Robust Image Classifications via Abstraction

#### 2. Authors: 
Zhaodi Zhang, Zhiyi Xue, Yang Chen, Si Liu, Yueling Zhang, Jing Liu, Min Zhang

#### 3. Affiliation: 
上海可信计算重点实验室

#### 4. Keywords: 
Robustness, Neural Networks, Abstraction, Verification, Image Classification

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Boosting_Verified_Training_for_Robust_Image_Classifications_via_Abstraction_CVPR_2021_paper.html  Github: https://github.com/zhangzhaodi233/ABSCERT.git

#### 6. Summary : 
- (1):本文研究的是神经网络图像分类的鲁棒性问题，提出了一种基于抽象的认证训练方法，通过将所有扰动图像映射到区间内，使得训练集的方差变小，模型的损失函数变得平滑，从而显著提高了训练模型的鲁棒性。

- (2):传统的防御方法通过增加训练集中的对抗样本来提高模型的鲁棒性，但是这些方法只针对特定的对手，无法提供鲁棒性保证。最近的方法尝试通过保证神经网络的鲁棒性来训练可证明的鲁棒模型，但是这些方法存在三个主要障碍：(i)将验证结果添加到训练的损失函数中会引入过度估计，从而对神经网络的鲁棒性提高有限；(ii)由于验证问题本身的高复杂度，这些方法非常耗时；(iii)验证与特定类型的神经网络相关联，无法扩展到任意类型的神经网络。为了克服这些障碍，本文提出了一种基于抽象的方法，将扰动图像映射到数值区间上进行训练，从而提高模型的鲁棒性。

- (3):本文提出了一种基于抽象的方法，将所有扰动图像映射到区间内，使得训练集的方差变小，模型的损失函数变得平滑，从而显著提高了训练模型的鲁棒性。对于抽象，本文的训练方法还实现了一种完整的黑盒验证方法，该方法与神经网络的大小和架构无关。本文在不同规模的基准测试中评估了该方法，实验结果表明，该方法优于现有方法，将训练模型的验证错误降低了高达95.64%。

- (4):本文提出的方法在多个基准测试中进行了评估，实验结果表明，该方法在提高模型鲁棒性方面具有显著的优势，验证错误降低了高达95.64%，并且可以扩展到较大的神经网络。
#### 7. 方法详细介绍：
本文提出了一种名为ABSCERT的训练方法，该方法使用抽象来提高神经网络的鲁棒性。该方法涉及扰动输入像素以获得扰动区间，然后将其映射到分类的训练区间。训练是在数值区间上进行的，这保证了映射到相同数值区间的所有扰动区间将具有相同的分类结果。该方法还涉及调整抽象粒度以获得更细的训练区间，并在精细的训练集上重新训练模型。使用黑盒验证方法来验证训练的神经网络的鲁棒性。具体步骤如下：
(1). 将输入空间划分为子区间。
(2). 将扰动图像映射到训练区间。
(3). 在训练区间上训练神经网络。
(4). 使用黑盒验证方法验证训练的神经网络的鲁棒性。

#### 8. 实验设置：
本文在三个数据集上进行了实验：MNIST、CIFAR-10和ImageNet。每个数据集使用不同的神经网络架构，包括CNN和FNN。批量大小设置为128，使用交叉熵损失函数和Adam优化器来更新参数。学习率在热身期后，按照0到π之间的余弦函数值递减，热身期间线性递增在0到1之间。实验在运行Ubuntu 18.04或Windows11的工作站上进行，使用一块NVIDIA GeForce RTX 3090 Ti GPU。

#### 9. 实验结果和分析：
本文提出的ABSCERT方法在MNIST和CIFAR-10数据集上实现了比竞争对手更低的验证错误率。验证错误率的改进范围从49%到95.64%不等。改进随着扰动大小的增加而增加，表明ABSCERT训练的模型比竞争对手训练的模型更具鲁棒性。在相同的扰动下，训练出的神经网络的准确率也比竞争对手更高。


# Paper:934     AdaptiveMix：通过特征空间收缩改善GAN训练



#### 1. Title: 
AdaptiveMix: Improving GAN Training via Feature Space Shrinkage

#### 2. Authors: 
Haozhe Liu, Wentian Zhang, Bing Li, Haoqian Wu, Nanjun He, Yawen Huang, Yuexiang Li, Bernard Ghanem, Yefeng Zheng

#### 3. Affiliation: 
第一作者：King Abdullah University of Science and Technology (KAUST)

#### 4. Keywords: 
Generative Adversarial Networks (GANs), feature space shrinkage, image classification, Out-Of-Distribution (OOD) detection

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Liu_AdaptiveMix_Improving_GAN_Training_via_Feature_Space_Shrinkage_CVPR_2021_paper.html
Github: https://github.com/WentianZhang-ML/AdaptiveMix

#### 6. Summary : 
- (1):本文研究了生成对抗网络（GANs）的训练问题，提出了一种新的模块AdaptiveMix，通过收缩鉴别器中的特征空间来改善GANs的训练。
- (2):过去的方法主要集中在生成器和鉴别器的网络架构设计、数据增强等方面，但是GANs的训练仍然存在困难和不稳定性。本文从鲁棒图像分类的角度出发，提出了AdaptiveMix模块，通过收缩特征空间来控制鉴别器的图像表示，从而提高GANs的训练稳定性和生成图像质量。
- (3):AdaptiveMix模块通过构造混合样本来缩小特征空间，其中混合样本是由一对训练图像混合而成的。本文还将AdaptiveMix模块应用于图像分类和OOD检测任务，并与现有方法进行了比较。实验结果表明，AdaptiveMix模块可以有效地提高GANs的训练效果和生成图像质量，并在图像分类和OOD检测任务中取得了良好的性能。
- (4):本文的方法在多个公开数据集上进行了实验，包括AFHQ-Cat-5k、FFHQ-5k等。实验结果表明，AdaptiveMix模块可以有效地提高GANs的训练效果和生成图像质量，并在图像分类和OOD检测任务中取得了良好的性能。
#### 7. 方法详细介绍：
本文提出的AdaptiveMix方法旨在通过缩小鉴别器特征提取器表示的特征空间中的训练数据区域来改善GAN的训练。AdaptiveMix由两个步骤组成：使用Mixup从训练数据生成困难样本和在特征空间中缩小困难样本和易样本之间的距离。AdaptiveMix损失是一种软损失，它在特征提取器表示的表示空间中减少困难样本和其对应的易样本之间的距离。AdaptiveMix方法可以应用于图像生成和图像分类任务。具体而言，AdaptiveMix方法可以应用于DCGAN和StyleGAN-V2等各种GAN架构。

#### 8. 实验设置：
本文在七个公开数据集上进行了实验，包括CIFAR-10、CelebA、FFHQ、AFHQ-CAT和ImageNet等。本文将AdaptiveMix方法与六个基线模型进行了比较，包括Mixup、R1正则化、Spectral Normalization等。本文还将AdaptiveMix方法与ADA和APA等数据增强方法相结合，进一步提高了GAN的生成性能。本文还对AdaptiveMix方法进行了有限训练数据的评估，并在FID上显著改善了基线。本文还对各种标准数据集进行了图像识别测试，并在所有情况下改善了基线。在OOD检测方面，本文将AdaptiveMix方法与各种最新的OOD检测方法进行了比较，并取得了竞争性的结果。

#### 9. 实验结果和分析：
本文提出的AdaptiveMix方法在各种任务和数据集上都取得了显著的改进。在图像生成任务中，AdaptiveMix方法在CIFAR10、CelebA、AFHQ和FFHQ等数据集上均取得了最新的最佳结果。在图像分类任务中，AdaptiveMix方法在各种标准数据集上均取得了最佳结果。在OOD检测任务中，AdaptiveMix方法在各种数据集上均取得了竞争性的结果。本文还对AdaptiveMix方法进行了Lipschitz连续性评估，并证明了该方法可以保证Lipschitz连续性。


# Paper:935     用于高帧率跟踪的帧-事件对齐与融合网络



#### 1. Title: 
Frame-Event Alignment and Fusion Network for High Frame Rate Tracking

#### 2. Authors: 
Jiqing Zhang, Yuanchen Wang, Wenxi Liu, Meng Li, Jinpeng Bai, Baocai Yin, Xin Yang

#### 3. Affiliation: 
大连理工大学

#### 4. Keywords: 
Event-based cameras, high frame rate tracking, alignment, fusion, deep learning

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Frame-Event_Alignment_and_Fusion_Network_for_High_Frame_Rate_Tracking_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究高帧率目标跟踪问题，传统的RGB-based跟踪器在低帧率下表现良好，但在高速运动下表现不佳。Event-based cameras作为一种生物启发式传感器，由于其高时间分辨率，提供了高帧率跟踪的潜力。然而，event-based cameras不能像传统相机一样提供细粒度的纹理信息，因此需要将传统帧和事件相结合，以在各种挑战性条件下实现高帧率目标跟踪。

- (2):传统的跟踪方法主要针对低帧率的跟踪，而event-based cameras的高时间分辨率可以实现更高的帧率跟踪。然而，event-based cameras不能提供细粒度的纹理信息，因此需要将传统帧和事件相结合。现有的方法主要是基于手工策略或者事件表示算法，但是这些方法在事件过于稀疏或不足时往往无法准确定位目标。本文提出了一种新的端到端框架，将传统帧和事件相结合，实现高帧率目标跟踪。

- (3):本文提出了一种基于深度学习的框架，包括多模态对齐和融合模块，以有效地结合不同测量速率下两种模态的有意义信息。对齐模块负责在事件提供的移动线索的指导下，在帧和事件模态之间进行跨样式和跨帧率对齐。融合模块负责通过两种模态之间的相互补充来强调有价值的特征并抑制噪声信息。本文的方法在各种具有挑战性的条件下实现了高帧率跟踪，并在FE240hz数据集上实现了高达240Hz的高帧率跟踪。

- (4):本文的方法在多个event-based tracking数据集上进行了实验，结果表明，与现有的跟踪器相比，本文的方法在高帧率跟踪方面具有显著的优势。本文的方法可以实现高达240Hz的高帧率跟踪，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种多模态跟踪架构，包括两个关键组件：事件引导的跨模态对齐（ECA）模块和交叉相关融合（CF）模块。该方法通过对齐和融合传统帧和事件数据，有效地结合了两种模态的有意义信息。其中，ECA模块通过事件提供的移动线索，同时完成跨样式和跨帧率的对齐。CF模块通过学习动态滤波器，将一种模态的有价值信息与另一种模态的特征表达相结合，从而强调有价值的信息并抑制干扰。该方法在FE240hz数据集上进行了评估，结果表明，在几乎相等的评估速度下，与其他多模态方法相比，该方法提供了最佳的跟踪精度。该方法在各种具有挑战性的条件下都表现出了有效性和鲁棒性。

#### 8. 实验设置：
本文在两个事件跟踪数据集上进行了评估：DAVIS346和FE240hz。DAVIS346数据集包含346个具有各种具有挑战性的条件的视频序列，包括遮挡、运动模糊和快速运动。FE240hz数据集是一个高帧率数据集，帧率高达240Hz，由事件相机捕获。本文将所提出的方法与几种最先进的跟踪器进行了比较，包括ToMP-MF、DeT、HMFT和FENet。

#### 9. 实验结果和分析：
本文在FE240hz和VisEvent两个数据集上评估了所提出的方法，结果表明，在RPR和RSR指标上，该方法在两个数据集上均优于其他最先进的跟踪器。本文还进行了消融实验，验证了所提出的多模态融合方法和事件引导的跨模态对齐模块的有效性。此外，本文比较了不同事件表示方法的性能，并表明自上一帧强化事件的累积提供了更好的结果。最后，本文比较了不同插值策略的性能，并证明了设计多模态对齐和融合网络以充分利用事件的高时间分辨率是可行且重要的方法。

#### 论文总结：
本文提出了一种多模态跟踪方法，通过对齐和融合传统帧和事件数据，有效地结合了两种模态的有意义信息。该方法在FE240hz和VisEvent两个数据集上进行了评估，结果表明，在RPR和RSR指标上，该方法在两个数据集上均优于其他最先进的跟踪器。本文还进行了消融实验，验证了所提出的多模态融合方法和事件引导的跨模态对齐模块的有效性。该方法具有较强的鲁棒性和适应性，可在各种具有挑战性的条件下实现高精度的跟踪。


# Paper:936     Auto-CARD：用于实时移动遥感的高效稳健编解码头像驱动



#### 1. Title: 
Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time Mobile Telepresence

#### 2. Authors: 
Yonggan Fu, Yuecheng Li, Chenghui Li, Jason Saragih, Peizhao Zhang, Xiaoliang Dai, Yingyan (Celine) Lin

#### 3. Affiliation: 
Georgia Institute of Technology（佐治亚理工学院）

#### 4. Keywords: 
Codec Avatar, telepresence, neural architecture search, real-time, mobile

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Fu_Auto-CARD_Efficient_and_Robust_Codec_Avatar_Driving_for_Real-Time_Mobile_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是实时的、鲁棒的光真实感头像在AR/VR中的应用，这是实现沉浸式真实感远程交互的关键技术之一。

- (2):过去的方法主要集中在减少解码器的计算成本，而编码器的设计则很少被探索，大多数现有的方法都假定在设备外进行编码。本文提出了一种名为Auto-CARD的框架，通过最小化两种冗余来实现实时的、鲁棒的Codec Avatar驱动，即架构冗余和时间冗余。本文的方法是通过神经架构搜索技术AVE-NAS和机制LATEX来实现的。AVE-NAS是一种专门为Avatar编码器设计的神经架构搜索技术，它显式地提高了搜索架构在极端面部表情存在的情况下的鲁棒性，并考虑了快速发展的AR/VR头戴式显示器的硬件友好性。LATEX机制则利用连续渲染中连续捕获的图像的时间冗余，并开发了一种自适应潜在外推机制来跳过冗余帧的计算。 

- (3):本文提出了一种名为Auto-CARD的框架，通过最小化两种冗余来实现实时的、鲁棒的Codec Avatar驱动，即架构冗余和时间冗余。本文的方法是通过神经架构搜索技术AVE-NAS和机制LATEX来实现的。AVE-NAS是一种专门为Avatar编码器设计的神经架构搜索技术，它显式地提高了搜索架构在极端面部表情存在的情况下的鲁棒性，并考虑了快速发展的AR/VR头戴式显示器的硬件友好性。LATEX机制则利用连续渲染中连续捕获的图像的时间冗余，并开发了一种自适应潜在外推机制来跳过冗余帧的计算。本文的贡献在于提出了一种实现实时的、鲁棒的Codec Avatar驱动的框架，这是首次仅使用设备上的计算资源实现的。 

- (4):本文的方法在Meta Quest 2上进行了实验，实现了5.05倍的加速，同时保持了与SOTA Avatar编码器设计相当甚至更好的动画质量。本文的方法可以在AR/VR设备上实现实时的、鲁棒的Codec Avatar驱动，这是实现沉浸式真实感远程交互的关键技术之一。
#### 7. 方法详细介绍：
本文提出了一种名为Auto-CARD的框架，用于实时移动遥控的高效和稳健的编解码器驱动。该框架由AVE-NAS和LATEX两个组件组成。AVE-NAS是一种可微分搜索方案，用于发现高效和有效的编码器架构。它采用了一个视角解耦的超网络，独立处理每个捕获的部分面部图像。搜索空间跨越运算符类型、深度、宽度和输入分辨率。LATEX是一种机制，通过潜在代码外推自适应地跳过冗余帧的计算。它旨在最小化模型和时间冗余，同时保持准确性。该方法使用混合可微搜索方案，集成了重新参数化技巧和策略梯度。运算符/通道搜索使用Gumbel Softmax函数，通道搜索遵循通道掩蔽策略。分辨率搜索采用策略梯度来估计梯度。搜索到的编码器将从头开始训练。

#### 8. 实验设置：
本文使用了一个大型多摄像头捕获设备捕获的约12k帧数据集来训练解码器，以及约42k帧HMC捕获的数据集来搜索/训练/测试编码器。每个身份的HMC捕获数据包括具有特定表情、面部动作和语音的不同片段，其中部分表情片段被随机选择为测试数据，其他片段作为训练数据。编码估计通过解码成3个不同的视角方向进行评估：前方、左侧和右侧视图。渲染的头像图像大小设置为1024×736。在两个具有不同资源设置的设备上测量了搜索到的编码器的实际设备延迟，包括AR/VR头戴式显示器Meta Quest 2上的SnapDragon 865 SoC和Google Pixel 3手机。

#### 9. 实验结果和分析：
本文提出的Auto-CARD方法在AR/VR中实现了编解码器驱动，实现了5.05倍的加速，同时保持了与最先进的编码器设计相当甚至更好的动画质量。AVE-NAS编码器在Meta Quest 2上的测量中始终实现更好的MSE效率权衡。搜索到的编码器在不同身份和视角下实现了显着更好的压缩效果，表明联合探索编码器架构的多个维度对于保持高渲染质量至关重要。极端表情感知目标显著提高了准确性。启用AVE-NAS和LATEX可以实现更好的延迟-MSE权衡，例如，在Quest 2上平均测量的1.43倍加速和AVE-T上的0.48 MSE降低。


# Paper:937     MVImgNet：一个大规模的多视角图像数据集



#### 1. Title: 
MVImgNet: A Large-scale Dataset of Multi-view Images

#### 2. Authors: 
Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, Guanying Chen, Shuguang Cui, Xiaoguang Han

#### 3. Affiliation: 
香港中文大学（深圳）FNii实验室

#### 4. Keywords: 
Multi-view images, 3D-aware signals, large-scale dataset, point clouds, 2D and 3D vision

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_MVImgNet_A_Large-Scale_Dataset_of_Multi-View_Images_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文的研究背景是深度学习算法的数据驱动特性，以及ImageNet在2D视觉领域的成功应用。然而，在3D视觉领域，由于真实世界3D数据的采集困难，目前还没有像ImageNet一样的通用数据集。因此，本文旨在构建一个大规模的多视角图像数据集，作为2D和3D视觉之间的软连接，以弥补这一缺陷。

- (2):过去的方法主要是使用3D合成数据集或者真实世界3D数据集，但是前者缺乏真实世界的线索，后者采集和标注困难，规模也不够大。本文提出的多视角图像数据集可以通过在普通移动设备上拍摄不同视角的对象来方便地获得，同时具有自然的3D视觉信号。本文的方法具有很好的动机。

- (3):本文提出了MVImgNet，一个包含6.5百万帧、238个类别的219,188个视频的大规模多视角图像数据集，以及一个基于MVImgNet的大规模真实世界3D对象点云数据集MVPNet。本文的方法通过多视角一致性进行密集重建，从而获得3D感知信号。本文的贡献在于构建了一个大规模的多视角图像数据集，为3D视觉领域提供了一个通用的数据集，同时提供了一个大规模的真实世界3D对象点云数据集。

- (4):本文的方法在多个3D和2D视觉任务上进行了实验，包括辐射场重建、多视角立体视觉和视角一致的图像理解。实验结果表明，MVImgNet在这些任务上表现出了很好的性能，为未来的探索提供了很多可能性。MVPNet在真实世界3D对象分类任务上表现出了很好的性能，同时也为点云理解提出了新的挑战。
#### 7. 方法详细介绍：
本文提出了一个大规模的多视角图像数据集MVImgNet，其中包含来自238个类别的219,188个真实物体的视频，共计6.5百万帧。数据集的采集过程高效，包括原始数据准备、视频拍摄和众包。对于每个合格的视频提交，采用自动化流程获取公共的2D和3D注释，包括对象掩模、相机内部和外部、深度图和点云。MVImgNet的多视角属性赋予了数据集3D感知信号，使其成为2D和3D视觉之间的软桥梁。本文还提出了一个新的3D物体点云数据集MVPNet，通过MVImgNet上的密集重建得到，包含150个类别的87,200个点云，每个点云都有类别标签。本文还介绍了使用MVImgNet进行对比学习和自监督预训练的方法，以及使用MVImgNet进行视觉任务的实验细节，包括辐射场重建、多视角立体和视角一致的图像理解。

#### 8. 实验设置：
本文对MVImgNet进行了对比学习和自监督预训练，并在DTU数据集上进行了实验评估。本文还对MVImgNet进行了视觉任务的实验，包括辐射场重建、多视角立体和视角一致的图像理解。本文还对MVPNet进行了实验评估，包括点云分类和点云预训练。

#### 9. 实验结果和分析：
本文在MVImgNet上进行了对比学习和自监督预训练，并在DTU数据集上进行了实验评估。实验结果表明，使用MVImgNet进行预训练可以提高模型的性能。本文还对MVImgNet进行了视觉任务的实验，包括辐射场重建、多视角立体和视角一致的图像理解，实验结果表明MVImgNet在这些任务上具有很好的表现。本文还对MVPNet进行了实验评估，包括点云分类和点云预训练，实验结果表明MVPNet在真实世界的3D物体分类中具有很好的表现。


# Paper:938     全面而精细：一种高效的图像恢复Transformer



#### 1. Title: 
Comprehensive and Delicate: An Efficient Transformer for Image Restoration

#### 2. Authors: 
Haiyu Zhao, Yuanbiao Gou, Boyun Li, Dezhong Peng, Jiancheng Lv, Xi Peng

#### 3. Affiliation: 
四川大学计算机学院

#### 4. Keywords: 
Image restoration, Vision Transformers, Superpixel-wise global dependency, Pixel-wise global dependency, Efficient Transformer

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Comprehensive_and_Delicate_An_Efficient_Transformer_for_Image_Restoration_CVPR_2022_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是图像恢复领域中，传统的卷积神经网络（CNN）在全局依赖捕捉和实例适应能力方面存在局限性。

- (2):过去的方法主要是基于CNN的方法，但是CNN的权重是静态的，不能动态适应实例，且CNN的连接是稀疏的，不能捕捉全局依赖。为了解决这些问题，本文提出了一种新的Transformer架构，通过超像素级别的全局依赖计算和转换，实现像素级别的全局依赖捕捉。与现有的方法不同，本文提出的方法包括两个神经块，即CA和DA，分别捕捉超像素级别的全局依赖和将全局性从超像素分配到像素级别。本文的方法在效率和性能方面都有很大的提升。

- (3):本文提出的方法是一种高效的Transformer，通过超像素级别的全局依赖计算和转换，实现像素级别的全局依赖捕捉。本文的方法包括两个神经块，即CA和DA，分别捕捉超像素级别的全局依赖和将全局性从超像素分配到像素级别。本文的方法在效率和性能方面都有很大的提升。

- (4):本文的方法在四个图像恢复任务上进行了广泛的实验，证明了其效率和有效性。与SwinIR相比，本文的方法只需要约6%的FLOPs，就能达到可比较的性能。
#### 7. 方法详细介绍：
本文提出了一种高效的图像恢复Transformer，称为CODE。该方法采用编码器-解码器架构，由多个Transformer块组成。编码器使用多尺度特征提取策略，解码器使用跳跃连接来融合分层多尺度特征。所提出的Transformer块包括压缩注意力神经块（CA）和双自适应神经块（DA）。CA通过特征聚合、注意力计算和特征恢复在超像素级别捕获全局依赖性。DA通过动态加权和组卷积进一步增强特征表示。最后，使用3x3卷积将最终特征融合为残差图像，将其添加到退化图像中以获得恢复图像。

#### 8. 实验设置：
本文在PyTorch框架下使用NVIDIA GeForce RTX 3090 GPU进行实验。使用Adam优化器，β1 = 0.9，β2 = 0.999。学习率从2e-4初始化，并通过余弦退火策略逐渐降至1e-6。模型的批量大小为16，补丁大小为128，训练1000K次迭代。使用水平和垂直翻转以及90°、180°和270°旋转进行数据增强。所有FLOPs都是在128×128图像上计算的。

#### 9. 实验结果与分析：
本文在四个图像恢复任务上评估了所提出的方法，包括灰度和彩色图像去噪、JPEG压缩伪影去除和运动去模糊。本文将所提出的方法与几种最先进的方法进行了比较，并以PSNR为指标报告了定量结果。本文还以图像形式呈现了定性结果。

#### 论文总结：
本文提出了一种高效的图像恢复Transformer，称为CODE。该方法采用编码器-解码器架构，由多个Transformer块组成。所提出的Transformer块包括压缩注意力神经块（CA）和双自适应神经块（DA）。本文在四个图像恢复任务上评估了所提出的方法，并将其与几种最先进的方法进行了比较。实验结果表明，所提出的方法在保持较少参数和FLOPs的情况下，取得了与最先进方法相当甚至更好的性能。


# Paper:939     ReLight My NeRF：用于真实世界物体的新视图合成和重新照明的数据集



#### 1. Title: 
ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects

#### 2. Authors: 
Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti, Daniele De Gregorio, Luigi Di Stefano, Samuele Salti

#### 3. Affiliation: 
Marco Toschi, Riccardo Spezialetti, Daniele De Gregorio: Eyecan.ai, 意大利
Riccardo De Matteo: University of Bologna, 意大利
Luigi Di Stefano, Samuele Salti: University of Bologna, 意大利

#### 4. Keywords: 
Neural Radiance Fields, Relighting, Novel View Synthesis, Dataset, Real-world Objects

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Toschi_ReLight_My_NeRF_A_Dataset_for_Novel_View_Synthesis_and_CVPR_2021_paper.html  Github: https://github.com/eyecan-ai/rene

#### 6. Summary : 
- (1):本文关注的是从神经辐射场（NeRF）中渲染新视图的问题，特别是在未观察到的光照条件下。为此，我们引入了一个新的数据集，称为ReNe（Relighting NeRF），在一次一个光源（OLAT）条件下捕捉了真实世界物体的图像，并注释了准确的地面真实相机和光源姿态。我们的采集管道利用两个机器人臂，分别持有相机和全向点光源。我们发布了总共20个场景，描绘了具有复杂几何形状和具有挑战性材料的各种物体。每个场景包括2000张图像，从50个不同的视点下以40个不同的OLAT条件进行捕获。
- (2):NeRF及其变体目前是逆向渲染流水线中新视图合成的最具吸引力的策略。然而，NeRF及其变体的学习场景表示将材料和照明纠缠在一起，因此不能直接用于在新的未观察到的照明条件下生成视图。本文提出了一种新的神经辐射场架构，以学习在新的未观察到的照明条件下执行重新照明的能力。我们的研究表明，通过估计颜色并使用两个单独的子网络，一个负责软阴影预测，另一个负责神经逼近BRDF，我们可以执行有效的重新照明，例如投射复杂阴影。我们提供了我们的新架构的结果作为新基准的参考线。
- (3):本文提出了一种数据采集方法，适用于收集具有高质量相机和光源姿态注释的物体的一组OLAT图像。我们的数据集包含20个场景，每个场景包括2000张图像，从50个不同的视点下以40个不同的OLAT条件进行捕获。我们还提出了一种新的神经辐射场架构，以学习在新的未观察到的照明条件下执行重新照明的能力。我们的研究表明，通过估计颜色并使用两个单独的子网络，我们可以执行有效的重新照明，例如投射复杂阴影。我们提供了我们的新架构的结果作为新基准的参考线。
- (4):本文提出的数据集和方法在新视图合成和重新照明任务上取得了良好的性能。我们的研究表明，我们的新架构可以在新的未观察到的照明条件下生成具有挑战性的物体的视图，并且可以投射复杂阴影。我们的数据集和基
#### 7. 方法详细介绍：
- 本文提出了一种改进的神经辐射场（NeRF）方法，用于实现真实物体的重照和新视角合成。该方法包括以下几个方面的改进：
  - 将光源位置信息注入颜色MLP中，使用3D单位向量表示光源与查询位置之间的相对位置；
  - 引入跳跃连接以保留查询点位置的信息；
  - 将颜色MLP的第一层输出与ReLU输出进行拼接；
  - 用基于物理的体积渲染替换沿射线的每个3D点的发射颜色，以处理非发射和非吸收体积的情况；
  - 使用位置编码将3D坐标和视线方向编码到高维空间中。
- 本文在包含20个场景的数据集上进行了实验，每个场景有50个前方视点和40个不同的光照条件，共计40000张图像。实验结果表明，所提出的方法在新视角合成和重照方面均优于标准NeRF方法。

#### 8. 实验设置：
- 本文使用两个工业机器人LightBot和CameraBot分别控制光源和相机的位置。机器人通过ChArUco板进行校准，以获得所有场景中光源和相机的准确6-DoF姿态信息。
- 数据集包含20个场景，每个场景有50个前方视点和40个不同的光照条件，共计40000张图像。数据集被分为训练、验证和测试子集，以比较不同的NeRF重照方法。测试集包括在训练时使用的光照下的新视角（简单分裂）和从未见过的光照下的新视角（困难分裂）。

#### 9. 实验结果与分析：
- 本文提出的NeRF改进方法在重照和新视角合成任务上均取得了优异的结果，比标准NeRF方法表现更好。在基准数据集上，本文方法的PSNR和SSIM指标均优于其他方法。此外，本文方法在处理具有挑战性的场景（如细线框结构、复杂阴影、镜面反射和半透明塑料线）时也表现出色。


# Paper:940     视觉依赖变换器：依赖树从反向注意力中出现



#### 1. Title: 
Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention

#### 2. Authors: 
Mingyu Ding, Yikang Shen, Lijie Fan, Zhenfang Chen, Zitian Chen, Ping Luo, Josh Tenenbaum, Chuang Gan

#### 3. Affiliation: 
Mingyu Ding: 香港大学
Yikang Shen, Zhenfang Chen: MIT-IBM Watson AI Lab
Lijie Fan: MIT
Zitian Chen: UMass Amherst
Ping Luo: 香港大学
Josh Tenenbaum: MIT
Chuang Gan: MIT-IBM Watson AI Lab, MIT

#### 4. Keywords: 
Visual Dependency Transformers, Reversed Attention, Dependency Tree, Self-Attention, Image Parsing

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Visual_Dependency_Transformers_Dependency_Tree_Emerges_From_Reversed_Attention_CVPR_2022_paper.html  Github: https://github.com/dingmyu/DependencyViT

#### 6. Summary : 
- (1):本文研究的背景是如何从图像中提取结构化的表示，包括实体、部件以及它们之间的依赖关系。
- (2):过去的方法主要是基于监督学习或者需要手动标注的方法，难以处理复杂的场景和大规模的数据集。本文提出了一种新的神经操作符——反向注意力，可以自然地捕捉图像块之间的长程依赖关系，从而无需标签即可诱导视觉依赖关系。该方法可以自然地从叶节点到根节点逐步地无监督地诱导依赖树。此外，本文还提出了一种动态视觉池化方案，可以减少计算和内存占用。 
- (3):本文提出了一种基于反向注意力的依赖关系视觉变换器（DependencyViT），可以自然地从图像块中诱导视觉依赖关系，无需标签。具体来说，我们将其构建为一个依赖图，其中反向注意力中的子标记被训练为关注其父标记并按照归一化的概率分布发送信息，而不是像传统的自注意力一样收集信息。通过这样的设计，层次结构自然地从反向注意力层中出现，并且依赖树从叶节点到根节点逐步无监督地诱导出来。 
- (4):本文在ImageNet数据集上进行了自监督和弱监督预训练的广泛研究，以及在8个数据集和5个任务上的实验，如无监督部分和显著性分割、识别和检测。实验结果表明，DependencyViT在处理图像中的实体和部件方面具有很好的效果，同时还可以进行动态视觉池化，减少计算和内存占用。
#### 7. 方法详细介绍：
本文提出了一种名为Visual Dependency Transformers (DependencyViT)的新型神经算子，用于在视觉任务中自动捕捉补丁之间的依赖关系。该方法基于反向自注意力机制，通过自监督或弱监督信号自动捕捉补丁之间的依赖关系。方法引入了反向自注意力机制，将邻接概率矩阵转置，使子节点向父节点发送信息，以及依赖块，包括头选择器和消息控制器，用于选择适当的反向注意力头进行依赖关系诱导，并确定节点或子树发送消息的程度。软依赖掩码是通过在头维度上对反向注意力矩阵进行求和来诱导的，依赖图和树结构是通过argmax（·）和chu-liu-edmonds算法获得的。该方法还包括一种动态视觉池化方案，大大降低了计算成本，并提出了轻量级模型DependencyViT-Lite。

#### 8. 实验设置：
本文在ImageNet数据集上进行了自监督和弱监督预训练以及五个下游任务的广泛实验，包括无监督部分和显著性分割、识别和检测。将提出的方法DependencyViT与几种最先进的方法进行了比较。

#### 9. 实验结果和分析：
本文的实验结果表明，DependencyViT在各种视觉任务中均优于基线方法，证明了视觉依赖分析的有效性。在COCO数据集上，本文还展示了对图像解析的依赖树的可视化。在附录中，还进行了语义分割、目标检测和视频识别等下游实验。


# Paper:941     在表示空间中学习测量点云重建损失



#### 1. Title: 
Learning to Measure the Point Cloud Reconstruction Loss in a Representation Space

#### 2. Authors: 
Tianxin Huang, Zhonggan Ding, Jiangning Zhang, Ying Tai, Zhenyu Zhang, Mingang Chen, Chengjie Wang, Yong Liu

#### 3. Affiliation: 
Tianxin Huang: APRIL Lab, Zhejiang University (浙江大学APRIL实验室)

#### 4. Keywords: 
Point cloud reconstruction, contrastive adversarial loss, representation space, shape similarity, shape differences

#### 5. Paper: 
https://openaccess.thecvf.com/content_CVPR_2021/papers/Huang_Learning_to_Measure_the_Point_Cloud_Reconstruction_Loss_in_a_CVPR_2021_paper.pdf
Github: None

#### 6. Summary:
- (1): 本文研究点云重建相关任务中的重建损失，以评估重建结果与真实值之间的形状差异。现有的方法通常使用点对点距离来测量训练损失，但这可能会引入额外的缺陷，因为预定义的匹配规则可能会偏离真实的形状差异。本文提出了一种基于学习的对比对抗损失（CALoss），通过将对比约束与对抗策略相结合，在非线性表示空间中动态地测量点云重建损失，以测量形状差异。
- (2): 现有的重建损失主要依赖于3D欧几里得空间中的距离来测量形状差异。本文提出的CALoss可以在高维表示空间中动态地测量点云重建损失，通过对比约束和对抗训练策略，CALoss可以构建一个表示空间，其中相似的形状应该具有相似的表示，并学习在该空间中搜索形状差异。与现有方法相比，CALoss可以更全面地搜索形状差异，提高训练效率。
- (3): 本文提出了一种基于学习的对比对抗损失（CALoss）来动态地测量点云重建损失。CALoss由Lp、Lr和Ladvr组成，这些损失是通过全局表示之间的距离获得的。Lp是对比约束，用于帮助CALoss构建具有形状相似性的表示空间。Ladvr是对抗损失，用于指导CALoss在Sg和So之间搜索形状差异。Lr用于训练任务网络。CALoss可以在非线性表示空间中动态地测量点云重建损失，以测量形状差异。
- (4): 本文在点云重建、无监督分类和点云完成等任务上进行了实验，结果表明CALoss可以帮助任务网络提高重建性能并学习更具代表性的表示。
#### 7. 方法详细介绍：
本文提出了一种新的方法CALoss，用于在高维表示空间中测量重建损失。CALoss的流程包括将重建结果和真实值输入CALoss中，以评估形状差异。通过小扰动构建正样本，将真实值、正样本和重建结果转换为特征，使用自适应池化将这些特征聚合成全局表示，并在训练过程中通过池化控制器动态更改和控制自适应池化。最后，通过任务的真实值和重建结果计算损失，通过这些损失优化CALoss和任务网络的整个训练过程。

#### 8. 实验设置：
本文在三个任务上进行了实验，包括基本点云重建、点云无监督分类和点云补全，以评估CALoss的性能。实验使用的数据集分别是ModelNet40、ShapeNet和ShapeNet Completion。实验中使用的网络架构是PointNet++，具有共享的编码器-解码器结构。优化器使用Adam，学习率为0.001。批量大小设置为16，训练时期设置为200。

#### 9. 实验结果与分析：
本文在点云重建、无监督分类和点云补全任务上评估了CALoss的性能。实验结果表明，CALoss在大多数情况下优于常用的重建损失，并可以帮助任务网络学习更具代表性的表示。具体而言，在ModelNet40数据集上，CALoss在无监督分类任务上实现了92.3％的准确率，在点云补全任务上实现了0.003的Chamfer距离。在ShapeNet数据集上，CALoss在点云补全任务上实现了0.002的Chamfer距离。在ShapeNet Completion数据集上，CALoss在点云补全任务上实现了0.002的Chamfer距离。本文还对CALoss中采用的组件进行了消融研究，证实了对比和对抗约束在CALoss中的重要性。此外，本文展示了在训练过程中动态更新CALoss的必要性，以持续搜索真实值和任务网络输出之间的形状差异。


# Paper:942     基于可微分Top-K的部分图匹配深度学习



#### 1. Title: 
Deep Learning of Partial Graph Matching via Differentiable Top-K

#### 2. Authors: 
Runzhong Wang, Ziao Guo, Shaofei Jiang, Xiaokang Yang, Junchi Yan

#### 3. Affiliation: 
上海交通大学人工智能研究院

#### 4. Keywords: 
Graph matching, deep learning, partial matching, top-k selection, attention mechanism

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2021_paper.html  Github: https://github.com/Thinklab-SJTU/ThinkMatch

#### 6. Summary : 
- (1):本文研究的背景是图匹配问题中的部分匹配问题，即存在离群点的情况下，如何进行图节点匹配。
- (2):过去的方法通常缺乏压制错误匹配的方案，而且往往需要手动设置阈值。本文提出了一种基于top-k选择的方案，通过可微分的top-k模块，可以有效地进行梯度下降，从而压制错误匹配。同时，本文提出了一种注意力机制，可以自动估计匹配中的离群点数量。这种方法可以与现有的SOTA图匹配神经网络相结合，具有较好的性能。
- (3):本文提出了一种基于top-k选择的方案，通过可微分的top-k模块，可以有效地进行梯度下降，从而压制错误匹配。同时，本文提出了一种注意力机制，可以自动估计匹配中的离群点数量。这种方法可以与现有的SOTA图匹配神经网络相结合，具有较好的性能。
- (4):本文提出的方法在多个图匹配数据集上进行了实验，结果表明，与现有的方法相比，本文提出的方法在部分匹配问题上具有更好的性能。
#### 7. 方法详细介绍：
本文提出了一种基于可微分top-k算法和注意力融合聚合模型的深度学习方法，用于解决部分图匹配问题。该方法包括三个模块：全局匹配网络（GM）、无锚点注意力（AFA）和可微分top-k（DTK）。在第一阶段中，GM网络被训练以预测两个图中节点之间的匹配得分。在第二阶段中，引入AFA模块以提高匹配性能，通过关注最具信息量的节点来实现。在第三阶段中，使用DTK模块选择前k个匹配得分，并通过选择过程反向传播梯度。该方法在基准数据集上与几种最先进的方法进行了比较，并取得了竞争性的性能。

#### 8. 实验设置：
本文在两个基准数据集上评估了所提出的方法：Willow Object Class和IMC-PT-SparseGM。Willow Object Class数据集包含六个对象类别，难度不同，IMC-PT-SparseGM数据集是基于Image Matching Challenge PhotoTourism（IMC-PT）2020的视觉图匹配新基准。实验设置包括基准中使用的锚点数量，向数据集添加的异常值数量以及用于测量匹配性能的评估指标。

#### 9. 实验结果和分析：
本文在两个基准数据集上提供了详细的实验结果。将所提出的方法与几种最先进的方法进行了比较，包括PMH、PCA-GM、BBGM、NGMv2和GCAN。使用的评估指标是F1分数，结果表明，所提出的方法在两个数据集上取得了竞争性的性能。本文还提供了有关所提出方法的可扩展性和内存效率的分析，并表明NGMv2网络的稀疏实现提高了该方法的可扩展性。


# Paper:943     延迟很重要：实时行动预测变压器



#### 1. Title: 
Latency Matters: Real-Time Action Forecasting Transformer

#### 2. Authors: 
Harshayu Girase, Nakul Agarwal, Chiho Choi, Karttikeya Mangalam

#### 3. Affiliation: 
Harshayu Girase: Honda Research Institute USA
Nakul Agarwal, Chiho Choi: Honda Research Institute USA, now at Samsung Semiconductor US
Karttikeya Mangalam: UC Berkeley

#### 4. Keywords: 
Action forecasting, real-time, transformer, self-supervised learning, latency-aware evaluation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2021_paper.html  Github: https://github.com/karttikeya/RAFTformer

#### 6. Summary : 
- (1):本文研究实时行动预测中的延迟问题，提出了一种实时行动预测变压器（RAFTformer）。
- (2):过去的方法忽略了模型推理延迟，而本文提出了一种考虑延迟的实时评估设置，并提出了一种新的自监督掩蔽方案来提高预测准确性。RAFTformer使用两阶段完全变压器架构，比以前的方法快9倍，同时在预测准确性方面表现更好。RAFTformer使用94％更少的训练计算和90％更少的训练参数，以比以前的最先进基线方法在EGTEA Gaze+和EPIC-Kitchens-100验证集上的Top-5召回率（T5R）分别提高4.9个点和1.4个点。在实时设置下，RAFTformer在EPIC-Kitchens-100数据集上的T5R分数比以前的方法高出4.4个点。
- (3):本文提出了RAFTformer，一种实时行动预测变压器，使用两阶段变压器编码器网络进行快速预测。RAFTformer使用一种新的自监督掩蔽方案和专门的预测令牌来学习预测多个时间范围内的行动，从而提高了短期行动预测的模型推理能力。RAFTformer的设计使其能够比以前的最先进方法快9倍，同时使用更少的训练计算和训练参数。 
- (4):RAFTformer在EGTEA Gaze+和EPIC-Kitchens-100验证集上的Top-5召回率（T5R）分别提高4.9个点和1.4个点。在实时设置下，RAFTformer在EPIC-Kitchens-100数据集上的T5R分数比以前的方法高出4.4个点。这些结果表明，RAFTformer在实时行动预测方面具有很高的性能和实用性。
#### 7. 方法详细介绍：
本文提出了一种名为Real-Time Action Forecasting Transformer（RAFTformer）的实时动作预测模型。该模型由两个阶段组成：第一个阶段是短期视频transformer骨干网络，用于特征提取；第二个阶段是长期头transformer编码器，用于跨多个片段进行长期时间聚合。模型使用预测令牌来聚合全局上下文，并使用洗牌因果掩码进行自监督，通过预测洗牌后的未来特征。模型使用自监督损失、未来特征预测损失和动作预测损失进行训练。模型在EPIC-KITCHENS和EGTEA+ Gaze数据集上进行了评估，并取得了最先进的性能。

#### 8. 实验设置：
本文在EPIC-KITCHENS和EGTEA+ Gaze数据集上评估了所提出的方法。使用了EPIC-55和EPIC-100两种不同的数据集划分，并报告了top-1准确率和top-5召回率。同时，本文还报告了模型的可训练参数数量、训练模型收敛所需的总计算量以及推理延迟时间。

#### 9. 实验结果与分析：
本文在EPIC-Kitchens-100和EPIC-Kitchens-55数据集以及EGTEA Gaze+数据集上进行了离线和实时评估。RAFTformer在所有数据集上均优于先前的基准，并在EGTEA Gaze+数据集上取得了最先进的性能。本文还讨论了模型大小和推理延迟之间的权衡，并表明RAFTformer在40毫秒的延迟下实现了最佳性能。


# Paper:944     如何防止噪声对模型训练的持续损害？



#### 1. Title: 
How to Prevent the Continuous Damage of Noises to Model Training?

#### 2. Authors: 
Xiaotian Yu, Yang Jiang, Tianqi Shi, Zunlei Feng, Yuexuan Wang, Mingli Song, Li Sun

#### 3. Affiliation: 
第一作者：浙江大学

#### 4. Keywords: 
Deep learning, Noisy labels, Gradient Switching Strategy, Model training, Misleading gradient directions

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yu_How_to_Prevent_the_Continuous_Damage_of_Noises_to_Model_Training_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是深度学习中的噪声标签问题，由于许多基准数据集中不可避免地包含噪声标签，因此如何处理噪声标签对于深度学习的应用具有重要意义。

- (2):过去的方法主要包括鲁棒损失函数和样本清理，但是这些方法仍然存在一些问题，例如减少低置信度样本的权重会导致模型泛化能力下降。本文提出的Gradient Switching Strategy（GSS）通过消除噪声标签导致的误导梯度方向来防止噪声标签对模型训练的持续损害，具有很好的动机。

- (3):本文提出的GSS方法通过为每个样本设计一个梯度方向池来防止噪声标签对模型训练的持续损害。该梯度方向池包含不同概率的所有类别梯度方向，对于不确定的样本，不同梯度方向的概率由随机性决定，而对于高置信度样本，模型将使用其潜在的主要方向进行优化。本文的创新点在于提出了一种新的防止噪声标签对模型训练的持续损害的方法。

- (4):本文在CIFAR-10数据集上进行了实验，结果表明，与现有的鲁棒损失函数和样本清理方法相比，本文提出的GSS方法可以有效地防止噪声标签对模型训练的持续损害，并且在高噪声率下可以实现1.23% ~ 9.22%的准确率提升，同时与使用干净数据训练的模型相比，本文提出的GSS方法在噪声数据上训练的模型可以实现可比较的性能。
#### 7. 方法详细介绍：
本文提出了一种名为梯度切换策略（Gradient Switching Strategy，GSS）的方法，用于防止噪声对模型训练的持续损害。GSS通过在不同概率下切换不确定样本的梯度方向，强制它们在不同方向上探索，而不是在固定方向上误导模型优化。理论分析表明，GSS可以有效减少标签噪声引起的梯度偏差。

具体而言，GSS包括以下步骤：
1. 对于每个样本，建立一个梯度方向池，包含所有类别的梯度方向，每个方向的概率动态变化。
2. 对于高置信度样本，以其梯度方向池中的主方向为优化方向，概率较大。
3. 对于不确定样本，梯度方向池中的概率更随机，以避免固定方向的误导。
4. 在训练过程中，根据样本的预测结果和原始标签，动态调整梯度方向池中各个方向的概率。
5. 根据梯度方向池中的概率，随机选择一个方向作为当前样本的梯度方向。

#### 8. 实验设置：
本文未提供具体的实验设置信息。

#### 9. 实验结果与分析：
本文提出的GSS方法在合成和真实噪声数据集上均优于现有方法，并且在使用噪声标签训练时，与使用干净标签训练的模型性能相当。在CIFAR-10和CIFAR-100数据集上，GSS方法在不同噪声比率下均取得了显著的改进。在真实噪声数据集Clothing1M和WebVision上，GSS方法也取得了比现有方法更好的结果。在top-5准确率上的改进要高于top-1准确率，这表明梯度方向池在训练模型时可以引导其在多个方向上进行探索，从而更好地应对不确定样本。


# Paper:945     Imagen Editor和EditBench：推进和评估文本引导的图像修复



#### 1. Title: 
Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting

#### 2. Authors: 
Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, William Chan

#### 3. Affiliation: 
谷歌研究院

#### 4. Keywords: 
text-guided image editing, image inpainting, object masking, evaluation benchmark

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的背景是文本引导的图像编辑，旨在支持创意应用。
- (2):过去的方法主要是基于随机掩蔽区域的训练，缺乏对文本提示的关注，导致生成的输出与文本提示不一致。本文提出了一种新的对象掩蔽技术，鼓励模型在训练过程中更多地依赖于文本提示，从而提高了文本-图像对齐的质量。本文的方法是有动机的，创新性地提出了一个系统性的基准测试集EditBench，以评估文本引导的图像修复的质量。
- (3):本文提出了一种级联扩散模型Imagen Editor，通过在Imagen上进行微调，实现了文本引导的图像修复。Imagen Editor通过使用对象检测器在训练过程中提出修复掩蔽区域，从而实现了对文本提示的忠实度。此外，Imagen Editor通过在级联管道中对原始高分辨率图像进行条件化，捕捉了输入图像的细节。本文的创新点在于提出了一种对象掩蔽技术，提高了文本-图像对齐的质量，并提出了一个系统性的基准测试集EditBench，以评估文本引导的图像修复的质量。
- (4):本文的方法在EditBench基准测试集上进行了广泛的人类评估，结果表明，对象掩蔽技术在文本-图像对齐方面取得了显著的改进。在文本-图像对齐方面，Imagen Editor在68%的比较中优于使用随机掩蔽区域的Imagen Editor。在图像质量方面，Imagen Editor相对于Stable Diffusion和DALL-E 2更受人类评估者的青睐。本文的方法在对象渲染方面表现更好，在文本渲染方面表现较差，在材料/颜色/大小属性方面表现更好，在数量/形状属性方面表现较差。本文的方法在文本引导的图像修复任务上取得了良好的性能，支持其目标。
#### 7. 方法详细介绍：
本文提出了一种基于Imgen的级联扩散模型，称为Imagen Editor，用于文本引导的图像修复。该模型通过三个卷积下采样图像编码器向每个扩散阶段添加图像和掩模上下文。在训练期间，模型使用目标检测器提出修复掩模，以确保生成的输出与文本提示相符。本文还提出了一种新颖的对象掩蔽技术，以鼓励模型在训练期间更多地依赖于文本提示。在方法部分，本文还描述了使用无分类器引导（CFG）来偏置样本以特定条件，以确保文本引导的图像修复生成的图像与输入文本提示之间具有强烈的对齐。

#### 8. 实验设置：
本文提出了一个名为EditBench的系统性基准，用于文本引导的图像修复。EditBench评估自然和生成图像上的修复编辑，探索对象、属性和场景。每个EditBench示例包括一个掩蔽的输入图像、一个输入文本提示和一个高质量的输出图像，可用作自动度量的参考。本文在EditBench上进行了广泛的人类评估，探究了Imagen Editor、Stable Diffusion和DALL-E 2的性能。

#### 9. 实验结果和分析：
本文在EditBench上进行了全面的人类评估，包括文本-图像对齐和图像质量。本文还分析了不同文本提示下人类编辑任务的表现。本文评估了所提出的掩蔽策略和CFG在提高文本引导的图像修复模型性能方面的有效性。本文表明，所提出的掩蔽策略效果出乎意料地好，减轻了使用随机掩蔽策略训练的模型所面临的大部分问题。本文还观察到，高引导权重与振荡引导相结合，可以在样本保真度和文本-图像对齐之间取得最佳平衡。


# Paper:946     教练一个可教的学生



#### 1. Title: 
Coaching a Teachable Student

#### 2. Authors: 
Jimuyang Zhang, Zanming Huang, Eshed Ohn-Bar

#### 3. Affiliation: 
Boston University（波士顿大学）

#### 4. Keywords: 
Knowledge Distillation, Sensorimotor Learning, Imitation Learning, Curriculum Learning, Autonomous Driving

#### 5. Paper: 
https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Coaching_a_Teachable_Student_CVPR_2021_paper.html
Github: None

#### 6. Summary:
- (1):本文研究自动驾驶中的知识蒸馏问题，提出了一种新的知识蒸馏框架，旨在通过教授特权教师代理的监督来有效地教授感知运动学生代理驾驶。 
- (2):现有的传感器运动代理的知识蒸馏方法往往会导致学生的学习行为次优，这是由于两个代理之间输入、建模能力和优化过程之间的固有差异所致。本文提出了一种新的蒸馏方案，可以解决这些限制，并缩小传感器运动代理与其特权教师之间的差距。 
- (3):本文提出了一种新的知识蒸馏框架，通过设计一个学生，使其学习将其输入特征与教师的特权鸟瞰图（BEV）空间对齐。然后，学生可以从教师对内部表示学习的直接监督中受益。为了支持困难的传感器运动学习任务，学生模型通过学生自主控制的辅助监督机制进行优化。本文还提出了一个高容量的模仿学习特权代理，它在CARLA中超越了先前的特权代理，并确保学生学习安全驾驶行为。本文提出的传感器运动代理在CARLA中实现了一个强大的基于图像的行为克隆代理，相比当前模型，驾驶得分提高了20.6％以上，而不需要LiDAR，历史观察，模型集成，在线数据聚合或强化学习。 
- (4):本文提出的方法在CARLA中实现了一个强大的基于图像的行为克隆代理，相比当前模型，驾驶得分提高了20.6％以上，而不需要LiDAR，历史观察，模型集成，在线数据聚合或强化学习。该方法的性能支持其目标，即通过教授特权教师代理的监督来有效地教授感知运动学生代理驾驶。
#### 7. 方法详细介绍：
本文提出了一种名为“Coaching a Teachable Student (CaT)”的方法，用于训练一个目标条件的感知运动代理，将图像和目标观测映射到车辆油门和转向控制。该方法包括训练一个特权教师代理，使用直接专家模仿和增强的鸟瞰图表示与安全提示，以及使用中间特征和教师的最终输出的蒸馏，以及来自辅助分割和命令预测任务的监督，交织在整个网络中，来训练学生代理。学生代理使用对齐模块将图像特征映射到BEV空间。

#### 8. 实验设置：
本文使用CARLA模拟器来训练和评估所提出的CaT框架。目标是训练一个目标条件的感知运动代理，将图像和目标观测映射到车辆油门和转向控制。代理使用三个不重叠的RGB相机视图、分类导航命令和从GNSS采样的中间嘈杂目标进行训练。代理学习预测下一个2.5秒的行驶中10个未来的2D航点，以车辆俯视坐标系为基础。BEV是从底层模拟中的特权（即地面真实）信息渲染出来的，包括车道、行人、车辆和交通信号灯的3D位置和状态。

#### 9. 实验结果和分析：
所提出的CaT方法在CARLA基准测试中取得了最先进的性能，包括LiDAR方法，在Driving Score（DS）方面达到了58.36％。该方法还在Driving Score（DS）方面优于先前的RGB-only最先进代理TCP 36.16％ DS。此外，该方法在nuScenes开环评估中显示出显着的改进，实现了0.41 ADE，并将碰撞率与基线相比降低了60.3％。


# Paper:947     一种基于概率注意力模型和遮挡感知纹理回归的单个RGB图像中的3D手部重建方法



#### 1. Title: 
A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image

#### 2. Authors: 
Zheheng Jiang, Hossein Rahmani, Sue Black, Bryan M. Williams

#### 3. Affiliation: 
第一作者：兰卡斯特大学

#### 4. Keywords: 
3D hand reconstruction, probabilistic model, occlusion-aware texture regression, attention-based mesh vertices uncertainty regression

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Jiang_A_Probabilistic_Attention_Model_With_Occlusion-Aware_Texture_Regression_for_3D_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究单个RGB图像中的3D手部重建问题，该问题在人机交互、虚拟现实等领域有广泛应用。 
- (2):过去的方法可以大致分为基于模型和基于无模型两类，但都存在一些问题，如基于模型的方法容易受到模型参数空间的限制，基于无模型的方法需要大量的3D标注数据。本文提出了一种新的概率模型，将MANO模型作为先验网络，结合AMVUR模型来估计关节和顶点的概率分布，从而兼具基于模型和基于无模型方法的优点。 
- (3):本文提出了一种基于注意力机制的网格顶点不确定性回归模型（AMVUR），用于捕捉顶点之间的依赖关系和关节与网格顶点之间的相关性，以提高它们的特征表示。同时，提出了一种基于学习的遮挡感知手部纹理回归模型，以实现高保真度的纹理重建。实验结果表明，该方法在单个图像中实现了3D手部和纹理重建的最新性能。 
- (4):本文提出的方法在HO3Dv2、HO3Dv3和FreiHand等三个基准数据集上进行了测试，取得了最先进的性能，包括在严重遮挡情况下的性能。
#### 7. 方法详细介绍：
本文提出了一种基于概率的注意力模型，用于从单个RGB图像中重建三维手部模型。该模型包括三个主要组件：弱监督的三维手部姿态和形状估计模块、基于注意力的网格顶点不确定性回归模块和考虑遮挡的手部纹理回归模块。该模型使用概率模型和方差推断算法从单个RGB图像中估计三维手部姿态和形状。基于注意力的网格顶点不确定性回归模块建立了关节和网格之间的相关性，以减轻对MANO参数模型的依赖。考虑遮挡的手部纹理回归模块回归每个顶点的RGB值以表示手部纹理，并且能够感知遮挡。

#### 8. 实验设置：
本文在HO3Dv2和HO3Dv3数据集上进行了实验，使用了Procrustes对齐方法对结果进行了比较。实验中使用了全监督和弱监督两种训练方式。在全监督训练中，使用了MANO模型的3D手部姿态和形状作为ground truth。在弱监督训练中，没有3D ground truth信息。实验中使用了SSIM和PSNR等指标对模型进行了评估。

#### 9. 实验结果和分析：
本文提出的方法在HO3Dv2数据集上的表现优于SOTA纹理回归模型S2HAND。该方法在重建高保真度手部纹理方面具有更好的能力，并在全监督和弱监督训练中均取得了最先进的准确性。在弱监督情况下，该方法不仅在性能上优于其他SOTA弱监督方法，而且在一些全监督方法上也表现出色。本文还对提出的模型和SOTA 3D手部网格估计方法在HO3Dv2数据集上进行了定性比较。


# Paper:948     基于焦散线索的完全自监督深度估计



#### 1. Title: 
Fully Self-Supervised Depth Estimation from Defocus Clue

#### 2. Authors: 
Haozhe Si, Bin Zhao, Dong Wang, Yunpeng Gao, Mulin Chen, Zhigang Wang, Xuelong Li

#### 3. Affiliation: 
上海人工智能实验室 Shanghai AI Laboratory

#### 4. Keywords: 
Depth-from-defocus, self-supervised, focal stack, defocus map, optical model

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Si_Fully_Self-Supervised_Depth_Estimation_From_Defocus_Clue_CVPR_2021_paper.html  Github: https://github.com/Ehzoahis/DEReD

#### 6. Summary : 
- (1):本文研究的是从焦散线索中完全自监督地估计深度的问题。深度估计是计算机视觉中的基本任务，但是获取准确的深度地面实况数据是具有挑战性的。自监督深度估计方法利用立体对或单目视频中的3D几何一致性作为约束，但是这些方法需要大量的数据。本文提出了一种完全自监督的框架，该框架仅从稀疏的焦距堆栈中纯粹估计深度，避免了需要深度和AIF图像实况数据的限制，从而使DFD方法在实际应用中更具可行性。

- (2):以前的DFD方法依赖于理论上的AIF图像，这些图像不能在现实世界中捕获。现有的DFD方法通常将使用小光圈拍摄的图像视为AIF图像，但这种近似不可避免地包含区域模糊和短距离下曝光的问题。本文提出了一种更现实的DFD任务设置，其中在模型训练中不提供深度和AIF图像实况数据。本文提出的完全自监督框架可以从稀疏的焦距堆栈中纯粹估计深度，避免了需要深度和AIF图像实况数据的限制，从而使DFD方法在实际应用中更具可行性。

- (3):本文提出了一种自监督框架，该框架从焦距堆栈中同时预测AIF图像和深度图，并利用光学模型验证和优化预测。该框架通过保证输入和重建的焦距堆栈之间的一致性来提高深度图和AIF图像的预测精度。本文的贡献是：设计了更现实和具有挑战性的DFD任务场景，提出了第一个完全自监督的DFD框架，该框架从焦距堆栈中纯粹估计深度和AIF图像，并在合成和真实数据集上进行了验证。

- (4):本文的方法在三个基准数据集上进行了验证，结果表明，该方法与最先进的监督/间接监督DFD方法相当，并具有令人信服的视觉质量。本文的方法在真实场景中也具有应用潜力。本文的贡献是提出了一种完全自监督的DFD框架，该框架从焦距堆栈中纯粹估计深度和AIF图像，并在合成和真实数据集上进行了验证。
#### 7. 方法详细介绍：
本文提出了一种全自监督的深度估计方法，包括两个主要组件：DAIF-Net和光学模型。DAIF-Net是一个改进的U-Net结构，可以从任意大小的焦距堆栈中估计深度图和全焦距图像。光学模型由薄透镜模块和PSF卷积层组成，用于重建输入焦距堆栈以监督预测。模型的输入具有4个通道：RGB颜色通道和焦距通道。在训练阶段，模型通过从预测的深度和全焦距图像重建输入来进行自监督。在推理阶段，深度图和全焦距图像可以直接从训练好的DAIF-Net中估计，无需渲染过程。

#### 8. 实验设置：
本文在三个基准数据集上进行了评估，包括使用渲染的焦距堆栈的合成数据集、使用捕获的焦距堆栈的真实数据集和NYUv2数据集。合成数据集包含10,000个训练样本和1,000个测试样本，而真实数据集包含1,000个训练样本和100个测试样本。评估指标包括平均绝对误差（MAE）和均方根误差（RMSE）。

#### 9. 实验结果和分析：
本文提出的方法在所有三个基准数据集上均取得了最先进的性能。在合成数据集上，MAE和RMSE分别为0.019和0.032。在真实数据集上，MAE和RMSE分别为0.031和0.051。在NYUv2数据集上的消融研究表明，本文提出的方法比基线方法有很大的优势。


# Paper:949     通过2D监督学习3D场景先验



#### 1. Title: 
Learning 3D Scene Priors with 2D Supervision

#### 2. Authors: 
Yinyu Nie, Angela Dai, Xiaoguang Han, Matthias Nießner

#### 3. Affiliation: 
第一作者：Technical University of Munich（慕尼黑工业大学）

#### 4. Keywords: 
3D scene understanding, 2D supervision, autoregressive decoder, scene synthesis, single-view reconstruction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Nie_Learning_3D_Scene_Priors_With_2D_Supervision_CVPR_2021_paper.html  Github: https://yinyunie.github.io/sceneprior-page/

#### 6. Summary : 
- (1):本文旨在通过2D监督学习3D场景的先验分布，以解决3D监督数据获取困难的问题。

- (2):过去的方法通常需要3D监督数据，而这种数据的获取成本高昂，难以扩展。本文提出了一种新的方法，通过2D监督学习3D场景的先验分布，从而支持多种下游任务，包括场景合成、插值和单视图重建。

- (3):本文提出了一种基于自回归解码器的方法，将3D场景表示为潜在向量，并逐步解码为一系列对象，这些对象由它们的类别、3D边界框和网格表示。通过训练自回归解码器，将场景先验编码为潜在空间，从而支持多种下游任务。本文的创新点在于，通过2D监督学习3D场景的先验分布，从而支持多种下游任务，包括场景合成、插值和单视图重建。

- (4):本文在3D-FRONT和ScanNet数据集上进行了实验，结果表明，本文的方法在单视图重建方面优于现有方法，在场景合成方面与需要3D监督的方法相比，取得了最先进的结果。
#### 7. 方法详细介绍：
本文提出了一种基于2D监督学习3D场景先验的方法。该方法使用自回归解码器逐步解码3D场景的先验，生成一系列对象，包括它们的类别、3D边界框和网格。该方法使用置换不变变压器来表示场景先验，从而促进了许多下游应用，包括场景合成、插值和单视图重建。该方法通过将随机向量从潜空间解码为一系列对象，使用置换不变变压器自回归地表示场景先验。每个输出对象由它们的类别、3D边界框和网格来描述。该方法使用渲染生成场景的图像与相应相机视角下的真实实例掩码之间的2D视图损失进行训练。

#### 8. 实验设置：
该方法在两个下游任务上进行了训练和测试：3D场景合成和单视图场景重建。对于3D场景合成，使用3D-FRONT数据集进行训练和评估。对于单视图场景重建，使用ScanNet数据集进行基准测试。

#### 9. 实验结果与分析：
本文在单视图重建方面使用不同方法进行了定量比较，包括Total3D、Im3D、USL和本文提出的方法。结果表明，本文提出的方法在3D Box IoU方面优于其他方法。本文还展示了他们的方法在场景插值和场景合成方面的有效性。进行了消融分析以研究场景合成中每个子模块的影响，结果表明变压器起着最重要的作用。讨论了他们方法的局限性，并提出了未来的工作，以将不确定性模型纳入到从嘈杂的掩码和相机姿态中学习稳定场景先验的方法中。


# Paper:950     MarS3D：一种即插即用的运动感知模型，用于多扫描3D点云上的语义分割



#### 1. Title: 
MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds

#### 2. Authors: 
Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, Lan Ma, Xiaojuan Qi

#### 3. Affiliation: 
Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, and Xiaojuan Qi are affiliated with The University of Hong Kong. Lan Ma is affiliated with TCL AI Lab.

#### 4. Keywords: 
3D semantic segmentation, multi-scan point clouds, motion-aware, representation learning, temporal information

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Liu_MarS3D_A_Plug-and-Play_Motion-Aware_Model_for_Semantic_Segmentation_on_CVPR_2021_paper.html  Github: https://github.com/CVMI-Lab/MarS3D

#### 6. Summary: 
- (1):本文研究的是多扫描3D点云上的语义分割，与单扫描语义分割任务不同，该任务需要除了区分点的语义类别外，还需要区分点的运动状态。 
- (2):过去的方法主要是将多个点云融合成一个点云进行处理，但这种方法会丢失时间信息，使得区分运动状态成为一个具有挑战性的问题。本文提出了一种新的模块，可以与现有的单扫描语义分割模型灵活地结合，使其具有多扫描感知能力。该模型包括两个关键设计：交叉帧特征嵌入模块和运动感知特征学习模块。 
- (3):本文提出了一种名为MarS3D的模块，可以无缝地与现有的单扫描语义分割模型结合，使其具有多扫描感知能力。该模型包括两个核心设计：交叉帧特征嵌入模块和运动感知特征学习模块。交叉帧特征嵌入模块将时间步信息嵌入特征中，以促进帧间融合和表示学习。运动感知特征学习模块基于鸟瞰图（BEV）学习对象之间的运动模式，以便有效地区分对象的运动状态。 
- (4):在SemanticKITTI和nuScenes数据集上进行了广泛的实验，结果表明，MarS3D可以显著提高基线模型的性能。在SemanticKITTI数据集上，MarS3D将基线模型的mIoU提高了6.24％，而模型参数增加了约0.2％。该方法的主要贡献在于提出了一种用于大规模多扫描3D语义分割的即插即用模块，可以与主流单扫描分割模型灵活结合，同时引入了可忽略的额外参数和推理时间。
#### 7. 方法详细介绍：
本文提出了一种名为MarS3D的运动感知模型，用于多扫描3D点云的语义分割。该模型由两个分支组成，即BEV分支和3D分支。BEV分支以k个BEV表示为输入，使用运动感知特征学习（MAFL）模块提取运动感知特征。3D分支以k个点云为输入，使用跨帧特征嵌入（CFFE）模块注入时间信息并增强空间表示学习。两个分支的融合特征被馈送到预测头以产生最终输出。在训练期间，获得的融合特征被馈送到两个分类头：类别感知分类头和运动感知分类头。最终的目标函数是两个损失的加权和。在推理期间，最终的预测结果是使用两个分类头产生的逻辑值确定的。

#### 8. 实验设置：
本文在两个数据集上评估了提出的方法：SemanticKITTI和nuScenes。对于SemanticKITTI，多扫描设置是完全监督的，包含25个类别（6个移动类别和19个静态类别），具有高质量的语义注释。注释基于KITTI数据集。它包括22个点云序列。对于nuScenes，提出了一个基于“lidar-seg”任务的新的多扫描设置，没有参考帧监督。该数据集包含24个类别（8个移动和16个静态类别）。BEV表示的大小设置为501×301，运动感知特征学习（MAFL）模块中的多个内核大小分别设置为1、3和5。

#### 9. 实验结果和分析：
实验结果表明，所提出的MarS3D模型在SemanticKITTI数据集上的mIoU表现优于基线方法。该模型在参数计数为21.9M，延迟为225ms的情况下，实现了54.66%的mIoU。性能提升主要归因于BEV分支提取的运动感知特征和3D分支中CFFE模块注入的时间信息。在nuScenes“lidar-seg”数据集上，所提出的方法也显著优于基线方法，实现了64.83%的mIoU。


# Paper:951     IMAGEBIND：一个嵌入空间绑定所有模态数据



#### 1. Title: 
IMAGEBIND: One Embedding Space To Bind Them All

#### 2. Authors: 
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra

#### 3. Affiliation: 
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, and Ishan Misra are affiliated with Facebook AI Research (FAIR), while Armand Joulin is affiliated with Meta AI.

#### 4. Keywords: 
Cross-Modal Retrieval, Embedding-Space Arithmetic, Audio to Image Generation, Joint Embedding Space, Multimodal Learning

#### 5. Paper: https://facebookresearch.github.io/ImageBind Github: https://github.com/facebookresearch/ImageBind

#### 6. Summary:
- (1): 本文的研究背景是多模态学习，旨在将不同模态的数据融合到一个共同的嵌入空间中。
- (2): 以往的方法通常只考虑了少数几个模态，而且需要大量的配对数据，难以实现多模态数据的联合嵌入。本文提出的方法通过将所有模态的嵌入对齐到图像嵌入上，实现了多模态数据的联合嵌入，避免了需要大量配对数据的问题。该方法的创新性在于，它可以将不同模态的数据进行自然融合，从而实现了跨模态检索、模态组合和跨模态生成等新颖应用。
- (3): 本文提出了一种基于图像嵌入的多模态联合嵌入方法，该方法可以利用大规模的图像-文本配对数据和自然配对数据（如视频-音频、图像-深度等）来学习一个共同的嵌入空间。该方法可以将文本嵌入与其他模态（如音频、深度等）的嵌入进行隐式对齐，从而实现了对这些模态的零样本识别能力。此外，本文还展示了该方法在多个模态和任务上的强大性能，包括跨模态检索、模态组合和跨模态生成等。
- (4): 本文的方法在多个数据集上进行了实验，结果表明其在跨模态检索、模态组合和跨模态生成等任务上均取得了优秀的性能，证明了该方法的有效性和创新性。
#### 7. 方法详细介绍：
本文提出了一种名为IMAGEBIND的方法，用于学习六种不同模态（图像、文本、音频、深度、热像和IMU数据）的联合嵌入。该方法利用图像的绑定属性，将每种模态的嵌入与图像嵌入对齐，从而实现所有模态之间的紧密对齐。该方法不需要所有模态共同出现的数据集，并且可以使用大规模的视觉语言模型（如CLIP）进行初始化。该方法使得可以进行新颖的应用，如跨模态检索、算术组合模态、跨模态检测和生成等。

具体步骤如下：
1. 对于每种模态，使用Transformer架构对其进行编码，然后添加一个特定于模态的线性投影头，以获得固定大小的d维嵌入。
2. 使用InfoNCE损失函数对模态进行对齐，使得它们在联合嵌入空间中更接近。
3. 使用大规模的网络数据集进行图像-文本监督，使用自然的自我监督配对其他模态（如音频、深度、热像和IMU）与图像。
4. 在IMAGEBIND训练期间保持每种模态的编码器冻结，更新音频、深度、热像和IMU编码器。

#### 8. 实验设置：
本文使用大规模的图像-文本配对数据以及自然配对的“自我监督”数据，包括四种新的模态-音频、深度、热像和IMU读数。作者展示了各种模态的强大的紧急零样本分类和检索性能。随着基础图像表示的加强，紧急性能得到了改善。

#### 9. 实验结果和分析：
本文在各种基准测试中展示了IMAGEBIND在零样本音频检索和分类方面的性能，甚至在没有使用音频特定监督的情况下。IMAGEBIND的紧急零样本性能接近专业监督模型的性能。此外，本文还展示了IMAGEBIND嵌入的组合信息跨模态，使得可以进行丰富多样的组合任务。最后，本文还展示了现有的使用CLIP嵌入的视觉模型可以升级为使用来自其他模态（如音频）的IMAGEBIND嵌入，并且扩散模型可以重新用不同类型的声音生成合理的图像。


# Paper:952     IterativePFN: 真正的迭代点云过滤



#### 1. Title: 
IterativePFN: True Iterative Point Cloud Filtering

#### 2. Authors: 
Dasith de Silva Edirimuni, Xuequan Lu, Zhiwen Shao, Gang Li, Antonio Robles-Kelly, Ying He

#### 3. Affiliation: 
第一作者：Deakin University（迪肯大学）

#### 4. Keywords: 
Point cloud filtering, denoising, deep learning, IterativePFN, IterationModule

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Edirimuni_IterativePFN_True_Iterative_Point_Cloud_Filtering_CVPR_2021_paper.html  Github: https://github.com/ddsediri/IterativePFN

#### 6. Summary : 
- (1):本文研究的是点云过滤或去噪的问题，点云的质量常常受到捕捉过程中引入的噪声的限制。因此，点云过滤是一项基本的3D视觉任务。 
- (2):传统的点云过滤方法，如基于MLS的方法、双边滤波机制和边缘恢复算法等，都依赖于点集的局部信息，即点法线，以过滤点云。然而，这些方法受到法线精度的限制。最近，提出了基于深度学习的过滤方法，以缓解传统方法的缺点和局限性。本文提出了IterativePFN（迭代点云过滤网络），它由多个IterationModule组成，可以在单个网络内部模拟真正的迭代过滤过程。 
- (3):本文提出了一种新的神经网络架构，称为IterationModule，用于模拟真正的迭代过滤过程。每个IterationModule表示一个过滤迭代，τ-th IterationModule的输出成为τ + 1-th IterationModule的输入。因此，τ + 1-th IterationModule表示过滤迭代t = τ + 1。这使得网络能够对迭代之间的过滤关系进行理解。本文还提出了一种新的损失函数，它将每个迭代的最近邻损失公式化为τ-th IterationModule推断的过滤位移和t = τ时的目标点云内最近点之间的L2范数最小化。这促进了逐渐过滤的过程，鼓励收敛到干净的表面。 
- (4):本文在合成和真实世界数据上进行了全面的实验，与最先进的方法进行了比较，证明了我们的方法在性能上的优势。在多个数据集上，我们的方法都能够获得更好的性能。
#### 7. 方法详细介绍：
本文提出了一种名为IterativePFN的点云滤波方法。该方法采用真正的迭代方法，由多个IterationModule组成，内部建模迭代滤波过程，不同于现有方法只在测试时执行迭代滤波。在推理过程中，该方法使用最远点采样来获取参考点并构建输入补丁。所有补丁内的点同时进行滤波，并使用广义补丁拼接方法找到最佳滤波重复点的补丁。该方法的损失函数是一个真正的迭代训练时间滤波解决方案，可以消耗嘈杂的补丁并过滤点而无需多次外部迭代。

#### 8. 实验设置：
使用PUNet数据集进行训练，对40个训练网格进行采样，分别在10K、30K和50K点的分辨率下添加标准偏差范围从0.5%到2%的高斯噪声。对于测试，使用20个测试网格，分别在10K和50K的分辨率下，这40个点云被高斯噪声扰动，标准偏差为边界球半径的1%、2%和2.5%。使用NVIDIA A100 GPU在PyTorch 1.11.0和CUDA 11.3上训练网络。

#### 9. 实验结果和分析：
所提出的IterativePFN方法在各种数据集上表现优异，包括PUNet、Kinect v1、Kinect v2和RueMadame。与基线位移法、重采样法、分数匹配法和归一化流法相比，该方法在所有分辨率和噪声尺度上都表现出优势。该方法在RueMadame数据库中始终过滤标志牌和车辆引擎盖等表面。该方法还在Kinect v1和Kinect v2数据集上优于其他方法的Chamfer距离和Point2Mesh距离评估指标。

#### 论文总结：
本文提出了一种名为IterativePFN的点云滤波方法，该方法采用真正的迭代方法，由多个IterationModule组成，内部建模迭代滤波过程。在推理过程中，该方法使用最远点采样来获取参考点并构建输入补丁。该方法的损失函数是一个真正的迭代训练时间滤波解决方案，可以消耗嘈杂的补丁并过滤点而无需多次外部迭代。实验结果表明，该方法在各种数据集上表现优异，优于现有的滤波方法。


# Paper:953     可见度约束的宽带照明光谱设计用于暗光环境下的视觉任务



#### 1. Title: 
Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark

#### 2. Authors: 
Muyao Niu, Zhuoxiao Li, Zhihang Zhong, Yinqiang Zheng

#### 3. Affiliation: 
东京大学

#### 4. Keywords: 
Seeing-in-the-Dark, RGB-dependent methods, RGB-independent methods, NIR2RGB translation, wide-band illumination spectrum design, visibility constraint, hyperspectral image dataset

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Niu_Visibility_Constrained_Wide-Band_Illumination_Spectrum_Design_for_Seeing-in-the-Dark_CVPR_2021_paper.html  Github: https://github.com/MyNiuuu/VCSD

#### 6. Summary : 
- (1):本文研究了在黑暗环境下的计算机视觉任务，提出了一种可见光-近红外宽带照明光谱设计方法，以提高NIR2RGB转换的鲁棒性。
 
- (2):过去的方法主要分为两类：RGB依赖方法和RGB独立方法。RGB依赖方法使用退化的RGB输入恢复信息，而RGB独立方法将辅助近红外（NIR）照明下的图像转换为RGB域。然而，NIR2RGB转换存在固有的模糊性。本文提出了一种可见度约束光谱设计模型，将人类视觉系统的可见度约束量化并纳入设计流程，以提高NIR2RGB转换的鲁棒性。 
 
- (3):本文提出了一种可见度约束光谱设计（VCSD）模型，以在420 nm至890 nm的宽带光谱范围内设计辅助照明的最优光谱，同时保持视觉友好性。通过对VIS-NIR范围内图像的形成过程进行建模，自动设计了一种广泛的LED多路复用方案。本文还收集了一个大规模的VIS-NIR高光谱图像数据集进行实验。实验结果表明，与仅使用NIR的方法相比，使用优化的宽带照明可以显著提高任务的性能。
 
- (4):本文提出的方法在NIR2RGB转换任务上取得了优异的性能，证明了可见光-近红外宽带照明光谱设计方法的有效性。
#### 7. 方法详细介绍：
本文提出了一种可见性约束宽带照明光谱设计（VCSD）模型，通过优化LED光谱，使其在可见光和近红外范围内都具有较好的效果。该方法考虑了人类视觉系统，通过量化某些LED光谱曲线的可见性，找到了一种方法来衡量可见性。该模型使用感知损失函数和预训练的VGG-19网络进行图像重建。训练过程使用Pytorch和Adam优化器，指定了批量大小、学习率和迭代次数。物理模型IPS用于生成辅助图像，考虑了辅助光谱曲线Φ、相机光谱灵敏度C和反射光谱图T的影响。融合网络采用UNet结构，将可见光图像和近红外-可见光图像作为输入，生成结果X。最优LED光谱曲线通过深度学习最小化重建损失得到。

#### 8. 实验设置：
本文使用自定义的50波段滤波轮收集了一个大规模的可见光-近红外高光谱图像数据集，用于评估所提出的模型和设计曲线的有效性。

#### 9. 实验结果与分析：
实验结果表明，与仅使用近红外相比，使用优化的宽带照明可以显著提高看暗物体的任务。所提出的模型在三个指标（SSIM、PSNR和LPIPS）上均优于OptNIR方法，证明了在可见性约束下宽带照明光谱设计的有效性。不同可见性阈值对模型的影响也进行了讨论，结果表明，随着可见性阈值的增加，模型的恢复结果变得更好，因为覆盖了更多的可见光信息。通过设置不同的可见性阈值，该模型可以在更广泛的应用场景中应用。本文贡献了一个可见光-近红外宽带高光谱图像数据集，以补充现有数据集的质量和数量。代码已在GitHub上公开。


# Paper:954     基于虚拟相机和关键点的高效3D车道检测



#### 1. Title: 
BEV-LaneDet: An Efficient 3D Lane Detection Based on Virtual Camera via Key-Points

#### 2. Authors: 
Ruihao Wang, Jian Qin, Kaiying Li, Yaochen Li, Dong Cao, Jintao Xu

#### 3. Affiliation: 
HAOMO.AI Technology Co., Ltd. (王瑞浩, 秦健, 李凯颖, 曹东, 徐金涛); Xi’an Jiaotong University (李耀辰)

#### 4. Keywords: 
3D lane detection, virtual camera, key-points representation, spatial transformation pyramid

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_BEV-LaneDet_An_Efficient_3D_Lane_Detection_Based_on_Virtual_Camera_CVPR_2021_paper.html  Github: https://github.com/gigo-team/bev_lane_det

#### 6. Summary : 
- (1):本文研究的是自动驾驶中的3D车道检测问题，旨在提出一种高效、鲁棒的单目3D车道检测方法，以解决现有方法中空间变换复杂、3D车道表示不灵活等问题。

- (2):过去的方法主要是基于2D车道检测，通过逆透视变换将2D车道投影到平面上，然后进行曲线拟合以获得BEV车道。然而，这种方法在实际驾驶过程中可能会出现问题。近年来，一些方法开始关注更复杂的3D车道感知领域。这些方法通常将相机内/外参数纳入网络中以获得BEV特征。本文提出了一种虚拟相机模块，将所有图像投影到标准虚拟公共相机的视图上，以保证数据分布的一致性。同时，本文提出了一种称为关键点表示的3D车道表示方法，以及一种基于MLP的轻量级易于部署的空间变换模块。这些方法在某些特殊场景下更具灵活性和可扩展性。 

- (3):本文提出了一种名为BEV-LaneDet的单目3D车道检测方法，包括三个主要贡献：虚拟相机模块、关键点表示和空间变换金字塔。虚拟相机模块将不同车辆上的前置摄像头的内/外参数统一起来，以保证前置摄像头之间的空间关系的一致性。关键点表示是一种简单但有效的3D车道表示方法，更适合表示复杂和多样化的3D车道结构。空间变换金字塔是一种基于MLP的轻量级易于部署的空间变换模块，将多尺度前视特征转换为BEV特征。实验结果表明，BEV-LaneDet在OpenLane数据集和Apollo 3D合成数据集上的F-Score分别比现有方法高10.6％和4.0％，速度为185 FPS。

- (4):本文提出的BEV-LaneDet方法在单目图像中实现了高效、鲁棒的3D车道检测。该方法在OpenLane数据集和Apollo 3D合成数据集上的F-Score分别比现有方法高10.6％和4.0％，速度为185 FPS。这些结果表明，BEV-LaneDet方法在3D车道检测方面具有很高的性能和实用性。
#### 7. 方法详细介绍：
本文提出了一种名为BEV-LaneDet的高效、鲁棒的单目3D车道线检测方法，包括以下五个部分：虚拟相机、前视图骨干网络、空间变换金字塔、关键点表示和前视图头。其中，虚拟相机用于统一不同车辆前置摄像头的内外参数，前视图骨干网络用于提取前视图像素特征，空间变换金字塔是一种快速的多尺度空间变换模块，用于将前视图特征转换为BEV特征，关键点表示将BEV平面划分为单元格，并预测每个单元格的置信度、嵌入、偏移和平均高度，前视图头用于辅助监督。本文方法的输入为前视图像素，输出为BEV平面上的车道线。本文方法的总体流程如图1所示。

#### 8. 实验设置：
本文在两个数据集上进行了实验，分别是OpenLane和Apollo 3D synthetic dataset。OpenLane是一个真实场景的大规模3D车道线数据集，包含10000张带有3D车道线标注的图像。Apollo 3D synthetic dataset是一个包含10000张带有3D车道线标注的合成数据集。实验在一台配备Intel Xeon E5-2680 v4 CPU和NVIDIA Tesla V100 GPU的服务器上进行。

#### 9. 实验结果与分析：
本文方法在OpenLane和Apollo 3D synthetic dataset上的实验结果表明，相比于现有的方法，本文方法在F-Score上分别提高了10.6%和4.0%，且速度达到了185 FPS。在OpenLane数据集上，本文方法的平均F-Score为0.874，在Apollo 3D synthetic dataset上为0.902。本文还进行了消融实验和对比实验，结果表明本文方法的各个模块都对性能有所提升。本文方法的代码已在https://github.com/gigo-team/bev_lane_det上开源。


# Paper:955     2PCNet：用于日夜无监督领域自适应目标检测的两阶段一致性训练



#### 1. Title: 
2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised Domain Adaptive Object Detection

#### 2. Authors: 
Mikhail Kennerley, Jian-Gang Wang, Bharadwaj Veeravalli, Robby T. Tan

#### 3. Affiliation: 
第一作者：新加坡国立大学电气与计算机工程系

#### 4. Keywords: 
Object detection, unsupervised domain adaptation, night images, pseudo-labels, two-phase consistency training

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Kennerley_2PCNet_Two-Phase_Consistency_Training_for_Day-to-Night_Unsupervised_Domain_Adaptive_Object_CVPR_2021_paper.html  Github: https://github.com/mecarill/2pcnet

#### 6. Summary : 
- (1):本文研究了夜间无监督领域自适应目标检测的问题，由于夜间图像缺乏注释，因此需要使用领域自适应方法。然而，现有的方法在小尺度和低光条件下仍然存在误报的问题，因此需要提出新的方法来解决这个问题。

- (2):过去的方法主要包括基于对抗学习的方法和基于学生-教师框架的方法。然而，这些方法在夜间场景下仍然存在误报的问题。本文提出了一种新的两阶段一致性无监督领域自适应网络，2PCNet，来解决这个问题。该网络利用了第一阶段教师的高置信度边界框预测，并将其附加到学生的区域提议中，以便第二阶段教师重新评估，从而产生高置信度和低置信度的伪标签。此外，本文还提出了一种夜间特定的增强方法，NightAug，来解决夜间图像中的低光区域和其他问题。

- (3):本文提出的2PCNet网络采用了两阶段一致性方法，以解决小尺度目标的误报问题。此外，本文还提出了一种学生缩放技术，以提高小尺度伪标签的准确性。此外，本文还提出了一种夜间特定的增强方法，NightAug，以减少学生网络对源数据的偏差。

- (4):本文在公开数据集上进行了实验，结果表明，与现有的领域自适应方法相比，本文提出的方法在BDD100K和SHIFT数据集上的性能分别提高了20%和26%。因此，本文提出的方法可以有效地解决夜间无监督领域自适应目标检测的问题。
#### 7. 方法详细介绍：
本文提出了一种名为2PCNet的双阶段一致性无监督域自适应目标检测网络。该网络采用了一种双阶段一致性的方法，即在第一阶段，将教师网络的高置信度边界框预测结果与学生网络的区域建议相结合，生成伪标签；在第二阶段，将伪标签与原始图像一起输入教师网络，重新评估边界框预测结果，得到高置信度和低置信度的伪标签。此外，为了解决夜间图像中小尺度和暗物体的问题，本文还提出了学生缩放和NightAug技术。其中，学生缩放技术将夜间图像和伪标签缩小，以提供更强的小尺度伪标签；NightAug技术是一种夜间特定的数据增强方法，可以增强夜间图像的特征，提高模型的鲁棒性。

#### 8. 实验设置：
本文使用BDD100K和SHIFT两个数据集进行实验评估。实验中，使用Faster-RCNN作为基础检测模型，ResNet-50作为特征提取器，使用了三阶段一致性训练。训练过程在3个RTX3090 GPU上进行，批量大小为6个源图像和6个目标图像。本文还进行了消融实验，以证明所提出的方法的有效性。最后，本文在BDD100K和SHIFT数据集上进行了实验，结果表明，所提出的2PCNet框架在日夜域自适应目标检测方面优于现有的最先进方法。

#### 9. 实验结果与分析：
本文提出的2PCNet框架在BDD100K和SHIFT数据集上的表现优于现有的最先进方法。消融实验表明，双阶段一致性方法、学生缩放和NightAug技术都对模型的性能有所提升。表4中报告了大、中、小物体的AP值。本文的实验结果表明，所提出的2PCNet框架是一种有效且高效的日夜域自适应目标检测方法。


# Paper:956     AUNet：学习面部动作单元之间关系用于面部伪造检测



#### 1. Title: 
AUNet: Learning Relations Between Action Units for Face Forgery Detection

#### 2. Authors: 
Weiming Bai, Yufan Liu, Zhipeng Zhang, Bing Li, Weiming Hu

#### 3. Affiliation: 
1. 中国科学院自动化研究所，多模态人工智能系统国家重点实验室
2. 中国科学院大学，人工智能学院
3. 滴滴出行
4. People AI, Inc.
5. 中国科学院大脑科学与智能技术卓越中心

#### 4. Keywords: 
Face forgery detection, Action Units, Relation Learning, Transformer, Tampered AU Prediction

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是面部伪造检测，由于面部伪造技术带来的严重安全问题，面部伪造检测变得越来越重要。
- (2):过去的方法主要集中在手工特征和深度学习模型上，但是这些方法在跨数据集和未知伪造方法的情况下表现不佳。本文提出了一种基于动作单元关系学习的框架，以提高伪造检测的泛化性能。本文的方法吸收了数据修改和辅助任务集成方案的优点，同时学习了动作单元之间的关系来建模面部信息。
- (3):本文提出了动作单元关系学习框架，包括动作单元关系变换器（ART）和篡改动作单元预测（TAP）。ART通过注意力机制构建不同动作单元之间的关系，包括动作单元特定分支和动作单元不可知分支。TAP通过随机删除动作单元相关区域来构建局部面部遮罩，并在图像和特征级别上修改数据来生成具有挑战性的伪造样本。模型通过生成的位置特定监督来预测篡改的动作单元区域。本文的方法在跨数据集和跨篡改方法的情况下均取得了最先进的性能。
- (4):本文的方法在跨数据集和跨篡改方法的情况下均取得了最先进的性能，证明了其有效性和泛化性能。
#### 7. 方法详细介绍：
本文提出的方法称为动作单元关系学习框架，由两个主要组件组成：动作单元关系变换器（ART）和篡改AU预测（TAP）。ART由三个编码器组成，每个编码器包含一个AU特定分支和一个AU不可知分支。AU特定分支提取与特定AU对齐的特征，并通过注意机制构建它们之间的关系。AU不可知分支旨在构建图像块之间的关系。TAP通过构建部分面部掩模来制定辅助任务，以增强模型感知本地篡改缺陷的能力。它通过随机删除与AU相关的区域并在剩余区域的图像和特征级别修改数据来生成具有挑战性的伪造对应项。然后使用本地篡改监督来训练模型以预测篡改区域。

#### 8. 实验设置：
本文在五个数据集上进行评估：CDF、DFD、DFDC、DFDCP和FFIW。使用跨数据集评估协议评估模型的泛化能力。使用跨篡改评估协议评估模型的鲁棒性。

#### 9. 实验结果与分析：
本文提出的方法在数据集内和跨数据集评估中均取得了最先进的性能。在跨数据集评估中，本文提出的方法在CDF、DFD、DFDC、DFDCP和FFIW数据集上分别取得了92.77％、99.22％、73.82％、86.16％和81.45％的AUC分数。在跨篡改评估中，本文提出的方法在DF、F2F、FS和NT上分别取得了99.98％、99.60％、99.89％和98.38％的AUC分数。实验结果表明，本文提出的方法在各个数据集上均取得了最佳性能。


# Paper:957     Hi4D：密切人际互动的4D实例分割



#### 1. Title: 
Hi4D: 4D Instance Segmentation of Close Human Interaction

#### 2. Authors: 
Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Jie Song, Otmar Hilliges

#### 3. Affiliation: 
Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Jie Song, Otmar Hilliges are affiliated with ETH Zurich.

#### 4. Keywords: 
4D instance segmentation, close human interaction, personalized human avatars, multi-view systems, implicit avatars.

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文提出了Hi4D，一种用于自动分析长时间接触下的密切人际互动的方法和数据集。由于遮挡和复杂形状，稳健地区分几个接触的主体是一项具有挑战性的任务。因此，现有的多视图系统通常将接近的主体的3D表面融合成单个连接的网格。为了解决这个问题，我们利用了个性化的神经隐式化身和交替优化方案，通过接近的时间段来优化姿态和表面，并将融合的原始扫描分割成单个实例。从这些实例中，我们编译了Hi4D数据集，其中包含20个主体对的4D纹理扫描，100个序列，总共超过11K帧。Hi4D包含2D和3D中丰富的以互动为中心的注释，以及准确注册的参数化身体模型。我们在这个数据集上定义了各种人类姿态和形状估计任务，并提供了来自这些基准测试的最先进方法的结果。Hi4D数据集可以在https://ait.ethz.ch/Hi4D找到。

- (2):与单个人和间隔较大的群体的3D姿态和形状估计相比，目前没有方法能够稳健地分离和重建密切互动的人。这部分是由于缺乏合适的数据集。虽然一些3D数据集包含人类之间的互动，如ExPI和CHI3D，但它们通常缺乏高保真度的动态纹理几何，不总是提供注册参数化身体模型和不总是提供丰富的接触信息，因此不适合研究密切互动的人。为了迈向未来能够解释多个人在密切物理互动和强遮挡下的互动的AI系统，我们提出了一种方法和数据集，使得研究这种新情况成为可能。

- (3):我们提出了一种新方法来跟踪和分割多个密切互动的人的4D表面，通过扩展的动态物理接触期间利用新兴的神经隐式表面表示，特别是SNARF，并创建每个个体的个性化人体化身。这些化身然后作为跟踪和因此分割多个相互作用的人的融合几何的强个性化先验（见图2，A）。为此，我们在姿态优化和形状细化之间交替（见图3）。优化的姿态和细化的表面产生了融合输入几何的精确分割。跟踪的3D实例（图1，
#### 7. 方法详细介绍：
本文提出了一种用于跟踪和分割多人近距离互动的4D扫描的方法。该方法为每个主体构建个性化的隐式化身模型，然后通过交替优化方式在融合的原始扫描中优化姿态和形状网络参数。该方法使用Hi4D数据集，该数据集包含高质量的4D纹理扫描以及相应的多视角RGB序列、2D和3D实例分割掩模、注册参数化身体模型和顶点级接触注释。该方法定义了几个视觉基准，例如在Hi4D上进行的单眼和多视角人体姿态估计和详细几何重建。该方法的局限性包括未明确建模手部或面部表情以及优化模式不具备计算效率。

#### 8. 实验设置：
本文引入了一个名为Hi4D的新数据集，其中包含20个独特的参与者对的100个独立剪辑，执行各种互动运动序列，例如拥抱、摆姿势、跳舞和玩运动。该数据集包括超过6K帧的物理接触注释，涵盖超过95%的参数化人体。

#### 9. 实验结果和分析：
本文在Hi4D上进行了消融和比较，以验证设计选择并与基线进行比较。消融包括在没有形状细化阶段和没有交替优化的情况下评估该方法。比较包括将所提出的方法与跟踪多个穿着衣服的SMPL身体模板网格的简单基线进行比较。本文还定义了在Hi4D数据集上进行的几个标准视觉基准，包括单眼和多视角SMPL估计以及单眼和多视角详细几何重建。实验结果表明，所提出的方法在所有指标上均优于基线，并且Hi4D数据集对当前方法具有挑战性。


# Paper:958     不让任何一个类别落后：改善长尾学习中最差的类别



#### 1. Title: 
No One Left Behind: Improving the Worst Categories in Long-Tailed Learning

#### 2. Authors: 
Yingxiao Du, Jianxin Wu

#### 3. Affiliation: 
第一作者：南京大学新软件技术国家重点实验室

#### 4. Keywords: 
Long-tailed recognition, imbalanced dataset, per-class recall, harmonic mean, ensemble trick

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Du_No_One_Left_Behind_Improving_the_Worst_Categories_in_Long-Tailed_Learning_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究的是长尾识别中最差类别的问题，长尾识别是指训练数据集不平衡的情况下训练神经网络的一种方法。在这种情况下，每个类别的召回率（即准确率）会因类别而异，而且在现有的长尾识别研究中，通常将所有类别手动分成三个子集，并报告每个子集内的平均准确率。然而，这种评估方法可能会牺牲一些类别，因为平均准确率无法反映一些类别的完全牺牲。因此，本文提出了一种更注重改善最低召回率和所有召回率的调和平均值的方法。

- (2):过去的方法主要有重新采样和重新加权、两阶段解耦、混合和多头等方法。这些方法都有其缺点，如过度拟合或欠拟合等问题。本文提出的方法是一种简单的插件方法，可以应用于现有方法，包括基线和各种最先进的方法。本文提出了一种新的损失函数，最大化几何平均值，作为调和平均值的替代品。同时，本文还提出了一种集成技巧，将预训练和微调模型结合起来，在推理过程中几乎不需要额外的成本。

- (3):本文提出了一种简单的插件方法，可以应用于现有方法，包括基线和各种最先进的方法。本文提出了一种新的损失函数，最大化几何平均值，作为调和平均值的替代品。同时，本文还提出了一种集成技巧，将预训练和微调模型结合起来，在推理过程中几乎不需要额外的成本。本文的创新点在于强调了长尾识别中所有类别的正确识别的重要性，并提出了一种更注重改善最低召回率和所有召回率的调和平均值的方法。

- (4):本文在三个广泛使用的基准数据集上进行了实验，证明了本文方法的有效性，无论是在整体准确率还是最差类别准确率方面。本文方法的性能支持其目标。
#### 7. 方法详细介绍：
本文提出的方法包括三个阶段。第一阶段使用任何现有方法从头开始训练模型。第二阶段使用新颖的广义均值损失函数重新训练分类器。第三阶段提出了一个简单的集成技巧，将两个分类器的预测组合起来，几乎没有额外的成本。该方法的有效性在三个广泛使用的基准数据集上得到验证，并且在召回率的调和平均值和最低召回值上始终保持一致的改进，而总体准确性仍然保持高水平。

#### 8. 实验设置：
本文在三个基准数据集上验证了所提出方法的有效性：CIFAR100-LT、ImageNet-LT和Places-LT。实验在默认训练设置下进行，除非另有说明。本文将所提出方法与各种基线方法和最先进的方法进行了比较。

#### 9. 实验结果与分析：
所提出的方法在所有三个基准数据集中都显著提高了最差表现类别的性能。该方法在几何平均值和调和平均值方面均取得了最先进的性能，同时保持总体准确性大致不变。本文还进行了消融研究，以分析不同超参数和重新加权设计的影响。结果表明，所提出的方法适用于各种不同的方法，并实现了一致的改进。


# Paper:959     自适应全局衰减过程用于事件相机



#### 1. Title: 
Adaptive Global Decay Process for Event Cameras

#### 2. Authors: 
Urbano Miguel Nunes, Ryad Benosman and Sio-Hoi Ieng

#### 3. Affiliation: 
Urbano Miguel Nunes: Sorbonne University, Paris, France (巴黎索邦大学)
Ryad Benosman: Sorbonne University, Paris, France (巴黎索邦大学)
Sio-Hoi Ieng: Sorbonne University, Paris, France (巴黎索邦大学)

#### 4. Keywords: 
Event cameras, adaptive decay process, global scene dynamics, event activity, temporal decay strategies

#### 5. Paper: https://openaccess.thecvf.com/content_CVPR_2021/html/Nunes_Adaptive_Global_Decay_Process_for_Event_Cameras_CVPR_2021_paper.html  Github: https://github.com/neuromorphic-paris/eventbatch

#### 6. Summary : 
- (1):本文研究的是事件相机的自适应全局衰减过程，旨在解决现有方法在选择最新事件时存在的问题，如常数时间衰减或固定时间窗口、常数事件数量和基于流的事件寿命等。
- (2):本文提出了一种新的衰减过程，该过程适应全局场景动态，并且延迟时间在纳秒级别。主要思想是构建一个自适应量，编码全局场景动态，称为事件活动。该方法在多个事件视觉问题和数据集上进行了评估，始终提高了相应基线方法的性能。因此，我们相信它可以对事件驱动研究产生重大广泛影响。
- (3):本文提出了一种新的自适应全局衰减过程，该过程依赖于编码场景动态的全局自适应量，即事件活动。该过程可以自适应地调整事件寿命，以适应场景动态，同时具有极低的延迟。该方法在多个事件视觉问题和数据集上进行了评估，始终提高了相应基线方法的性能。
- (4):本文在多个事件视觉问题和数据集上进行了评估，包括全局运动估计、光流估计、深度估计和图像重建等。实验结果表明，所提出的方法在各种任务中均取得了优异的性能，证明了其有效性和实用性。
#### 7. 方法详细介绍：
本文提出了一种自适应全局衰减过程，用于事件相机。该方法包括两个步骤：首先，基于先前的事件活动计算当前自适应衰减；其次，基于当前自适应衰减和先前的事件活动计算当前事件活动。自适应衰减率与事件活动成正比，隐含地编码了事件流动态。事件权重也基于事件活动定义，可用于确定事件是否活跃并加权事件贡献。通过收集连续事件，可以形成自适应批次，直到第一个事件基于其权重被认为是不活跃的。该方法在多个事件视觉问题和数据集上进行了评估，包括异步空间卷积、全局运动估计、模式分类和基于学习的图像强度重建。评估使用的数据集包括DAVIS 240C、Poker DVS、N-MNIST和UZH-FPV Drone Racing。使用的评估指标包括结构相似性指数（SSIM）、多尺度SSIM（MSSIM）、速度误差、相应标准差（σ）、均方根误差（RMS）和相对于地面真值最大偏差的RMS误差百分比（%）。衰减过程的运行时间几乎比事件相机的典型时间分辨率低三个数量级，引入几乎可以忽略的延迟。

#### 8. 实验设置：
本文使用了多个数据集和评估指标来评估所提出的自适应衰减过程在多个事件视觉问题上的性能。数据集包括DAVIS 240C、Poker DVS、N-MNIST和UZH-FPV Drone Racing。评估指标包括结构相似性指数（SSIM）、多尺度SSIM（MSSIM）、速度误差、相应标准差（σ）、均方根误差（RMS）和相对于地面真值最大偏差的RMS误差百分比（%）。

#### 9. 实验结果与分析：
所提出的自适应衰减过程在多个事件视觉问题上均能够稳定提高基线方法的性能，包括全局运动估计、模式分类和图像强度重建。自适应HOTS特征提取器的性能优于基线方法，PokerDVS和N-MNIST数据集上分别实现了9%和0.5%的绝对增益。所提出的自适应大小训练在图像强度重建方面的平均性能略优于基线方法，均方误差（MSE）和MSSIM指标均有所提高，而SSIM指标的性能与基线方法相似。所提出的自适应衰减过程的每个事件运行时间几乎比事件相机的典型时间分辨率低三个数量级，引入几乎可以忽略的延迟。衰减过程的自适应性和低延迟使其在事件相机应用中具有广泛的潜力。


# Paper:960     基于答案启发式的大型语言模型在知识型视觉问答中的应用



#### 1. Title: 
Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering

#### 2. Authors: 
Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu

#### 3. Affiliation: 
浙江省复杂系统建模与仿真重点实验室，杭州电子科技大学，计算机科学与技术学院，中国。

#### 4. Keywords: 
Visual Question Answering, Knowledge-based VQA, Large Language Model, GPT-3, Answer Heuristics

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_CVPR_2021_paper.html  Github: https://github.com/MILVLG/prophet

#### 6. Summary : 
- (1):本文研究的背景是知识型视觉问答（VQA）需要外部知识来回答问题，而早期的方法从显式知识库（KB）中检索所需的知识，这经常会引入与问题无关的信息，从而限制了模型的性能。最近的方法使用大型语言模型（即GPT-3）作为隐式知识引擎来获取回答所需的必要知识。
- (2):过去的方法是从显式知识库中检索所需的知识，但这些方法的性能受到两个限制：（i）所需的知识可能无法成功从知识库中检索出来；（ii）即使检索到所需的知识，也不可避免地引入大量无关的知识，这会妨碍VQA模型的学习。本文提出了Prophet框架，通过回答启发式来提示GPT-3，以增强其知识型VQA的能力。Prophet显著优于所有现有的最先进方法，在两个具有挑战性的知识型VQA数据集OK-VQA和A-OKVQA上提供了61.1％和55.7％的准确性。
- (3):本文提出了Prophet框架，通过回答启发式来提示GPT-3，以增强其知识型VQA的能力。具体而言，我们首先在特定的知识型VQA数据集上训练一个vanilla VQA模型，不使用外部知识。然后，我们从模型中提取两种互补的答案启发式：答案候选和答案感知示例。最后，将这两种答案启发式编码到提示中，以使GPT-3更好地理解任务，从而增强其能力。Prophet框架的创新点在于，它可以同时从任何vanilla VQA模型中获取这两种答案启发式。 
- (4):本文在两个具有挑战性的知识型VQA数据集OK-VQA和A-OKVQA上进行了实验，Prophet显著优于所有现有的最先进方法，包括使用1.8B图像文本对训练的Flamingo-80B模型。Prophet的结果可以使用单个GPU和可承受的GPT-3调用次数进行复现。
#### 7. 方法详细介绍：
本文提出了Prophet框架，用于知识型视觉问答。该框架包括两个阶段：第一阶段，使用一个基础的VQA模型在特定的知识型VQA数据集上进行训练，提取出两种互补的答案启发式，即答案候选和答案感知示例；第二阶段，将这两种答案启发式编码到提示中，以便GPT-3更好地理解任务，从而增强其能力。提示由提示头、上下文示例和测试输入组成。在第二阶段中，使用答案感知示例和答案候选生成多个提示，然后使用多查询集成策略将这些提示合并为一个输出。 

#### 8. 实验设置：
本文使用了两个具有挑战性的知识型VQA数据集，即OK-VQA和A-OKVQA。OK-VQA包含9K个训练图像-问题对和5K个测试图像-问题对，而A-OKVQA分为三个子集：17K个训练图像-问题对、1K个验证图像-问题对和7K个测试图像-问题对。本文还提供了VQA模型的实现细节和Prophet模型的设置，包括答案候选、上下文示例和查询的数量等。

#### 9. 实验结果和分析：
本文的Prophet框架在OK-VQA和A-OKVQA数据集上均取得了显著的性能提升，分别达到了61.1%和55.7%的准确率。此外，Prophet在MC任务上的表现也优于其他方法。这些结果表明了Prophet的有效性和泛化性。


# Paper:961     通过反向特征投影在连续学习中保持线性可分性



#### 1. Title: 
Preserving Linear Separability in Continual Learning by Backward Feature Projection

#### 2. Authors: 
Qiao Gu, Dongsub Shim, Florian Shkurti

#### 3. Affiliation: 
Qiao Gu: University of Toronto (多伦多大学)

#### 4. Keywords: 
Continual learning, catastrophic forgetting, feature distillation, linear separability, Backward Feature Projection

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2021/html/Gu_Preserving_Linear_Separability_in_Continual_Learning_by_Backward_Feature_Projection_CVPR_2021_paper.html  Github: None

#### 6. Summary : 
- (1):本文研究了在连续学习中的灾难性遗忘问题，即模型在学习新任务时会忘记旧任务的知识，这是深度神经网络的一个主要挑战。 

- (2):过去的方法包括经验回放、参数正则化、知识蒸馏和架构方法等，但它们大多数直接约束新特征与旧特征匹配，忽略了可塑性的需求。本文提出了一种名为Backward Feature Projection (BFP)的方法，允许新特征在旧特征的可学习线性变换下发生变化，以实现更好的稳定性和可塑性平衡。BFP保留了旧类别的线性可分性，同时允许新特征方向的出现以适应新类别。 

- (3):本文提出了一种基于特征空间的知识蒸馏方法，即BFP。BFP通过可学习的线性变换来约束新特征与旧特征的一致性，以保留旧类别的线性可分性。BFP可以与现有的经验回放方法相结合，并显著提高性能。 

- (4):本文在MNIST、Split-CIFAR10和Split-CIFAR100等数据集上进行了实验，结果表明BFP可以提高基线方法6%-8%的性能，并实现了最先进的类增量学习准确性。线性探测实验表明，BFP可以帮助学习更好的特征空间，其中不同类别更易于分离。
#### 1. 方法详细介绍：
本文提出了一种称为“Backward Feature Projection”（BFP）的方法，用于在连续学习中保持特征空间的线性可分性。该方法将CL模型分解为两部分：非线性特征提取器h和线性分类头g。BFP损失定义为投影特征与先前模型提取的特征之间的L2范数。线性变换矩阵A被学习用于将新的特征空间投影回旧的特征空间。BFP损失可以保持旧类别的线性可分性，同时允许新类别沿着旧特征空间中未使用的方向进行分类。

#### 2. 实验设置：
本文在具有挑战性的Split-CIFAR10和Split-CIFAR100数据集上评估了所提出的方法，这是一种类增量学习基准。实验使用ResNet-18架构和批量大小为128。学习率设置为0.1，并在60和80个epoch时降低10倍。模型总共训练100个epoch。

#### 3. 实验结果与分析：
本文将所提出的BFP方法与几种最先进的连续学习方法进行了比较，包括经验回放（ER）、梯度情节记忆（GEM）和无遗忘学习（LwF）。实验结果表明，BFP在Split-CIFAR10和Split-CIFAR100数据集上均取得了最先进的类增量学习准确率，且优于其他方法。线性探测实验也表明，BFP有助于学习更好的特征空间，使不同类别更易于分离。


# Paper:962     基于课程学习的LiDAR三维物体检测中的物体操作



#### 1. Title: 
Curricular Object Manipulation in LiDAR-based Object Detection

#### 2. Authors: 
Ziyue Zhu, Qiang Meng, Xiao Wang, Ke Wang, Liujiang Yan, Jian Yang

#### 3. Affiliation: 
Ziyue Zhu, Jian Yang: 天津市视觉计算与智能感知重点实验室, 南开大学计算机科学学院
Qiang Meng, Xiao Wang, Ke Wang, Liujiang Yan: 滴滴出行

#### 4. Keywords: 
LiDAR, object detection, curriculum learning, data augmentation

#### 5. Paper: https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Curricular_Object_Manipulation_in_LiDAR-Based_Object_Detection_CVPR_2022_paper.html  Github: https://github.com/ZZY816/COM

#### 6. Summary : 
- (1):本文研究基于LiDAR的三维物体检测中的课程学习问题。
- (2):过去的方法主要集中在数据表示和数据增强上，但是点云数据的稀疏性仍然是一个挑战。本文提出了一种基于课程学习的框架，通过在损失设计和数据增强过程中嵌入课程训练策略，来解决这个问题。与现有的数据增强方法相比，本文提出的方法可以更好地控制训练过程中难度的逐步增加，从而提高模型的性能和泛化能力。
- (3):本文提出了一种基于课程学习的物体操作（COM）框架，该框架将课程训练策略嵌入到损失设计和数据增强过程中。对于损失设计，本文提出了COMLoss来动态预测物体级别的难度，并根据训练阶段强调不同难度的物体。在数据增强方面，本文提出了一种新颖的COMAug策略，该策略首先根据精心设计的启发式方法将地面真实数据库中的物体聚类。然后，在训练过程中，对组级别的难度进行预测和更新，以获得稳定的结果。通过将逐渐增加难度的物体逐步采样和增强到训练样本中，可以提高模型的性能和泛化能力。本文的实验结果和消融研究表明了所提出的框架的优越性和通用性。
- (4):本文在KITTI数据集上进行了实验，结果表明，所提出的方法在三维物体检测任务中取得了最先进的性能，超过了现有的方法。
#### 7. 方法详细介绍：
本文提出了一种基于课程学习的目标检测框架——Curricular Object Manipulation (COM)。该框架包括两个组件：COMLoss和COMAug。COMLoss通过引入自适应加权函数来动态调整优化目标的重点，从而调整不同难度对象的损失。COMAug通过将地面真实数据库中的对象聚类，并更新组级别得分而不是个体得分，以便对增强进行难度适应性采样。该框架旨在通过课程学习方案提高模型性能和泛化能力。

具体步骤如下：
1. 对象聚类：将具有相似属性的对象分配到同一组中，基于四个因素：到LiDAR传感器的距离、边界框的大小、边界框方向与盒子中心方位之间的相对角度以及观察区域占整个边界框的占用比率。
2. 组级别得分更新：在每个时期结束时，将每个组的得分更新为得分池的平均值。
3. 难度适应性采样：选择一个组，然后根据以µt为中心的高斯采样曲线和超参数σ随机采样一个对象，其中µt随时期编号t而移动。

#### 8. 实验设置：
本文在KITTI数据集上进行了实验，使用CenterPoint检测器作为基线，并对COMLoss和COMAug模块进行了消融研究。实验中使用了Adam优化器，学习率为0.001，批量大小为4，训练时长为70个时期。在Waymo Open数据集上进行了泛化实验。

#### 9. 实验结果和分析：
实验结果表明，COM框架在KITTI数据集上的3D目标检测mAP指标优于现有的方法。在Waymo Open数据集上，该方法也表现出卓越的泛化能力。消融研究验证了COMLoss和COMAug的有效性。
